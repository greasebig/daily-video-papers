#!/usr/bin/env python3
"""
arXiv Papers Fetcher V2 (Extended Video Version with AI Analysis)
åœ¨ V1 åŸºç¡€ä¸Šå¢åŠ  PDF å…¨æ–‡ä¸‹è½½å’Œ AI æ·±åº¦åˆ†æåŠŸèƒ½ï¼Œæ”¯æŒ DeepSeek API
"""

import os
import re
import json
import time
import urllib.request
import urllib.parse
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from pathlib import Path
from openai import OpenAI
import tempfile
import subprocess

# é…ç½®
CATEGORIES = ["cs.CV", "cs.AI", "cs.MM", "cs.RO", "cs.LG"]
DAYS_TO_CHECK = 3
DAYS_TO_COMPARE = 5

# æ‰©å±•åçš„è§†é¢‘ç›¸å…³å…³é”®è¯
VIDEO_KEYWORDS = [
    # ç”Ÿæˆä¸ç¼–è¾‘
    "video generation", "video synthesis", "video editing", "video edit",
    "video diffusion", "text-to-video", "image-to-video", "video-to-video",
    "motion generation", "character animation", "talking head", "human motion",
    # ç†è§£ä¸åˆ†æ
    "video understanding", "video recognition", "video classification", "action recognition",
    "action detection", "temporal action", "video retrieval", "video captioning",
    "video question answering", "video QA", "video summarization",
    # å¤„ç†ä¸å¢å¼º
    "video super-resolution", "video enhancement", "video restoration", "video denoising",
    "video interpolation", "frame interpolation", "video compression", "video coding",
    # åˆ†å‰²ä¸è·Ÿè¸ª
    "video segmentation", "video object segmentation", "VOS", "video instance segmentation",
    "VIS", "object tracking", "multi-object tracking", "MOT", "video matting",
    # åŸºç¡€æ¨¡å‹ä¸æ—¶åº
    "video model", "temporal modeling", "spatio-temporal", "video transformer",
    "video representation", "optical flow", "video prediction", "future frame prediction"
]

# åˆå§‹åŒ–å®¢æˆ·ç«¯
deepseek_key = os.environ.get("DEEPSEEK_API_KEY")
openai_key = os.environ.get("OPENAI_API_KEY")

if deepseek_key:
    print("Using DeepSeek API")
    client = OpenAI(api_key=deepseek_key, base_url="https://api.deepseek.com")
    model_name = "deepseek-chat"
elif openai_key and openai_key.startswith("AIza"):
    print("Using Gemini API (via Google AI Studio)")
    client = OpenAI(
        api_key=openai_key,
        base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
    )
    model_name = "gemini-2.0-flash"
else:
    print("Using Standard OpenAI API")
    client = OpenAI(api_key=openai_key)
    model_name = "gpt-4o-mini"

def fetch_arxiv_papers(category, days=3, max_retries=3):
    """ä» arXiv API è·å–æŒ‡å®šç±»åˆ«çš„è®ºæ–‡"""
    base_url = "http://export.arxiv.org/api/query?"
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days)
    query = f"cat:{category}"
    params = {
        "search_query": query,
        "start": 0,
        "max_results": 500,
        "sortBy": "submittedDate",
        "sortOrder": "descending"
    }
    url = base_url + urllib.parse.urlencode(params)
    for attempt in range(max_retries):
        try:
            if attempt > 0: time.sleep(5 * attempt)
            with urllib.request.urlopen(url, timeout=30) as response:
                data = response.read()
            root = ET.fromstring(data)
            namespace = {"atom": "http://www.w3.org/2005/Atom"}
            papers = []
            for entry in root.findall("atom:entry", namespace):
                published = entry.find("atom:published", namespace).text
                pub_date = datetime.strptime(published, "%Y-%m-%dT%H:%M:%SZ")
                if pub_date < start_date: continue
                paper = {
                    "id": entry.find("atom:id", namespace).text.split("/abs/")[-1],
                    "title": entry.find("atom:title", namespace).text.strip().replace("\n", " "),
                    "summary": entry.find("atom:summary", namespace).text.strip().replace("\n", " "),
                    "authors": [author.find("atom:name", namespace).text for author in entry.findall("atom:author", namespace)],
                    "published": pub_date.strftime("%Y-%m-%d"),
                    "pdf_url": entry.find("atom:id", namespace).text.replace("/abs/", "/pdf/"),
                    "abs_url": entry.find("atom:id", namespace).text,
                    "categories": [cat.attrib["term"] for cat in entry.findall("atom:category", namespace)]
                }
                papers.append(paper)
            return papers
        except Exception as e:
            print(f"  Attempt {attempt + 1} failed: {e}")
            if attempt == max_retries - 1: return []
    return []

def is_video_related(paper):
    """åˆ¤æ–­è®ºæ–‡æ˜¯å¦ä¸è§†é¢‘ç›¸å…³"""
    text = (paper["title"] + " " + paper["summary"]).lower()
    return any(keyword.lower() in text for keyword in VIDEO_KEYWORDS)

def extract_links(paper):
    """ä»æ‘˜è¦ä¸­æå–é¡¹ç›®å’Œä»£ç é“¾æ¥"""
    text = paper["summary"]
    url_pattern = r'https?://[^\s<>"{}\\|\\^`\\[\\]]+'
    urls = re.findall(url_pattern, text)
    links = {"project": None, "code": None}
    for url in urls:
        url_lower = url.lower()
        if "github.com" in url_lower or "gitlab.com" in url_lower:
            if not links["code"]: links["code"] = url
        elif any(k in url_lower for k in ["project", "page", "site"]):
            if not links["project"]: links["project"] = url
        elif not links["project"]: links["project"] = url
    return links

def download_pdf(pdf_url, output_path):
    """ä¸‹è½½ PDF æ–‡ä»¶"""
    try:
        urllib.request.urlretrieve(pdf_url, output_path)
        return True
    except Exception as e:
        print(f"  PDF download failed: {e}")
        return False

def extract_text_from_pdf(pdf_path):
    """ä» PDF æå–æ–‡æœ¬"""
    try:
        result = subprocess.run(["pdftotext", "-layout", pdf_path, "-"], capture_output=True, text=True, timeout=30)
        return result.stdout if result.returncode == 0 else None
    except Exception as e:
        print(f"  PDF text extraction error: {e}")
        return None

def analyze_paper_with_ai(paper, pdf_text):
    """ä½¿ç”¨ AI åˆ†æè®ºæ–‡å…¨æ–‡"""
    max_chars = 30000
    if len(pdf_text) > max_chars:
        half = max_chars // 2
        pdf_text = pdf_text[:half] + "\n\n[... Omitted ...]\n\n" + pdf_text[-half:]
    
    print(f"  [Debug] Using model: {model_name} for AI analysis...")
    
    prompt = f'''ä½ æ˜¯ä¸€ä½èµ„æ·±çš„è®¡ç®—æœºè§†è§‰ç ”ç©¶ä¸“å®¶ã€‚è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹è®ºæ–‡çš„å…¨æ–‡å†…å®¹ï¼Œå¹¶è¿›è¡Œæ·±åº¦åˆ†æã€‚
è®ºæ–‡æ ‡é¢˜: {paper["title"]}
è®ºæ–‡å…¨æ–‡:
{pdf_text}

è¯·ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œæ‰¹åˆ¤æ€§åˆ†æï¼š
1. **æ ¸å¿ƒè§‚ç‚¹**: ç”¨æœ€ç®€å•ç›´ç™½çš„è¯­è¨€ï¼ˆ1-2å¥è¯ï¼‰è¯´æ˜è®ºæ–‡çš„æ ¸å¿ƒåˆ›æ–°ç‚¹
2. **æŠ€æœ¯æ–¹æ³•**: ç®€è¦è¯´æ˜é‡‡ç”¨çš„ä¸»è¦æŠ€æœ¯æ–¹æ³•å’Œæ¶æ„
3. **å®éªŒéªŒè¯**: è¯„ä¼°å®éªŒè®¾è®¡çš„åˆç†æ€§ã€æ•°æ®é›†é€‰æ‹©ã€å¯¹æ¯”æ–¹æ³•æ˜¯å¦å……åˆ†
4. **ç»“æœå¯é æ€§**: åˆ†æå®éªŒç»“æœçš„å¯ä¿¡åº¦ï¼Œæ˜¯å¦å­˜åœ¨è¿‡æ‹Ÿåˆã€cherry-picking ç­‰é—®é¢˜
5. **å®ç”¨ä»·å€¼**: è¯„ä¼°è¯¥ç ”ç©¶çš„å®é™…åº”ç”¨ä»·å€¼å’Œå±€é™æ€§
6. **æ‰¹åˆ¤æ€§è¯„ä»·**: æŒ‡å‡ºè®ºæ–‡çš„ä¼˜ç‚¹å’Œä¸è¶³ï¼Œä»¥åŠå¯èƒ½å­˜åœ¨çš„é—®é¢˜

è¯·ç”¨ç®€æ´ã€ä¸“ä¸šä½†æ˜“æ‡‚çš„ä¸­æ–‡å›ç­”ï¼Œæ¯ä¸ªæ–¹é¢æ§åˆ¶åœ¨ 2-3 å¥è¯ä»¥å†…ã€‚'''

    try:
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„å­¦æœ¯ç ”ç©¶åˆ†æä¸“å®¶ï¼Œæ“…é•¿æ‰¹åˆ¤æ€§é˜…è¯»å’Œè¯„ä¼°è®ºæ–‡è´¨é‡ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.5,
            max_tokens=2000
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"  [Error] AI analysis failed: {str(e)}")
        if "402" in str(e) or "Insufficient Balance" in str(e):
            return "AI åˆ†æå¤±è´¥ï¼šAPI ä½™é¢ä¸è¶³"
        return f"AI åˆ†ææš‚æ—¶ä¸å¯ç”¨: {e}"

def translate_text(text):
    """ä½¿ç”¨ AI ç¿»è¯‘æ–‡æœ¬"""
    if not text or not text.strip():
        return ""
    print(f"  [Debug] Using model: {model_name} for translation...")
    try:
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": "You are a professional translator. Translate the following academic text to Chinese. Keep technical terms in English when appropriate. Only return the translation, no explanations."},
                {"role": "user", "content": text}
            ],
            temperature=0.3
        )
        result = response.choices[0].message.content.strip()
        return result if result else text
    except Exception as e:
        print(f"  [Error] Translation failed: {str(e)}")
        return text

def load_recent_papers(days=5):
    """åŠ è½½æœ€è¿‘å‡ å¤©çš„è®ºæ–‡ ID ç”¨äºå»é‡"""
    papers_dir = Path(__file__).parent.parent / "papers"
    recent_ids = set()
    if not papers_dir.exists(): return recent_ids
    start_date = datetime.now() - timedelta(days=days)
    for md_file in papers_dir.glob("*.md"):
        try:
            if datetime.strptime(md_file.stem, "%Y-%m-%d") >= start_date:
                content = md_file.read_text(encoding="utf-8")
                recent_ids.update(re.findall(r'arxiv\.org/abs/(\d+\.\d+)', content))
        except: continue
    return recent_ids

def generate_markdown_v2(papers, date_str):
    """ç”Ÿæˆ V2 ç‰ˆæœ¬çš„ Markdownï¼ˆåŒ…å« AI åˆ†æï¼‰"""
    md_content = f"# arXiv Video Papers - {date_str} (V2 Enhanced)\n\n"
    md_content += f"**Update Time**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    md_content += f"**Paper Count**: {len(papers)} | **Version**: V2 (AI Analysis)\n\n---\n\n"
    
    for i, paper in enumerate(papers, 1):
        print(f"Processing {i}/{len(papers)}: {paper['id']}")
        title_zh = translate_text(paper["title"])
        summary_zh = translate_text(paper["summary"])
        links = extract_links(paper)
        
        ai_analysis = "PDF ä¸‹è½½æˆ–åˆ†æå¤±è´¥"
        with tempfile.TemporaryDirectory() as tmpdir:
            pdf_path = Path(tmpdir) / f'{paper["id"]}.pdf'
            if download_pdf(paper["pdf_url"], pdf_path):
                pdf_text = extract_text_from_pdf(pdf_path)
                if pdf_text and len(pdf_text.strip()) > 500:
                    ai_analysis = analyze_paper_with_ai(paper, pdf_text)
                else:
                    ai_analysis = "PDF æ–‡æœ¬æå–å¤±è´¥æˆ–å†…å®¹è¿‡çŸ­"
        
        md_content += f"## {i}. {paper['title']}\n\n"
        md_content += f"**ä¸­æ–‡æ ‡é¢˜**: {title_zh}\n\n"
        md_content += f"**Authors**: {', '.join(paper['authors'][:5])}{' et al.' if len(paper['authors']) > 5 else ''}\n\n"
        md_content += f"**Date**: {paper['published']} | **arXiv**: [{paper['id']}]({paper['abs_url']}) | **PDF**: [Link]({paper['pdf_url']})\n\n"
        if links["project"]: md_content += f"**Project**: {links['project']}  "
        if links["code"]: md_content += f"**Code**: {links['code']}\n\n"
        md_content += f"**Categories**: {', '.join(paper['categories'])}\n\n"
        md_content += f'<details><summary><b>Abstract</b></summary>\n\n{paper["summary"]}\n\n</details>\n\n'
        md_content += f'<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>\n\n{summary_zh}\n\n</details>\n\n'
        md_content += f'<details><summary><b>ğŸ¤– AI é˜…è¯»åˆ†æ</b></summary>\n\n{ai_analysis}\n\n</details>\n\n---\n\n'
        time.sleep(2)
    return md_content

def update_readme_index():
    """æ›´æ–° README ä¸­çš„è®ºæ–‡ç´¢å¼•"""
    base_dir = Path(__file__).parent.parent
    papers_dir = base_dir / "papers"
    readme_path = base_dir / "README.md"
    if not papers_dir.exists(): return
    paper_files = sorted(papers_dir.glob("*.md"), reverse=True)
    index_content = "\n"
    for f in paper_files:
        content = f.read_text(encoding="utf-8")
        count = re.search(r'\*\*Paper Count\*\*: (\d+)', content)
        index_content += f"- [{f.stem}](papers/{f.name}) - {count.group(1) if count else '0'} papers\n"
    
    readme_content = readme_path.read_text(encoding="utf-8")
    pattern = r'<!-- PAPERS_INDEX_START -->.*?<!-- PAPERS_INDEX_END -->'
    replacement = f'<!-- PAPERS_INDEX_START -->{index_content}<!-- PAPERS_INDEX_END -->'
    readme_path.write_text(re.sub(pattern, replacement, readme_content, flags=re.DOTALL), encoding="utf-8")

def main():
    recent_ids = load_recent_papers(DAYS_TO_COMPARE)
    all_papers = []
    for i, cat in enumerate(CATEGORIES):
        print(f"Fetching {cat}...")
        papers = fetch_arxiv_papers(cat, DAYS_TO_CHECK)
        all_papers.extend(papers)
        if i < len(CATEGORIES) - 1: time.sleep(3)
    
    unique_papers = {p["id"]: p for p in all_papers}
    video_papers = [p for p in unique_papers.values() if is_video_related(p)]
    new_papers = [p for p in video_papers if p["id"] not in recent_ids]
    
    if not new_papers:
        print("No new papers found.")
        return
    
    new_papers.sort(key=lambda x: x["published"], reverse=True)
    date_str = datetime.now().strftime("%Y-%m-%d")
    md_content = generate_markdown_v2(new_papers, date_str)
    
    papers_dir = Path(__file__).parent.parent / "papers"
    papers_dir.mkdir(parents=True, exist_ok=True)
    (papers_dir / f"{date_str}.md").write_text(md_content, encoding="utf-8")
    update_readme_index()

if __name__ == "__main__":
    main()
