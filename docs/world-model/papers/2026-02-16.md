# arXiv World Model Papers - 2026-02-16

**Paper Count**: 1

---

## 1. Information-theoretic analysis of world models in optimal reward maximizers / 最优奖励最大化世界模型的信息论分析

**Date**: 2026-02-13 | **arXiv**: [2602.12963v1](http://arxiv.org/abs/2602.12963v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.12963v1)

**Categories**: cs.AI

<details><summary><b>Abstract / 摘要</b></summary>

An important question in the field of AI is the extent to which successful behaviour requires an internal representation of the world. In this work, we quantify the amount of information an optimal policy provides about the underlying environment. We consider a Controlled Markov Process (CMP) with $n$ states and $m$ actions, assuming a uniform prior over the space of possible transition dynamics. We prove that observing a deterministic policy that is optimal for any non-constant reward function then conveys exactly $n \log m$ bits of information about the environment. Specifically, we show that the mutual information between the environment and the optimal policy is $n \log m$ bits. This bound holds across a broad class of objectives, including finite-horizon, infinite-horizon discounted, and time-averaged reward maximization. These findings provide a precise information-theoretic lower bound on the "implicit world model'' necessary for optimality.

人工智能领域的一个重要问题是成功的行为在多大程度上需要对世界的内部表征。在这项工作中，我们量化了最佳策略提供的有关底层环境的信息量。我们考虑一个具有 $n$ 状态和 $m$ 动作的受控马尔可夫过程 (CMP)，假设在可能的过渡动态空间上有一个统一的先验。我们证明，观察对于任何非恒定奖励函数来说都是最佳的确定性策略可以准确地传达 $n \log m$ 位有关环境的信息。具体来说，我们表明环境和最优策略之间的互信息为 $n \log m$ 位。这一界限适用于广泛的目标，包括有限范围、无限范围贴现和时间平均奖励最大化。这些发现为最优性所需的“隐式世界模型”提供了精确的信息论下界。

</details>

---

