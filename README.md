# Daily Video Papers ğŸ¥

?? **Website**: https://greasebig.github.io/daily-video-papers/

![Actions](https://github.com/greasebig/daily-video-papers/actions/workflows/daily-update.yml/badge.svg) ![Pages](https://img.shields.io/badge/pages-online-brightgreen) ![Visitors](https://visitor-badge.laobi.icu/badge?page_id=greasebig.daily-video-papers)

## ğŸ“š è®ºæ–‡ç´¢å¼•

<!-- PAPERS_INDEX_START -->
- [2026-02-17](papers/2026-02-17.md) - 3 papers
- [2026-02-16](papers/2026-02-16.md) - 4 papers
- [2026-02-14](papers/2026-02-14.md) - 16 papers
<!-- PAPERS_INDEX_END -->

## Other Topics

- [World Model Papers](world-model/README.md)
- [Agent Papers](agent/README.md)

## Daily Papers

<!-- PAPERS_CONTENT_START -->
<details><summary><b>2026-02-17 (3 papers)</b></summary>

# arXiv Video Papers - 2026-02-17

**Paper Count**: 3

---

## 1. When Test-Time Guidance Is Enough: Fast Image and Video Editing with Diffusion Guidance / å½“æµ‹è¯•æ—¶é—´æŒ‡å¯¼è¶³å¤Ÿæ—¶ï¼šä½¿ç”¨æ‰©æ•£æŒ‡å¯¼è¿›è¡Œå¿«é€Ÿå›¾åƒå’Œè§†é¢‘ç¼–è¾‘

**Date**: 2026-02-15 | **arXiv**: [2602.14157v1](http://arxiv.org/abs/2602.14157v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.14157v1)

**Categories**: cs.CV, cs.AI, cs.LG

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Text-driven image and video editing can be naturally cast as inpainting problems, where masked regions are reconstructed to remain consistent with both the observed content and the editing prompt. Recent advances in test-time guidance for diffusion and flow models provide a principled framework for this task; however, existing methods rely on costly vector--Jacobian product (VJP) computations to approximate the intractable guidance term, limiting their practical applicability. Building upon the recent work of Moufad et al. (2025), we provide theoretical insights into their VJP-free approximation and substantially extend their empirical evaluation to large-scale image and video editing benchmarks. Our results demonstrate that test-time guidance alone can achieve performance comparable to, and in some cases surpass, training-based methods.

æ–‡æœ¬é©±åŠ¨çš„å›¾åƒå’Œè§†é¢‘ç¼–è¾‘å¯ä»¥è‡ªç„¶åœ°è½¬åŒ–ä¸ºä¿®å¤é—®é¢˜ï¼Œå…¶ä¸­é‡å»ºé®ç½©åŒºåŸŸä»¥ä¸è§‚å¯Ÿåˆ°çš„å†…å®¹å’Œç¼–è¾‘æç¤ºä¿æŒä¸€è‡´ã€‚æ‰©æ•£å’ŒæµåŠ¨æ¨¡å‹æµ‹è¯•æ—¶æŒ‡å¯¼çš„æœ€æ–°è¿›å±•ä¸ºè¿™é¡¹ä»»åŠ¡æä¾›äº†åŸåˆ™æ¡†æ¶ï¼›ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºæ˜‚è´µçš„çŸ¢é‡é›…å¯æ¯”ç§¯ï¼ˆVJPï¼‰è®¡ç®—æ¥è¿‘ä¼¼æ£˜æ‰‹çš„æŒ‡å¯¼é¡¹ï¼Œé™åˆ¶äº†å®ƒä»¬çš„å®é™…é€‚ç”¨æ€§ã€‚ä»¥ Moufad ç­‰äººæœ€è¿‘çš„å·¥ä½œä¸ºåŸºç¡€ã€‚ (2025)ï¼Œæˆ‘ä»¬æä¾›äº†å¯¹å…¶æ—  VJP è¿‘ä¼¼çš„ç†è®ºè§è§£ï¼Œå¹¶å°†å…¶å®è¯è¯„ä¼°å¤§å¹…æ‰©å±•åˆ°å¤§è§„æ¨¡å›¾åƒå’Œè§†é¢‘ç¼–è¾‘åŸºå‡†ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä»…æµ‹è¯•æ—¶æŒ‡å¯¼å°±å¯ä»¥å®ç°ä¸åŸºäºè®­ç»ƒçš„æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³è¶…è¶ŠåŸºäºè®­ç»ƒçš„æ–¹æ³•ã€‚

</details>

---

## 2. Train Short, Inference Long: Training-free Horizon Extension for Autoregressive Video Generation / è®­ç»ƒçŸ­ï¼Œæ¨ç†é•¿ï¼šç”¨äºè‡ªå›å½’è§†é¢‘ç”Ÿæˆçš„å…è®­ç»ƒåœ°å¹³çº¿æ‰©å±•

**Date**: 2026-02-15 | **arXiv**: [2602.14027v1](http://arxiv.org/abs/2602.14027v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.14027v1)

**Categories**: cs.CV

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Autoregressive video diffusion models have emerged as a scalable paradigm for long video generation. However, they often suffer from severe extrapolation failure, where rapid error accumulation leads to significant temporal degradation when extending beyond training horizons. We identify that this failure primarily stems from the \textit{spectral bias} of 3D positional embeddings and the lack of \textit{dynamic priors} in noise sampling. To address these issues, we propose \textbf{FLEX} (\textbf{F}requency-aware \textbf{L}ength \textbf{EX}tension), a training-free inference-time framework that bridges the gap between short-term training and long-term inference. FLEX introduces Frequency-aware RoPE Modulation to adaptively interpolate under-trained low-frequency components while extrapolating high-frequency ones to preserve multi-scale temporal discriminability. This is integrated with Antiphase Noise Sampling (ANS) to inject high-frequency dynamic priors and Inference-only Attention Sink to anchor global structure. Extensive evaluations on VBench demonstrate that FLEX significantly outperforms state-of-the-art models at $6\times$ extrapolation (30s duration) and matches the performance of long-video fine-tuned baselines at $12\times$ scale (60s duration). As a plug-and-play augmentation, FLEX seamlessly integrates into existing inference pipelines for horizon extension. It effectively pushes the generation limits of models such as LongLive, supporting consistent and dynamic video synthesis at a 4-minute scale. Project page is available at \href{https://ga-lee.github.io/FLEX_demo}{https://ga-lee.github.io/FLEX}.

è‡ªå›å½’è§†é¢‘æ‰©æ•£æ¨¡å‹å·²æˆä¸ºé•¿è§†é¢‘ç”Ÿæˆçš„å¯æ‰©å±•èŒƒä¾‹ã€‚ç„¶è€Œï¼Œå®ƒä»¬ç»å¸¸é­å—ä¸¥é‡çš„å¤–æ¨å¤±è´¥ï¼Œå½“è¶…å‡ºè®­ç»ƒèŒƒå›´æ—¶ï¼Œå¿«é€Ÿçš„è¯¯å·®ç§¯ç´¯ä¼šå¯¼è‡´æ˜¾ç€çš„æ—¶é—´é€€åŒ–ã€‚æˆ‘ä»¬å‘ç°è¿™ç§å¤±è´¥ä¸»è¦æºäº 3D ä½ç½®åµŒå…¥çš„ \textit{é¢‘è°±åå·®} ä»¥åŠå™ªå£°é‡‡æ ·ä¸­ \textit{åŠ¨æ€å…ˆéªŒ} çš„ç¼ºä¹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº† \textbf{FLEX} ï¼ˆ\textbf{F}requency-aware \textbf{L}ength \textbf{EX}tensionï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ¨ç†æ—¶é—´æ¡†æ¶ï¼Œå¯ä»¥å¼¥è¡¥çŸ­æœŸè®­ç»ƒå’Œé•¿æœŸæ¨ç†ä¹‹é—´çš„å·®è·ã€‚ FLEX å¼•å…¥äº†é¢‘ç‡æ„ŸçŸ¥ RoPE è°ƒåˆ¶ï¼Œå¯è‡ªé€‚åº”åœ°å†…æ’æœªç»è®­ç»ƒçš„ä½é¢‘åˆ†é‡ï¼ŒåŒæ—¶å¤–æ¨é«˜é¢‘åˆ†é‡ä»¥ä¿æŒå¤šå°ºåº¦æ—¶é—´è¾¨åˆ«èƒ½åŠ›ã€‚å®ƒä¸åç›¸å™ªå£°é‡‡æ · (ANS) é›†æˆï¼Œä»¥æ³¨å…¥é«˜é¢‘åŠ¨æ€å…ˆéªŒå’Œä»…æ¨ç†æ³¨æ„æ± æ¥é”šå®šå…¨å±€ç»“æ„ã€‚å¯¹ VBench çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒFLEX åœ¨ $6\times$ å¤–æ¨ï¼ˆ30 ç§’æŒç»­æ—¶é—´ï¼‰ä¸‹æ˜¾ç€ä¼˜äºæœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¹¶ä¸é•¿è§†é¢‘å¾®è°ƒåŸºçº¿åœ¨ $12\times$ è§„æ¨¡ï¼ˆ60 ç§’æŒç»­æ—¶é—´ï¼‰ä¸‹çš„æ€§èƒ½ç›¸åŒ¹é…ã€‚ä½œä¸ºä¸€ç§å³æ’å³ç”¨çš„å¢å¼ºåŠŸèƒ½ï¼ŒFLEX å¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„æ¨ç†ç®¡é“ä¸­ï¼Œä»¥å®ç°èŒƒå›´æ‰©å±•ã€‚å®ƒæœ‰æ•ˆåœ°çªç ´äº† LongLive ç­‰æ¨¡å‹çš„ç”Ÿæˆé™åˆ¶ï¼Œæ”¯æŒ 4 åˆ†é’Ÿè§„æ¨¡çš„ä¸€è‡´åŠ¨æ€è§†é¢‘åˆæˆã€‚é¡¹ç›®é¡µé¢ä½äº \href{https://ga-lee.github.io/FLEX_demo}{https://ga-lee.github.io/FLEX}ã€‚

</details>

---

## 3. High-Fidelity Causal Video Diffusion Models for Real-Time Ultra-Low-Bitrate Semantic Communication / ç”¨äºå®æ—¶è¶…ä½æ¯”ç‰¹ç‡è¯­ä¹‰é€šä¿¡çš„é«˜ä¿çœŸå› æœè§†é¢‘æ‰©æ•£æ¨¡å‹

**Date**: 2026-02-14 | **arXiv**: [2602.13837v1](http://arxiv.org/abs/2602.13837v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.13837v1)

**Categories**: cs.CV

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

We introduce a video diffusion model for high-fidelity, causal, and real-time video generation under ultra-low-bitrate semantic communication constraints. Our approach utilizes lossy semantic video coding to transmit the semantic scene structure, complemented by a stream of highly compressed, low-resolution frames that provide sufficient texture information to preserve fidelity. Building on these inputs, we introduce a modular video diffusion model that contains Semantic Control, Restoration Adapter, and Temporal Adapter. We further introduce an efficient temporal distillation procedure that enables extension to real-time and causal synthesis, reducing trainable parameters by 300x and training time by 2x, while adhering to communication constraints. Evaluated across diverse datasets, the framework achieves strong perceptual quality, semantic fidelity, and temporal consistency at ultra-low bitrates (< 0.0003 bpp), outperforming classical, neural, and generative baselines in extensive quantitative, qualitative, and subjective evaluations.

æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºåœ¨è¶…ä½æ¯”ç‰¹ç‡è¯­ä¹‰é€šä¿¡çº¦æŸä¸‹ç”Ÿæˆé«˜ä¿çœŸã€å› æœå’Œå®æ—¶è§†é¢‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æœ‰æŸè¯­ä¹‰è§†é¢‘ç¼–ç æ¥ä¼ è¾“è¯­ä¹‰åœºæ™¯ç»“æ„ï¼Œå¹¶è¾…ä»¥é«˜åº¦å‹ç¼©çš„ä½åˆ†è¾¨ç‡å¸§æµï¼Œæä¾›è¶³å¤Ÿçš„çº¹ç†ä¿¡æ¯ä»¥ä¿æŒä¿çœŸåº¦ã€‚åŸºäºè¿™äº›è¾“å…¥ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ¨¡å—åŒ–è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå…¶ä¸­åŒ…å«è¯­ä¹‰æ§åˆ¶ã€æ¢å¤é€‚é…å™¨å’Œæ—¶é—´é€‚é…å™¨ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ç§æœ‰æ•ˆçš„æ—¶é—´è’¸é¦ç¨‹åºï¼Œå¯ä»¥æ‰©å±•åˆ°å®æ—¶å’Œå› æœåˆæˆï¼Œå°†å¯è®­ç»ƒå‚æ•°å‡å°‘ 300 å€ï¼Œå°†è®­ç»ƒæ—¶é—´å‡å°‘ 2 å€ï¼ŒåŒæ—¶éµå®ˆé€šä¿¡é™åˆ¶ã€‚ç»è¿‡ä¸åŒæ•°æ®é›†çš„è¯„ä¼°ï¼Œè¯¥æ¡†æ¶åœ¨è¶…ä½æ¯”ç‰¹ç‡ (< 0.0003 bpp) ä¸‹å®ç°äº†å¼ºå¤§çš„æ„ŸçŸ¥è´¨é‡ã€è¯­ä¹‰ä¿çœŸåº¦å’Œæ—¶é—´ä¸€è‡´æ€§ï¼Œåœ¨å¹¿æ³›çš„å®šé‡ã€å®šæ€§å’Œä¸»è§‚è¯„ä¼°ä¸­ä¼˜äºç»å…¸ã€ç¥ç»å’Œç”ŸæˆåŸºçº¿ã€‚

</details>

---



</details>

<details><summary><b>2026-02-16 (4 papers)</b></summary>

# arXiv Video Papers - 2026-02-16

**Paper Count**: 4

---

## 1. CoPE-VideoLM: Codec Primitives For Efficient Video Language Models / CoPE-VideoLMï¼šç”¨äºé«˜æ•ˆè§†é¢‘è¯­è¨€æ¨¡å‹çš„ç¼–è§£ç å™¨åŸè¯­

**Date**: 2026-02-13 | **arXiv**: [2602.13191v1](http://arxiv.org/abs/2602.13191v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.13191v1)

**Categories**: cs.CV, cs.AI, cs.CL

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos. To fit to the maximum context window constraint, current methods use keyframe sampling which can miss both macro-level events and micro-level details due to the sparse temporal coverage. Furthermore, processing full images and their tokens for each frame incurs substantial computational overhead. To address these limitations, we propose to leverage video codec primitives (specifically motion vectors and residuals) which natively encode video redundancy and sparsity without requiring expensive full-image encoding for most frames. To this end, we introduce lightweight transformer-based encoders that aggregate codec primitives and align their representations with image encoder embeddings through a pre-training strategy that accelerates convergence during end-to-end fine-tuning. Our approach reduces the time-to-first-token by up to $86\%$ and token usage by up to $93\%$ compared to standard VideoLMs. Moreover, by varying the keyframe and codec primitive densities we are able to maintain or exceed performance on $14$ diverse video understanding benchmarks spanning general question answering, temporal reasoning, long-form understanding, and spatial scene understanding.

è§†é¢‘è¯­è¨€æ¨¡å‹ (VideoLM) ä½¿ AI ç³»ç»Ÿèƒ½å¤Ÿç†è§£è§†é¢‘ä¸­çš„æ—¶é—´åŠ¨æ€ã€‚ä¸ºäº†é€‚åº”æœ€å¤§ä¸Šä¸‹æ–‡çª—å£çº¦æŸï¼Œå½“å‰çš„æ–¹æ³•ä½¿ç”¨å…³é”®å¸§é‡‡æ ·ï¼Œç”±äºç¨€ç–çš„æ—¶é—´è¦†ç›–ï¼Œå¯èƒ½ä¼šé”™è¿‡å®è§‚çº§åˆ«çš„äº‹ä»¶å’Œå¾®è§‚çº§åˆ«çš„ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œå¤„ç†å®Œæ•´å›¾åƒåŠå…¶æ¯å¸§çš„æ ‡è®°ä¼šäº§ç”Ÿå¤§é‡çš„è®¡ç®—å¼€é”€ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å»ºè®®åˆ©ç”¨è§†é¢‘ç¼–è§£ç å™¨åŸè¯­ï¼ˆç‰¹åˆ«æ˜¯è¿åŠ¨å‘é‡å’Œæ®‹å·®ï¼‰ï¼Œå®ƒå¯ä»¥å¯¹è§†é¢‘å†—ä½™å’Œç¨€ç–æ€§è¿›è¡Œæœ¬æœºç¼–ç ï¼Œè€Œæ— éœ€å¯¹å¤§å¤šæ•°å¸§è¿›è¡Œæ˜‚è´µçš„å…¨å›¾åƒç¼–ç ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäº Transformer çš„è½»é‡çº§ç¼–ç å™¨ï¼Œå®ƒèšåˆç¼–è§£ç å™¨åŸè¯­ï¼Œå¹¶é€šè¿‡é¢„è®­ç»ƒç­–ç•¥å°†å…¶è¡¨ç¤ºä¸å›¾åƒç¼–ç å™¨åµŒå…¥å¯¹é½ï¼Œè¯¥é¢„è®­ç»ƒç­–ç•¥å¯åœ¨ç«¯åˆ°ç«¯å¾®è°ƒè¿‡ç¨‹ä¸­åŠ é€Ÿæ”¶æ•›ã€‚ä¸æ ‡å‡† VideoLM ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†é¦–æ¬¡ä½¿ç”¨ä»¤ç‰Œçš„æ—¶é—´å‡å°‘äº†é«˜è¾¾ 86\%$ï¼Œä»¤ç‰Œä½¿ç”¨é‡å‡å°‘äº†é«˜è¾¾ 93\%$ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ”¹å˜å…³é”®å¸§å’Œç¼–è§£ç å™¨åŸºå…ƒå¯†åº¦ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨ 14 ç¾å…ƒçš„å„ç§è§†é¢‘ç†è§£åŸºå‡†ä¸Šä¿æŒæˆ–è¶…è¿‡æ€§èƒ½ï¼Œæ¶µç›–ä¸€èˆ¬é—®ç­”ã€æ—¶é—´æ¨ç†ã€é•¿æ ¼å¼ç†è§£å’Œç©ºé—´åœºæ™¯ç†è§£ã€‚

</details>

---

## 2. FlexAM: Flexible Appearance-Motion Decomposition for Versatile Video Generation Control / FlexAMï¼šçµæ´»çš„å¤–è§‚è¿åŠ¨åˆ†è§£ï¼Œç”¨äºå¤šåŠŸèƒ½è§†é¢‘ç”Ÿæˆæ§åˆ¶

**Date**: 2026-02-13 | **arXiv**: [2602.13185v1](http://arxiv.org/abs/2602.13185v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.13185v1)

**Categories**: cs.CV, cs.GR

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Effective and generalizable control in video generation remains a significant challenge. While many methods rely on ambiguous or task-specific signals, we argue that a fundamental disentanglement of "appearance" and "motion" provides a more robust and scalable pathway. We propose FlexAM, a unified framework built upon a novel 3D control signal. This signal represents video dynamics as a point cloud, introducing three key enhancements: multi-frequency positional encoding to distinguish fine-grained motion, depth-aware positional encoding, and a flexible control signal for balancing precision and generative quality. This representation allows FlexAM to effectively disentangle appearance and motion, enabling a wide range of tasks including I2V/V2V editing, camera control, and spatial object editing. Extensive experiments demonstrate that FlexAM achieves superior performance across all evaluated tasks.

è§†é¢‘ç”Ÿæˆçš„æœ‰æ•ˆä¸”é€šç”¨çš„æ§åˆ¶ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶è®¸å¤šæ–¹æ³•ä¾èµ–äºæ¨¡ç³Šæˆ–ç‰¹å®šäºä»»åŠ¡çš„ä¿¡å·ï¼Œä½†æˆ‘ä»¬è®¤ä¸ºâ€œå¤–è§‚â€å’Œâ€œè¿åŠ¨â€çš„æ ¹æœ¬åˆ†ç¦»æä¾›äº†ä¸€æ¡æ›´ç¨³å¥å’Œå¯æ‰©å±•çš„é€”å¾„ã€‚æˆ‘ä»¬æå‡ºäº† FlexAMï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ–°é¢–çš„ 3D æ§åˆ¶ä¿¡å·æ„å»ºçš„ç»Ÿä¸€æ¡†æ¶ã€‚è¯¥ä¿¡å·å°†è§†é¢‘åŠ¨æ€è¡¨ç¤ºä¸ºç‚¹äº‘ï¼Œå¼•å…¥äº†ä¸‰ä¸ªå…³é”®å¢å¼ºåŠŸèƒ½ï¼šç”¨äºåŒºåˆ†ç»†ç²’åº¦è¿åŠ¨çš„å¤šé¢‘ä½ç½®ç¼–ç ã€æ·±åº¦æ„ŸçŸ¥ä½ç½®ç¼–ç ä»¥åŠç”¨äºå¹³è¡¡ç²¾åº¦å’Œç”Ÿæˆè´¨é‡çš„çµæ´»æ§åˆ¶ä¿¡å·ã€‚è¿™ç§è¡¨ç¤ºæ–¹å¼ä½¿ FlexAM èƒ½å¤Ÿæœ‰æ•ˆåœ°ç†æ¸…å¤–è§‚å’Œè¿åŠ¨ï¼Œä»è€Œå®ç°å¹¿æ³›çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬ I2V/V2V ç¼–è¾‘ã€ç›¸æœºæ§åˆ¶å’Œç©ºé—´å¯¹è±¡ç¼–è¾‘ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFlexAM åœ¨æ‰€æœ‰è¯„ä¼°ä»»åŠ¡ä¸­å‡å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚

</details>

---

## 3. Towards Universal Video MLLMs with Attribute-Structured and Quality-Verified Instructions / è¿ˆå‘å…·æœ‰å±æ€§ç»“æ„å’Œè´¨é‡éªŒè¯æŒ‡ä»¤çš„é€šç”¨è§†é¢‘ MLLM

**Date**: 2026-02-13 | **arXiv**: [2602.13013v1](http://arxiv.org/abs/2602.13013v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.13013v1)

**Categories**: cs.CV

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Universal video understanding requires modeling fine-grained visual and audio information over time in diverse real-world scenarios. However, the performance of existing models is primarily constrained by video-instruction data that represents complex audiovisual content as single, incomplete descriptions, lacking fine-grained organization and reliable annotation. To address this, we introduce: (i) ASID-1M, an open-source collection of one million structured, fine-grained audiovisual instruction annotations with single- and multi-attribute supervision; (ii) ASID-Verify, a scalable data curation pipeline for annotation, with automatic verification and refinement that enforces semantic and temporal consistency between descriptions and the corresponding audiovisual content; and (iii) ASID-Captioner, a video understanding model trained via Supervised Fine-Tuning (SFT) on the ASID-1M. Experiments across seven benchmarks covering audiovisual captioning, attribute-wise captioning, caption-based QA, and caption-based temporal grounding show that ASID-Captioner improves fine-grained caption quality while reducing hallucinations and improving instruction following. It achieves state-of-the-art performance among open-source models and is competitive with Gemini-3-Pro.

é€šç”¨è§†é¢‘ç†è§£éœ€è¦åœ¨ä¸åŒçš„ç°å®åœºæ™¯ä¸­éšç€æ—¶é—´çš„æ¨ç§»å¯¹ç»†ç²’åº¦çš„è§†è§‰å’ŒéŸ³é¢‘ä¿¡æ¯è¿›è¡Œå»ºæ¨¡ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹çš„æ€§èƒ½ä¸»è¦å—åˆ°è§†é¢‘æ•™å­¦æ•°æ®çš„é™åˆ¶ï¼Œè§†é¢‘æ•™å­¦æ•°æ®å°†å¤æ‚çš„è§†å¬å†…å®¹è¡¨ç¤ºä¸ºå•ä¸€çš„ã€ä¸å®Œæ•´çš„æè¿°ï¼Œç¼ºä¹ç»†ç²’åº¦çš„ç»„ç»‡å’Œå¯é çš„æ³¨é‡Šã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥ï¼šï¼ˆiï¼‰ASID-1Mï¼Œä¸€ä¸ªå¼€æºé›†åˆï¼ŒåŒ…å«ä¸€ç™¾ä¸‡ä¸ªç»“æ„åŒ–ã€ç»†ç²’åº¦çš„è§†å¬æŒ‡ä»¤æ³¨é‡Šï¼Œå…·æœ‰å•å±æ€§å’Œå¤šå±æ€§ç›‘ç£ï¼› (ii) ASID-Verifyï¼Œä¸€ç§å¯æ‰©å±•çš„æ³¨é‡Šæ•°æ®ç®¡ç†ç®¡é“ï¼Œå…·æœ‰è‡ªåŠ¨éªŒè¯å’Œç»†åŒ–åŠŸèƒ½ï¼Œå¯å¼ºåˆ¶æè¿°ä¸ç›¸åº”è§†å¬å†…å®¹ä¹‹é—´çš„è¯­ä¹‰å’Œæ—¶é—´ä¸€è‡´æ€§ï¼› (iii) ASID-Captionerï¼Œä¸€ç§é€šè¿‡ç›‘ç£å¾®è°ƒ (SFT) åœ¨ ASID-1M ä¸Šè®­ç»ƒçš„è§†é¢‘ç†è§£æ¨¡å‹ã€‚æ¶µç›–è§†å¬å­—å¹•ã€æŒ‰å±æ€§å­—å¹•ã€åŸºäºå­—å¹•çš„ QA å’ŒåŸºäºå­—å¹•çš„æ—¶é—´æ¥åœ°çš„ä¸ƒä¸ªåŸºå‡†çš„å®éªŒè¡¨æ˜ï¼ŒASID-Captioner æé«˜äº†ç»†ç²’åº¦å­—å¹•è´¨é‡ï¼ŒåŒæ—¶å‡å°‘å¹»è§‰å¹¶æ”¹å–„æŒ‡ä»¤éµå¾ªã€‚å®ƒåœ¨å¼€æºæ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”ä¸Gemini-3-Proå…·æœ‰ç«äº‰åŠ›ã€‚

</details>

---

## 4. Detecting Object Tracking Failure via Sequential Hypothesis Testing / é€šè¿‡åºè´¯å‡è®¾æ£€éªŒæ£€æµ‹å¯¹è±¡è·Ÿè¸ªå¤±è´¥

**Date**: 2026-02-13 | **arXiv**: [2602.12983v1](http://arxiv.org/abs/2602.12983v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.12983v1)

**Categories**: cs.CV, cs.AI

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Real-time online object tracking in videos constitutes a core task in computer vision, with wide-ranging applications including video surveillance, motion capture, and robotics. Deployed tracking systems usually lack formal safety assurances to convey when tracking is reliable and when it may fail, at best relying on heuristic measures of model confidence to raise alerts. To obtain such assurances we propose interpreting object tracking as a sequential hypothesis test, wherein evidence for or against tracking failures is gradually accumulated over time. Leveraging recent advancements in the field, our sequential test (formalized as an e-process) quickly identifies when tracking failures set in whilst provably containing false alerts at a desired rate, and thus limiting potentially costly re-calibration or intervention steps. The approach is computationally light-weight, requires no extra training or fine-tuning, and is in principle model-agnostic. We propose both supervised and unsupervised variants by leveraging either ground-truth or solely internal tracking information, and demonstrate its effectiveness for two established tracking models across four video benchmarks. As such, sequential testing can offer a statistically grounded and efficient mechanism to incorporate safety assurances into real-time tracking systems.

è§†é¢‘ä¸­çš„å®æ—¶åœ¨çº¿å¯¹è±¡è·Ÿè¸ªæ„æˆäº†è®¡ç®—æœºè§†è§‰çš„æ ¸å¿ƒä»»åŠ¡ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨ï¼ŒåŒ…æ‹¬è§†é¢‘ç›‘æ§ã€åŠ¨ä½œæ•æ‰å’Œæœºå™¨äººæŠ€æœ¯ã€‚å·²éƒ¨ç½²çš„è·Ÿè¸ªç³»ç»Ÿé€šå¸¸ç¼ºä¹æ­£å¼çš„å®‰å…¨ä¿è¯æ¥ä¼ è¾¾è·Ÿè¸ªä½•æ—¶å¯é ä»¥åŠä½•æ—¶å¯èƒ½å¤±è´¥ï¼Œæœ€å¤šåªèƒ½ä¾é æ¨¡å‹ç½®ä¿¡åº¦çš„å¯å‘å¼æªæ–½æ¥å‘å‡ºè­¦æŠ¥ã€‚ä¸ºäº†è·å¾—è¿™æ ·çš„ä¿è¯ï¼Œæˆ‘ä»¬å»ºè®®å°†å¯¹è±¡è·Ÿè¸ªè§£é‡Šä¸ºé¡ºåºå‡è®¾æ£€éªŒï¼Œå…¶ä¸­æ”¯æŒæˆ–åå¯¹è·Ÿè¸ªå¤±è´¥çš„è¯æ®éšç€æ—¶é—´çš„æ¨ç§»é€æ¸ç§¯ç´¯ã€‚åˆ©ç”¨è¯¥é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œæˆ‘ä»¬çš„é¡ºåºæµ‹è¯•ï¼ˆå½¢å¼åŒ–ä¸ºç”µå­æµç¨‹ï¼‰å¯ä»¥å¿«é€Ÿè¯†åˆ«ä½•æ—¶å‡ºç°è·Ÿè¸ªæ•…éšœï¼ŒåŒæ—¶ä»¥æ‰€éœ€çš„é€Ÿåº¦è¯æ˜åŒ…å«é”™è¯¯è­¦æŠ¥ï¼Œä»è€Œé™åˆ¶å¯èƒ½æˆæœ¬é«˜æ˜‚çš„é‡æ–°æ ¡å‡†æˆ–å¹²é¢„æ­¥éª¤ã€‚è¯¥æ–¹æ³•è®¡ç®—é‡è½»ï¼Œä¸éœ€è¦é¢å¤–çš„è®­ç»ƒæˆ–å¾®è°ƒï¼Œå¹¶ä¸”åŸåˆ™ä¸Šä¸æ¨¡å‹æ— å…³ã€‚æˆ‘ä»¬é€šè¿‡åˆ©ç”¨çœŸå®æƒ…å†µæˆ–ä»…å†…éƒ¨è·Ÿè¸ªä¿¡æ¯æå‡ºç›‘ç£å’Œæ— ç›‘ç£å˜ä½“ï¼Œå¹¶åœ¨å››ä¸ªè§†é¢‘åŸºå‡†ä¸­è¯æ˜å…¶å¯¹ä¸¤ä¸ªå·²å»ºç«‹çš„è·Ÿè¸ªæ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚å› æ­¤ï¼Œé¡ºåºæµ‹è¯•å¯ä»¥æä¾›ä¸€ç§åŸºäºç»Ÿè®¡çš„æœ‰æ•ˆæœºåˆ¶ï¼Œå°†å®‰å…¨ä¿è¯çº³å…¥å®æ—¶è·Ÿè¸ªç³»ç»Ÿã€‚

</details>

---



</details>

<details><summary><b>2026-02-14 (16 papers)</b></summary>

# arXiv Video Papers - 2026-02-14

**Paper Count**: 16

---

## 1. MonarchRT: Efficient Attention for Real-Time Video Generation / MonarchRTï¼šå®æ—¶è§†é¢‘ç”Ÿæˆçš„é«˜æ•ˆå…³æ³¨

**Date**: 2026-02-12 | **arXiv**: [2602.12271v1](http://arxiv.org/abs/2602.12271v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.12271v1)

**Categories**: cs.CV, cs.LG

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Real-time video generation with Diffusion Transformers is bottlenecked by the quadratic cost of 3D self-attention, especially in real-time regimes that are both few-step and autoregressive, where errors compound across time and each denoising step must carry substantially more information. In this setting, we find that prior sparse-attention approximations break down, despite showing strong results for bidirectional, many-step diffusion. Specifically, we observe that video attention is not reliably sparse, but instead combines pronounced periodic structure driven by spatiotemporal position with dynamic, sparse semantic correspondences and dense mixing, exceeding the representational capacity of even oracle top-k attention. Building on this insight, we propose Monarch-RT, a structured attention parameterization for video diffusion models that factorizes attention using Monarch matrices. Through appropriately aligned block structure and our extended tiled Monarch parameterization, we achieve high expressivity while preserving computational efficiency. We further overcome the overhead of parameterization through finetuning, with custom Triton kernels. We first validate the high efficacy of Monarch-RT over existing sparse baselines designed only for bidirectional models. We further observe that Monarch-RT attains up to 95% attention sparsity with no loss in quality when applied to the state-of-the-art model Self-Forcing, making Monarch-RT a pioneering work on highly-capable sparse attention parameterization for real-time video generation. Our optimized implementation outperforms FlashAttention-2, FlashAttention-3, and FlashAttention-4 kernels on Nvidia RTX 5090, H100, and B200 GPUs respectively, providing kernel speedups in the range of 1.4-11.8X. This enables us, for the first time, to achieve true real-time video generation with Self-Forcing at 16 FPS on a single RTX 5090.

ä½¿ç”¨æ‰©æ•£å˜å‹å™¨çš„å®æ—¶è§†é¢‘ç”Ÿæˆå—åˆ° 3D è‡ªæ³¨æ„åŠ›äºŒæ¬¡æˆæœ¬çš„ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯åœ¨å°‘æ­¥å’Œè‡ªå›å½’çš„å®æ—¶æœºåˆ¶ä¸­ï¼Œå…¶ä¸­è¯¯å·®éšæ—¶é—´å¤åˆï¼Œæ¯ä¸ªå»å™ªæ­¥éª¤å¿…é¡»æºå¸¦æ›´å¤šä¿¡æ¯ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å‘ç°å°½ç®¡åŒå‘ã€å¤šæ­¥æ‰©æ•£æ˜¾ç¤ºå‡ºå¾ˆå¼ºçš„ç»“æœï¼Œä½†å…ˆå‰çš„ç¨€ç–æ³¨æ„åŠ›è¿‘ä¼¼æ–¹æ³•è¿˜æ˜¯å¤±æ•ˆäº†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è§†é¢‘æ³¨æ„åŠ›å¹¶ä¸æ˜¯å¯é çš„ç¨€ç–ï¼Œè€Œæ˜¯å°†ç”±æ—¶ç©ºä½ç½®é©±åŠ¨çš„æ˜¾ç€å‘¨æœŸç»“æ„ä¸åŠ¨æ€ã€ç¨€ç–è¯­ä¹‰å¯¹åº”å’Œå¯†é›†æ··åˆç›¸ç»“åˆï¼Œç”šè‡³è¶…è¿‡äº† oracle top-k æ³¨æ„åŠ›çš„è¡¨ç¤ºèƒ½åŠ›ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº† Monarch-RTï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè§†é¢‘æ‰©æ•£æ¨¡å‹çš„ç»“æ„åŒ–æ³¨æ„åŠ›å‚æ•°åŒ–ï¼Œå®ƒä½¿ç”¨ Monarch çŸ©é˜µåˆ†è§£æ³¨æ„åŠ›ã€‚é€šè¿‡é€‚å½“å¯¹é½çš„å—ç»“æ„å’Œæ‰©å±•çš„å¹³é“º Monarch å‚æ•°åŒ–ï¼Œæˆ‘ä»¬åœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶å®ç°äº†é«˜è¡¨è¾¾åŠ›ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨è‡ªå®šä¹‰ Triton å†…æ ¸è¿›è¡Œå¾®è°ƒï¼Œè¿›ä¸€æ­¥å…‹æœäº†å‚æ•°åŒ–çš„å¼€é”€ã€‚æˆ‘ä»¬é¦–å…ˆéªŒè¯ Monarch-RT ç›¸å¯¹äºä»…ä¸ºåŒå‘æ¨¡å‹è®¾è®¡çš„ç°æœ‰ç¨€ç–åŸºçº¿çš„é«˜æ•ˆæ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è§‚å¯Ÿåˆ°ï¼Œå½“åº”ç”¨äºæœ€å…ˆè¿›çš„æ¨¡å‹ Self-Forcing æ—¶ï¼ŒMonarch-RT è·å¾—äº†é«˜è¾¾ 95% çš„æ³¨æ„åŠ›ç¨€ç–åº¦ï¼Œä¸”è´¨é‡æ²¡æœ‰æŸå¤±ï¼Œè¿™ä½¿å¾— Monarch-RT æˆä¸ºå®æ—¶è§†é¢‘ç”Ÿæˆçš„é«˜æ€§èƒ½ç¨€ç–æ³¨æ„åŠ›å‚æ•°åŒ–çš„å¼€åˆ›æ€§å·¥ä½œã€‚æˆ‘ä»¬çš„ä¼˜åŒ–å®ç°åœ¨ Nvidia RTX 5090ã€H100 å’Œ B200 GPU ä¸Šçš„æ€§èƒ½åˆ†åˆ«ä¼˜äº FlashAttention-2ã€FlashAttention-3 å’Œ FlashAttention-4 å†…æ ¸ï¼Œæä¾› 1.4-11.8 å€çš„å†…æ ¸åŠ é€Ÿã€‚è¿™ä½¿æˆ‘ä»¬ç¬¬ä¸€æ¬¡èƒ½å¤Ÿåœ¨å•ä¸ª RTX 5090 ä¸Šä»¥ 16 FPS çš„é€Ÿåº¦å®ç°çœŸæ­£çš„å®æ—¶è§†é¢‘ç”Ÿæˆã€‚

</details>

---

## 2. DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation / DreamID-Omniï¼šå¯æ§çš„ä»¥äººä¸ºæœ¬çš„éŸ³é¢‘è§†é¢‘ç”Ÿæˆçš„ç»Ÿä¸€æ¡†æ¶

**Date**: 2026-02-12 | **arXiv**: [2602.12160v1](http://arxiv.org/abs/2602.12160v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.12160v1)

**Categories**: cs.CV

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audio-video generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives. Furthermore, achieving precise, disentangled control over multiple character identities and voice timbres within a single framework remains an open challenge. In this paper, we propose DreamID-Omni, a unified framework for controllable human-centric audio-video generation. Specifically, we design a Symmetric Conditional Diffusion Transformer that integrates heterogeneous conditioning signals via a symmetric conditional injection scheme. To resolve the pervasive identity-timbre binding failures and speaker confusion in multi-person scenarios, we introduce a Dual-Level Disentanglement strategy: Synchronized RoPE at the signal level to ensure rigid attention-space binding, and Structured Captions at the semantic level to establish explicit attribute-subject mappings. Furthermore, we devise a Multi-Task Progressive Training scheme that leverages weakly-constrained generative priors to regularize strongly-constrained tasks, preventing overfitting and harmonizing disparate objectives. Extensive experiments demonstrate that DreamID-Omni achieves comprehensive state-of-the-art performance across video, audio, and audio-visual consistency, even outperforming leading proprietary commercial models. We will release our code to bridge the gap between academic research and commercial-grade applications.

åŸºç¡€æ¨¡å‹çš„æœ€æ–°è¿›å±•å½»åº•æ”¹å˜äº†è”åˆéŸ³é¢‘è§†é¢‘ç”Ÿæˆã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸å°†ä»¥äººä¸ºä¸­å¿ƒçš„ä»»åŠ¡è§†ä¸ºå­¤ç«‹çš„ç›®æ ‡ï¼ŒåŒ…æ‹¬åŸºäºå‚è€ƒçš„éŸ³é¢‘è§†é¢‘ç”Ÿæˆï¼ˆR2AVï¼‰ã€è§†é¢‘ç¼–è¾‘ï¼ˆRV2AVï¼‰å’ŒéŸ³é¢‘é©±åŠ¨è§†é¢‘åŠ¨ç”»ï¼ˆRA2Vï¼‰ã€‚æ­¤å¤–ï¼Œåœ¨å•ä¸€æ¡†æ¶å†…å®ç°å¯¹å¤šä¸ªè§’è‰²èº«ä»½å’ŒéŸ³è‰²çš„ç²¾ç¡®ã€åˆ†ç¦»çš„æ§åˆ¶ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† DreamID-Omniï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¯æ§çš„ä»¥äººä¸ºä¸­å¿ƒçš„éŸ³é¢‘è§†é¢‘ç”Ÿæˆçš„ç»Ÿä¸€æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¯¹ç§°æ¡ä»¶æ‰©æ•£å˜å‹å™¨ï¼Œå®ƒé€šè¿‡å¯¹ç§°æ¡ä»¶æ³¨å…¥æ–¹æ¡ˆé›†æˆå¼‚æ„è°ƒèŠ‚ä¿¡å·ã€‚ä¸ºäº†è§£å†³å¤šäººåœºæ™¯ä¸­æ™®éå­˜åœ¨çš„èº«ä»½-éŸ³è‰²ç»‘å®šå¤±è´¥å’Œè¯´è¯è€…æ··ä¹±çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŒå±‚è§£ç¼ ç­–ç•¥ï¼šä¿¡å·çº§åˆ«çš„åŒæ­¥ RoPE ä»¥ç¡®ä¿ä¸¥æ ¼çš„æ³¨æ„åŠ›ç©ºé—´ç»‘å®šï¼Œè¯­ä¹‰çº§åˆ«çš„ç»“æ„åŒ–å­—å¹•ä»¥å»ºç«‹æ˜ç¡®çš„å±æ€§-ä¸»é¢˜æ˜ å°„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å¤šä»»åŠ¡æ¸è¿›è®­ç»ƒæ–¹æ¡ˆï¼Œåˆ©ç”¨å¼±çº¦æŸçš„ç”Ÿæˆå…ˆéªŒæ¥è§„èŒƒå¼ºçº¦æŸçš„ä»»åŠ¡ï¼Œé˜²æ­¢è¿‡åº¦æ‹Ÿåˆå¹¶åè°ƒä¸åŒçš„ç›®æ ‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDreamID-Omni åœ¨è§†é¢‘ã€éŸ³é¢‘å’Œè§†å¬ä¸€è‡´æ€§æ–¹é¢å®ç°äº†å…¨é¢çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œç”šè‡³è¶…è¶Šäº†é¢†å…ˆçš„ä¸“æœ‰å•†ä¸šæ¨¡å‹ã€‚æˆ‘ä»¬å°†å‘å¸ƒæˆ‘ä»¬çš„ä»£ç ï¼Œä»¥å¼¥åˆå­¦æœ¯ç ”ç©¶å’Œå•†ä¸šçº§åº”ç”¨ç¨‹åºä¹‹é—´çš„å·®è·ã€‚

</details>

---

## 3. FAIL: Flow Matching Adversarial Imitation Learning for Image Generation / å¤±è´¥ï¼šç”¨äºå›¾åƒç”Ÿæˆçš„æµåŒ¹é…å¯¹æŠ—æ€§æ¨¡ä»¿å­¦ä¹ 

**Date**: 2026-02-12 | **arXiv**: [2602.12155v1](http://arxiv.org/abs/2602.12155v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.12155v1)

**Categories**: cs.CV

**Code**: https://github.com/HansPolo113/FAIL.

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Post-training of flow matching models-aligning the output distribution with a high-quality target-is mathematically equivalent to imitation learning. While Supervised Fine-Tuning mimics expert demonstrations effectively, it cannot correct policy drift in unseen states. Preference optimization methods address this but require costly preference pairs or reward modeling. We propose Flow Matching Adversarial Imitation Learning (FAIL), which minimizes policy-expert divergence through adversarial training without explicit rewards or pairwise comparisons. We derive two algorithms: FAIL-PD exploits differentiable ODE solvers for low-variance pathwise gradients, while FAIL-PG provides a black-box alternative for discrete or computationally constrained settings. Fine-tuning FLUX with only 13,000 demonstrations from Nano Banana pro, FAIL achieves competitive performance on prompt following and aesthetic benchmarks. Furthermore, the framework generalizes effectively to discrete image and video generation, and functions as a robust regularizer to mitigate reward hacking in reward-based optimization. Code and data are available at https://github.com/HansPolo113/FAIL.

æµåŒ¹é…æ¨¡å‹çš„åè®­ç»ƒâ€”â€”å°†è¾“å‡ºåˆ†å¸ƒä¸é«˜è´¨é‡ç›®æ ‡å¯¹é½â€”â€”åœ¨æ•°å­¦ä¸Šç­‰åŒäºæ¨¡ä»¿å­¦ä¹ ã€‚è™½ç„¶ç›‘ç£å¾®è°ƒæœ‰æ•ˆåœ°æ¨¡ä»¿äº†ä¸“å®¶çš„æ¼”ç¤ºï¼Œä½†å®ƒæ— æ³•çº æ­£çœ‹ä¸è§çš„çŠ¶æ€ä¸­çš„æ”¿ç­–æ¼‚ç§»ã€‚åå¥½ä¼˜åŒ–æ–¹æ³•å¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†éœ€è¦æ˜‚è´µçš„åå¥½å¯¹æˆ–å¥–åŠ±å»ºæ¨¡ã€‚æˆ‘ä»¬æå‡ºäº†æµåŒ¹é…å¯¹æŠ—æ€§æ¨¡ä»¿å­¦ä¹ ï¼ˆFAILï¼‰ï¼Œå®ƒé€šè¿‡å¯¹æŠ—æ€§è®­ç»ƒæ¥æœ€å°åŒ–æ”¿ç­–ä¸“å®¶åˆ†æ­§ï¼Œè€Œæ— éœ€æ˜ç¡®çš„å¥–åŠ±æˆ–æˆå¯¹æ¯”è¾ƒã€‚æˆ‘ä»¬æ¨å¯¼å‡ºä¸¤ç§ç®—æ³•ï¼šFAIL-PD åˆ©ç”¨å¯å¾®åˆ† ODE æ±‚è§£å™¨æ¥å®ç°ä½æ–¹å·®è·¯å¾„æ¢¯åº¦ï¼Œè€Œ FAIL-PG åˆ™ä¸ºç¦»æ•£æˆ–è®¡ç®—çº¦æŸè®¾ç½®æä¾›é»‘ç›’æ›¿ä»£æ–¹æ¡ˆã€‚ä»…é€šè¿‡ Nano Banana pro çš„ 13,000 æ¬¡æ¼”ç¤ºå¯¹ FLUX è¿›è¡Œå¾®è°ƒï¼ŒFAIL åœ¨å¿«é€Ÿè·Ÿéšå’Œç¾å­¦åŸºå‡†æ–¹é¢å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆåœ°æ¨å¹¿åˆ°ç¦»æ•£å›¾åƒå’Œè§†é¢‘ç”Ÿæˆï¼Œå¹¶ä½œä¸ºå¼ºå¤§çš„æ­£åˆ™åŒ–å™¨æ¥å‡è½»åŸºäºå¥–åŠ±çš„ä¼˜åŒ–ä¸­çš„å¥–åŠ±é»‘å®¢è¡Œä¸ºã€‚ä»£ç å’Œæ•°æ®å¯åœ¨ https://github.com/HansPolo113/FAIL è·å–ã€‚

</details>

---

## 4. How to Sample High Quality 3D Fractals for Action Recognition Pre-Training? / å¦‚ä½•å¯¹é«˜è´¨é‡ 3D åˆ†å½¢è¿›è¡Œé‡‡æ ·ä»¥è¿›è¡ŒåŠ¨ä½œè¯†åˆ«é¢„è®­ç»ƒï¼Ÿ

**Date**: 2026-02-12 | **arXiv**: [2602.11810v1](http://arxiv.org/abs/2602.11810v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11810v1)

**Categories**: cs.CV, cs.LG

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Synthetic datasets are being recognized in the deep learning realm as a valuable alternative to exhaustively labeled real data. One such synthetic data generation method is Formula Driven Supervised Learning (FDSL), which can provide an infinite number of perfectly labeled data through a formula driven approach, such as fractals or contours. FDSL does not have common drawbacks like manual labor, privacy and other ethical concerns. In this work we generate 3D fractals using 3D Iterated Function Systems (IFS) for pre-training an action recognition model. The fractals are temporally transformed to form a video that is used as a pre-training dataset for downstream task of action recognition. We find that standard methods of generating fractals are slow and produce degenerate 3D fractals. Therefore, we systematically explore alternative ways of generating fractals and finds that overly-restrictive approaches, while generating aesthetically pleasing fractals, are detrimental for downstream task performance. We propose a novel method, Targeted Smart Filtering, to address both the generation speed and fractal diversity issue. The method reports roughly 100 times faster sampling speed and achieves superior downstream performance against other 3D fractal filtering methods.

åˆæˆæ•°æ®é›†åœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸè¢«è®¤ä¸ºæ˜¯è¯¦å°½æ ‡è®°çš„çœŸå®æ•°æ®çš„æœ‰ä»·å€¼çš„æ›¿ä»£æ–¹æ¡ˆã€‚å…¶ä¸­ä¸€ç§åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•æ˜¯å…¬å¼é©±åŠ¨ç›‘ç£å­¦ä¹ ï¼ˆFDSLï¼‰ï¼Œå®ƒå¯ä»¥é€šè¿‡å…¬å¼é©±åŠ¨æ–¹æ³•ï¼ˆä¾‹å¦‚åˆ†å½¢æˆ–è½®å»“ï¼‰æä¾›æ— é™æ•°é‡çš„å®Œç¾æ ‡è®°æ•°æ®ã€‚ FDSL æ²¡æœ‰ä½“åŠ›åŠ³åŠ¨ã€éšç§å’Œå…¶ä»–é“å¾·é—®é¢˜ç­‰å¸¸è§ç¼ºç‚¹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ 3D è¿­ä»£å‡½æ•°ç³»ç»Ÿ (IFS) ç”Ÿæˆ 3D åˆ†å½¢ï¼Œä»¥é¢„è®­ç»ƒåŠ¨ä½œè¯†åˆ«æ¨¡å‹ã€‚åˆ†å½¢åœ¨æ—¶é—´ä¸Šè¿›è¡Œå˜æ¢ä»¥å½¢æˆè§†é¢‘ï¼Œè¯¥è§†é¢‘ç”¨ä½œåŠ¨ä½œè¯†åˆ«ä¸‹æ¸¸ä»»åŠ¡çš„é¢„è®­ç»ƒæ•°æ®é›†ã€‚æˆ‘ä»¬å‘ç°ç”Ÿæˆåˆ†å½¢çš„æ ‡å‡†æ–¹æ³•å¾ˆæ…¢å¹¶ä¸”ä¼šäº§ç”Ÿç®€å¹¶çš„ 3D åˆ†å½¢ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ¢ç´¢äº†ç”Ÿæˆåˆ†å½¢çš„æ›¿ä»£æ–¹æ³•ï¼Œå¹¶å‘ç°è¿‡åº¦é™åˆ¶çš„æ–¹æ³•è™½ç„¶ç”Ÿæˆç¾è§‚çš„åˆ†å½¢ï¼Œä½†ä¸åˆ©äºä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå³ç›®æ ‡æ™ºèƒ½è¿‡æ»¤ï¼Œæ¥è§£å†³ç”Ÿæˆé€Ÿåº¦å’Œåˆ†å½¢å¤šæ ·æ€§é—®é¢˜ã€‚ä¸å…¶ä»– 3D åˆ†å½¢è¿‡æ»¤æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•çš„é‡‡æ ·é€Ÿåº¦å¿«äº†å¤§çº¦ 100 å€ï¼Œå¹¶å®ç°äº†å“è¶Šçš„ä¸‹æ¸¸æ€§èƒ½ã€‚

</details>

---

## 5. STVG-R1: Incentivizing Instance-Level Reasoning and Grounding in Videos via Reinforcement Learning / STVG-R1ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¿€åŠ±è§†é¢‘ä¸­çš„å®ä¾‹çº§æ¨ç†å’ŒåŸºç¡€

**Date**: 2026-02-12 | **arXiv**: [2602.11730v1](http://arxiv.org/abs/2602.11730v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11730v1)

**Categories**: cs.CV

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

In vision-language models (VLMs), misalignment between textual descriptions and visual coordinates often induces hallucinations. This issue becomes particularly severe in dense prediction tasks such as spatial-temporal video grounding (STVG). Prior approaches typically focus on enhancing visual-textual alignment or attaching auxiliary decoders. However, these strategies inevitably introduce additional trainable modules, leading to significant annotation costs and computational overhead. In this work, we propose a novel visual prompting paradigm that avoids the difficult problem of aligning coordinates across modalities. Specifically, we reformulate per-frame coordinate prediction as a compact instance-level identification problem by assigning each object a unique, temporally consistent ID. These IDs are embedded into the video as visual prompts, providing explicit and interpretable inputs to the VLMs. Furthermore, we introduce STVG-R1, the first reinforcement learning framework for STVG, which employs a task-driven reward to jointly optimize temporal accuracy, spatial consistency, and structural format regularization. Extensive experiments on six benchmarks demonstrate the effectiveness of our approach. STVG-R1 surpasses the baseline Qwen2.5-VL-7B by a remarkable margin of 20.9% on m_IoU on the HCSTVG-v2 benchmark, establishing a new state of the art (SOTA). Surprisingly, STVG-R1 also exhibits strong zero-shot generalization to multi-object referring video object segmentation tasks, achieving a SOTA 47.3% J&F on MeViS.

åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸­ï¼Œæ–‡æœ¬æè¿°å’Œè§†è§‰åæ ‡ä¹‹é—´çš„é”™ä½é€šå¸¸ä¼šå¼•èµ·å¹»è§‰ã€‚è¿™ä¸ªé—®é¢˜åœ¨æ—¶ç©ºè§†é¢‘æ¥åœ°ï¼ˆSTVGï¼‰ç­‰å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­å˜å¾—å°¤ä¸ºä¸¥é‡ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾§é‡äºå¢å¼ºè§†è§‰æ–‡æœ¬å¯¹é½æˆ–é™„åŠ è¾…åŠ©è§£ç å™¨ã€‚ç„¶è€Œï¼Œè¿™äº›ç­–ç•¥ä¸å¯é¿å…åœ°å¼•å…¥é¢å¤–çš„å¯è®­ç»ƒæ¨¡å—ï¼Œå¯¼è‡´æ˜¾ç€çš„æ³¨é‡Šæˆæœ¬å’Œè®¡ç®—å¼€é”€ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†è§‰æç¤ºèŒƒä¾‹ï¼Œé¿å…äº†è·¨æ¨¡æ€å¯¹é½åæ ‡çš„éš¾é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡ä¸ºæ¯ä¸ªå¯¹è±¡åˆ†é…ä¸€ä¸ªå”¯ä¸€çš„ã€æ—¶é—´ä¸€è‡´çš„ IDï¼Œå°†æ¯å¸§åæ ‡é¢„æµ‹é‡æ–°è¡¨è¿°ä¸ºä¸€ä¸ªç´§å‡‘çš„å®ä¾‹çº§è¯†åˆ«é—®é¢˜ã€‚è¿™äº› ID ä½œä¸ºè§†è§‰æç¤ºåµŒå…¥åˆ°è§†é¢‘ä¸­ï¼Œä¸º VLM æä¾›æ˜ç¡®ä¸”å¯è§£é‡Šçš„è¾“å…¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº† STVG-R1ï¼Œè¿™æ˜¯ STVG çš„ç¬¬ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒé‡‡ç”¨ä»»åŠ¡é©±åŠ¨çš„å¥–åŠ±æ¥è”åˆä¼˜åŒ–æ—¶é—´å‡†ç¡®æ€§ã€ç©ºé—´ä¸€è‡´æ€§å’Œç»“æ„æ ¼å¼æ­£åˆ™åŒ–ã€‚å¯¹å…­ä¸ªåŸºå‡†çš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ STVG-R1 åœ¨ HCSTVG-v2 åŸºå‡†ä¸Šçš„ m_IoU ä¸Šè¶…è¶Šäº†åŸºçº¿ Qwen2.5-VL-7Bï¼Œæ˜¾ç€æé«˜äº† 20.9%ï¼Œå»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›æŠ€æœ¯ (SOTA)ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼ŒSTVG-R1 è¿˜å¯¹å¤šå¯¹è±¡å‚è€ƒè§†é¢‘å¯¹è±¡åˆ†å‰²ä»»åŠ¡è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨ MeViS ä¸Šå®ç°äº† SOTA 47.3% J&Fã€‚

</details>

---

## 6. LUVE : Latent-Cascaded Ultra-High-Resolution Video Generation with Dual Frequency Experts / LUVEï¼šåŒé¢‘ä¸“å®¶çš„æ½œåœ¨çº§è”è¶…é«˜åˆ†è¾¨ç‡è§†é¢‘ç”Ÿæˆ

**Date**: 2026-02-12 | **arXiv**: [2602.11564v1](http://arxiv.org/abs/2602.11564v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11564v1)

**Categories**: cs.CV

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Recent advances in video diffusion models have significantly improved visual quality, yet ultra-high-resolution (UHR) video generation remains a formidable challenge due to the compounded difficulties of motion modeling, semantic planning, and detail synthesis. To address these limitations, we propose \textbf{LUVE}, a \textbf{L}atent-cascaded \textbf{U}HR \textbf{V}ideo generation framework built upon dual frequency \textbf{E}xperts. LUVE employs a three-stage architecture comprising low-resolution motion generation for motion-consistent latent synthesis, video latent upsampling that performs resolution upsampling directly in the latent space to mitigate memory and computational overhead, and high-resolution content refinement that integrates low-frequency and high-frequency experts to jointly enhance semantic coherence and fine-grained detail generation. Extensive experiments demonstrate that our LUVE achieves superior photorealism and content fidelity in UHR video generation, and comprehensive ablation studies further validate the effectiveness of each component. The project is available at \href{https://unicornanrocinu.github.io/LUVE_web/}{https://github.io/LUVE/}.

è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•æ˜¾ç€æé«˜äº†è§†è§‰è´¨é‡ï¼Œä½†ç”±äºè¿åŠ¨å»ºæ¨¡ã€è¯­ä¹‰è§„åˆ’å’Œç»†èŠ‚åˆæˆçš„å¤æ‚å›°éš¾ï¼Œè¶…é«˜åˆ†è¾¨ç‡ (UHR) è§†é¢‘ç”Ÿæˆä»ç„¶æ˜¯ä¸€ä¸ªè‰°å·¨çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº† \textbf{LUVE}ï¼Œä¸€ä¸ªåŸºäºåŒé¢‘ \textbf{E}xperts æ„å»ºçš„ \textbf{L}atent çº§è” \textbf{U}HR \textbf{V}ideo ç”Ÿæˆæ¡†æ¶ã€‚ LUVE é‡‡ç”¨ä¸‰é˜¶æ®µæ¶æ„ï¼ŒåŒ…æ‹¬ç”¨äºè¿åŠ¨ä¸€è‡´æ½œåœ¨åˆæˆçš„ä½åˆ†è¾¨ç‡è¿åŠ¨ç”Ÿæˆã€ç›´æ¥åœ¨æ½œåœ¨ç©ºé—´ä¸­æ‰§è¡Œåˆ†è¾¨ç‡ä¸Šé‡‡æ ·ä»¥å‡è½»å†…å­˜å’Œè®¡ç®—å¼€é”€çš„è§†é¢‘æ½œåœ¨ä¸Šé‡‡æ ·ï¼Œä»¥åŠé›†æˆä½é¢‘å’Œé«˜é¢‘ä¸“å®¶ä»¥å…±åŒå¢å¼ºè¯­ä¹‰ä¸€è‡´æ€§å’Œç»†ç²’åº¦ç»†èŠ‚ç”Ÿæˆçš„é«˜åˆ†è¾¨ç‡å†…å®¹ç»†åŒ–ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ LUVE åœ¨ UHR è§†é¢‘ç”Ÿæˆä¸­å®ç°äº†å“è¶Šçš„ç…§ç‰‡çœŸå®æ„Ÿå’Œå†…å®¹ä¿çœŸåº¦ï¼Œå…¨é¢çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†æ¯ä¸ªç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚è¯¥é¡¹ç›®ä½äº \href{https://unicornanrocinu.github.io/LUVE_web/}{https://github.io/LUVE/}ã€‚

</details>

---

## 7. SAM3-LiteText: An Anatomical Study of the SAM3 Text Encoder for Efficient Vision-Language Segmentation / SAM3-LiteTextï¼šç”¨äºé«˜æ•ˆè§†è§‰è¯­è¨€åˆ†å‰²çš„ SAM3 æ–‡æœ¬ç¼–ç å™¨çš„è§£å‰–ç ”ç©¶

**Date**: 2026-02-12 | **arXiv**: [2602.12173v1](http://arxiv.org/abs/2602.12173v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.12173v1)

**Categories**: cs.AI

**Code**: https://github.com/SimonZeng7108/efficientsam3/tree/sam3_litetext.

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Vision-language segmentation models such as SAM3 enable flexible, prompt-driven visual grounding, but inherit large, general-purpose text encoders originally designed for open-ended language understanding. In practice, segmentation prompts are short, structured, and semantically constrained, leading to substantial over-provisioning in text encoder capacity and persistent computational and memory overhead. In this paper, we perform a large-scale anatomical analysis of text prompting in vision-language segmentation, covering 404,796 real prompts across multiple benchmarks. Our analysis reveals severe redundancy: most context windows are underutilized, vocabulary usage is highly sparse, and text embeddings lie on low-dimensional manifold despite high-dimensional representations. Motivated by these findings, we propose SAM3-LiteText, a lightweight text encoding framework that replaces the original SAM3 text encoder with a compact MobileCLIP student that is optimized by knowledge distillation. Extensive experiments on image and video segmentation benchmarks show that SAM3-LiteText reduces text encoder parameters by up to 88%, substantially reducing static memory footprint, while maintaining segmentation performance comparable to the original model. Code: https://github.com/SimonZeng7108/efficientsam3/tree/sam3_litetext.

SAM3 ç­‰è§†è§‰è¯­è¨€åˆ†å‰²æ¨¡å‹å¯å®ç°çµæ´»ã€æç¤ºé©±åŠ¨çš„è§†è§‰åŸºç¡€ï¼Œä½†ç»§æ‰¿äº†æœ€åˆä¸ºå¼€æ”¾å¼è¯­è¨€ç†è§£è€Œè®¾è®¡çš„å¤§å‹é€šç”¨æ–‡æœ¬ç¼–ç å™¨ã€‚åœ¨å®è·µä¸­ï¼Œåˆ†æ®µæç¤ºå¾ˆçŸ­ã€ç»“æ„åŒ–ä¸”è¯­ä¹‰å—é™ï¼Œå¯¼è‡´æ–‡æœ¬ç¼–ç å™¨å®¹é‡çš„å¤§é‡è¿‡åº¦é…ç½®ä»¥åŠæŒç»­çš„è®¡ç®—å’Œå†…å­˜å¼€é”€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹è§†è§‰è¯­è¨€åˆ†å‰²ä¸­çš„æ–‡æœ¬æç¤ºè¿›è¡Œäº†å¤§è§„æ¨¡çš„è§£å‰–åˆ†æï¼Œæ¶µç›–äº†è·¨å¤šä¸ªåŸºå‡†çš„ 404,796 ä¸ªçœŸå®æç¤ºã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ä¸¥é‡çš„å†—ä½™ï¼šå¤§å¤šæ•°ä¸Šä¸‹æ–‡çª—å£æœªå¾—åˆ°å……åˆ†åˆ©ç”¨ï¼Œè¯æ±‡ä½¿ç”¨é«˜åº¦ç¨€ç–ï¼Œå°½ç®¡å…·æœ‰é«˜ç»´è¡¨ç¤ºï¼Œä½†æ–‡æœ¬åµŒå…¥ä½äºä½ç»´æµå½¢ä¸Šã€‚å—è¿™äº›å‘ç°çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº† SAM3-LiteTextï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§æ–‡æœ¬ç¼–ç æ¡†æ¶ï¼Œç”¨é€šè¿‡çŸ¥è¯†è’¸é¦ä¼˜åŒ–çš„ç´§å‡‘å‹ MobileCLIP Student å–ä»£äº†åŸå§‹çš„ SAM3 æ–‡æœ¬ç¼–ç å™¨ã€‚å¯¹å›¾åƒå’Œè§†é¢‘åˆ†å‰²åŸºå‡†çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSAM3-LiteText å°†æ–‡æœ¬ç¼–ç å™¨å‚æ•°å‡å°‘äº†é«˜è¾¾ 88%ï¼Œå¤§å¤§å‡å°‘äº†é™æ€å†…å­˜å ç”¨ï¼ŒåŒæ—¶ä¿æŒäº†ä¸åŸå§‹æ¨¡å‹ç›¸å½“çš„åˆ†å‰²æ€§èƒ½ã€‚ä»£ç ï¼šhttps://github.com/SimonZeng7108/efficientsam3/tree/sam3_litetextã€‚

</details>

---

## 8. HLA: Hadamard Linear Attention / HLAï¼šHadamard çº¿æ€§æ³¨æ„åŠ›

**Date**: 2026-02-12 | **arXiv**: [2602.12128v1](http://arxiv.org/abs/2602.12128v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.12128v1)

**Categories**: cs.AI

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

The attention mechanism is an important reason for the success of transformers. It relies on computing pairwise relations between tokens. To reduce the high computational cost of standard quadratic attention, linear attention has been proposed as an efficient approximation. It employs kernel functions that are applied independently to the inputs before the pairwise similarities are calculated. That allows for an efficient computational procedure which, however, amounts to a low-degree rational function approximating softmax.   We propose Hadamard Linear Attention (HLA). Unlike previous works on linear attention, the nonlinearity in HLA is not applied separately to queries and keys, but, analogously to standard softmax attention, after the pairwise similarities have been computed. It will be shown that the proposed nonlinearity amounts to a higher-degree rational function to approximate softmax. An efficient computational scheme for the proposed method is derived that is similar to that of standard linear attention. In contrast to other approaches, no time-consuming tensor reshaping is necessary to apply the proposed algorithm. The effectiveness of the approach is demonstrated by applying it to a large diffusion transformer model for video generation, an application that involves very large amounts of tokens.

æ³¨æ„åŠ›æœºåˆ¶æ˜¯ Transformer æˆåŠŸçš„é‡è¦åŸå› ã€‚å®ƒä¾èµ–äºè®¡ç®—æ ‡è®°ä¹‹é—´çš„æˆå¯¹å…³ç³»ã€‚ä¸ºäº†é™ä½æ ‡å‡†äºŒæ¬¡æ³¨æ„åŠ›çš„é«˜è®¡ç®—æˆæœ¬ï¼Œçº¿æ€§æ³¨æ„åŠ›è¢«æå‡ºä½œä¸ºä¸€ç§æœ‰æ•ˆçš„è¿‘ä¼¼ã€‚å®ƒé‡‡ç”¨åœ¨è®¡ç®—æˆå¯¹ç›¸ä¼¼åº¦ä¹‹å‰ç‹¬ç«‹åº”ç”¨äºè¾“å…¥çš„æ ¸å‡½æ•°ã€‚è¿™å…è®¸é«˜æ•ˆçš„è®¡ç®—è¿‡ç¨‹ï¼Œç„¶è€Œï¼Œè¿™ç›¸å½“äºä¸€ä¸ªé€¼è¿‘ softmax çš„ä½æ¬¡æœ‰ç†å‡½æ•°ã€‚   æˆ‘ä»¬æå‡ºå“ˆè¾¾ç›çº¿æ€§æ³¨æ„åŠ›ï¼ˆHLAï¼‰ã€‚ä¸ä¹‹å‰çš„çº¿æ€§æ³¨æ„åŠ›å·¥ä½œä¸åŒï¼ŒHLA ä¸­çš„éçº¿æ€§ä¸æ˜¯å•ç‹¬åº”ç”¨äºæŸ¥è¯¢å’Œé”®ï¼Œè€Œæ˜¯ç±»ä¼¼äºæ ‡å‡†çš„ softmax æ³¨æ„åŠ›ï¼Œåœ¨è®¡ç®—æˆå¯¹ç›¸ä¼¼æ€§ä¹‹ååº”ç”¨ã€‚å°†ä¼šè¡¨æ˜ï¼Œæ‰€æå‡ºçš„éçº¿æ€§ç›¸å½“äºä¸€ä¸ªæ›´é«˜é˜¶çš„æœ‰ç†å‡½æ•°æ¥è¿‘ä¼¼ softmaxã€‚æ¨å¯¼äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆè®¡ç®—æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆç±»ä¼¼äºæ ‡å‡†çº¿æ€§æ³¨æ„çš„æ–¹æ¡ˆã€‚ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œåº”ç”¨æ‰€æå‡ºçš„ç®—æ³•ä¸éœ€è¦è€—æ—¶çš„å¼ é‡æ•´å½¢ã€‚è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§é€šè¿‡å°†å…¶åº”ç”¨äºç”¨äºè§†é¢‘ç”Ÿæˆçš„å¤§å‹æ‰©æ•£å˜å‹å™¨æ¨¡å‹ï¼ˆæ¶‰åŠå¤§é‡ä»¤ç‰Œçš„åº”ç”¨ç¨‹åºï¼‰æ¥è¯æ˜ã€‚

</details>

---

## 9. Beyond End-to-End Video Models: An LLM-Based Multi-Agent System for Educational Video Generation / è¶…è¶Šç«¯åˆ°ç«¯è§†é¢‘æ¨¡å‹ï¼šç”¨äºæ•™è‚²è§†é¢‘ç”Ÿæˆçš„åŸºäº LLM çš„å¤šä»£ç†ç³»ç»Ÿ

**Date**: 2026-02-12 | **arXiv**: [2602.11790v1](http://arxiv.org/abs/2602.11790v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11790v1)

**Categories**: cs.AI, cs.CL

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Although recent end-to-end video generation models demonstrate impressive performance in visually oriented content creation, they remain limited in scenarios that require strict logical rigor and precise knowledge representation, such as instructional and educational media. To address this problem, we propose LAVES, a hierarchical LLM-based multi-agent system for generating high-quality instructional videos from educational problems. The LAVES formulates educational video generation as a multi-objective task that simultaneously demands correct step-by-step reasoning, pedagogically coherent narration, semantically faithful visual demonstrations, and precise audio--visual alignment. To address the limitations of prior approaches--including low procedural fidelity, high production cost, and limited controllability--LAVES decomposes the generation workflow into specialized agents coordinated by a central Orchestrating Agent with explicit quality gates and iterative critique mechanisms. Specifically, the Orchestrating Agent supervises a Solution Agent for rigorous problem solving, an Illustration Agent that produces executable visualization codes, and a Narration Agent for learner-oriented instructional scripts. In addition, all outputs from the working agents are subject to semantic critique, rule-based constraints, and tool-based compilation checks. Rather than directly synthesizing pixels, the system constructs a structured executable video script that is deterministically compiled into synchronized visuals and narration using template-driven assembly rules, enabling fully automated end-to-end production without manual editing. In large-scale deployments, LAVES achieves a throughput exceeding one million videos per day, delivering over a 95% reduction in cost compared to current industry-standard approaches while maintaining a high acceptance rate.

å°½ç®¡æœ€è¿‘çš„ç«¯åˆ°ç«¯è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨é¢å‘è§†è§‰çš„å†…å®¹åˆ›å»ºæ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åœ¨éœ€è¦ä¸¥æ ¼é€»è¾‘ä¸¥è°¨æ€§å’Œç²¾ç¡®çŸ¥è¯†è¡¨ç¤ºçš„åœºæ™¯ä¸­ä»ç„¶å—åˆ°é™åˆ¶ï¼Œä¾‹å¦‚æ•™å­¦å’Œæ•™è‚²åª’ä½“ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº† LAVESï¼Œä¸€ç§åŸºäº LLM çš„åˆ†å±‚å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œç”¨äºæ ¹æ®æ•™è‚²é—®é¢˜ç”Ÿæˆé«˜è´¨é‡çš„æ•™å­¦è§†é¢‘ã€‚ LAVES å°†æ•™è‚²è§†é¢‘ç”Ÿæˆåˆ¶å®šä¸ºä¸€é¡¹å¤šç›®æ ‡ä»»åŠ¡ï¼ŒåŒæ—¶è¦æ±‚æ­£ç¡®çš„é€æ­¥æ¨ç†ã€æ•™å­¦ä¸Šè¿è´¯çš„å™è¿°ã€è¯­ä¹‰ä¸Šå¿ å®çš„è§†è§‰æ¼”ç¤ºä»¥åŠç²¾ç¡®çš„è§†å¬å¯¹é½ã€‚ä¸ºäº†è§£å†³å…ˆå‰æ–¹æ³•çš„å±€é™æ€§ï¼ˆåŒ…æ‹¬ç¨‹åºä¿çœŸåº¦ä½ã€ç”Ÿäº§æˆæœ¬é«˜å’Œå¯æ§æ€§æœ‰é™ï¼‰ï¼ŒLAVES å°†ç”Ÿæˆå·¥ä½œæµç¨‹åˆ†è§£ä¸ºç”±å…·æœ‰æ˜ç¡®è´¨é‡é—¨å’Œè¿­ä»£æ‰¹è¯„æœºåˆ¶çš„ä¸­å¤®ç¼–æ’ä»£ç†åè°ƒçš„ä¸“é—¨ä»£ç†ã€‚å…·ä½“æ¥è¯´ï¼Œç¼–æ’ä»£ç†ç›‘ç£è§£å†³æ–¹æ¡ˆä»£ç†ä»¥ä¸¥æ ¼è§£å†³é—®é¢˜ï¼Œæ’å›¾ä»£ç†ç”Ÿæˆå¯æ‰§è¡Œçš„å¯è§†åŒ–ä»£ç ï¼Œä»¥åŠå™è¿°ä»£ç†ä»¥ç”¨äºé¢å‘å­¦ä¹ è€…çš„æ•™å­¦è„šæœ¬ã€‚æ­¤å¤–ï¼Œå·¥ä½œä»£ç†çš„æ‰€æœ‰è¾“å‡ºéƒ½å—åˆ°è¯­ä¹‰æ‰¹è¯„ã€åŸºäºè§„åˆ™çš„çº¦æŸå’ŒåŸºäºå·¥å…·çš„ç¼–è¯‘æ£€æŸ¥ã€‚è¯¥ç³»ç»Ÿä¸æ˜¯ç›´æ¥åˆæˆåƒç´ ï¼Œè€Œæ˜¯æ„å»ºä¸€ä¸ªç»“æ„åŒ–çš„å¯æ‰§è¡Œè§†é¢‘è„šæœ¬ï¼Œè¯¥è„šæœ¬ä½¿ç”¨æ¨¡æ¿é©±åŠ¨çš„ç»„è£…è§„åˆ™ç¡®å®šæ€§åœ°ç¼–è¯‘æˆåŒæ­¥çš„è§†è§‰æ•ˆæœå’Œæ—ç™½ï¼Œä»è€Œå®ç°å®Œå…¨è‡ªåŠ¨åŒ–çš„ç«¯åˆ°ç«¯åˆ¶ä½œï¼Œæ— éœ€æ‰‹åŠ¨ç¼–è¾‘ã€‚åœ¨å¤§è§„æ¨¡éƒ¨ç½²ä¸­ï¼ŒLAVES çš„ååé‡æ¯å¤©è¶…è¿‡ 100 ä¸‡ä¸ªè§†é¢‘ï¼Œä¸å½“å‰è¡Œä¸šæ ‡å‡†æ–¹æ³•ç›¸æ¯”ï¼Œæˆæœ¬é™ä½äº† 95% ä»¥ä¸Šï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„æ¥å—ç‡ã€‚

</details>

---

## 10. VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model / VLAWï¼šæ„¿æ™¯-è¯­è¨€-è¡ŒåŠ¨æ”¿ç­–å’Œä¸–ç•Œæ¨¡å‹çš„è¿­ä»£å…±åŒæ”¹è¿›

**Date**: 2026-02-12 | **arXiv**: [2602.12063v1](http://arxiv.org/abs/2602.12063v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.12063v1)

**Categories**: cs.RO

**Project**: https://sites.google.com/view/vla-w  <details><summary><b>Abstract / æ‘˜è¦</b></summary>

The goal of this paper is to improve the performance and reliability of vision-language-action (VLA) models through iterative online interaction. Since collecting policy rollouts in the real world is expensive, we investigate whether a learned simulator-specifically, an action-conditioned video generation model-can be used to generate additional rollout data. Unfortunately, existing world models lack the physical fidelity necessary for policy improvement: they are predominantly trained on demonstration datasets that lack coverage of many different physical interactions (particularly failure cases) and struggle to accurately model small yet critical physical details in contact-rich object manipulation. We propose a simple iterative improvement algorithm that uses real-world roll-out data to improve the fidelity of the world model, which can then, in turn, be used to generate supplemental synthetic data for improving the VLA model. In our experiments on a real robot, we use this approach to improve the performance of a state-of-the-art VLA model on multiple downstream tasks. We achieve a 39.2% absolute success rate improvement over the base policy and 11.6% improvement from training with the generated synthetic rollouts. Videos can be found at this anonymous website: https://sites.google.com/view/vla-w

æœ¬æ–‡çš„ç›®æ ‡æ˜¯é€šè¿‡è¿­ä»£åœ¨çº¿äº¤äº’æ¥æé«˜è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„æ€§èƒ½å’Œå¯é æ€§ã€‚ç”±äºåœ¨ç°å®ä¸–ç•Œä¸­æ”¶é›†ç­–ç•¥æ¨å‡ºçš„æˆæœ¬å¾ˆé«˜ï¼Œå› æ­¤æˆ‘ä»¬ç ”ç©¶äº†æ˜¯å¦å¯ä»¥ä½¿ç”¨å­¦ä¹ çš„æ¨¡æ‹Ÿå™¨ï¼ˆå…·ä½“è€Œè¨€ï¼ŒåŠ¨ä½œæ¡ä»¶è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼‰æ¥ç”Ÿæˆé¢å¤–çš„æ¨å‡ºæ•°æ®ã€‚ä¸å¹¸çš„æ˜¯ï¼Œç°æœ‰çš„ä¸–ç•Œæ¨¡å‹ç¼ºä¹æ”¿ç­–æ”¹è¿›æ‰€éœ€çš„ç‰©ç†ä¿çœŸåº¦ï¼šå®ƒä»¬ä¸»è¦æ˜¯åœ¨æ¼”ç¤ºæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œè¿™äº›æ•°æ®é›†ç¼ºä¹å¯¹è®¸å¤šä¸åŒç‰©ç†äº¤äº’ï¼ˆç‰¹åˆ«æ˜¯å¤±è´¥æ¡ˆä¾‹ï¼‰çš„è¦†ç›–ï¼Œå¹¶ä¸”å¾ˆéš¾åœ¨æ¥è§¦ä¸°å¯Œçš„å¯¹è±¡æ“ä½œä¸­å‡†ç¡®åœ°æ¨¡æ‹Ÿå¾®å°ä½†å…³é”®çš„ç‰©ç†ç»†èŠ‚ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„è¿­ä»£æ”¹è¿›ç®—æ³•ï¼Œè¯¥ç®—æ³•ä½¿ç”¨ç°å®ä¸–ç•Œçš„è½¬å‡ºæ•°æ®æ¥æé«˜ä¸–ç•Œæ¨¡å‹çš„ä¿çœŸåº¦ï¼Œç„¶åå¯ä»¥ä½¿ç”¨è¯¥ç®—æ³•ç”Ÿæˆè¡¥å……åˆæˆæ•°æ®ä»¥æ”¹è¿› VLA æ¨¡å‹ã€‚åœ¨æˆ‘ä»¬å¯¹çœŸå®æœºå™¨äººçš„å®éªŒä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™ç§æ–¹æ³•æ¥æé«˜æœ€å…ˆè¿›çš„ VLA æ¨¡å‹åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ä¸åŸºæœ¬ç­–ç•¥ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ç»å¯¹æˆåŠŸç‡æé«˜äº† 39.2%ï¼Œé€šè¿‡ç”Ÿæˆçš„ç»¼åˆéƒ¨ç½²è¿›è¡Œè®­ç»ƒï¼Œç»å¯¹æˆåŠŸç‡æé«˜äº† 11.6%ã€‚è§†é¢‘å¯ä»¥åœ¨è¿™ä¸ªåŒ¿åç½‘ç«™ä¸Šæ‰¾åˆ°ï¼šhttps://sites.google.com/view/vla-w

</details>

---

## 11. SurfPhase: 3D Interfacial Dynamics in Two-Phase Flows from Sparse Videos / SurfPhaseï¼šç¨€ç–è§†é¢‘ä¸­ä¸¤ç›¸æµçš„ 3D ç•Œé¢åŠ¨åŠ›å­¦

**Date**: 2026-02-11 | **arXiv**: [2602.11154v1](http://arxiv.org/abs/2602.11154v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11154v1)

**Categories**: cs.CV

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Interfacial dynamics in two-phase flows govern momentum, heat, and mass transfer, yet remain difficult to measure experimentally. Classical techniques face intrinsic limitations near moving interfaces, while existing neural rendering methods target single-phase flows with diffuse boundaries and cannot handle sharp, deformable liquid-vapor interfaces. We propose SurfPhase, a novel model for reconstructing 3D interfacial dynamics from sparse camera views. Our approach integrates dynamic Gaussian surfels with a signed distance function formulation for geometric consistency, and leverages a video diffusion model to synthesize novel-view videos to refine reconstruction from sparse observations. We evaluate on a new dataset of high-speed pool boiling videos, demonstrating high-quality view synthesis and velocity estimation from only two camera views. Project website: https://yuegao.me/SurfPhase.

ä¸¤ç›¸æµä¸­çš„ç•Œé¢åŠ¨åŠ›å­¦æ§åˆ¶åŠ¨é‡ã€çƒ­é‡å’Œè´¨é‡ä¼ é€’ï¼Œä½†ä»ç„¶éš¾ä»¥é€šè¿‡å®éªŒæµ‹é‡ã€‚ç»å…¸æŠ€æœ¯åœ¨ç§»åŠ¨ç•Œé¢é™„è¿‘é¢ä¸´å›ºæœ‰çš„å±€é™æ€§ï¼Œè€Œç°æœ‰çš„ç¥ç»æ¸²æŸ“æ–¹æ³•é’ˆå¯¹å…·æœ‰æ‰©æ•£è¾¹ç•Œçš„å•ç›¸æµï¼Œæ— æ³•å¤„ç†å°–é”ã€å¯å˜å½¢çš„æ¶²-æ°”ç•Œé¢ã€‚æˆ‘ä»¬æå‡ºäº† SurfPhaseï¼Œä¸€ç§ä»ç¨€ç–ç›¸æœºè§†å›¾é‡å»º 3D ç•Œé¢åŠ¨åŠ›å­¦çš„æ–°é¢–æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†åŠ¨æ€é«˜æ–¯é¢å…ƒä¸å¸¦ç¬¦å·è·ç¦»å‡½æ•°å…¬å¼ç›¸ç»“åˆä»¥å®ç°å‡ ä½•ä¸€è‡´æ€§ï¼Œå¹¶åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹æ¥åˆæˆæ–°é¢–çš„è§†å›¾è§†é¢‘ï¼Œä»¥æ”¹è¿›ç¨€ç–è§‚æµ‹çš„é‡å»ºã€‚æˆ‘ä»¬å¯¹é«˜é€Ÿæ± æ²¸è…¾è§†é¢‘çš„æ–°æ•°æ®é›†è¿›è¡Œäº†è¯„ä¼°ï¼Œä»…é€šè¿‡ä¸¤ä¸ªæ‘„åƒæœºè§†å›¾æ¼”ç¤ºäº†é«˜è´¨é‡çš„è§†å›¾åˆæˆå’Œé€Ÿåº¦ä¼°è®¡ã€‚é¡¹ç›®ç½‘ç«™ï¼šhttps://yuegao.me/SurfPhaseã€‚

</details>

---

## 12. HairWeaver: Few-Shot Photorealistic Hair Motion Synthesis with Sim-to-Real Guided Video Diffusion / HairWeaverï¼šé€šè¿‡æ¨¡æ‹Ÿåˆ°çœŸå®å¼•å¯¼è§†é¢‘æ‰©æ•£è¿›è¡Œå°‘é•œå¤´çœŸå®æ„Ÿå¤´å‘è¿åŠ¨åˆæˆ

**Date**: 2026-02-11 | **arXiv**: [2602.11117v1](http://arxiv.org/abs/2602.11117v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11117v1)

**Categories**: cs.CV

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

We present HairWeaver, a diffusion-based pipeline that animates a single human image with realistic and expressive hair dynamics. While existing methods successfully control body pose, they lack specific control over hair, and as a result, fail to capture the intricate hair motions, resulting in stiff and unrealistic animations. HairWeaver overcomes this limitation using two specialized modules: a Motion-Context-LoRA to integrate motion conditions and a Sim2Real-Domain-LoRA to preserve the subject's photoreal appearance across different data domains. These lightweight components are designed to guide a video diffusion backbone while maintaining its core generative capabilities. By training on a specialized dataset of dynamic human motion generated from a CG simulator, HairWeaver affords fine control over hair motion and ultimately learns to produce highly realistic hair that responds naturally to movement. Comprehensive evaluations demonstrate that our approach sets a new state of the art, producing lifelike human hair animations with dynamic details.

æˆ‘ä»¬æ¨å‡ºäº† HairWeaverï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„ç®¡é“ï¼Œå¯ä»¥é€šè¿‡é€¼çœŸä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„å¤´å‘åŠ¨æ€æ¥å¯¹å•ä¸ªäººç±»å›¾åƒè¿›è¡ŒåŠ¨ç”»å¤„ç†ã€‚è™½ç„¶ç°æœ‰æ–¹æ³•æˆåŠŸåœ°æ§åˆ¶äº†èº«ä½“å§¿åŠ¿ï¼Œä½†å®ƒä»¬ç¼ºä¹å¯¹å¤´å‘çš„å…·ä½“æ§åˆ¶ï¼Œå› æ­¤æ— æ³•æ•æ‰å¤æ‚çš„å¤´å‘è¿åŠ¨ï¼Œå¯¼è‡´åŠ¨ç”»åƒµç¡¬ä¸”ä¸åˆ‡å®é™…ã€‚ HairWeaver ä½¿ç”¨ä¸¤ä¸ªä¸“ç”¨æ¨¡å—å…‹æœäº†è¿™ä¸€é™åˆ¶ï¼šä¸€ä¸ªç”¨äºé›†æˆè¿åŠ¨æ¡ä»¶çš„ Motion-Context-LoRAï¼Œå¦ä¸€ä¸ªæ˜¯ Sim2Real-Domain-LoRAï¼Œç”¨äºåœ¨ä¸åŒæ•°æ®åŸŸä¸­ä¿ç•™ä¸»ä½“çš„çœŸå®å¤–è§‚ã€‚è¿™äº›è½»é‡çº§ç»„ä»¶æ—¨åœ¨æŒ‡å¯¼è§†é¢‘ä¼ æ’­ä¸»å¹²ï¼ŒåŒæ—¶ä¿æŒå…¶æ ¸å¿ƒç”ŸæˆåŠŸèƒ½ã€‚é€šè¿‡å¯¹ CG æ¨¡æ‹Ÿå™¨ç”Ÿæˆçš„åŠ¨æ€äººä½“è¿åŠ¨çš„ä¸“é—¨æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼ŒHairWeaver å¯ä»¥å¯¹å¤´å‘è¿åŠ¨è¿›è¡Œç²¾ç»†æ§åˆ¶ï¼Œå¹¶æœ€ç»ˆå­¦ä¼šç”Ÿæˆå¯¹è¿åŠ¨åšå‡ºè‡ªç„¶å“åº”çš„é«˜åº¦é€¼çœŸçš„å¤´å‘ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ ‘ç«‹äº†æ–°çš„æŠ€æœ¯æ°´å¹³ï¼Œå¯ä»¥åˆ¶ä½œå…·æœ‰åŠ¨æ€ç»†èŠ‚çš„é€¼çœŸçš„äººå‘åŠ¨ç”»ã€‚

</details>

---

## 13. FastFlow: Accelerating The Generative Flow Matching Models with Bandit Inference / FastFlowï¼šé€šè¿‡å¼ºç›—æ¨ç†åŠ é€Ÿç”ŸæˆæµåŒ¹é…æ¨¡å‹

**Date**: 2026-02-11 | **arXiv**: [2602.11105v1](http://arxiv.org/abs/2602.11105v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11105v1)

**Categories**: cs.CV

**Code**: https://github.com/Div290/FastFlow.

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Flow-matching models deliver state-of-the-art fidelity in image and video generation, but the inherent sequential denoising process renders them slower. Existing acceleration methods like distillation, trajectory truncation, and consistency approaches are static, require retraining, and often fail to generalize across tasks. We propose FastFlow, a plug-and-play adaptive inference framework that accelerates generation in flow matching models. FastFlow identifies denoising steps that produce only minor adjustments to the denoising path and approximates them without using the full neural network models used for velocity predictions. The approximation utilizes finite-difference velocity estimates from prior predictions to efficiently extrapolate future states, enabling faster advancements along the denoising path at zero compute cost. This enables skipping computation at intermediary steps. We model the decision of how many steps to safely skip before requiring a full model computation as a multi-armed bandit problem. The bandit learns the optimal skips to balance speed with performance. FastFlow integrates seamlessly with existing pipelines and generalizes across image generation, video generation, and editing tasks. Experiments demonstrate a speedup of over 2.6x while maintaining high-quality outputs. The source code for this work can be found at https://github.com/Div290/FastFlow.

æµåŒ¹é…æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä¸­æä¾›æœ€å…ˆè¿›çš„ä¿çœŸåº¦ï¼Œä½†å›ºæœ‰çš„é¡ºåºå»å™ªè¿‡ç¨‹ä½¿å®ƒä»¬é€Ÿåº¦å˜æ…¢ã€‚ç°æœ‰çš„åŠ é€Ÿæ–¹æ³•ï¼ˆä¾‹å¦‚è’¸é¦ã€è½¨è¿¹æˆªæ–­å’Œä¸€è‡´æ€§æ–¹æ³•ï¼‰æ˜¯é™æ€çš„ï¼Œéœ€è¦é‡æ–°è®­ç»ƒï¼Œå¹¶ä¸”é€šå¸¸æ— æ³•è·¨ä»»åŠ¡æ³›åŒ–ã€‚æˆ‘ä»¬æå‡ºäº† FastFlowï¼Œä¸€ç§å³æ’å³ç”¨çš„è‡ªé€‚åº”æ¨ç†æ¡†æ¶ï¼Œå¯åŠ é€ŸæµåŒ¹é…æ¨¡å‹çš„ç”Ÿæˆã€‚ FastFlow è¯†åˆ«ä»…å¯¹å»å™ªè·¯å¾„äº§ç”Ÿå¾®å°è°ƒæ•´çš„å»å™ªæ­¥éª¤ï¼Œå¹¶åœ¨ä¸ä½¿ç”¨ç”¨äºé€Ÿåº¦é¢„æµ‹çš„å®Œæ•´ç¥ç»ç½‘ç»œæ¨¡å‹çš„æƒ…å†µä¸‹å¯¹å…¶è¿›è¡Œè¿‘ä¼¼ã€‚è¯¥è¿‘ä¼¼åˆ©ç”¨å…ˆå‰é¢„æµ‹çš„æœ‰é™å·®åˆ†é€Ÿåº¦ä¼°è®¡æ¥æœ‰æ•ˆåœ°æ¨æ–­æœªæ¥çŠ¶æ€ï¼Œä»è€Œä»¥é›¶è®¡ç®—æˆæœ¬æ²¿ç€å»å™ªè·¯å¾„å®ç°æ›´å¿«çš„è¿›å±•ã€‚è¿™ä½¿å¾—èƒ½å¤Ÿè·³è¿‡ä¸­é—´æ­¥éª¤çš„è®¡ç®—ã€‚æˆ‘ä»¬å°†åœ¨éœ€è¦å®Œæ•´æ¨¡å‹è®¡ç®—ä¹‹å‰å®‰å…¨è·³è¿‡å¤šå°‘æ­¥éª¤çš„å†³ç­–å»ºæ¨¡ä¸ºå¤šè‡‚è€è™æœºé—®é¢˜ã€‚è€è™æœºå­¦ä¹ æœ€ä½³è·³è·ƒä»¥å¹³è¡¡é€Ÿåº¦ä¸æ€§èƒ½ã€‚ FastFlow ä¸ç°æœ‰ç®¡é“æ— ç¼é›†æˆï¼Œå¹¶å¯æ³›åŒ–å›¾åƒç”Ÿæˆã€è§†é¢‘ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ä¿æŒé«˜è´¨é‡è¾“å‡ºçš„åŒæ—¶ï¼Œé€Ÿåº¦æé«˜äº† 2.6 å€ä»¥ä¸Šã€‚è¿™é¡¹å·¥ä½œçš„æºä»£ç å¯ä»¥åœ¨ https://github.com/Div290/FastFlow æ‰¾åˆ°ã€‚

</details>

---

## 14. ReTracing: An Archaeological Approach Through Body, Machine, and Generative Systems / è¿½æº¯ï¼šé€šè¿‡èº«ä½“ã€æœºå™¨å’Œç”Ÿæˆç³»ç»Ÿçš„è€ƒå¤æ–¹æ³•

**Date**: 2026-02-11 | **arXiv**: [2602.11242v1](http://arxiv.org/abs/2602.11242v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11242v1)

**Categories**: cs.CV

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

We present ReTracing, a multi-agent embodied performance art that adopts an archaeological approach to examine how artificial intelligence shapes, constrains, and produces bodily movement. Drawing from science-fiction novels, the project extracts sentences that describe human-machine interaction. We use large language models (LLMs) to generate paired prompts "what to do" and "what not to do" for each excerpt. A diffusion-based text-to-video model transforms these prompts into choreographic guides for a human performer and motor commands for a quadruped robot. Both agents enact the actions on a mirrored floor, captured by multi-camera motion tracking and reconstructed into 3D point clouds and motion trails, forming a digital archive of motion traces. Through this process, ReTracing serves as a novel approach to reveal how generative systems encode socio-cultural biases through choreographed movements. Through an immersive interplay of AI, human, and robot, ReTracing confronts a critical question of our time: What does it mean to be human among AIs that also move, think, and leave traces behind?

æˆ‘ä»¬å±•ç¤ºäº† ReTracingï¼Œè¿™æ˜¯ä¸€ç§å¤šæ™ºèƒ½ä½“ä½“ç°çš„è¡¨æ¼”è‰ºæœ¯ï¼Œå®ƒé‡‡ç”¨è€ƒå¤å­¦çš„æ–¹æ³•æ¥ç ”ç©¶äººå·¥æ™ºèƒ½å¦‚ä½•å¡‘é€ ã€çº¦æŸå’Œäº§ç”Ÿèº«ä½“è¿åŠ¨ã€‚è¯¥é¡¹ç›®å€Ÿé‰´ç§‘å¹»å°è¯´ï¼Œæå–æè¿°äººæœºäº¤äº’çš„å¥å­ã€‚æˆ‘ä»¬ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) ä¸ºæ¯ä¸ªæ‘˜å½•ç”Ÿæˆé…å¯¹æç¤ºâ€œè¯¥åšä»€ä¹ˆâ€å’Œâ€œä¸è¯¥åšä»€ä¹ˆâ€ã€‚åŸºäºæ‰©æ•£çš„æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹å°†è¿™äº›æç¤ºè½¬æ¢ä¸ºäººç±»è¡¨æ¼”è€…çš„ç¼–èˆæŒ‡å—å’Œå››è¶³æœºå™¨äººçš„è¿åŠ¨å‘½ä»¤ã€‚ä¸¤ä¸ªä»£ç†éƒ½åœ¨é•œåƒåœ°æ¿ä¸Šæ‰§è¡ŒåŠ¨ä½œï¼Œé€šè¿‡å¤šæ‘„åƒå¤´è¿åŠ¨è·Ÿè¸ªæ•è·å¹¶é‡å»ºä¸º 3D ç‚¹äº‘å’Œè¿åŠ¨è½¨è¿¹ï¼Œå½¢æˆè¿åŠ¨è½¨è¿¹çš„æ•°å­—æ¡£æ¡ˆã€‚é€šè¿‡è¿™ä¸ªè¿‡ç¨‹ï¼ŒReTracing ä½œä¸ºä¸€ç§æ–°é¢–çš„æ–¹æ³•æ¥æ­ç¤ºç”Ÿæˆç³»ç»Ÿå¦‚ä½•é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„åŠ¨ä½œæ¥ç¼–ç ç¤¾ä¼šæ–‡åŒ–åè§ã€‚é€šè¿‡äººå·¥æ™ºèƒ½ã€äººç±»å’Œæœºå™¨äººçš„æ²‰æµ¸å¼äº’åŠ¨ï¼Œã€ŠReTracingã€‹é¢ä¸´ç€æˆ‘ä»¬è¿™ä¸ªæ—¶ä»£çš„ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šåœ¨åŒæ ·ä¼šç§»åŠ¨ã€æ€è€ƒå’Œç•™ä¸‹ç—•è¿¹çš„äººå·¥æ™ºèƒ½ä¸­ï¼Œä½œä¸ºäººç±»æ„å‘³ç€ä»€ä¹ˆï¼Ÿ

</details>

---

## 15. Flow caching for autoregressive video generation / ç”¨äºè‡ªå›å½’è§†é¢‘ç”Ÿæˆçš„æµç¼“å­˜

**Date**: 2026-02-11 | **arXiv**: [2602.10825v1](http://arxiv.org/abs/2602.10825v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10825v1)

**Categories**: cs.CV, cs.AI

**Code**: https://github.com/mikeallen39/FlowCache.

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Autoregressive models, often built on Transformer architectures, represent a powerful paradigm for generating ultra-long videos by synthesizing content in sequential chunks. However, this sequential generation process is notoriously slow. While caching strategies have proven effective for accelerating traditional video diffusion models, existing methods assume uniform denoising across all frames-an assumption that breaks down in autoregressive models where different video chunks exhibit varying similarity patterns at identical timesteps. In this paper, we present FlowCache, the first caching framework specifically designed for autoregressive video generation. Our key insight is that each video chunk should maintain independent caching policies, allowing fine-grained control over which chunks require recomputation at each timestep. We introduce a chunkwise caching strategy that dynamically adapts to the unique denoising characteristics of each chunk, complemented by a joint importance-redundancy optimized KV cache compression mechanism that maintains fixed memory bounds while preserving generation quality. Our method achieves remarkable speedups of 2.38 times on MAGI-1 and 6.7 times on SkyReels-V2, with negligible quality degradation (VBench: 0.87 increase and 0.79 decrease respectively). These results demonstrate that FlowCache successfully unlocks the potential of autoregressive models for real-time, ultra-long video generation-establishing a new benchmark for efficient video synthesis at scale. The code is available at https://github.com/mikeallen39/FlowCache.

è‡ªå›å½’æ¨¡å‹é€šå¸¸å»ºç«‹åœ¨ Transformer æ¶æ„ä¹‹ä¸Šï¼Œä»£è¡¨äº†é€šè¿‡åˆæˆè¿ç»­å—ä¸­çš„å†…å®¹æ¥ç”Ÿæˆè¶…é•¿è§†é¢‘çš„å¼ºå¤§èŒƒä¾‹ã€‚ç„¶è€Œï¼Œè¿™ç§é¡ºåºç”Ÿæˆè¿‡ç¨‹æ˜¯å‡ºäº†åçš„æ…¢ã€‚è™½ç„¶ç¼“å­˜ç­–ç•¥å·²è¢«è¯æ˜å¯¹äºåŠ é€Ÿä¼ ç»Ÿè§†é¢‘æ‰©æ•£æ¨¡å‹æ˜¯æœ‰æ•ˆçš„ï¼Œä½†ç°æœ‰æ–¹æ³•å‡è®¾æ‰€æœ‰å¸§éƒ½é‡‡ç”¨ç»Ÿä¸€çš„å»å™ªâ€”â€”è¿™ç§å‡è®¾åœ¨è‡ªå›å½’æ¨¡å‹ä¸­è¢«æ‰“ç ´ï¼Œå…¶ä¸­ä¸åŒçš„è§†é¢‘å—åœ¨ç›¸åŒçš„æ—¶é—´æ­¥é•¿è¡¨ç°å‡ºä¸åŒçš„ç›¸ä¼¼æ€§æ¨¡å¼ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† FlowCacheï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“ä¸ºè‡ªå›å½’è§†é¢‘ç”Ÿæˆè€Œè®¾è®¡çš„ç¼“å­˜æ¡†æ¶ã€‚æˆ‘ä»¬çš„ä¸»è¦è§è§£æ˜¯æ¯ä¸ªè§†é¢‘å—åº”è¯¥ç»´æŠ¤ç‹¬ç«‹çš„ç¼“å­˜ç­–ç•¥ï¼Œä»è€Œå¯ä»¥å¯¹æ¯ä¸ªæ—¶é—´æ­¥éœ€è¦é‡æ–°è®¡ç®—çš„å—è¿›è¡Œç»†ç²’åº¦æ§åˆ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ†å—ç¼“å­˜ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åŠ¨æ€é€‚åº”æ¯ä¸ªå—çš„ç‹¬ç‰¹å»å™ªç‰¹æ€§ï¼Œå¹¶è¾…ä»¥è”åˆé‡è¦æ€§å†—ä½™ä¼˜åŒ–çš„ KV ç¼“å­˜å‹ç¼©æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åœ¨ä¿æŒå›ºå®šå†…å­˜è¾¹ç•Œçš„åŒæ—¶ä¿æŒç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ MAGI-1 ä¸Šå®ç°äº† 2.38 å€çš„æ˜¾ç€åŠ é€Ÿï¼Œåœ¨ SkyReels-V2 ä¸Šå®ç°äº† 6.7 å€çš„æ˜¾ç€åŠ é€Ÿï¼Œè€Œè´¨é‡ä¸‹é™å¯ä»¥å¿½ç•¥ä¸è®¡ï¼ˆVBenchï¼šåˆ†åˆ«å¢åŠ  0.87 å€å’Œå‡å°‘ 0.79 å€ï¼‰ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒFlowCache æˆåŠŸé‡Šæ”¾äº†è‡ªå›å½’æ¨¡å‹åœ¨å®æ—¶ã€è¶…é•¿è§†é¢‘ç”Ÿæˆæ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºå¤§è§„æ¨¡é«˜æ•ˆè§†é¢‘åˆæˆå»ºç«‹äº†æ–°åŸºå‡†ã€‚è¯¥ä»£ç å¯ä» https://github.com/mikeallen39/FlowCache è·å–ã€‚

</details>

---

## 16. H-WM: Robotic Task and Motion Planning Guided by Hierarchical World Model / H-WMï¼šåˆ†å±‚ä¸–ç•Œæ¨¡å‹å¼•å¯¼çš„æœºå™¨äººä»»åŠ¡å’Œè¿åŠ¨è§„åˆ’

**Date**: 2026-02-11 | **arXiv**: [2602.11291v1](http://arxiv.org/abs/2602.11291v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11291v1)

**Categories**: cs.RO

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

World models are becoming central to robotic planning and control, as they enable prediction of future state transitions. Existing approaches often emphasize video generation or natural language prediction, which are difficult to directly ground in robot actions and suffer from compounding errors over long horizons. Traditional task and motion planning relies on symbolic logic world models, such as planning domains, that are robot-executable and robust for long-horizon reasoning. However, these methods typically operate independently of visual perception, preventing synchronized symbolic and perceptual state prediction. We propose a Hierarchical World Model (H-WM) that jointly predicts logical and visual state transitions within a unified bilevel framework. H-WM combines a high-level logical world model with a low-level visual world model, integrating the robot-executable, long-horizon robustness of symbolic reasoning with perceptual grounding from visual observations. The hierarchical outputs provide stable and consistent intermediate guidance for long-horizon tasks, mitigating error accumulation and enabling robust execution across extended task sequences. To train H-WM, we introduce a robotic dataset that aligns robot motion with symbolic states, actions, and visual observations. Experiments across vision-language-action (VLA) control policies demonstrate the effectiveness and generality of the approach.

ä¸–ç•Œæ¨¡å‹æ­£åœ¨æˆä¸ºæœºå™¨äººè§„åˆ’å’Œæ§åˆ¶çš„æ ¸å¿ƒï¼Œå› ä¸ºå®ƒä»¬èƒ½å¤Ÿé¢„æµ‹æœªæ¥çš„çŠ¶æ€è½¬æ¢ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸å¼ºè°ƒè§†é¢‘ç”Ÿæˆæˆ–è‡ªç„¶è¯­è¨€é¢„æµ‹ï¼Œè¿™äº›æ–¹æ³•å¾ˆéš¾ç›´æ¥åæ˜ æœºå™¨äººçš„åŠ¨ä½œï¼Œå¹¶ä¸”åœ¨é•¿æœŸèŒƒå›´å†…ä¼šå‡ºç°å¤åˆé”™è¯¯ã€‚ä¼ ç»Ÿçš„ä»»åŠ¡å’Œè¿åŠ¨è§„åˆ’ä¾èµ–äºç¬¦å·é€»è¾‘ä¸–ç•Œæ¨¡å‹ï¼Œä¾‹å¦‚è§„åˆ’åŸŸï¼Œè¿™äº›æ¨¡å‹æ˜¯æœºå™¨äººå¯æ‰§è¡Œçš„å¹¶ä¸”å¯¹äºé•¿è§†é‡æ¨ç†æ¥è¯´æ˜¯é²æ£’çš„ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ç‹¬ç«‹äºè§†è§‰æ„ŸçŸ¥è¿›è¡Œæ“ä½œï¼Œä»è€Œé˜»æ­¢äº†åŒæ­¥çš„ç¬¦å·å’Œæ„ŸçŸ¥çŠ¶æ€é¢„æµ‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†å±‚ä¸–ç•Œæ¨¡å‹ï¼ˆH-WMï¼‰ï¼Œå®ƒåœ¨ç»Ÿä¸€çš„åŒå±‚æ¡†æ¶å†…è”åˆé¢„æµ‹é€»è¾‘å’Œè§†è§‰çŠ¶æ€è½¬æ¢ã€‚ H-WM å°†é«˜çº§é€»è¾‘ä¸–ç•Œæ¨¡å‹ä¸ä½çº§è§†è§‰ä¸–ç•Œæ¨¡å‹ç›¸ç»“åˆï¼Œå°†æœºå™¨äººå¯æ‰§è¡Œçš„ç¬¦å·æ¨ç†çš„é•¿æœŸé²æ£’æ€§ä¸è§†è§‰è§‚å¯Ÿçš„æ„ŸçŸ¥åŸºç¡€ç›¸ç»“åˆã€‚åˆ†å±‚è¾“å‡ºä¸ºé•¿æœŸä»»åŠ¡æä¾›ç¨³å®šä¸€è‡´çš„ä¸­é—´æŒ‡å¯¼ï¼Œå‡å°‘é”™è¯¯ç´¯ç§¯å¹¶å®ç°è·¨æ‰©å±•ä»»åŠ¡åºåˆ—çš„ç¨³å¥æ‰§è¡Œã€‚ä¸ºäº†è®­ç»ƒ H-WMï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæœºå™¨äººæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†å°†æœºå™¨äººè¿åŠ¨ä¸ç¬¦å·çŠ¶æ€ã€åŠ¨ä½œå’Œè§†è§‰è§‚å¯Ÿå¯¹é½ã€‚è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ§åˆ¶ç­–ç•¥çš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚

</details>

---



</details>

<!-- PAPERS_CONTENT_END -->

## ğŸš€ å¿«é€Ÿå¼€å§‹

1. **é…ç½® API Key**ï¼šåœ¨ä»“åº“ `Settings -> Secrets and variables -> Actions -> Secrets` ä¸­æ·»åŠ  `DEEPSEEK_API_KEY`ã€‚
2. **æ‰‹åŠ¨è¿è¡Œ**ï¼šåœ¨ `Actions` æ ‡ç­¾é¡µé€‰æ‹© `Daily Video Papers Update` å¹¶ç‚¹å‡» `Run workflow`ã€‚
3. **åˆ‡æ¢ç‰ˆæœ¬**ï¼šåœ¨ `Variables` ä¸­è®¾ç½® `VERSION` ä¸º `v2` å³å¯å¼€å¯ AI æ·±åº¦åˆ†ææ¨¡å¼ã€‚

## ğŸ› ï¸ æŠ€æœ¯ç»†èŠ‚

- **æ•°æ®æº**ï¼šarXiv API (cs.CV, cs.AI, cs.MM, cs.RO, cs.LG)
- **ç¿»è¯‘/åˆ†æ**ï¼šDeepSeek API (ä¼˜å…ˆ) / Gemini (å¤‡ç”¨)
- **è‡ªåŠ¨åŒ–**ï¼šGitHub Actions

---
*æœ¬é¡¹ç›®ç”± Manus è‡ªåŠ¨ç”Ÿæˆå¹¶ç»´æŠ¤ã€‚*
