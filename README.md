# Daily Video Papers ğŸ¥

æ¯æ—¥è‡ªåŠ¨æ›´æ–° arXiv è§†é¢‘ç›¸å…³ç ”ç©¶è®ºæ–‡ã€‚

## ğŸŒŸ åŠŸèƒ½ç‰¹æ€§

- **å¹¿æ³›ç­›é€‰**ï¼šæ¶µç›–è§†é¢‘ç”Ÿæˆã€ç¼–è¾‘ã€ç†è§£ã€åˆ†å‰²ã€è·Ÿè¸ªã€å¢å¼ºç­‰å…¨æ–¹ä½è§†é¢‘ç ”ç©¶ã€‚
- **åŒè¯­æ”¯æŒ**ï¼šæ‰€æœ‰è®ºæ–‡æ ‡é¢˜ and æ‘˜è¦å‡é…å¤‡ä¸­æ–‡ç¿»è¯‘ã€‚
- **AI æ·±åº¦åˆ†æ**ï¼šæ”¯æŒ V2 ç‰ˆæœ¬ï¼Œåˆ©ç”¨ AI è¿›è¡Œå…¨æ–‡é˜…è¯»å’Œæ‰¹åˆ¤æ€§åˆ†æã€‚
- **è‡ªåŠ¨å»é‡**ï¼šæ™ºèƒ½å¯¹æ¯”æœ€è¿‘ 5 å¤©å†…å®¹ï¼Œç¡®ä¿ä¸é‡å¤æ›´æ–°ã€‚
- **ä¸€é”®è®¿é—®**ï¼šè‡ªåŠ¨æå–é¡¹ç›®ä¸»é¡µå’Œä»£ç ä»“åº“é“¾æ¥ã€‚

## ğŸ“š è®ºæ–‡ç´¢å¼•

<!-- PAPERS_INDEX_START -->
- [2026-02-12](papers/2026-02-12.md) - 288 papers
- [2026-02-11](papers/2026-02-11.md) - 355 papers
<!-- PAPERS_INDEX_END -->

## Daily Papers

<!-- PAPERS_CONTENT_START -->
<details><summary><b>2026-02-12 (288 papers)</b></summary>

# arXiv Video Papers - 2026-02-12

**Paper Count**: 288

---

## 1. Interpretable Vision Transformers in Monocular Depth Estimation via SVDA

**ä¸­æ–‡æ ‡é¢˜**: é€šè¿‡ SVDA è¿›è¡Œå•çœ¼æ·±åº¦ä¼°è®¡çš„å¯è§£é‡Šè§†è§‰å˜æ¢å™¨

**Date**: 2026-02-11 | **arXiv**: [2602.11005v1](http://arxiv.org/abs/2602.11005v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11005v1)

<details><summary><b>Abstract</b></summary>

Monocular depth estimation is a central problem in computer vision with applications in robotics, AR, and autonomous driving, yet the self-attention mechanisms that drive modern Transformer architectures remain opaque. We introduce SVD-Inspired Attention (SVDA) into the Dense Prediction Transformer (DPT), providing the first spectrally structured formulation of attention for dense prediction tasks. SVDA decouples directional alignment from spectral modulation by embedding a learnable diagonal matrix into normalized query-key interactions, enabling attention maps that are intrinsically interpretable rather than post-hoc approximations. Experiments on KITTI and NYU-v2 show that SVDA preserves or slightly improves predictive accuracy while adding only minor computational overhead. More importantly, SVDA unlocks six spectral indicators that quantify entropy, rank, sparsity, alignment, selectivity, and robustness. These reveal consistent cross-dataset and depth-wise patterns in how attention organizes during training, insights that remain inaccessible in standard Transformers. By shifting the role of attention from opaque mechanism to quantifiable descriptor, SVDA redefines interpretability in monocular depth estimation and opens a principled avenue toward transparent dense prediction models.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å•ç›®æ·±åº¦ä¼°è®¡æ˜¯è®¡ç®—æœºè§†è§‰åœ¨æœºå™¨äººã€AR å’Œè‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åº”ç”¨çš„æ ¸å¿ƒé—®é¢˜ï¼Œä½†é©±åŠ¨ç°ä»£ Transformer æ¶æ„çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ä»ç„¶ä¸é€æ˜ã€‚æˆ‘ä»¬å°† SVD-Inspired Attention (SVDA) å¼•å…¥åˆ°å¯†é›†é¢„æµ‹å˜æ¢å™¨ (DPT) ä¸­ï¼Œä¸ºå¯†é›†é¢„æµ‹ä»»åŠ¡æä¾›äº†ç¬¬ä¸€ä¸ªé¢‘è°±ç»“æ„çš„æ³¨æ„åŠ›å…¬å¼ã€‚ SVDA é€šè¿‡å°†å¯å­¦ä¹ çš„å¯¹è§’çŸ©é˜µåµŒå…¥åˆ°æ ‡å‡†åŒ–çš„æŸ¥è¯¢é”®äº¤äº’ä¸­ï¼Œå°†æ–¹å‘å¯¹é½ä¸é¢‘è°±è°ƒåˆ¶è§£è€¦ï¼Œä»è€Œå®ç°æœ¬è´¨ä¸Šå¯è§£é‡Šçš„æ³¨æ„åŠ›å›¾ï¼Œè€Œä¸æ˜¯äº‹åè¿‘ä¼¼ã€‚ KITTI å’Œ NYU-v2 ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSVDA ä¿ç•™æˆ–ç•¥å¾®æé«˜äº†é¢„æµ‹å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä»…å¢åŠ äº†å°‘é‡è®¡ç®—å¼€é”€ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒSVDA è§£é”äº†å…­ä¸ªå…‰è°±æŒ‡æ ‡ï¼Œå¯é‡åŒ–ç†µã€ç§©ã€ç¨€ç–æ€§ã€å¯¹é½ã€é€‰æ‹©æ€§å’Œé²æ£’æ€§ã€‚è¿™äº›æ­ç¤ºäº†è®­ç»ƒè¿‡ç¨‹ä¸­æ³¨æ„åŠ›å¦‚ä½•ç»„ç»‡çš„ä¸€è‡´çš„è·¨æ•°æ®é›†å’Œæ·±åº¦æ¨¡å¼ï¼Œè¿™äº›è§è§£åœ¨æ ‡å‡† Transformer ä¸­ä»ç„¶æ— æ³•è·å¾—ã€‚é€šè¿‡å°†æ³¨æ„åŠ›çš„ä½œç”¨ä»ä¸é€æ˜æœºåˆ¶è½¬ç§»åˆ°å¯é‡åŒ–æè¿°ç¬¦ï¼ŒSVDA é‡æ–°å®šä¹‰äº†å•ç›®æ·±åº¦ä¼°è®¡çš„å¯è§£é‡Šæ€§ï¼Œå¹¶ä¸ºé€æ˜å¯†é›†é¢„æµ‹æ¨¡å‹å¼€è¾Ÿäº†ä¸€æ¡åŸåˆ™æ€§é€”å¾„ã€‚

</details>

---

## 2. Interpretable Vision Transformers in Image Classification via SVDA

**ä¸­æ–‡æ ‡é¢˜**: é€šè¿‡ SVDA è¿›è¡Œå›¾åƒåˆ†ç±»ä¸­çš„å¯è§£é‡Šè§†è§‰è½¬æ¢å™¨

**Date**: 2026-02-11 | **arXiv**: [2602.10994v1](http://arxiv.org/abs/2602.10994v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10994v1)

<details><summary><b>Abstract</b></summary>

Vision Transformers (ViTs) have achieved state-of-the-art performance in image classification, yet their attention mechanisms often remain opaque and exhibit dense, non-structured behaviors. In this work, we adapt our previously proposed SVD-Inspired Attention (SVDA) mechanism to the ViT architecture, introducing a geometrically grounded formulation that enhances interpretability, sparsity, and spectral structure. We apply the use of interpretability indicators -- originally proposed with SVDA -- to monitor attention dynamics during training and assess structural properties of the learned representations. Experimental evaluations on four widely used benchmarks -- CIFAR-10, FashionMNIST, CIFAR-100, and ImageNet-100 -- demonstrate that SVDA consistently yields more interpretable attention patterns without sacrificing classification accuracy. While the current framework offers descriptive insights rather than prescriptive guidance, our results establish SVDA as a comprehensive and informative tool for analyzing and developing structured attention models in computer vision. This work lays the foundation for future advances in explainable AI, spectral diagnostics, and attention-based model compression.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è§†è§‰å˜æ¢å™¨ (ViT) åœ¨å›¾åƒåˆ†ç±»æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬çš„æ³¨æ„åŠ›æœºåˆ¶é€šå¸¸ä»ç„¶ä¸é€æ˜ï¼Œå¹¶è¡¨ç°å‡ºå¯†é›†çš„éç»“æ„åŒ–è¡Œä¸ºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†ä¹‹å‰æå‡ºçš„ SVD-Inspired Attention (SVDA) æœºåˆ¶åº”ç”¨äº ViT æ¶æ„ï¼Œå¼•å…¥äº†ä¸€ç§åŸºäºå‡ ä½•çš„å…¬å¼ï¼Œå¢å¼ºäº†å¯è§£é‡Šæ€§ã€ç¨€ç–æ€§å’Œè°±ç»“æ„ã€‚æˆ‘ä»¬åº”ç”¨å¯è§£é‡Šæ€§æŒ‡æ ‡ï¼ˆæœ€åˆç”± SVDA æå‡ºï¼‰æ¥ç›‘æ§è®­ç»ƒæœŸé—´çš„æ³¨æ„åŠ›åŠ¨æ€å¹¶è¯„ä¼°æ‰€å­¦ä¹ è¡¨å¾çš„ç»“æ„å±æ€§ã€‚å¯¹å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†ï¼ˆCIFAR-10ã€FashionMNISTã€CIFAR-100 å’Œ ImageNet-100ï¼‰çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒSVDA åœ¨ä¸ç‰ºç‰²åˆ†ç±»å‡†ç¡®æ€§çš„æƒ…å†µä¸‹å§‹ç»ˆèƒ½äº§ç”Ÿæ›´å¯è§£é‡Šçš„æ³¨æ„åŠ›æ¨¡å¼ã€‚è™½ç„¶å½“å‰çš„æ¡†æ¶æä¾›äº†æè¿°æ€§è§è§£è€Œä¸æ˜¯è§„å®šæ€§æŒ‡å¯¼ï¼Œä½†æˆ‘ä»¬çš„ç»“æœå°† SVDA ç¡®ç«‹ä¸ºä¸€ç§ç”¨äºåˆ†æå’Œå¼€å‘è®¡ç®—æœºè§†è§‰ä¸­ç»“æ„åŒ–æ³¨æ„åŠ›æ¨¡å‹çš„å…¨é¢ä¸”ä¿¡æ¯ä¸°å¯Œçš„å·¥å…·ã€‚è¿™é¡¹å·¥ä½œä¸ºå¯è§£é‡Šçš„äººå·¥æ™ºèƒ½ã€å…‰è°±è¯Šæ–­å’ŒåŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹å‹ç¼©çš„æœªæ¥å‘å±•å¥ å®šäº†åŸºç¡€ã€‚

</details>

---

## 3. DFIC: Towards a balanced facial image dataset for automatic ICAO compliance verification

**ä¸­æ–‡æ ‡é¢˜**: DFICï¼šå»ºç«‹å¹³è¡¡çš„é¢éƒ¨å›¾åƒæ•°æ®é›†ï¼Œç”¨äºè‡ªåŠ¨ ICAO åˆè§„æ€§éªŒè¯

**Date**: 2026-02-11 | **arXiv**: [2602.10985v1](http://arxiv.org/abs/2602.10985v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10985v1)

**Code**: https://github.com/visteam-isr-uc/DFIC)

<details><summary><b>Abstract</b></summary>

Ensuring compliance with ISO/IEC and ICAO standards for facial images in machine-readable travel documents (MRTDs) is essential for reliable identity verification, but current manual inspection methods are inefficient in high-demand environments. This paper introduces the DFIC dataset, a novel comprehensive facial image dataset comprising around 58,000 annotated images and 2706 videos of more than 1000 subjects, that cover a broad range of non-compliant conditions, in addition to compliant portraits. Our dataset provides a more balanced demographic distribution than the existing public datasets, with one partition that is nearly uniformly distributed, facilitating the development of automated ICAO compliance verification methods.   Using DFIC, we fine-tuned a novel method that heavily relies on spatial attention mechanisms for the automatic validation of ICAO compliance requirements, and we have compared it with the state-of-the-art aimed at ICAO compliance verification, demonstrating improved results. DFIC dataset is now made public (https://github.com/visteam-isr-uc/DFIC) for the training and validation of new models, offering an unprecedented diversity of faces, that will improve both robustness and adaptability to the intrinsically diverse combinations of faces and props that can be presented to the validation system. These results emphasize the potential of DFIC to enhance automated ICAO compliance methods but it can also be used in many other applications that aim to improve the security, privacy, and fairness of facial recognition systems.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç¡®ä¿æœºè¯»æ—…è¡Œè¯ä»¶ (MRTD) ä¸­çš„é¢éƒ¨å›¾åƒç¬¦åˆ ISO/IEC å’Œ ICAO æ ‡å‡†å¯¹äºå¯é çš„èº«ä»½éªŒè¯è‡³å…³é‡è¦ï¼Œä½†å½“å‰çš„æ‰‹åŠ¨æ£€æŸ¥æ–¹æ³•åœ¨é«˜è¦æ±‚ç¯å¢ƒä¸­æ•ˆç‡ä½ä¸‹ã€‚æœ¬æ–‡ä»‹ç»äº† DFIC æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„ç»¼åˆé¢éƒ¨å›¾åƒæ•°æ®é›†ï¼ŒåŒ…å« 1000 å¤šä¸ªä¸»é¢˜çš„çº¦ 58,000 å¼ å¸¦æ³¨é‡Šå›¾åƒå’Œ 2706 ä¸ªè§†é¢‘ï¼Œé™¤äº†åˆè§„è‚–åƒä¹‹å¤–ï¼Œè¿˜æ¶µç›–äº†å¹¿æ³›çš„ä¸åˆè§„æ¡ä»¶ã€‚æˆ‘ä»¬çš„æ•°æ®é›†æä¾›äº†æ¯”ç°æœ‰å…¬å…±æ•°æ®é›†æ›´å¹³è¡¡çš„äººå£åˆ†å¸ƒï¼Œå…¶ä¸­ä¸€ä¸ªåˆ†åŒºå‡ ä¹å‡åŒ€åˆ†å¸ƒï¼Œæœ‰åˆ©äºè‡ªåŠ¨åŒ–å›½é™…æ°‘èˆªç»„ç»‡åˆè§„æ€§éªŒè¯æ–¹æ³•çš„å¼€å‘ã€‚   ä½¿ç”¨ DFICï¼Œæˆ‘ä»¬å¾®è°ƒäº†ä¸€ç§ä¸¥é‡ä¾èµ–ç©ºé—´æ³¨æ„æœºåˆ¶æ¥è‡ªåŠ¨éªŒè¯ ICAO åˆè§„æ€§è¦æ±‚çš„æ–°é¢–æ–¹æ³•ï¼Œå¹¶å°†å…¶ä¸é’ˆå¯¹ ICAO åˆè§„æ€§éªŒè¯çš„æœ€å…ˆè¿›æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼Œå±•ç¤ºäº†æ”¹è¿›çš„ç»“æœã€‚ DFIC æ•°æ®é›†ç°å·²å…¬å¼€ (https://github.com/visteam-isr-uc/DFIC)ï¼Œç”¨äºæ–°æ¨¡å‹çš„è®­ç»ƒå’ŒéªŒè¯ï¼Œæä¾›å‰æ‰€æœªæœ‰çš„äººè„¸å¤šæ ·æ€§ï¼Œè¿™å°†æé«˜å¯å‘ˆç°ç»™éªŒè¯ç³»ç»Ÿçš„äººè„¸å’Œé“å…·çš„æœ¬è´¨å¤šæ ·åŒ–ç»„åˆçš„é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº† DFIC åœ¨å¢å¼ºè‡ªåŠ¨åŒ– ICAO åˆè§„æ–¹æ³•æ–¹é¢çš„æ½œåŠ›ï¼Œä½†å®ƒä¹Ÿå¯ç”¨äºè®¸å¤šå…¶ä»–æ—¨åœ¨æé«˜é¢éƒ¨è¯†åˆ«ç³»ç»Ÿçš„å®‰å…¨æ€§ã€éšç§æ€§å’Œå…¬å¹³æ€§çš„åº”ç”¨ã€‚

</details>

---

## 4. Towards Learning a Generalizable 3D Scene Representation from 2D Observations

**ä¸­æ–‡æ ‡é¢˜**: ä» 2D è§‚å¯Ÿä¸­å­¦ä¹ å¯æ¦‚æ‹¬çš„ 3D åœºæ™¯è¡¨ç¤º

**Date**: 2026-02-11 | **arXiv**: [2602.10943v1](http://arxiv.org/abs/2602.10943v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10943v1)

<details><summary><b>Abstract</b></summary>

We introduce a Generalizable Neural Radiance Field approach for predicting 3D workspace occupancy from egocentric robot observations. Unlike prior methods operating in camera-centric coordinates, our model constructs occupancy representations in a global workspace frame, making it directly applicable to robotic manipulation. The model integrates flexible source views and generalizes to unseen object arrangements without scene-specific finetuning. We demonstrate the approach on a humanoid robot and evaluate predicted geometry against 3D sensor ground truth. Trained on 40 real scenes, our model achieves 26mm reconstruction error, including occluded regions, validating its ability to infer complete 3D occupancy beyond traditional stereo vision methods.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é€šç”¨ç¥ç»è¾å°„åœºæ–¹æ³•ï¼Œç”¨äºæ ¹æ®ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æœºå™¨äººè§‚å¯Ÿæ¥é¢„æµ‹ 3D å·¥ä½œç©ºé—´å ç”¨æƒ…å†µã€‚ä¸ä¹‹å‰åœ¨ä»¥ç›¸æœºä¸ºä¸­å¿ƒçš„åæ ‡ä¸­æ“ä½œçš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å…¨å±€å·¥ä½œç©ºé—´æ¡†æ¶ä¸­æ„å»ºå ç”¨è¡¨ç¤ºï¼Œä½¿å…¶ç›´æ¥é€‚ç”¨äºæœºå™¨äººæ“ä½œã€‚è¯¥æ¨¡å‹é›†æˆäº†çµæ´»çš„æºè§†å›¾ï¼Œå¹¶å¯æ¨å¹¿åˆ°çœ‹ä¸è§çš„å¯¹è±¡æ’åˆ—ï¼Œè€Œæ— éœ€é’ˆå¯¹ç‰¹å®šåœºæ™¯è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬åœ¨äººå½¢æœºå™¨äººä¸Šæ¼”ç¤ºäº†è¯¥æ–¹æ³•ï¼Œå¹¶æ ¹æ® 3D ä¼ æ„Ÿå™¨åœ°é¢å®å†µè¯„ä¼°é¢„æµ‹çš„å‡ ä½•å½¢çŠ¶ã€‚ç»è¿‡ 40 ä¸ªçœŸå®åœºæ™¯çš„è®­ç»ƒï¼Œæˆ‘ä»¬çš„æ¨¡å‹å®ç°äº† 26mm çš„é‡å»ºè¯¯å·®ï¼ˆåŒ…æ‹¬é®æŒ¡åŒºåŸŸï¼‰ï¼ŒéªŒè¯äº†å…¶è¶…è¶Šä¼ ç»Ÿç«‹ä½“è§†è§‰æ–¹æ³•æ¨æ–­å®Œæ•´ 3D å ç”¨çš„èƒ½åŠ›ã€‚

</details>

---

## 5. ResWorld: Temporal Residual World Model for End-to-End Autonomous Driving

**ä¸­æ–‡æ ‡é¢˜**: ResWorldï¼šç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶çš„æ—¶é—´æ®‹å·®ä¸–ç•Œæ¨¡å‹

**Date**: 2026-02-11 | **arXiv**: [2602.10884v1](http://arxiv.org/abs/2602.10884v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10884v1)

**Code**: https://github.com/mengtan00/ResWorld.git.

<details><summary><b>Abstract</b></summary>

The comprehensive understanding capabilities of world models for driving scenarios have significantly improved the planning accuracy of end-to-end autonomous driving frameworks. However, the redundant modeling of static regions and the lack of deep interaction with trajectories hinder world models from exerting their full effectiveness. In this paper, we propose Temporal Residual World Model (TR-World), which focuses on dynamic object modeling. By calculating the temporal residuals of scene representations, the information of dynamic objects can be extracted without relying on detection and tracking. TR-World takes only temporal residuals as input, thus predicting the future spatial distribution of dynamic objects more precisely. By combining the prediction with the static object information contained in the current BEV features, accurate future BEV features can be obtained. Furthermore, we propose Future-Guided Trajectory Refinement (FGTR) module, which conducts interaction between prior trajectories (predicted from the current scene representation) and the future BEV features. This module can not only utilize future road conditions to refine trajectories, but also provides sparse spatial-temporal supervision on future BEV features to prevent world model collapse. Comprehensive experiments conducted on the nuScenes and NAVSIM datasets demonstrate that our method, namely ResWorld, achieves state-of-the-art planning performance. The code is available at https://github.com/mengtan00/ResWorld.git.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ä¸–ç•Œæ¨¡å‹å¯¹é©¾é©¶åœºæ™¯çš„å…¨é¢ç†è§£èƒ½åŠ›ï¼Œæ˜¾ç€æé«˜äº†ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶æ¡†æ¶çš„è§„åˆ’ç²¾åº¦ã€‚ç„¶è€Œï¼Œé™æ€åŒºåŸŸçš„å†—ä½™å»ºæ¨¡ä»¥åŠç¼ºä¹ä¸è½¨è¿¹çš„æ·±åº¦äº¤äº’é˜»ç¢äº†ä¸–ç•Œæ¨¡å‹å‘æŒ¥å…¶å…¨éƒ¨æœ‰æ•ˆæ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ—¶é—´æ®‹å·®ä¸–ç•Œæ¨¡å‹ï¼ˆTR-Worldï¼‰ï¼Œå®ƒä¸“æ³¨äºåŠ¨æ€å¯¹è±¡å»ºæ¨¡ã€‚é€šè¿‡è®¡ç®—åœºæ™¯è¡¨ç¤ºçš„æ—¶é—´æ®‹å·®ï¼Œå¯ä»¥åœ¨ä¸ä¾èµ–æ£€æµ‹å’Œè·Ÿè¸ªçš„æƒ…å†µä¸‹æå–åŠ¨æ€å¯¹è±¡çš„ä¿¡æ¯ã€‚ TR-Worldä»…å°†æ—¶é—´æ®‹å·®ä½œä¸ºè¾“å…¥ï¼Œä»è€Œæ›´å‡†ç¡®åœ°é¢„æµ‹åŠ¨æ€å¯¹è±¡çš„æœªæ¥ç©ºé—´åˆ†å¸ƒã€‚é€šè¿‡å°†é¢„æµ‹ä¸å½“å‰BEVç‰¹å¾ä¸­åŒ…å«çš„é™æ€ç‰©ä½“ä¿¡æ¯ç›¸ç»“åˆï¼Œå¯ä»¥è·å¾—å‡†ç¡®çš„æœªæ¥BEVç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†æœªæ¥å¼•å¯¼è½¨è¿¹ç»†åŒ–ï¼ˆFGTRï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—åœ¨å…ˆå‰è½¨è¿¹ï¼ˆæ ¹æ®å½“å‰åœºæ™¯è¡¨ç¤ºé¢„æµ‹ï¼‰å’Œæœªæ¥ BEV ç‰¹å¾ä¹‹é—´è¿›è¡Œäº¤äº’ã€‚è¯¥æ¨¡å—ä¸ä»…å¯ä»¥åˆ©ç”¨æœªæ¥çš„è·¯å†µæ¥ç»†åŒ–è½¨è¿¹ï¼Œè¿˜å¯ä»¥å¯¹æœªæ¥çš„ BEV ç‰¹å¾æä¾›ç¨€ç–æ—¶ç©ºç›‘ç£ï¼Œâ€‹â€‹ä»¥é˜²æ­¢ä¸–ç•Œæ¨¡å‹å´©æºƒã€‚åœ¨ nuScenes å’Œ NAVSIM æ•°æ®é›†ä¸Šè¿›è¡Œçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ï¼ˆå³ ResWorldï¼‰å®ç°äº†æœ€å…ˆè¿›çš„è§„åˆ’æ€§èƒ½ã€‚ä»£ç å¯åœ¨ https://github.com/mengtan00/ResWorld.git è·å–ã€‚

</details>

---

## 6. Chart Specification: Structural Representations for Incentivizing VLM Reasoning in Chart-to-Code Generation

**ä¸­æ–‡æ ‡é¢˜**: å›¾è¡¨è§„èŒƒï¼šåœ¨å›¾è¡¨åˆ°ä»£ç ç”Ÿæˆä¸­æ¿€åŠ± VLM æ¨ç†çš„ç»“æ„è¡¨ç¤º

**Date**: 2026-02-11 | **arXiv**: [2602.10880v1](http://arxiv.org/abs/2602.10880v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10880v1)

**Code**: https://github.com/Mighten/chart-specification-paper

<details><summary><b>Abstract</b></summary>

Vision-Language Models (VLMs) have shown promise in generating plotting code from chart images, yet achieving structural fidelity remains challenging. Existing approaches largely rely on supervised fine-tuning, encouraging surface-level token imitation rather than faithful modeling of underlying chart structure, which often leads to hallucinated or semantically inconsistent outputs. We propose Chart Specification, a structured intermediate representation that shifts training from text imitation to semantically grounded supervision. Chart Specification filters syntactic noise to construct a structurally balanced training set and supports a Spec-Align Reward that provides fine-grained, verifiable feedback on structural correctness, enabling reinforcement learning to enforce consistent plotting logic. Experiments on three public benchmarks show that our method consistently outperforms prior approaches. With only 3K training samples, we achieve strong data efficiency, surpassing leading baselines by up to 61.7% on complex benchmarks, and scaling to 4K samples establishes new state-of-the-art results across all evaluated metrics. Overall, our results demonstrate that precise structural supervision offers an efficient pathway to high-fidelity chart-to-code generation. Code and dataset are available at: https://github.com/Mighten/chart-specification-paper

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è§†è§‰è¯­è¨€æ¨¡å‹ (VLM) åœ¨ä»å›¾è¡¨å›¾åƒç”Ÿæˆç»˜å›¾ä»£ç æ–¹é¢æ˜¾ç¤ºå‡ºäº†å‰æ™¯ï¼Œä½†å®ç°ç»“æ„ä¿çœŸåº¦ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰çš„æ–¹æ³•åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºç›‘ç£å¾®è°ƒï¼Œé¼“åŠ±è¡¨é¢çº§åˆ«çš„ä»¤ç‰Œæ¨¡ä»¿ï¼Œè€Œä¸æ˜¯å¯¹åº•å±‚å›¾è¡¨ç»“æ„çš„å¿ å®å»ºæ¨¡ï¼Œè¿™é€šå¸¸ä¼šå¯¼è‡´å¹»è§‰æˆ–è¯­ä¹‰ä¸ä¸€è‡´çš„è¾“å‡ºã€‚æˆ‘ä»¬æå‡ºäº†å›¾è¡¨è§„èŒƒï¼Œä¸€ç§ç»“æ„åŒ–çš„ä¸­é—´è¡¨ç¤ºï¼Œå°†è®­ç»ƒä»æ–‡æœ¬æ¨¡ä»¿è½¬å˜ä¸ºåŸºäºè¯­ä¹‰çš„ç›‘ç£ã€‚å›¾è¡¨è§„èŒƒè¿‡æ»¤è¯­æ³•å™ªéŸ³ä»¥æ„å»ºç»“æ„å¹³è¡¡çš„è®­ç»ƒé›†ï¼Œå¹¶æ”¯æŒè§„èŒƒå¯¹é½å¥–åŠ±ï¼Œè¯¥å¥–åŠ±æä¾›æœ‰å…³ç»“æ„æ­£ç¡®æ€§çš„ç»†ç²’åº¦ã€å¯éªŒè¯çš„åé¦ˆï¼Œä½¿å¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿå¼ºåˆ¶æ‰§è¡Œä¸€è‡´çš„ç»˜å›¾é€»è¾‘ã€‚å¯¹ä¸‰ä¸ªå…¬å…±åŸºå‡†çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚ä»…é€šè¿‡ 3K è®­ç»ƒæ ·æœ¬ï¼Œæˆ‘ä»¬å°±å®ç°äº†å¼ºå¤§çš„æ•°æ®æ•ˆç‡ï¼Œåœ¨å¤æ‚çš„åŸºå‡†æµ‹è¯•ä¸­è¶…å‡ºäº†é¢†å…ˆåŸºçº¿é«˜è¾¾ 61.7%ï¼Œå¹¶ä¸”æ‰©å±•åˆ° 4K æ ·æœ¬åœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸­å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„ç»“æœã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç²¾ç¡®çš„ç»“æ„ç›‘ç£ä¸ºé«˜ä¿çœŸå›¾è¡¨åˆ°ä»£ç ç”Ÿæˆæä¾›äº†ä¸€æ¡æœ‰æ•ˆçš„é€”å¾„ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨ä»¥ä¸‹ä½ç½®è·å–ï¼šhttps://github.com/Mighten/chart-specification-paper

</details>

---

## 7. Stride-Net: Fairness-Aware Disentangled Representation Learning for Chest X-Ray Diagnosis

**ä¸­æ–‡æ ‡é¢˜**: Stride-Netï¼šç”¨äºèƒ¸éƒ¨ X å°„çº¿è¯Šæ–­çš„å…¬å¹³æ„è¯†è§£ç¼ è¡¨ç¤ºå­¦ä¹ 

**Date**: 2026-02-11 | **arXiv**: [2602.10875v1](http://arxiv.org/abs/2602.10875v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10875v1)

**Code**: https://github.com/Daraksh/Fairness_StrideNet.

<details><summary><b>Abstract</b></summary>

Deep neural networks for chest X-ray classification achieve strong average performance, yet often underperform for specific demographic subgroups, raising critical concerns about clinical safety and equity. Existing debiasing methods frequently yield inconsistent improvements across datasets or attain fairness by degrading overall diagnostic utility, treating fairness as a post hoc constraint rather than a property of the learned representation. In this work, we propose Stride-Net (Sensitive Attribute Resilient Learning via Disentanglement and Learnable Masking with Embedding Alignment), a fairness-aware framework that learns disease-discriminative yet demographically invariant representations for chest X-ray analysis. Stride-Net operates at the patch level, using a learnable stride-based mask to select label-aligned image regions while suppressing sensitive attribute information through adversarial confusion loss. To anchor representations in clinical semantics and discourage shortcut learning, we further enforce semantic alignment between image features and BioBERT-based disease label embeddings via Group Optimal Transport. We evaluate Stride-Net on the MIMIC-CXR and CheXpert benchmarks across race and intersectional race-gender subgroups. Across architectures including ResNet and Vision Transformers, Stride-Net consistently improves fairness metrics while matching or exceeding baseline accuracy, achieving a more favorable accuracy-fairness trade-off than prior debiasing approaches. Our code is available at https://github.com/Daraksh/Fairness_StrideNet.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç”¨äºèƒ¸éƒ¨ X å°„çº¿åˆ†ç±»çš„æ·±åº¦ç¥ç»ç½‘ç»œå®ç°äº†å¼ºåŠ²çš„å¹³å‡æ€§èƒ½ï¼Œä½†å¯¹äºç‰¹å®šçš„äººå£äºšç»„é€šå¸¸è¡¨ç°ä¸ä½³ï¼Œå¼•å‘äº†å¯¹ä¸´åºŠå®‰å…¨æ€§å’Œå…¬å¹³æ€§çš„ä¸¥é‡æ‹…å¿§ã€‚ç°æœ‰çš„å»åå·®æ–¹æ³•ç»å¸¸ä¼šåœ¨æ•°æ®é›†ä¸­äº§ç”Ÿä¸ä¸€è‡´çš„æ”¹è¿›ï¼Œæˆ–è€…é€šè¿‡é™ä½æ•´ä½“è¯Šæ–­æ•ˆç”¨æ¥å®ç°å…¬å¹³æ€§ï¼Œå°†å…¬å¹³æ€§è§†ä¸ºäº‹åçº¦æŸè€Œä¸æ˜¯å­¦ä¹ è¡¨ç¤ºçš„å±æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† Stride-Netï¼ˆé€šè¿‡è§£ç¼ ç»“å’Œå¯å­¦ä¹ æ©ç ä¸åµŒå…¥å¯¹é½è¿›è¡Œæ•æ„Ÿå±æ€§å¼¹æ€§å­¦ä¹ ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å…¬å¹³æ„è¯†æ¡†æ¶ï¼Œå¯ä»¥å­¦ä¹ ç”¨äºèƒ¸éƒ¨ X å°„çº¿åˆ†æçš„ç–¾ç—…åŒºåˆ†æ€§ä½†äººå£ç»Ÿè®¡ä¸å˜çš„è¡¨ç¤ºã€‚ Stride-Net åœ¨è¡¥ä¸çº§åˆ«è¿è¡Œï¼Œä½¿ç”¨å¯å­¦ä¹ çš„åŸºäºæ­¥å¹…çš„æ©æ¨¡æ¥é€‰æ‹©æ ‡ç­¾å¯¹é½çš„å›¾åƒåŒºåŸŸï¼ŒåŒæ—¶é€šè¿‡å¯¹æŠ—æ€§æ··æ·†æŸå¤±æŠ‘åˆ¶æ•æ„Ÿå±æ€§ä¿¡æ¯ã€‚ä¸ºäº†é”šå®šä¸´åºŠè¯­ä¹‰ä¸­çš„è¡¨ç¤ºå¹¶é˜»æ­¢å¿«æ·å­¦ä¹ ï¼Œæˆ‘ä»¬é€šè¿‡ç»„æœ€ä¼˜ä¼ è¾“è¿›ä¸€æ­¥åŠ å¼ºå›¾åƒç‰¹å¾å’ŒåŸºäº BioBERT çš„ç–¾ç—…æ ‡ç­¾åµŒå…¥ä¹‹é—´çš„è¯­ä¹‰å¯¹é½ã€‚æˆ‘ä»¬åœ¨ MIMIC-CXR å’Œ CheXpert åŸºå‡†ä¸Šè¯„ä¼°è·¨ç§æ—å’Œäº¤å‰ç§æ—æ€§åˆ«å­ç»„çš„ Stride-Netã€‚åœ¨åŒ…æ‹¬ ResNet å’Œ Vision Transformers åœ¨å†…çš„æ¶æ„ä¸­ï¼ŒStride-Net æŒç»­æ”¹è¿›å…¬å¹³æ€§æŒ‡æ ‡ï¼ŒåŒæ—¶åŒ¹é…æˆ–è¶…è¿‡åŸºçº¿å‡†ç¡®åº¦ï¼Œå®ç°æ¯”ä¹‹å‰çš„å»åæ–¹æ³•æ›´æœ‰åˆ©çš„å‡†ç¡®åº¦ä¸å…¬å¹³æ€§æƒè¡¡ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ https://github.com/Daraksh/Fairness_StrideNet ä¸Šè·å–ã€‚

</details>

---

## 8. Hyperspectral Smoke Segmentation via Mixture of Prototypes

**ä¸­æ–‡æ ‡é¢˜**: é€šè¿‡æ··åˆåŸå‹è¿›è¡Œé«˜å…‰è°±çƒŸé›¾åˆ†å‰²

**Date**: 2026-02-11 | **arXiv**: [2602.10858v1](http://arxiv.org/abs/2602.10858v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10858v1)

<details><summary><b>Abstract</b></summary>

Smoke segmentation is critical for wildfire management and industrial safety applications. Traditional visible-light-based methods face limitations due to insufficient spectral information, particularly struggling with cloud interference and semi-transparent smoke regions. To address these challenges, we introduce hyperspectral imaging for smoke segmentation and present the first hyperspectral smoke segmentation dataset (HSSDataset) with carefully annotated samples collected from over 18,000 frames across 20 real-world scenarios using a Many-to-One annotations protocol. However, different spectral bands exhibit varying discriminative capabilities across spatial regions, necessitating adaptive band weighting strategies. We decompose this into three technical challenges: spectral interaction contamination, limited spectral pattern modeling, and complex weighting router problems. We propose a mixture of prototypes (MoP) network with: (1) Band split for spectral isolation, (2) Prototype-based spectral representation for diverse patterns, and (3) Dual-level router for adaptive spatial-aware band weighting. We further construct a multispectral dataset (MSSDataset) with RGB-infrared images. Extensive experiments validate superior performance across both hyperspectral and multispectral modalities, establishing a new paradigm for spectral-based smoke segmentation.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

çƒŸé›¾åˆ†å‰²å¯¹äºé‡ç«ç®¡ç†å’Œå·¥ä¸šå®‰å…¨åº”ç”¨è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„åŸºäºå¯è§å…‰çš„æ–¹æ³•ç”±äºå…‰è°±ä¿¡æ¯ä¸è¶³è€Œé¢ä¸´å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨äº‘å¹²æ‰°å’ŒåŠé€æ˜çƒŸé›¾åŒºåŸŸæ–¹é¢é‡åˆ°å›°éš¾ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç”¨äºçƒŸé›¾åˆ†å‰²çš„é«˜å…‰è°±æˆåƒï¼Œå¹¶æå‡ºäº†ç¬¬ä¸€ä¸ªé«˜å…‰è°±çƒŸé›¾åˆ†å‰²æ•°æ®é›† (HSSDataset)ï¼Œå…¶ä¸­ä½¿ç”¨å¤šå¯¹ä¸€æ³¨é‡Šåè®®ä» 20 ä¸ªçœŸå®åœºæ™¯çš„ 18,000 å¤šä¸ªå¸§ä¸­æ”¶é›†äº†ä»”ç»†æ³¨é‡Šçš„æ ·æœ¬ã€‚ç„¶è€Œï¼Œä¸åŒçš„å…‰è°±å¸¦åœ¨ç©ºé—´åŒºåŸŸä¸Šè¡¨ç°å‡ºä¸åŒçš„è¾¨åˆ«èƒ½åŠ›ï¼Œå› æ­¤éœ€è¦è‡ªé€‚åº”é¢‘å¸¦åŠ æƒç­–ç•¥ã€‚æˆ‘ä»¬å°†å…¶åˆ†è§£ä¸ºä¸‰ä¸ªæŠ€æœ¯æŒ‘æˆ˜ï¼šå…‰è°±ç›¸äº’ä½œç”¨æ±¡æŸ“ã€æœ‰é™çš„å…‰è°±æ¨¡å¼å»ºæ¨¡å’Œå¤æ‚çš„åŠ æƒè·¯ç”±å™¨é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆåŸå‹ï¼ˆMoPï¼‰ç½‘ç»œï¼šï¼ˆ1ï¼‰ç”¨äºå…‰è°±éš”ç¦»çš„é¢‘å¸¦åˆ†å‰²ï¼Œï¼ˆ2ï¼‰ç”¨äºä¸åŒæ¨¡å¼çš„åŸºäºåŸå‹çš„å…‰è°±è¡¨ç¤ºï¼Œä»¥åŠï¼ˆ3ï¼‰ç”¨äºè‡ªé€‚åº”ç©ºé—´æ„ŸçŸ¥é¢‘å¸¦åŠ æƒçš„åŒå±‚è·¯ç”±å™¨ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ç”¨ RGB çº¢å¤–å›¾åƒæ„å»ºäº†å¤šå…‰è°±æ•°æ®é›† (MSSDataset)ã€‚å¤§é‡çš„å®éªŒéªŒè¯äº†é«˜å…‰è°±å’Œå¤šå…‰è°±æ¨¡å¼çš„å“è¶Šæ€§èƒ½ï¼Œä¸ºåŸºäºå…‰è°±çš„çƒŸé›¾åˆ†å‰²å»ºç«‹äº†æ–°çš„èŒƒä¾‹ã€‚

</details>

---

## 9. Flow caching for autoregressive video generation

**ä¸­æ–‡æ ‡é¢˜**: ç”¨äºè‡ªå›å½’è§†é¢‘ç”Ÿæˆçš„æµç¼“å­˜

**Date**: 2026-02-11 | **arXiv**: [2602.10825v1](http://arxiv.org/abs/2602.10825v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10825v1)

**Code**: https://github.com/mikeallen39/FlowCache.

<details><summary><b>Abstract</b></summary>

Autoregressive models, often built on Transformer architectures, represent a powerful paradigm for generating ultra-long videos by synthesizing content in sequential chunks. However, this sequential generation process is notoriously slow. While caching strategies have proven effective for accelerating traditional video diffusion models, existing methods assume uniform denoising across all frames-an assumption that breaks down in autoregressive models where different video chunks exhibit varying similarity patterns at identical timesteps. In this paper, we present FlowCache, the first caching framework specifically designed for autoregressive video generation. Our key insight is that each video chunk should maintain independent caching policies, allowing fine-grained control over which chunks require recomputation at each timestep. We introduce a chunkwise caching strategy that dynamically adapts to the unique denoising characteristics of each chunk, complemented by a joint importance-redundancy optimized KV cache compression mechanism that maintains fixed memory bounds while preserving generation quality. Our method achieves remarkable speedups of 2.38 times on MAGI-1 and 6.7 times on SkyReels-V2, with negligible quality degradation (VBench: 0.87 increase and 0.79 decrease respectively). These results demonstrate that FlowCache successfully unlocks the potential of autoregressive models for real-time, ultra-long video generation-establishing a new benchmark for efficient video synthesis at scale. The code is available at https://github.com/mikeallen39/FlowCache.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è‡ªå›å½’æ¨¡å‹é€šå¸¸å»ºç«‹åœ¨ Transformer æ¶æ„ä¹‹ä¸Šï¼Œä»£è¡¨äº†é€šè¿‡åˆæˆè¿ç»­å—ä¸­çš„å†…å®¹æ¥ç”Ÿæˆè¶…é•¿è§†é¢‘çš„å¼ºå¤§èŒƒä¾‹ã€‚ç„¶è€Œï¼Œè¿™ç§é¡ºåºç”Ÿæˆè¿‡ç¨‹æ˜¯å‡ºäº†åçš„æ…¢ã€‚è™½ç„¶ç¼“å­˜ç­–ç•¥å·²è¢«è¯æ˜å¯¹äºåŠ é€Ÿä¼ ç»Ÿè§†é¢‘æ‰©æ•£æ¨¡å‹æ˜¯æœ‰æ•ˆçš„ï¼Œä½†ç°æœ‰æ–¹æ³•å‡è®¾æ‰€æœ‰å¸§éƒ½é‡‡ç”¨ç»Ÿä¸€çš„å»å™ªâ€”â€”è¿™ç§å‡è®¾åœ¨è‡ªå›å½’æ¨¡å‹ä¸­è¢«æ‰“ç ´ï¼Œå…¶ä¸­ä¸åŒçš„è§†é¢‘å—åœ¨ç›¸åŒçš„æ—¶é—´æ­¥é•¿è¡¨ç°å‡ºä¸åŒçš„ç›¸ä¼¼æ€§æ¨¡å¼ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† FlowCacheï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“ä¸ºè‡ªå›å½’è§†é¢‘ç”Ÿæˆè€Œè®¾è®¡çš„ç¼“å­˜æ¡†æ¶ã€‚æˆ‘ä»¬çš„ä¸»è¦è§è§£æ˜¯æ¯ä¸ªè§†é¢‘å—åº”è¯¥ç»´æŠ¤ç‹¬ç«‹çš„ç¼“å­˜ç­–ç•¥ï¼Œä»è€Œå¯ä»¥å¯¹æ¯ä¸ªæ—¶é—´æ­¥éœ€è¦é‡æ–°è®¡ç®—çš„å—è¿›è¡Œç»†ç²’åº¦æ§åˆ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ†å—ç¼“å­˜ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åŠ¨æ€é€‚åº”æ¯ä¸ªå—çš„ç‹¬ç‰¹å»å™ªç‰¹æ€§ï¼Œå¹¶è¾…ä»¥è”åˆé‡è¦æ€§å†—ä½™ä¼˜åŒ–çš„ KV ç¼“å­˜å‹ç¼©æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åœ¨ä¿æŒå›ºå®šå†…å­˜è¾¹ç•Œçš„åŒæ—¶ä¿æŒç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ MAGI-1 ä¸Šå®ç°äº† 2.38 å€çš„æ˜¾ç€åŠ é€Ÿï¼Œåœ¨ SkyReels-V2 ä¸Šå®ç°äº† 6.7 å€çš„æ˜¾ç€åŠ é€Ÿï¼Œè€Œè´¨é‡ä¸‹é™å¯ä»¥å¿½ç•¥ä¸è®¡ï¼ˆVBenchï¼šåˆ†åˆ«å¢åŠ  0.87 å€å’Œå‡å°‘ 0.79 å€ï¼‰ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒFlowCache æˆåŠŸé‡Šæ”¾äº†è‡ªå›å½’æ¨¡å‹åœ¨å®æ—¶ã€è¶…é•¿è§†é¢‘ç”Ÿæˆæ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºå¤§è§„æ¨¡é«˜æ•ˆè§†é¢‘åˆæˆå»ºç«‹äº†æ–°åŸºå‡†ã€‚è¯¥ä»£ç å¯ä» https://github.com/mikeallen39/FlowCache è·å–ã€‚

</details>

---

## 10. Resource-Efficient RGB-Only Action Recognition for Edge Deployment

**ä¸­æ–‡æ ‡é¢˜**: é€‚ç”¨äºè¾¹ç¼˜éƒ¨ç½²çš„èµ„æºé«˜æ•ˆå‹çº¯ RGB åŠ¨ä½œè¯†åˆ«

**Date**: 2026-02-11 | **arXiv**: [2602.10818v1](http://arxiv.org/abs/2602.10818v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10818v1)

<details><summary><b>Abstract</b></summary>

Action recognition on edge devices poses stringent constraints on latency, memory, storage, and power consumption. While auxiliary modalities such as skeleton and depth information can enhance recognition performance, they often require additional sensors or computationally expensive pose-estimation pipelines, limiting practicality for edge use. In this work, we propose a compact RGB-only network tailored for efficient on-device inference. Our approach builds upon an X3D-style backbone augmented with Temporal Shift, and further introduces selective temporal adaptation and parameter-free attention. Extensive experiments on the NTU RGB+D 60 and 120 benchmarks demonstrate a strong accuracy-efficiency balance. Moreover, deployment-level profiling on the Jetson Orin Nano verifies a smaller on-device footprint and practical resource utilization compared to existing RGB-based action recognition techniques.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è¾¹ç¼˜è®¾å¤‡ä¸Šçš„åŠ¨ä½œè¯†åˆ«å¯¹å»¶è¿Ÿã€å†…å­˜ã€å­˜å‚¨å’ŒåŠŸè€—æå‡ºäº†ä¸¥æ ¼çš„é™åˆ¶ã€‚è™½ç„¶éª¨æ¶å’Œæ·±åº¦ä¿¡æ¯ç­‰è¾…åŠ©æ¨¡æ€å¯ä»¥å¢å¼ºè¯†åˆ«æ€§èƒ½ï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦é¢å¤–çš„ä¼ æ„Ÿå™¨æˆ–è®¡ç®—æˆæœ¬é«˜æ˜‚çš„å§¿æ€ä¼°è®¡ç®¡é“ï¼Œä»è€Œé™åˆ¶äº†è¾¹ç¼˜ä½¿ç”¨çš„å®ç”¨æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç´§å‡‘çš„çº¯ RGB ç½‘ç»œï¼Œä¸“ä¸ºé«˜æ•ˆçš„è®¾å¤‡ä¸Šæ¨ç†è€Œå®šåˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å»ºç«‹åœ¨ X3D é£æ ¼çš„ä¸»å¹²ä¸Šï¼Œå¹¶é€šè¿‡æ—¶é—´ç§»ä½è¿›è¡Œå¢å¼ºï¼Œå¹¶è¿›ä¸€æ­¥å¼•å…¥äº†é€‰æ‹©æ€§æ—¶é—´é€‚åº”å’Œæ— å‚æ•°æ³¨æ„åŠ›ã€‚ NTU RGB+D 60 å’Œ 120 åŸºå‡†çš„å¤§é‡å®éªŒè¯æ˜äº†å¼ºå¤§çš„å‡†ç¡®æ€§ä¸æ•ˆç‡å¹³è¡¡ã€‚æ­¤å¤–ï¼Œä¸ç°æœ‰åŸºäº RGB çš„åŠ¨ä½œè¯†åˆ«æŠ€æœ¯ç›¸æ¯”ï¼ŒJetson Orin Nano ä¸Šçš„éƒ¨ç½²çº§åˆ†æéªŒè¯äº†æ›´å°çš„è®¾å¤‡å ç”¨ç©ºé—´å’Œå®é™…èµ„æºåˆ©ç”¨ç‡ã€‚

</details>

---

## 11. Why Does RL Generalize Better Than SFT? A Data-Centric Perspective on VLM Post-Training

**ä¸­æ–‡æ ‡é¢˜**: ä¸ºä»€ä¹ˆ RL çš„æ³›åŒ–èƒ½åŠ›æ¯” SFT æ›´å¥½ï¼Ÿä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„ VLM è®­ç»ƒåè§†è§’

**Date**: 2026-02-11 | **arXiv**: [2602.10815v1](http://arxiv.org/abs/2602.10815v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10815v1)

**Code**: https://github.com/byyx666/DC-SFT.

<details><summary><b>Abstract</b></summary>

The adaptation of large-scale Vision-Language Models (VLMs) through post-training reveals a pronounced generalization gap: models fine-tuned with Reinforcement Learning (RL) consistently achieve superior out-of-distribution (OOD) performance compared to those trained with Supervised Fine-Tuning (SFT). This paper posits a data-centric explanation for this phenomenon, contending that RL's generalization advantage arises from an implicit data filtering mechanism that inherently prioritizes medium-difficulty training samples. To test this hypothesis, we systematically evaluate the OOD generalization of SFT models across training datasets of varying difficulty levels. Our results confirm that data difficulty is a critical factor, revealing that training on hard samples significantly degrades OOD performance. Motivated by this finding, we introduce Difficulty-Curated SFT (DC-SFT), a straightforward method that explicitly filters the training set based on sample difficulty. Experiments show that DC-SFT not only substantially enhances OOD generalization over standard SFT, but also surpasses the performance of RL-based training, all while providing greater stability and computational efficiency. This work offers a data-centric account of the OOD generalization gap in VLMs and establishes a more efficient pathway to achieving robust generalization. Code is available at https://github.com/byyx666/DC-SFT.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

é€šè¿‡åæœŸè®­ç»ƒå¯¹å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ (VLM) è¿›è¡Œçš„è°ƒæ•´æ­ç¤ºäº†æ˜æ˜¾çš„æ³›åŒ–å·®è·ï¼šä¸ä½¿ç”¨ç›‘ç£å¾®è°ƒ (SFT) è®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”ï¼Œä½¿ç”¨å¼ºåŒ–å­¦ä¹  (RL) å¾®è°ƒçš„æ¨¡å‹å§‹ç»ˆèƒ½å¤Ÿå®ç°å“è¶Šçš„åˆ†å¸ƒå¤– (OOD) æ€§èƒ½ã€‚æœ¬æ–‡å¯¹è¿™ç§ç°è±¡æå‡ºäº†ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„è§£é‡Šï¼Œè®¤ä¸ºå¼ºåŒ–å­¦ä¹ çš„æ³›åŒ–ä¼˜åŠ¿æºäºéšå¼æ•°æ®è¿‡æ»¤æœºåˆ¶ï¼Œè¯¥æœºåˆ¶æœ¬è´¨ä¸Šä¼˜å…ˆè€ƒè™‘ä¸­ç­‰éš¾åº¦çš„è®­ç»ƒæ ·æœ¬ã€‚ä¸ºäº†æ£€éªŒè¿™ä¸€å‡è®¾ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº† SFT æ¨¡å‹åœ¨ä¸åŒéš¾åº¦çº§åˆ«çš„è®­ç»ƒæ•°æ®é›†ä¸Šçš„ OOD æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç»“æœè¯å®ï¼Œæ•°æ®éš¾åº¦æ˜¯ä¸€ä¸ªå…³é”®å› ç´ ï¼Œè¡¨æ˜å¯¹ç¡¬æ ·æœ¬è¿›è¡Œè®­ç»ƒä¼šæ˜¾ç€é™ä½ OOD æ€§èƒ½ã€‚å—è¿™ä¸€å‘ç°çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†éš¾åº¦ç­–åˆ’ SFT (DC-SFT)ï¼Œè¿™æ˜¯ä¸€ç§æ ¹æ®æ ·æœ¬éš¾åº¦æ˜¾å¼è¿‡æ»¤è®­ç»ƒé›†çš„ç®€å•æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼ŒDC-SFT ä¸ä»…æ¯”æ ‡å‡† SFT æ˜¾ç€å¢å¼ºäº† OOD æ³›åŒ–èƒ½åŠ›ï¼Œè€Œä¸”è¶…è¶Šäº†åŸºäº RL çš„è®­ç»ƒæ€§èƒ½ï¼ŒåŒæ—¶æä¾›äº†æ›´é«˜çš„ç¨³å®šæ€§å’Œè®¡ç®—æ•ˆç‡ã€‚è¿™é¡¹å·¥ä½œæä¾›äº†ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„ VLM ä¸­ OOD æ³›åŒ–å·®è·çš„è§£é‡Šï¼Œå¹¶å»ºç«‹äº†ä¸€æ¡æ›´æœ‰æ•ˆçš„é€”å¾„æ¥å®ç°ç¨³å¥çš„æ³›åŒ–ã€‚ä»£ç å¯åœ¨ https://github.com/byyx666/DC-SFT è·å–ã€‚

</details>

---

## 12. DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories

**ä¸­æ–‡æ ‡é¢˜**: DeepImageSearchï¼šè§†è§‰å†å²ä¸­ä¸Šä¸‹æ–‡æ„ŸçŸ¥å›¾åƒæ£€ç´¢çš„å¤šæ¨¡æ€ä»£ç†åŸºå‡†æµ‹è¯•

**Date**: 2026-02-11 | **arXiv**: [2602.10809v1](http://arxiv.org/abs/2602.10809v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10809v1)

<details><summary><b>Abstract</b></summary>

Existing multimodal retrieval systems excel at semantic matching but implicitly assume that query-image relevance can be measured in isolation. This paradigm overlooks the rich dependencies inherent in realistic visual streams, where information is distributed across temporal sequences rather than confined to single snapshots. To bridge this gap, we introduce DeepImageSearch, a novel agentic paradigm that reformulates image retrieval as an autonomous exploration task. Models must plan and perform multi-step reasoning over raw visual histories to locate targets based on implicit contextual cues. We construct DISBench, a challenging benchmark built on interconnected visual data. To address the scalability challenge of creating context-dependent queries, we propose a human-model collaborative pipeline that employs vision-language models to mine latent spatiotemporal associations, effectively offloading intensive context discovery before human verification. Furthermore, we build a robust baseline using a modular agent framework equipped with fine-grained tools and a dual-memory system for long-horizon navigation. Extensive experiments demonstrate that DISBench poses significant challenges to state-of-the-art models, highlighting the necessity of incorporating agentic reasoning into next-generation retrieval systems.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç°æœ‰çš„å¤šæ¨¡æ€æ£€ç´¢ç³»ç»Ÿæ“…é•¿è¯­ä¹‰åŒ¹é…ï¼Œä½†éšå«åœ°å‡è®¾æŸ¥è¯¢å›¾åƒç›¸å…³æ€§å¯ä»¥å•ç‹¬æµ‹é‡ã€‚è¿™ç§èŒƒä¾‹å¿½ç•¥äº†ç°å®è§†è§‰æµä¸­å›ºæœ‰çš„ä¸°å¯Œä¾èµ–å…³ç³»ï¼Œå…¶ä¸­ä¿¡æ¯åˆ†å¸ƒåœ¨æ—¶é—´åºåˆ—ä¸Šï¼Œè€Œä¸æ˜¯å±€é™äºå•ä¸ªå¿«ç…§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº† DeepImageSearchï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„ä»£ç†èŒƒå¼ï¼Œå®ƒå°†å›¾åƒæ£€ç´¢é‡æ–°å®šä¹‰ä¸ºä¸€é¡¹è‡ªä¸»æ¢ç´¢ä»»åŠ¡ã€‚æ¨¡å‹å¿…é¡»å¯¹åŸå§‹è§†è§‰å†å²è¿›è¡Œè§„åˆ’å’Œæ‰§è¡Œå¤šæ­¥éª¤æ¨ç†ï¼Œä»¥æ ¹æ®éšå¼ä¸Šä¸‹æ–‡çº¿ç´¢å®šä½ç›®æ ‡ã€‚æˆ‘ä»¬æ„å»ºäº† DISBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºäº’è¿è§†è§‰æ•°æ®çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†ã€‚ä¸ºäº†è§£å†³åˆ›å»ºä¸Šä¸‹æ–‡ç›¸å…³æŸ¥è¯¢çš„å¯æ‰©å±•æ€§æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§äººç±»æ¨¡å‹åä½œç®¡é“ï¼Œè¯¥ç®¡é“é‡‡ç”¨è§†è§‰è¯­è¨€æ¨¡å‹æ¥æŒ–æ˜æ½œåœ¨çš„æ—¶ç©ºå…³è”ï¼Œä»è€Œåœ¨äººç±»éªŒè¯ä¹‹å‰æœ‰æ•ˆåœ°å¸è½½å¯†é›†çš„ä¸Šä¸‹æ–‡å‘ç°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨é…å¤‡ç»†ç²’åº¦å·¥å…·çš„æ¨¡å—åŒ–ä»£ç†æ¡†æ¶å’Œç”¨äºé•¿è§†é‡å¯¼èˆªçš„åŒå†…å­˜ç³»ç»Ÿæ„å»ºäº†å¼ºå¤§çš„åŸºçº¿ã€‚å¤§é‡å®éªŒè¡¨æ˜ DISBench å¯¹æœ€å…ˆè¿›çš„æ¨¡å‹æå‡ºäº†é‡å¤§æŒ‘æˆ˜ï¼Œå‡¸æ˜¾äº†å°†ä»£ç†æ¨ç†çº³å…¥ä¸‹ä¸€ä»£æ£€ç´¢ç³»ç»Ÿçš„å¿…è¦æ€§ã€‚

</details>

---

## 13. DMP-3DAD: Cross-Category 3D Anomaly Detection via Realistic Depth Map Projection with Few Normal Samples

**ä¸­æ–‡æ ‡é¢˜**: DMP-3DADï¼šé€šè¿‡çœŸå®æ·±åº¦å›¾æŠ•å½±ä¸å°‘é‡æ­£å¸¸æ ·æœ¬è¿›è¡Œè·¨ç±»åˆ« 3D å¼‚å¸¸æ£€æµ‹

**Date**: 2026-02-11 | **arXiv**: [2602.10806v1](http://arxiv.org/abs/2602.10806v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10806v1)

<details><summary><b>Abstract</b></summary>

Cross-category anomaly detection for 3D point clouds aims to determine whether an unseen object belongs to a target category using only a few normal examples. Most existing methods rely on category-specific training, which limits their flexibility in few-shot scenarios. In this paper, we propose DMP-3DAD, a training-free framework for cross-category 3D anomaly detection based on multi-view realistic depth map projection. Specifically, by converting point clouds into a fixed set of realistic depth images, our method leverages a frozen CLIP visual encoder to extract multi-view representations and performs anomaly detection via weighted feature similarity, which does not require any fine-tuning or category-dependent adaptation. Extensive experiments on the ShapeNetPart dataset demonstrate that DMP-3DAD achieves state-of-the-art performance under few-shot setting. The results show that the proposed approach provides a simple yet effective solution for practical cross-category 3D anomaly detection.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

3D ç‚¹äº‘çš„è·¨ç±»åˆ«å¼‚å¸¸æ£€æµ‹æ—¨åœ¨ä»…ä½¿ç”¨å‡ ä¸ªæ­£å¸¸ç¤ºä¾‹æ¥ç¡®å®šæœªè§è¿‡çš„å¯¹è±¡æ˜¯å¦å±äºç›®æ ‡ç±»åˆ«ã€‚å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¾èµ–äºç‰¹å®šç±»åˆ«çš„è®­ç»ƒï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å°‘æ•°åœºæ™¯ä¸­çš„çµæ´»æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† DMP-3DADï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¤šè§†å›¾çœŸå®æ·±åº¦å›¾æŠ•å½±çš„è·¨ç±»åˆ« 3D å¼‚å¸¸æ£€æµ‹çš„å…è®­ç»ƒæ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡å°†ç‚¹äº‘è½¬æ¢ä¸ºä¸€ç»„å›ºå®šçš„çœŸå®æ·±åº¦å›¾åƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å†»ç»“çš„ CLIP è§†è§‰ç¼–ç å™¨æ¥æå–å¤šè§†å›¾è¡¨ç¤ºï¼Œå¹¶é€šè¿‡åŠ æƒç‰¹å¾ç›¸ä¼¼æ€§æ‰§è¡Œå¼‚å¸¸æ£€æµ‹ï¼Œè¿™ä¸éœ€è¦ä»»ä½•å¾®è°ƒæˆ–ä¾èµ–äºç±»åˆ«çš„é€‚åº”ã€‚åœ¨ ShapeNetPart æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDMP-3DAD åœ¨å°‘é‡é•œå¤´è®¾ç½®ä¸‹å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¸ºå®é™…çš„è·¨ç±»åˆ« 3D å¼‚å¸¸æ£€æµ‹æä¾›äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

</details>

---

## 14. RSHallu: Dual-Mode Hallucination Evaluation for Remote-Sensing Multimodal Large Language Models with Domain-Tailored Mitigation

**ä¸­æ–‡æ ‡é¢˜**: RSHalluï¼šå…·æœ‰é¢†åŸŸå®šåˆ¶ç¼“è§£åŠŸèƒ½çš„é¥æ„Ÿå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„åŒæ¨¡å¹»è§‰è¯„ä¼°

**Date**: 2026-02-11 | **arXiv**: [2602.10799v1](http://arxiv.org/abs/2602.10799v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10799v1)

<details><summary><b>Abstract</b></summary>

Multimodal large language models (MLLMs) are increasingly adopted in remote sensing (RS) and have shown strong performance on tasks such as RS visual grounding (RSVG), RS visual question answering (RSVQA), and multimodal dialogue. However, hallucinations, which are responses inconsistent with the input RS images, severely hinder their deployment in high-stakes scenarios (e.g., emergency management and agricultural monitoring) and remain under-explored in RS. In this work, we present RSHallu, a systematic study with three deliverables: (1) we formalize RS hallucinations with an RS-oriented taxonomy and introduce image-level hallucination to capture RS-specific inconsistencies beyond object-centric errors (e.g., modality, resolution, and scene-level semantics); (2) we build a hallucination benchmark RSHalluEval (2,023 QA pairs) and enable dual-mode checking, supporting high-precision cloud auditing and low-cost reproducible local checking via a compact checker fine-tuned on RSHalluCheck dataset (15,396 QA pairs); and (3) we introduce a domain-tailored dataset RSHalluShield (30k QA pairs) for training-friendly mitigation and further propose training-free plug-and-play strategies, including decoding-time logit correction and RS-aware prompting. Across representative RS-MLLMs, our mitigation improves the hallucination-free rate by up to 21.63 percentage points under a unified protocol, while maintaining competitive performance on downstream RS tasks (RSVQA/RSVG). Code and datasets will be released.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLM) åœ¨é¥æ„Ÿ (RS) ä¸­è¶Šæ¥è¶Šå¤šåœ°é‡‡ç”¨ï¼Œå¹¶ä¸”åœ¨ RS è§†è§‰æ¥åœ° (RSVG)ã€RS è§†è§‰é—®ç­” (RSVQA) å’Œå¤šæ¨¡æ€å¯¹è¯ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¹»è§‰æ˜¯ä¸è¾“å…¥çš„é¥æ„Ÿå›¾åƒä¸ä¸€è‡´çš„å“åº”ï¼Œä¸¥é‡é˜»ç¢äº†å®ƒä»¬åœ¨é«˜é£é™©åœºæ™¯ï¼ˆä¾‹å¦‚åº”æ€¥ç®¡ç†å’Œå†œä¸šç›‘æµ‹ï¼‰ä¸­çš„éƒ¨ç½²ï¼Œå¹¶ä¸”åœ¨é¥æ„Ÿæ–¹é¢ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†RSHalluï¼Œè¿™æ˜¯ä¸€é¡¹å…·æœ‰ä¸‰ä¸ªå¯äº¤ä»˜æˆæœçš„ç³»ç»Ÿæ€§ç ”ç©¶ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬ç”¨é¢å‘RSçš„åˆ†ç±»æ³•å°†RSå¹»è§‰å½¢å¼åŒ–ï¼Œå¹¶å¼•å…¥å›¾åƒçº§å¹»è§‰æ¥æ•è·ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„é”™è¯¯ä¹‹å¤–çš„RSç‰¹å®šçš„ä¸ä¸€è‡´ï¼ˆä¾‹å¦‚æ¨¡æ€ã€åˆ†è¾¨ç‡å’Œåœºæ™¯çº§è¯­ä¹‰ï¼‰ï¼› ï¼ˆ2ï¼‰æˆ‘ä»¬å»ºç«‹äº†å¹»è§‰åŸºå‡†RSHalluEvalï¼ˆ2,023ä¸ªQAå¯¹ï¼‰å¹¶å¯ç”¨åŒæ¨¡å¼æ£€æŸ¥ï¼Œé€šè¿‡åœ¨RSHalluCheckæ•°æ®é›†ï¼ˆ15,396ä¸ªQAå¯¹ï¼‰ä¸Šå¾®è°ƒçš„ç´§å‡‘æ£€æŸ¥å™¨æ”¯æŒé«˜ç²¾åº¦äº‘å®¡è®¡å’Œä½æˆæœ¬å¯é‡å¤çš„æœ¬åœ°æ£€æŸ¥ï¼› (3)æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé¢†åŸŸå®šåˆ¶çš„æ•°æ®é›†RSHalluShieldï¼ˆ30k QAå¯¹ï¼‰ç”¨äºè®­ç»ƒå‹å¥½çš„ç¼“è§£ï¼Œå¹¶è¿›ä¸€æ­¥æå‡ºäº†å…è®­ç»ƒçš„å³æ’å³ç”¨ç­–ç•¥ï¼ŒåŒ…æ‹¬è§£ç æ—¶logitæ ¡æ­£å’ŒRSæ„ŸçŸ¥æç¤ºã€‚åœ¨ä»£è¡¨æ€§çš„ RS-MLLM ä¸­ï¼Œæˆ‘ä»¬çš„ç¼“è§£æªæ–½åœ¨ç»Ÿä¸€åè®®ä¸‹å°†æ— å¹»è§‰ç‡æé«˜äº†é«˜è¾¾ 21.63 ä¸ªç™¾åˆ†ç‚¹ï¼ŒåŒæ—¶ä¿æŒäº†ä¸‹æ¸¸ RS ä»»åŠ¡ (RSVQA/RSVG) çš„ç«äº‰æ€§èƒ½ã€‚ä»£ç å’Œæ•°æ®é›†å°†è¢«å‘å¸ƒã€‚

</details>

---

## 15. From Steering to Pedalling: Do Autonomous Driving VLMs Generalize to Cyclist-Assistive Spatial Perception and Planning?

**ä¸­æ–‡æ ‡é¢˜**: ä»è½¬å‘åˆ°è¸æ¿ï¼šè‡ªåŠ¨é©¾é©¶ VLM æ˜¯å¦å¯ä»¥æ¨å¹¿åˆ°éª‘è½¦äººè¾…åŠ©ç©ºé—´æ„ŸçŸ¥å’Œè§„åˆ’ï¼Ÿ

**Date**: 2026-02-11 | **arXiv**: [2602.10771v1](http://arxiv.org/abs/2602.10771v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10771v1)

<details><summary><b>Abstract</b></summary>

Cyclists often encounter safety-critical situations in urban traffic, highlighting the need for assistive systems that support safe and informed decision-making. Recently, vision-language models (VLMs) have demonstrated strong performance on autonomous driving benchmarks, suggesting their potential for general traffic understanding and navigation-related reasoning. However, existing evaluations are predominantly vehicle-centric and fail to assess perception and reasoning from a cyclist-centric viewpoint. To address this gap, we introduce CyclingVQA, a diagnostic benchmark designed to probe perception, spatio-temporal understanding, and traffic-rule-to-lane reasoning from a cyclist's perspective. Evaluating 31+ recent VLMs spanning general-purpose, spatially enhanced, and autonomous-driving-specialized models, we find that current models demonstrate encouraging capabilities, while also revealing clear areas for improvement in cyclist-centric perception and reasoning, particularly in interpreting cyclist-specific traffic cues and associating signs with the correct navigational lanes. Notably, several driving-specialized models underperform strong generalist VLMs, indicating limited transfer from vehicle-centric training to cyclist-assistive scenarios. Finally, through systematic error analysis, we identify recurring failure modes to guide the development of more effective cyclist-assistive intelligent systems.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

éª‘è‡ªè¡Œè½¦çš„äººç»å¸¸åœ¨åŸå¸‚äº¤é€šä¸­é‡åˆ°å®‰å…¨å±æ€¥çš„æƒ…å†µï¼Œè¿™çªå‡ºè¡¨æ˜éœ€è¦è¾…åŠ©ç³»ç»Ÿæ¥æ”¯æŒå®‰å…¨å’Œæ˜æ™ºçš„å†³ç­–ã€‚æœ€è¿‘ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨è‡ªåŠ¨é©¾é©¶åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œè¡¨æ˜å®ƒä»¬åœ¨ä¸€èˆ¬äº¤é€šç†è§£å’Œå¯¼èˆªç›¸å…³æ¨ç†æ–¹é¢çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°ä¸»è¦ä»¥è½¦è¾†ä¸ºä¸­å¿ƒï¼Œæœªèƒ½ä»ä»¥éª‘è½¦äººä¸ºä¸­å¿ƒçš„è§’åº¦è¯„ä¼°æ„ŸçŸ¥å’Œæ¨ç†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº† CyclingVQAï¼Œè¿™æ˜¯ä¸€ä¸ªè¯Šæ–­åŸºå‡†ï¼Œæ—¨åœ¨ä»éª‘è½¦äººçš„è§’åº¦æ¢è®¨æ„ŸçŸ¥ã€æ—¶ç©ºç†è§£ä»¥åŠäº¤é€šè§„åˆ™åˆ°è½¦é“æ¨ç†ã€‚é€šè¿‡è¯„ä¼°è¶…è¿‡ 31 ä¸ªæœ€æ–°çš„ VLMï¼Œæ¶µç›–é€šç”¨ã€ç©ºé—´å¢å¼ºå’Œè‡ªåŠ¨é©¾é©¶ä¸“ç”¨æ¨¡å‹ï¼Œæˆ‘ä»¬å‘ç°å½“å‰æ¨¡å‹è¡¨ç°å‡ºäº†ä»¤äººé¼“èˆçš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¹Ÿæ­ç¤ºäº†ä»¥éª‘è½¦äººä¸ºä¸­å¿ƒçš„æ„ŸçŸ¥å’Œæ¨ç†æ–¹é¢æœ‰å¾…æ”¹è¿›çš„æ˜ç¡®é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨è§£é‡Šéª‘è½¦äººç‰¹å®šçš„äº¤é€šçº¿ç´¢ä»¥åŠå°†æ ‡å¿—ä¸æ­£ç¡®çš„å¯¼èˆªè½¦é“ç›¸å…³è”æ–¹é¢ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸€äº›é©¾é©¶ä¸“ç”¨æ¨¡å‹çš„è¡¨ç°ä¸å¦‚å¼ºå¤§çš„é€šç”¨ VLMï¼Œè¿™è¡¨æ˜ä»ä»¥è½¦è¾†ä¸ºä¸­å¿ƒçš„è®­ç»ƒåˆ°éª‘è½¦äººè¾…åŠ©åœºæ™¯çš„è½¬ç§»æœ‰é™ã€‚æœ€åï¼Œé€šè¿‡ç³»ç»Ÿè¯¯å·®åˆ†æï¼Œæˆ‘ä»¬è¯†åˆ«é‡å¤å‡ºç°çš„æ•…éšœæ¨¡å¼ï¼Œä»¥æŒ‡å¯¼å¼€å‘æ›´æœ‰æ•ˆçš„éª‘è¡Œè€…è¾…åŠ©æ™ºèƒ½ç³»ç»Ÿã€‚

</details>

---

## 16. Dual-End Consistency Model

**ä¸­æ–‡æ ‡é¢˜**: åŒç«¯ä¸€è‡´æ€§æ¨¡å‹

**Date**: 2026-02-11 | **arXiv**: [2602.10764v1](http://arxiv.org/abs/2602.10764v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10764v1)

<details><summary><b>Abstract</b></summary>

The slow iterative sampling nature remains a major bottleneck for the practical deployment of diffusion and flow-based generative models. While consistency models (CMs) represent a state-of-the-art distillation-based approach for efficient generation, their large-scale application is still limited by two key issues: training instability and inflexible sampling. Existing methods seek to mitigate these problems through architectural adjustments or regularized objectives, yet overlook the critical reliance on trajectory selection. In this work, we first conduct an analysis on these two limitations: training instability originates from loss divergence induced by unstable self-supervised term, whereas sampling inflexibility arises from error accumulation. Based on these insights and analysis, we propose the Dual-End Consistency Model (DE-CM) that selects vital sub-trajectory clusters to achieve stable and effective training. DE-CM decomposes the PF-ODE trajectory and selects three critical sub-trajectories as optimization targets. Specifically, our approach leverages continuous-time CMs objectives to achieve few-step distillation and utilizes flow matching as a boundary regularizer to stabilize the training process. Furthermore, we propose a novel noise-to-noisy (N2N) mapping that can map noise to any point, thereby alleviating the error accumulation in the first step. Extensive experimental results show the effectiveness of our method: it achieves a state-of-the-art FID score of 1.70 in one-step generation on the ImageNet 256x256 dataset, outperforming existing CM-based one-step approaches.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç¼“æ…¢çš„è¿­ä»£é‡‡æ ·æ€§è´¨ä»ç„¶æ˜¯æ‰©æ•£å’ŒåŸºäºæµçš„ç”Ÿæˆæ¨¡å‹çš„å®é™…éƒ¨ç½²çš„ä¸»è¦ç“¶é¢ˆã€‚è™½ç„¶ä¸€è‡´æ€§æ¨¡å‹ï¼ˆCMï¼‰ä»£è¡¨äº†ä¸€ç§æœ€å…ˆè¿›çš„åŸºäºè’¸é¦çš„é«˜æ•ˆç”Ÿæˆæ–¹æ³•ï¼Œä½†å…¶å¤§è§„æ¨¡åº”ç”¨ä»ç„¶å—åˆ°ä¸¤ä¸ªå…³é”®é—®é¢˜çš„é™åˆ¶ï¼šè®­ç»ƒä¸ç¨³å®šå’Œé‡‡æ ·ä¸çµæ´»ã€‚ç°æœ‰æ–¹æ³•è¯•å›¾é€šè¿‡æ¶æ„è°ƒæ•´æˆ–è§„èŒƒåŒ–ç›®æ ‡æ¥ç¼“è§£è¿™äº›é—®é¢˜ï¼Œä½†å¿½è§†äº†å¯¹è½¨è¿¹é€‰æ‹©çš„å…³é”®ä¾èµ–ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹è¿™ä¸¤ä¸ªå±€é™æ€§è¿›è¡Œäº†åˆ†æï¼šè®­ç»ƒçš„ä¸ç¨³å®šæ€§æºäºä¸ç¨³å®šçš„è‡ªç›‘ç£é¡¹å¼•èµ·çš„æŸå¤±å‘æ•£ï¼Œè€Œé‡‡æ ·çš„ä¸çµæ´»æ€§åˆ™æºäºè¯¯å·®ç´¯ç§¯ã€‚åŸºäºè¿™äº›è§è§£å’Œåˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†åŒç«¯ä¸€è‡´æ€§æ¨¡å‹ï¼ˆDE-CMï¼‰ï¼Œè¯¥æ¨¡å‹é€‰æ‹©é‡è¦çš„å­è½¨è¿¹ç°‡æ¥å®ç°ç¨³å®šæœ‰æ•ˆçš„è®­ç»ƒã€‚ DE-CMåˆ†è§£PF-ODEè½¨è¿¹å¹¶é€‰æ‹©ä¸‰ä¸ªå…³é”®å­è½¨è¿¹ä½œä¸ºä¼˜åŒ–ç›®æ ‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨è¿ç»­æ—¶é—´ CM ç›®æ ‡æ¥å®ç°å‡ æ­¥è’¸é¦ï¼Œå¹¶åˆ©ç”¨æµåŒ¹é…ä½œä¸ºè¾¹ç•Œæ­£åˆ™åŒ–å™¨æ¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å™ªå£°åˆ°å™ªå£°ï¼ˆN2Nï¼‰æ˜ å°„ï¼Œå¯ä»¥å°†å™ªå£°æ˜ å°„åˆ°ä»»ä½•ç‚¹ï¼Œä»è€Œå‡è½»ç¬¬ä¸€æ­¥ä¸­çš„è¯¯å·®ç´¯ç§¯ã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼šå®ƒåœ¨ ImageNet 256x256 æ•°æ®é›†ä¸Šçš„ä¸€æ­¥ç”Ÿæˆä¸­è¾¾åˆ°äº† 1.70 çš„æœ€å…ˆè¿›çš„ FID åˆ†æ•°ï¼Œä¼˜äºç°æœ‰çš„åŸºäº CM çš„ä¸€æ­¥æ–¹æ³•ã€‚

</details>

---

## 17. Text-to-Vector Conversion for Residential Plan Design

**ä¸­æ–‡æ ‡é¢˜**: ä½å®…è§„åˆ’è®¾è®¡çš„æ–‡æœ¬åˆ°çŸ¢é‡è½¬æ¢

**Date**: 2026-02-11 | **arXiv**: [2602.10757v1](http://arxiv.org/abs/2602.10757v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10757v1)

<details><summary><b>Abstract</b></summary>

Computer graphics, comprising both raster and vector components, is a fundamental part of modern science, industry, and digital communication. While raster graphics offer ease of use, its pixel-based structure limits scalability. Vector graphics, defined by mathematical primitives, provides scalability without quality loss, however, it is more complex to produce. For design and architecture, the versatility of vector graphics is paramount, despite its computational demands. This paper introduces a novel method for generating vector residential plans from textual descriptions. Our approach surpasses existing solutions by approximately 5% in CLIPScore-based visual quality, benefiting from its inherent handling of right angles and flexible settings. Additionally, we present a new algorithm for vectorizing raster plans into structured vector images. Such images have a better CLIPscore compared to others by about 4%.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è®¡ç®—æœºå›¾å½¢å­¦ç”±å…‰æ …å’ŒçŸ¢é‡ç»„æˆï¼Œæ˜¯ç°ä»£ç§‘å­¦ã€å·¥ä¸šå’Œæ•°å­—é€šä¿¡çš„åŸºæœ¬ç»„æˆéƒ¨åˆ†ã€‚è™½ç„¶å…‰æ …å›¾å½¢æ˜“äºä½¿ç”¨ï¼Œä½†å…¶åŸºäºåƒç´ çš„ç»“æ„é™åˆ¶äº†å¯æ‰©å±•æ€§ã€‚ç”±æ•°å­¦åŸºå…ƒå®šä¹‰çš„çŸ¢é‡å›¾å½¢æä¾›äº†å¯æ‰©å±•æ€§è€Œä¸ä¼šé€ æˆè´¨é‡æŸå¤±ï¼Œä½†å…¶ç”Ÿæˆæ›´ä¸ºå¤æ‚ã€‚å¯¹äºè®¾è®¡å’Œå»ºç­‘æ¥è¯´ï¼ŒçŸ¢é‡å›¾å½¢çš„å¤šåŠŸèƒ½æ€§è‡³å…³é‡è¦ï¼Œå°½ç®¡å®ƒå…·æœ‰è®¡ç®—éœ€æ±‚ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ ¹æ®æ–‡æœ¬æè¿°ç”ŸæˆçŸ¢é‡ä½å®…è§„åˆ’çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸºäº CLIPScore çš„è§†è§‰è´¨é‡æ–¹é¢æ¯”ç°æœ‰è§£å†³æ–¹æ¡ˆé«˜å‡ºçº¦ 5%ï¼Œè¿™å¾—ç›Šäºå…¶å›ºæœ‰çš„ç›´è§’å¤„ç†å’Œçµæ´»çš„è®¾ç½®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å°†å…‰æ …è®¡åˆ’çŸ¢é‡åŒ–ä¸ºç»“æ„åŒ–çŸ¢é‡å›¾åƒçš„æ–°ç®—æ³•ã€‚ä¸å…¶ä»–å›¾åƒç›¸æ¯”ï¼Œæ­¤ç±»å›¾åƒçš„ CLIPscore é«˜å‡ºçº¦ 4%ã€‚

</details>

---

## 18. Self-Supervised Image Super-Resolution Quality Assessment based on Content-Free Multi-Model Oriented Representation Learning

**ä¸­æ–‡æ ‡é¢˜**: åŸºäºå†…å®¹æ— å…³çš„é¢å‘å¤šæ¨¡å‹è¡¨ç¤ºå­¦ä¹ çš„è‡ªç›‘ç£å›¾åƒè¶…åˆ†è¾¨ç‡è´¨é‡è¯„ä¼°

**Date**: 2026-02-11 | **arXiv**: [2602.10744v1](http://arxiv.org/abs/2602.10744v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10744v1)

<details><summary><b>Abstract</b></summary>

Super-resolution (SR) applied to real-world low-resolution (LR) images often results in complex, irregular degradations that stem from the inherent complexity of natural scene acquisition. In contrast to SR artifacts arising from synthetic LR images created under well-defined scenarios, those distortions are highly unpredictable and vary significantly across different real-life contexts. Consequently, assessing the quality of SR images (SR-IQA) obtained from realistic LR, remains a challenging and underexplored problem. In this work, we introduce a no-reference SR-IQA approach tailored for such highly ill-posed realistic settings. The proposed method enables domain-adaptive IQA for real-world SR applications, particularly in data-scarce domains. We hypothesize that degradations in super-resolved images are strongly dependent on the underlying SR algorithms, rather than being solely determined by image content. To this end, we introduce a self-supervised learning (SSL) strategy that first pretrains multiple SR model oriented representations in a pretext stage. Our contrastive learning framework forms positive pairs from images produced by the same SR model and negative pairs from those generated by different methods, independent of image content. The proposed approach S3 RIQA, further incorporates targeted preprocessing to extract complementary quality information and an auxiliary task to better handle the various degradation profiles associated with different SR scaling factors. To this end, we constructed a new dataset, SRMORSS, to support unsupervised pretext training; it includes a wide range of SR algorithms applied to numerous real LR images, which addresses a gap in existing datasets. Experiments on real SR-IQA benchmarks demonstrate that S3 RIQA consistently outperforms most state-of-the-art relevant metrics.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å°†è¶…åˆ†è¾¨ç‡ (SR) åº”ç”¨äºç°å®ä¸–ç•Œçš„ä½åˆ†è¾¨ç‡ (LR) å›¾åƒé€šå¸¸ä¼šå¯¼è‡´å¤æ‚çš„ã€ä¸è§„åˆ™çš„é™çº§ï¼Œè¿™æ˜¯ç”±äºè‡ªç„¶åœºæ™¯é‡‡é›†çš„å›ºæœ‰å¤æ‚æ€§é€ æˆçš„ã€‚ä¸åœ¨æ˜ç¡®åœºæ™¯ä¸‹åˆ›å»ºçš„åˆæˆ LR å›¾åƒäº§ç”Ÿçš„ SR ä¼ªåƒç›¸æ¯”ï¼Œè¿™äº›å¤±çœŸæ˜¯é«˜åº¦ä¸å¯é¢„æµ‹çš„ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„ç°å®ç”Ÿæ´»ç¯å¢ƒä¸­å­˜åœ¨æ˜¾ç€å·®å¼‚ã€‚å› æ­¤ï¼Œè¯„ä¼°ä»ç°å® LR è·å¾—çš„ SR å›¾åƒ (SR-IQA) çš„è´¨é‡ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§ä¸”å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é’ˆå¯¹è¿™ç§é«˜åº¦ä¸é€‚å®šçš„ç°å®ç¯å¢ƒé‡èº«å®šåˆ¶çš„æ— å‚è€ƒ SR-IQA æ–¹æ³•ã€‚æ‰€æå‡ºçš„æ–¹æ³•èƒ½å¤Ÿä¸ºç°å®ä¸–ç•Œçš„ SR åº”ç”¨æä¾›åŸŸè‡ªé€‚åº” IQAï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºé¢†åŸŸã€‚æˆ‘ä»¬å‡è®¾è¶…åˆ†è¾¨ç‡å›¾åƒçš„é€€åŒ–å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºåº•å±‚çš„ SR ç®—æ³•ï¼Œè€Œä¸æ˜¯ä»…ä»…ç”±å›¾åƒå†…å®¹å†³å®šã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é¦–å…ˆåœ¨å€Ÿå£é˜¶æ®µé¢„è®­ç»ƒå¤šä¸ªé¢å‘ SR æ¨¡å‹çš„è¡¨ç¤ºã€‚æˆ‘ä»¬çš„å¯¹æ¯”å­¦ä¹ æ¡†æ¶ä»ç›¸åŒ SR æ¨¡å‹ç”Ÿæˆçš„å›¾åƒä¸­å½¢æˆæ­£å¯¹ï¼Œä»ä¸åŒæ–¹æ³•ç”Ÿæˆçš„å›¾åƒä¸­å½¢æˆè´Ÿå¯¹ï¼Œä¸å›¾åƒå†…å®¹æ— å…³ã€‚æ‰€æå‡ºçš„æ–¹æ³• S3 RIQA è¿›ä¸€æ­¥ç»“åˆäº†æœ‰é’ˆå¯¹æ€§çš„é¢„å¤„ç†ä»¥æå–è¡¥å……è´¨é‡ä¿¡æ¯å’Œè¾…åŠ©ä»»åŠ¡ä»¥æ›´å¥½åœ°å¤„ç†ä¸ä¸åŒ SR ç¼©æ”¾å› å­ç›¸å…³çš„å„ç§é€€åŒ–æ¦‚å†µã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†SRMORSSï¼Œä»¥æ”¯æŒæ— ç›‘ç£å€Ÿå£è®­ç»ƒï¼›å®ƒåŒ…æ‹¬åº”ç”¨äºå¤§é‡çœŸå® LR å›¾åƒçš„å¹¿æ³› SR ç®—æ³•ï¼Œå¼¥è¡¥äº†ç°æœ‰æ•°æ®é›†çš„ç©ºç™½ã€‚å¯¹çœŸå® SR-IQA åŸºå‡†æµ‹è¯•çš„å®éªŒè¡¨æ˜ï¼ŒS3 RIQA å§‹ç»ˆä¼˜äºå¤§å¤šæ•°æœ€å…ˆè¿›çš„ç›¸å…³æŒ‡æ ‡ã€‚

</details>

---

## 19. OccFace: Unified Occlusion-Aware Facial Landmark Detection with Per-Point Visibility

**ä¸­æ–‡æ ‡é¢˜**: OccFaceï¼šå…·æœ‰æ¯ç‚¹å¯è§æ€§çš„ç»Ÿä¸€é®æŒ¡æ„ŸçŸ¥é¢éƒ¨æ ‡å¿—æ£€æµ‹

**Date**: 2026-02-11 | **arXiv**: [2602.10728v1](http://arxiv.org/abs/2602.10728v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10728v1)

<details><summary><b>Abstract</b></summary>

Accurate facial landmark detection under occlusion remains challenging, especially for human-like faces with large appearance variation and rotation-driven self-occlusion. Existing detectors typically localize landmarks while handling occlusion implicitly, without predicting per-point visibility that downstream applications can benefits. We present OccFace, an occlusion-aware framework for universal human-like faces, including humans, stylized characters, and other non-human designs. OccFace adopts a unified dense 100-point layout and a heatmap-based backbone, and adds an occlusion module that jointly predicts landmark coordinates and per-point visibility by combining local evidence with cross-landmark context. Visibility supervision mixes manual labels with landmark-aware masking that derives pseudo visibility from mask-heatmap overlap. We also create an occlusion-aware evaluation suite reporting NME on visible vs. occluded landmarks and benchmarking visibility with Occ AP, F1@0.5, and ROC-AUC, together with a dataset annotated with 100-point landmarks and per-point visibility. Experiments show improved robustness under external occlusion and large head rotations, especially on occluded regions, while preserving accuracy on visible landmarks.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

é®æŒ¡ä¸‹å‡†ç¡®çš„é¢éƒ¨æ ‡å¿—æ£€æµ‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå…·æœ‰è¾ƒå¤§å¤–è§‚å˜åŒ–å’Œæ—‹è½¬é©±åŠ¨çš„è‡ªé®æŒ¡çš„ç±»äººé¢éƒ¨ã€‚ç°æœ‰çš„æ£€æµ‹å™¨é€šå¸¸ä¼šåœ¨éšå¼å¤„ç†é®æŒ¡çš„åŒæ—¶å®šä½åœ°æ ‡ï¼Œè€Œä¸ä¼šé¢„æµ‹ä¸‹æ¸¸åº”ç”¨ç¨‹åºå¯ä»¥å—ç›Šçš„æ¯ç‚¹å¯è§æ€§ã€‚æˆ‘ä»¬æ¨å‡ºäº† OccFaceï¼Œè¿™æ˜¯ä¸€ç§é®æŒ¡æ„ŸçŸ¥æ¡†æ¶ï¼Œé€‚ç”¨äºé€šç”¨çš„ç±»äººé¢å­”ï¼ŒåŒ…æ‹¬äººç±»ã€é£æ ¼åŒ–è§’è‰²å’Œå…¶ä»–éäººç±»è®¾è®¡ã€‚ OccFaceé‡‡ç”¨ç»Ÿä¸€çš„å¯†é›†100ç‚¹å¸ƒå±€å’ŒåŸºäºçƒ­å›¾çš„ä¸»å¹²ç½‘ï¼Œå¹¶æ·»åŠ äº†é®æŒ¡æ¨¡å—ï¼Œé€šè¿‡ç»“åˆå±€éƒ¨è¯æ®å’Œè·¨åœ°æ ‡ä¸Šä¸‹æ–‡æ¥è”åˆé¢„æµ‹åœ°æ ‡åæ ‡å’Œæ¯ç‚¹å¯è§æ€§ã€‚å¯è§æ€§ç›‘ç£å°†æ‰‹åŠ¨æ ‡ç­¾ä¸åœ°æ ‡æ„ŸçŸ¥æ©è”½ç›¸ç»“åˆï¼Œä»æ©ç çƒ­å›¾é‡å ä¸­å¾—å‡ºä¼ªå¯è§æ€§ã€‚æˆ‘ä»¬è¿˜åˆ›å»ºäº†ä¸€ä¸ªé®æŒ¡æ„ŸçŸ¥è¯„ä¼°å¥—ä»¶ï¼ŒæŠ¥å‘Šå¯è§ä¸é®æŒ¡åœ°æ ‡çš„ NME ä»¥åŠä½¿ç”¨ Occ APã€F1@0.5 å’Œ ROC-AUC çš„åŸºå‡†å¯è§æ€§ï¼Œä»¥åŠç”¨ 100 ç‚¹åœ°æ ‡å’Œæ¯ç‚¹å¯è§æ€§æ³¨é‡Šçš„æ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å¤–éƒ¨é®æŒ¡å’Œå¤§çš„å¤´éƒ¨æ—‹è½¬ä¸‹ï¼Œå°¤å…¶æ˜¯åœ¨é®æŒ¡åŒºåŸŸï¼Œé²æ£’æ€§å¾—åˆ°äº†æé«˜ï¼ŒåŒæ—¶ä¿æŒäº†å¯è§åœ°æ ‡çš„å‡†ç¡®æ€§ã€‚

</details>

---

## 20. From Representational Complementarity to Dual Systems: Synergizing VLM and Vision-Only Backbones for End-to-End Driving

**ä¸­æ–‡æ ‡é¢˜**: ä»ä»£è¡¨æ€§äº’è¡¥åˆ°åŒç³»ç»Ÿï¼šååŒ VLM å’Œçº¯è§†è§‰éª¨å¹²ç½‘å®ç°ç«¯åˆ°ç«¯é©¾é©¶

**Date**: 2026-02-11 | **arXiv**: [2602.10719v1](http://arxiv.org/abs/2602.10719v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10719v1)

<details><summary><b>Abstract</b></summary>

Vision-Language-Action (VLA) driving augments end-to-end (E2E) planning with language-enabled backbones, yet it remains unclear what changes beyond the usual accuracy--cost trade-off. We revisit this question with 3--RQ analysis in RecogDrive by instantiating the system with a full VLM and vision-only backbones, all under an identical diffusion Transformer planner. RQ1: At the backbone level, the VLM can introduce additional subspaces upon the vision-only backbones. RQ2: This unique subspace leads to a different behavioral in some long-tail scenario: the VLM tends to be more aggressive whereas ViT is more conservative, and each decisively wins on about 2--3% of test scenarios; With an oracle that selects, per scenario, the better trajectory between the VLM and ViT branches, we obtain an upper bound of 93.58 PDMS. RQ3: To fully harness this observation, we propose HybridDriveVLA, which runs both ViT and VLM branches and selects between their endpoint trajectories using a learned scorer, improving PDMS to 92.10. Finally, DualDriveVLA implements a practical fast--slow policy: it runs ViT by default and invokes the VLM only when the scorer's confidence falls below a threshold; calling the VLM on 15% of scenarios achieves 91.00 PDMS while improving throughput by 3.2x. Code will be released.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ (VLA) é©±åŠ¨é€šè¿‡æ”¯æŒè¯­è¨€çš„ä¸»å¹²å¢å¼ºäº†ç«¯åˆ°ç«¯ (E2E) è§„åˆ’ï¼Œä½†ç›®å‰å°šä¸æ¸…æ¥šé™¤äº†é€šå¸¸çš„å‡†ç¡®æ€§å’Œæˆæœ¬æƒè¡¡ä¹‹å¤–è¿˜æœ‰å“ªäº›å˜åŒ–ã€‚æˆ‘ä»¬é€šè¿‡ RecogDrive ä¸­çš„ 3--RQ åˆ†æé‡æ–°å®¡è§†è¿™ä¸ªé—®é¢˜ï¼Œé€šè¿‡ä½¿ç”¨å®Œæ•´çš„ VLM å’Œä»…è§†è§‰ä¸»å¹²æ¥å®ä¾‹åŒ–ç³»ç»Ÿï¼Œæ‰€æœ‰è¿™äº›éƒ½åœ¨ç›¸åŒçš„æ‰©æ•£ Transformer è§„åˆ’å™¨ä¸‹è¿›è¡Œã€‚ RQ1ï¼šåœ¨ä¸»å¹²å±‚ï¼ŒVLM å¯ä»¥åœ¨ä»…è§†è§‰ä¸»å¹²ä¸Šå¼•å…¥é¢å¤–çš„å­ç©ºé—´ã€‚ RQ2ï¼šè¿™ä¸ªç‹¬ç‰¹çš„å­ç©ºé—´å¯¼è‡´åœ¨æŸäº›é•¿å°¾åœºæ™¯ä¸­å‡ºç°ä¸åŒçš„è¡Œä¸ºï¼šVLM å¾€å¾€æ›´æ¿€è¿›ï¼Œè€Œ ViT æ›´ä¿å®ˆï¼Œå¹¶ä¸”æ¯ä¸ªéƒ½åœ¨å¤§çº¦ 2--3% çš„æµ‹è¯•åœºæ™¯ä¸­å†³å®šæ€§åœ°è·èƒœï¼›é€šè¿‡æ ¹æ®åœºæ™¯é€‰æ‹© VLM å’Œ ViT åˆ†æ”¯ä¹‹é—´æ›´å¥½çš„è½¨è¿¹çš„é¢„è¨€æœºï¼Œæˆ‘ä»¬è·å¾—äº† 93.58 PDMS çš„ä¸Šé™ã€‚ RQ3ï¼šä¸ºäº†å……åˆ†åˆ©ç”¨è¿™ä¸€è§‚å¯Ÿç»“æœï¼Œæˆ‘ä»¬æå‡ºäº† HybridDriveVLAï¼Œå®ƒè¿è¡Œ ViT å’Œ VLM åˆ†æ”¯ï¼Œå¹¶ä½¿ç”¨å­¦ä¹ è¯„åˆ†å™¨åœ¨å®ƒä»¬çš„ç«¯ç‚¹è½¨è¿¹ä¹‹é—´è¿›è¡Œé€‰æ‹©ï¼Œå°† PDMS æé«˜åˆ° 92.10ã€‚æœ€åï¼ŒDualDriveVLA å®ç°äº†å®ç”¨çš„å¿«-æ…¢ç­–ç•¥ï¼šå®ƒé»˜è®¤è¿è¡Œ ViTï¼Œä»…å½“è¯„åˆ†è€…çš„ç½®ä¿¡åº¦ä½äºé˜ˆå€¼æ—¶æ‰è°ƒç”¨ VLMï¼›åœ¨ 15% çš„åœºæ™¯ä¸­è°ƒç”¨ VLM å¯å®ç° 91.00 PDMSï¼ŒåŒæ—¶å°†ååé‡æé«˜ 3.2 å€ã€‚ä»£ç å°†è¢«å‘å¸ƒã€‚

</details>

---

## 21. FGAA-FPN: Foreground-Guided Angle-Aware Feature Pyramid Network for Oriented Object Detection

**ä¸­æ–‡æ ‡é¢˜**: FGAA-FPNï¼šç”¨äºå®šå‘ç‰©ä½“æ£€æµ‹çš„å‰æ™¯å¼•å¯¼è§’åº¦æ„ŸçŸ¥ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œ

**Date**: 2026-02-11 | **arXiv**: [2602.10710v1](http://arxiv.org/abs/2602.10710v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10710v1)

<details><summary><b>Abstract</b></summary>

With the increasing availability of high-resolution remote sensing and aerial imagery, oriented object detection has become a key capability for geographic information updating, maritime surveillance, and disaster response. However, it remains challenging due to cluttered backgrounds, severe scale variation, and large orientation changes. Existing approaches largely improve performance through multi-scale feature fusion with feature pyramid networks or contextual modeling with attention, but they often lack explicit foreground modeling and do not leverage geometric orientation priors, which limits feature discriminability. To overcome these limitations, we propose FGAA-FPN, a Foreground-Guided Angle-Aware Feature Pyramid Network for oriented object detection. FGAA-FPN is built on a hierarchical functional decomposition that accounts for the distinct spatial resolution and semantic abstraction across pyramid levels, thereby strengthening multi-scale representations. Concretely, a Foreground-Guided Feature Modulation module learns foreground saliency under weak supervision to enhance object regions and suppress background interference in low-level features. In parallel, an Angle-Aware Multi-Head Attention module encodes relative orientation relationships to guide global interactions among high-level semantic features. Extensive experiments on DOTA v1.0 and DOTA v1.5 demonstrate that FGAA-FPN achieves state-of-the-art results, reaching 75.5% and 68.3% mAP, respectively.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

éšç€é«˜åˆ†è¾¨ç‡é¥æ„Ÿå’Œèˆªç©ºå›¾åƒçš„æ—¥ç›Šæ™®åŠï¼Œå®šå‘ç›®æ ‡æ£€æµ‹å·²æˆä¸ºåœ°ç†ä¿¡æ¯æ›´æ–°ã€æµ·ä¸Šç›‘è§†å’Œç¾å®³å“åº”çš„å…³é”®èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºæ‚ä¹±çš„èƒŒæ™¯ã€ä¸¥é‡çš„å°ºåº¦å˜åŒ–å’Œå¤§çš„æ–¹å‘å˜åŒ–ï¼Œå®ƒä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œçš„å¤šå°ºåº¦ç‰¹å¾èåˆæˆ–å¸¦æœ‰æ³¨æ„åŠ›çš„ä¸Šä¸‹æ–‡å»ºæ¨¡åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæé«˜äº†æ€§èƒ½ï¼Œä½†å®ƒä»¬é€šå¸¸ç¼ºä¹æ˜ç¡®çš„å‰æ™¯å»ºæ¨¡å¹¶ä¸”ä¸åˆ©ç”¨å‡ ä½•æ–¹å‘å…ˆéªŒï¼Œè¿™é™åˆ¶äº†ç‰¹å¾çš„å¯è¾¨åˆ«æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº† FGAA-FPNï¼Œä¸€ç§ç”¨äºå®šå‘å¯¹è±¡æ£€æµ‹çš„å‰æ™¯å¼•å¯¼è§’åº¦æ„ŸçŸ¥ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œã€‚ FGAA-FPN å»ºç«‹åœ¨åˆ†å±‚åŠŸèƒ½åˆ†è§£çš„åŸºç¡€ä¸Šï¼Œè¯¥åˆ†è§£è€ƒè™‘äº†è·¨é‡‘å­—å¡”çº§åˆ«çš„ä¸åŒç©ºé—´åˆ†è¾¨ç‡å’Œè¯­ä¹‰æŠ½è±¡ï¼Œä»è€ŒåŠ å¼ºäº†å¤šå°ºåº¦è¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œå‰æ™¯å¼•å¯¼ç‰¹å¾è°ƒåˆ¶æ¨¡å—åœ¨å¼±ç›‘ç£ä¸‹å­¦ä¹ å‰æ™¯æ˜¾ç€æ€§ï¼Œä»¥å¢å¼ºå¯¹è±¡åŒºåŸŸå¹¶æŠ‘åˆ¶ä½çº§ç‰¹å¾ä¸­çš„èƒŒæ™¯å¹²æ‰°ã€‚åŒæ—¶ï¼Œè§’åº¦æ„ŸçŸ¥å¤šå¤´æ³¨æ„åŠ›æ¨¡å—å¯¹ç›¸å¯¹æ–¹å‘å…³ç³»è¿›è¡Œç¼–ç ï¼Œä»¥æŒ‡å¯¼é«˜çº§è¯­ä¹‰ç‰¹å¾ä¹‹é—´çš„å…¨å±€äº¤äº’ã€‚åœ¨ DOTA v1.0 å’Œ DOTA v1.5 ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFGAA-FPN å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œåˆ†åˆ«è¾¾åˆ° 75.5% å’Œ 68.3% mAPã€‚

</details>

---

## 22. AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models

**ä¸­æ–‡æ ‡é¢˜**: AugVLA-3Dï¼šè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„æ·±åº¦é©±åŠ¨ç‰¹å¾å¢å¼º

**Date**: 2026-02-11 | **arXiv**: [2602.10698v1](http://arxiv.org/abs/2602.10698v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10698v1)

<details><summary><b>Abstract</b></summary>

Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic perception and control, yet most existing approaches primarily rely on VLM trained using 2D images, which limits their spatial understanding and action grounding in complex 3D environments. To address this limitation, we propose a novel framework that integrates depth estimation into VLA models to enrich 3D feature representations. Specifically, we employ a depth estimation baseline called VGGT to extract geometry-aware 3D cues from standard RGB inputs, enabling efficient utilization of existing large-scale 2D datasets while implicitly recovering 3D structural information. To further enhance the reliability of these depth-derived features, we introduce a new module called action assistant, which constrains the learned 3D representations with action priors and ensures their consistency with downstream control tasks. By fusing the enhanced 3D features with conventional 2D visual tokens, our approach significantly improves the generalization ability and robustness of VLA models. Experimental results demonstrate that the proposed method not only strengthens perception in geometrically ambiguous scenarios but also leads to superior action prediction accuracy. This work highlights the potential of depth-driven data augmentation and auxiliary expert supervision for bridging the gap between 2D observations and 3D-aware decision-making in robotic systems.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è§†è§‰-è¯­è¨€-åŠ¨ä½œ (VLA) æ¨¡å‹æœ€è¿‘åœ¨æœºå™¨äººæ„ŸçŸ¥å’Œæ§åˆ¶æ–¹é¢å–å¾—äº†æ˜¾ç€è¿›å±•ï¼Œä½†å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºä½¿ç”¨ 2D å›¾åƒè®­ç»ƒçš„ VLMï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å¤æ‚ 3D ç¯å¢ƒä¸­çš„ç©ºé—´ç†è§£å’ŒåŠ¨ä½œåŸºç¡€ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œå°†æ·±åº¦ä¼°è®¡é›†æˆåˆ° VLA æ¨¡å‹ä¸­ä»¥ä¸°å¯Œ 3D ç‰¹å¾è¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨ç§°ä¸º VGGT çš„æ·±åº¦ä¼°è®¡åŸºçº¿ä»æ ‡å‡† RGB è¾“å…¥ä¸­æå–å‡ ä½•æ„ŸçŸ¥çš„ 3D çº¿ç´¢ï¼Œä»è€Œèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨ç°æœ‰çš„å¤§è§„æ¨¡ 2D æ•°æ®é›†ï¼ŒåŒæ—¶éšå¼æ¢å¤ 3D ç»“æ„ä¿¡æ¯ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºè¿™äº›æ·±åº¦è¡ç”Ÿç‰¹å¾çš„å¯é æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåä¸ºåŠ¨ä½œåŠ©æ‰‹çš„æ–°æ¨¡å—ï¼Œå®ƒç”¨åŠ¨ä½œå…ˆéªŒæ¥çº¦æŸå­¦ä¹ åˆ°çš„ 3D è¡¨ç¤ºï¼Œå¹¶ç¡®ä¿å®ƒä»¬ä¸ä¸‹æ¸¸æ§åˆ¶ä»»åŠ¡çš„ä¸€è‡´æ€§ã€‚é€šè¿‡å°†å¢å¼ºçš„ 3D ç‰¹å¾ä¸ä¼ ç»Ÿçš„ 2D è§†è§‰æ ‡è®°èåˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾ç€æé«˜äº† VLA æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¸ä»…å¢å¼ºäº†å‡ ä½•æ¨¡ç³Šåœºæ™¯ä¸­çš„æ„ŸçŸ¥ï¼Œè€Œä¸”è¿˜å¸¦æ¥äº†å“è¶Šçš„åŠ¨ä½œé¢„æµ‹ç²¾åº¦ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†æ·±åº¦é©±åŠ¨çš„æ•°æ®å¢å¼ºå’Œè¾…åŠ©ä¸“å®¶ç›‘ç£åœ¨ç¼©å°æœºå™¨äººç³»ç»Ÿä¸­ 2D è§‚å¯Ÿå’Œ 3D æ„ŸçŸ¥å†³ç­–ä¹‹é—´å·®è·çš„æ½œåŠ›ã€‚

</details>

---

## 23. OmniVL-Guard: Towards Unified Vision-Language Forgery Detection and Grounding via Balanced RL

**ä¸­æ–‡æ ‡é¢˜**: OmniVL-Guardï¼šé€šè¿‡å¹³è¡¡ RL å®ç°ç»Ÿä¸€è§†è§‰è¯­è¨€ä¼ªé€ æ£€æµ‹å’Œæ¥åœ°

**Date**: 2026-02-11 | **arXiv**: [2602.10687v1](http://arxiv.org/abs/2602.10687v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10687v1)

<details><summary><b>Abstract</b></summary>

Existing forgery detection methods are often limited to uni-modal or bi-modal settings, failing to handle the interleaved text, images, and videos prevalent in real-world misinformation. To bridge this gap, this paper targets to develop a unified framework for omnibus vision-language forgery detection and grounding. In this unified setting, the {interplay} between diverse modalities and the dual requirements of simultaneous detection and localization pose a critical ``difficulty bias`` problem: the simpler veracity classification task tends to dominate the gradients, leading to suboptimal performance in fine-grained grounding during multi-task optimization. To address this challenge, we propose \textbf{OmniVL-Guard}, a balanced reinforcement learning framework for omnibus vision-language forgery detection and grounding. Particularly, OmniVL-Guard comprises two core designs: Self-Evolving CoT Generatio and Adaptive Reward Scaling Policy Optimization (ARSPO). {Self-Evolving CoT Generation} synthesizes high-quality reasoning paths, effectively overcoming the cold-start challenge. Building upon this, {Adaptive Reward Scaling Policy Optimization (ARSPO)} dynamically modulates reward scales and task weights, ensuring a balanced joint optimization. Extensive experiments demonstrate that OmniVL-Guard significantly outperforms state-of-the-art methods and exhibits zero-shot robust generalization across out-of-domain scenarios.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç°æœ‰çš„ä¼ªé€ æ£€æµ‹æ–¹æ³•é€šå¸¸ä»…é™äºå•æ¨¡æ€æˆ–åŒæ¨¡æ€è®¾ç½®ï¼Œæ— æ³•å¤„ç†ç°å®ä¸–ç•Œé”™è¯¯ä¿¡æ¯ä¸­æ™®éå­˜åœ¨çš„äº¤é”™æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡çš„ç›®æ ‡æ˜¯å¼€å‘ä¸€ä¸ªç”¨äºç»¼åˆè§†è§‰è¯­è¨€ä¼ªé€ æ£€æµ‹å’ŒåŸºç¡€çš„ç»Ÿä¸€æ¡†æ¶ã€‚åœ¨è¿™ç§ç»Ÿä¸€çš„è®¾ç½®ä¸­ï¼Œä¸åŒæ¨¡æ€ä¹‹é—´çš„ç›¸äº’ä½œç”¨ä»¥åŠåŒæ—¶æ£€æµ‹å’Œå®šä½çš„åŒé‡è¦æ±‚æå‡ºäº†ä¸€ä¸ªå…³é”®çš„â€œéš¾åº¦åå·®â€é—®é¢˜ï¼šæ›´ç®€å•çš„å‡†ç¡®æ€§åˆ†ç±»ä»»åŠ¡å¾€å¾€ä¼šä¸»å¯¼æ¢¯åº¦ï¼Œå¯¼è‡´å¤šä»»åŠ¡ä¼˜åŒ–æœŸé—´ç»†ç²’åº¦åŸºç¡€çš„æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº† \textbf{OmniVL-Guard}ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç»¼åˆè§†è§‰è¯­è¨€ä¼ªé€ æ£€æµ‹å’ŒåŸºç¡€çš„å¹³è¡¡å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚ç‰¹åˆ«æ˜¯ï¼ŒOmniVL-GuardåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒè®¾è®¡ï¼šè‡ªæˆ‘è¿›åŒ–CoTç”Ÿæˆå’Œè‡ªé€‚åº”å¥–åŠ±ç¼©æ”¾ç­–ç•¥ä¼˜åŒ–ï¼ˆARSPOï¼‰ã€‚ {Self-Evolving CoT Generation}ç»¼åˆé«˜è´¨é‡æ¨ç†è·¯å¾„ï¼Œæœ‰æ•ˆå…‹æœå†·å¯åŠ¨æŒ‘æˆ˜ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œ{è‡ªé€‚åº”å¥–åŠ±ç¼©æ”¾ç­–ç•¥ä¼˜åŒ–ï¼ˆARSPOï¼‰}åŠ¨æ€è°ƒæ•´å¥–åŠ±è§„æ¨¡å’Œä»»åŠ¡æƒé‡ï¼Œç¡®ä¿å¹³è¡¡çš„è”åˆä¼˜åŒ–ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒOmniVL-Guard çš„æ€§èƒ½æ˜¾ç€ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶åœ¨åŸŸå¤–åœºæ™¯ä¸­å±•ç°å‡ºé›¶æ ·æœ¬çš„é²æ£’æ³›åŒ–èƒ½åŠ›ã€‚

</details>

---

## 24. TwiFF (Think With Future Frames): A Large-Scale Dataset for Dynamic Visual Reasoning

**ä¸­æ–‡æ ‡é¢˜**: TwiFFï¼ˆæ€è€ƒæœªæ¥æ¡†æ¶ï¼‰ï¼šç”¨äºåŠ¨æ€è§†è§‰æ¨ç†çš„å¤§è§„æ¨¡æ•°æ®é›†

**Date**: 2026-02-11 | **arXiv**: [2602.10675v1](http://arxiv.org/abs/2602.10675v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10675v1)

**Code**: https://github.com/LiuJunhua02/TwiFF.

<details><summary><b>Abstract</b></summary>

Visual Chain-of-Thought (VCoT) has emerged as a promising paradigm for enhancing multimodal reasoning by integrating visual perception into intermediate reasoning steps. However, existing VCoT approaches are largely confined to static scenarios and struggle to capture the temporal dynamics essential for tasks such as instruction, prediction, and camera motion. To bridge this gap, we propose TwiFF-2.7M, the first large-scale, temporally grounded VCoT dataset derived from $2.7$ million video clips, explicitly designed for dynamic visual question and answer. Accompanying this, we introduce TwiFF-Bench, a high-quality evaluation benchmark of $1,078$ samples that assesses both the plausibility of reasoning trajectories and the correctness of final answers in open-ended dynamic settings. Building on these foundations, we propose the TwiFF model, a unified modal that synergistically leverages pre-trained video generation and image comprehension capabilities to produce temporally coherent visual reasoning cues-iteratively generating future action frames and textual reasoning. Extensive experiments demonstrate that TwiFF significantly outperforms existing VCoT methods and Textual Chain-of-Thought baselines on dynamic reasoning tasks, which fully validates the effectiveness for visual question answering in dynamic scenarios. Our code and data is available at https://github.com/LiuJunhua02/TwiFF.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è§†è§‰æ€ç»´é“¾ï¼ˆVCoTï¼‰å·²æˆä¸ºä¸€ç§æœ‰å‰é€”çš„èŒƒå¼ï¼Œé€šè¿‡å°†è§†è§‰æ„ŸçŸ¥é›†æˆåˆ°ä¸­é—´æ¨ç†æ­¥éª¤æ¥å¢å¼ºå¤šæ¨¡æ€æ¨ç†ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ VCoT æ–¹æ³•ä¸»è¦å±€é™äºé™æ€åœºæ™¯ï¼Œéš¾ä»¥æ•æ‰æŒ‡ä»¤ã€é¢„æµ‹å’Œç›¸æœºè¿åŠ¨ç­‰ä»»åŠ¡æ‰€å¿…éœ€çš„æ—¶é—´åŠ¨æ€ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº† TwiFF-2.7Mï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡ã€åŸºäºæ—¶é—´çš„ VCoT æ•°æ®é›†ï¼Œæºè‡ªä»·å€¼ 270 ä¸‡ç¾å…ƒçš„è§†é¢‘å‰ªè¾‘ï¼Œä¸“é—¨ä¸ºåŠ¨æ€è§†è§‰é—®ç­”è€Œè®¾è®¡ã€‚ä¸æ­¤åŒæ—¶ï¼Œæˆ‘ä»¬æ¨å‡ºäº† TwiFF-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« 1,078 ç¾å…ƒæ ·æœ¬çš„é«˜è´¨é‡è¯„ä¼°åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¼€æ”¾å¼åŠ¨æ€è®¾ç½®ä¸­æ¨ç†è½¨è¿¹çš„åˆç†æ€§å’Œæœ€ç»ˆç­”æ¡ˆçš„æ­£ç¡®æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº† TwiFF æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€æ¨¡å¼ï¼ŒååŒåˆ©ç”¨é¢„å…ˆè®­ç»ƒçš„è§†é¢‘ç”Ÿæˆå’Œå›¾åƒç†è§£åŠŸèƒ½æ¥äº§ç”Ÿæ—¶é—´è¿è´¯çš„è§†è§‰æ¨ç†çº¿ç´¢ï¼Œè¿­ä»£åœ°ç”Ÿæˆæœªæ¥çš„åŠ¨ä½œæ¡†æ¶å’Œæ–‡æœ¬æ¨ç†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTwiFF åœ¨åŠ¨æ€æ¨ç†ä»»åŠ¡ä¸Šæ˜¾ç€ä¼˜äºç°æœ‰çš„ VCoT æ–¹æ³•å’Œæ–‡æœ¬æ€ç»´é“¾åŸºçº¿ï¼Œå……åˆ†éªŒè¯äº†åŠ¨æ€åœºæ™¯ä¸‹è§†è§‰é—®ç­”çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯åœ¨ https://github.com/LiuJunhua02/TwiFF è·å–ã€‚

</details>

---

## 25. Multimodal Priors-Augmented Text-Driven 3D Human-Object Interaction Generation

**ä¸­æ–‡æ ‡é¢˜**: å¤šæ¨¡æ€å…ˆéªŒå¢å¼ºæ–‡æœ¬é©±åŠ¨ 3D äººæœºäº¤äº’ç”Ÿæˆ

**Date**: 2026-02-11 | **arXiv**: [2602.10659v1](http://arxiv.org/abs/2602.10659v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10659v1)

<details><summary><b>Abstract</b></summary>

We address the challenging task of text-driven 3D human-object interaction (HOI) motion generation. Existing methods primarily rely on a direct text-to-HOI mapping, which suffers from three key limitations due to the significant cross-modality gap: (Q1) sub-optimal human motion, (Q2) unnatural object motion, and (Q3) weak interaction between humans and objects. To address these challenges, we propose MP-HOI, a novel framework grounded in four core insights: (1) Multimodal Data Priors: We leverage multimodal data (text, image, pose/object) from large multimodal models as priors to guide HOI generation, which tackles Q1 and Q2 in data modeling. (2) Enhanced Object Representation: We improve existing object representations by incorporating geometric keypoints, contact features, and dynamic properties, enabling expressive object representations, which tackles Q2 in data representation. (3) Multimodal-Aware Mixture-of-Experts (MoE) Model: We propose a modality-aware MoE model for effective multimodal feature fusion paradigm, which tackles Q1 and Q2 in feature fusion. (4) Cascaded Diffusion with Interaction Supervision: We design a cascaded diffusion framework that progressively refines human-object interaction features under dedicated supervision, which tackles Q3 in interaction refinement. Comprehensive experiments demonstrate that MP-HOI outperforms existing approaches in generating high-fidelity and fine-grained HOI motions.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æˆ‘ä»¬è§£å†³æ–‡æœ¬é©±åŠ¨çš„ 3D äººæœºäº¤äº’ (HOI) è¿åŠ¨ç”Ÿæˆè¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºç›´æ¥æ–‡æœ¬åˆ° HOI çš„æ˜ å°„ï¼Œç”±äºå­˜åœ¨æ˜¾ç€çš„è·¨æ¨¡æ€å·®è·ï¼Œè¯¥æ–¹æ³•å—åˆ°ä¸‰ä¸ªå…³é”®é™åˆ¶ï¼šï¼ˆQ1ï¼‰æ¬¡ä¼˜çš„äººä½“è¿åŠ¨ï¼Œï¼ˆQ2ï¼‰ä¸è‡ªç„¶çš„ç‰©ä½“è¿åŠ¨ï¼Œä»¥åŠï¼ˆQ3ï¼‰äººç±»å’Œç‰©ä½“ä¹‹é—´çš„å¼±äº¤äº’ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº† MP-HOIï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå››ä¸ªæ ¸å¿ƒè§è§£çš„æ–°é¢–æ¡†æ¶ï¼šï¼ˆ1ï¼‰å¤šæ¨¡æ€æ•°æ®å…ˆéªŒï¼šæˆ‘ä»¬åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„å¤šæ¨¡æ€æ•°æ®ï¼ˆæ–‡æœ¬ã€å›¾åƒã€å§¿åŠ¿/å¯¹è±¡ï¼‰ä½œä¸ºå…ˆéªŒæ¥æŒ‡å¯¼ HOI ç”Ÿæˆï¼Œä»è€Œè§£å†³æ•°æ®å»ºæ¨¡ä¸­çš„ Q1 å’Œ Q2 é—®é¢˜ã€‚ ï¼ˆ2ï¼‰å¢å¼ºçš„å¯¹è±¡è¡¨ç¤ºï¼šæˆ‘ä»¬é€šè¿‡åˆå¹¶å‡ ä½•å…³é”®ç‚¹ã€æ¥è§¦ç‰¹å¾å’ŒåŠ¨æ€å±æ€§æ¥æ”¹è¿›ç°æœ‰çš„å¯¹è±¡è¡¨ç¤ºï¼Œä»è€Œå®ç°å¯Œæœ‰è¡¨ç°åŠ›çš„å¯¹è±¡è¡¨ç¤ºï¼Œä»è€Œè§£å†³äº†æ•°æ®è¡¨ç¤ºä¸­çš„é—®é¢˜2ã€‚ ï¼ˆ3ï¼‰å¤šæ¨¡æ€æ„ŸçŸ¥æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºæœ‰æ•ˆå¤šæ¨¡æ€ç‰¹å¾èåˆèŒƒå¼çš„æ¨¡æ€æ„ŸçŸ¥MoEæ¨¡å‹ï¼Œè¯¥æ¨¡å‹è§£å†³äº†ç‰¹å¾èåˆä¸­çš„Q1å’ŒQ2é—®é¢˜ã€‚ ï¼ˆ4ï¼‰å…·æœ‰äº¤äº’ç›‘ç£çš„çº§è”æ‰©æ•£ï¼šæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªçº§è”æ‰©æ•£æ¡†æ¶ï¼Œåœ¨ä¸“é—¨çš„ç›‘ç£ä¸‹é€æ­¥ç»†åŒ–äººä¸ç‰©ä½“çš„äº¤äº’ç‰¹å¾ï¼Œè§£å†³äº†äº¤äº’ç»†åŒ–ä¸­çš„é—®é¢˜3ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒMP-HOI åœ¨ç”Ÿæˆé«˜ä¿çœŸå’Œç»†ç²’åº¦ HOI è¿åŠ¨æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

</details>

---

## 26. VideoSTF: Stress-Testing Output Repetition in Video Large Language Models

**ä¸­æ–‡æ ‡é¢˜**: VideoSTFï¼šè§†é¢‘å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è¾“å‡ºé‡å¤å‹åŠ›æµ‹è¯•

**Date**: 2026-02-11 | **arXiv**: [2602.10639v1](http://arxiv.org/abs/2602.10639v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10639v1)

**Code**: https://github.com/yuxincao22/VideoSTF_benchmark.

<details><summary><b>Abstract</b></summary>

Video Large Language Models (VideoLLMs) have recently achieved strong performance in video understanding tasks. However, we identify a previously underexplored generation failure: severe output repetition, where models degenerate into self-reinforcing loops of repeated phrases or sentences. This failure mode is not captured by existing VideoLLM benchmarks, which focus primarily on task accuracy and factual correctness. We introduce VideoSTF, the first framework for systematically measuring and stress-testing output repetition in VideoLLMs. VideoSTF formalizes repetition using three complementary n-gram-based metrics and provides a standardized testbed of 10,000 diverse videos together with a library of controlled temporal transformations. Using VideoSTF, we conduct pervasive testing, temporal stress testing, and adversarial exploitation across 10 advanced VideoLLMs. We find that output repetition is widespread and, critically, highly sensitive to temporal perturbations of video inputs. Moreover, we show that simple temporal transformations can efficiently induce repetitive degeneration in a black-box setting, exposing output repetition as an exploitable security vulnerability. Our results reveal output repetition as a fundamental stability issue in modern VideoLLMs and motivate stability-aware evaluation for video-language systems. Our evaluation code and scripts are available at: https://github.com/yuxincao22/VideoSTF_benchmark.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideoLLMï¼‰æœ€è¿‘åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­å–å¾—äº†å¼ºåŠ²çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªå…ˆå‰æœªè¢«å……åˆ†æ¢ç´¢çš„ç”Ÿæˆå¤±è´¥ï¼šä¸¥é‡çš„è¾“å‡ºé‡å¤ï¼Œå…¶ä¸­æ¨¡å‹é€€åŒ–ä¸ºé‡å¤çŸ­è¯­æˆ–å¥å­çš„è‡ªæˆ‘å¼ºåŒ–å¾ªç¯ã€‚ç°æœ‰çš„ VideoLLM åŸºå‡†æµ‹è¯•æœªæ•è·æ­¤æ•…éšœæ¨¡å¼ï¼Œè¯¥åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨ä»»åŠ¡å‡†ç¡®æ€§å’Œäº‹å®æ­£ç¡®æ€§ã€‚æˆ‘ä»¬ä»‹ç» VideoSTFï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåœ¨ VideoLLM ä¸­ç³»ç»Ÿæµ‹é‡å’Œå‹åŠ›æµ‹è¯•è¾“å‡ºé‡å¤çš„æ¡†æ¶ã€‚ VideoSTF ä½¿ç”¨ä¸‰ä¸ªäº’è¡¥çš„åŸºäº n-gram çš„æŒ‡æ ‡æ¥å½¢å¼åŒ–é‡å¤ï¼Œå¹¶æä¾›åŒ…å« 10,000 ä¸ªä¸åŒè§†é¢‘çš„æ ‡å‡†åŒ–æµ‹è¯•åºŠä»¥åŠå—æ§æ—¶é—´è½¬æ¢åº“ã€‚ä½¿ç”¨ VideoSTFï¼Œæˆ‘ä»¬åœ¨ 10 ä¸ªé«˜çº§ VideoLLM ä¸­è¿›è¡Œæ™®éæµ‹è¯•ã€æ—¶é—´å‹åŠ›æµ‹è¯•å’Œå¯¹æŠ—æ€§åˆ©ç”¨ã€‚æˆ‘ä»¬å‘ç°è¾“å‡ºé‡å¤å¾ˆæ™®éï¼Œè€Œä¸”è‡³å…³é‡è¦çš„æ˜¯ï¼Œå®ƒå¯¹è§†é¢‘è¾“å…¥çš„æ—¶é—´æ‰°åŠ¨é«˜åº¦æ•æ„Ÿã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¡¨æ˜ç®€å•çš„æ—¶é—´å˜æ¢å¯ä»¥æœ‰æ•ˆåœ°åœ¨é»‘ç›’è®¾ç½®ä¸­å¼•èµ·é‡å¤é€€åŒ–ï¼Œä»è€Œå°†è¾“å‡ºé‡å¤æš´éœ²ä¸ºå¯åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¾“å‡ºé‡å¤æ˜¯ç°ä»£ VideoLLM ä¸­çš„ä¸€ä¸ªåŸºæœ¬ç¨³å®šæ€§é—®é¢˜ï¼Œå¹¶æ¿€å‘äº†å¯¹è§†é¢‘è¯­è¨€ç³»ç»Ÿçš„ç¨³å®šæ€§æ„ŸçŸ¥è¯„ä¼°ã€‚æˆ‘ä»¬çš„è¯„ä¼°ä»£ç å’Œè„šæœ¬ä½äºï¼šhttps://github.com/yuxincao22/VideoSTF_benchmarkã€‚

</details>

---

## 27. Eliminating VAE for Fast and High-Resolution Generative Detail Restoration

**ä¸­æ–‡æ ‡é¢˜**: æ¶ˆé™¤ VAE ä»¥å®ç°å¿«é€Ÿã€é«˜åˆ†è¾¨ç‡çš„ç”Ÿæˆç»†èŠ‚æ¢å¤

**Date**: 2026-02-11 | **arXiv**: [2602.10630v1](http://arxiv.org/abs/2602.10630v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10630v1)

<details><summary><b>Abstract</b></summary>

Diffusion models have attained remarkable breakthroughs in the real-world super-resolution (SR) task, albeit at slow inference and high demand on devices. To accelerate inference, recent works like GenDR adopt step distillation to minimize the step number to one. However, the memory boundary still restricts the maximum processing size, necessitating tile-by-tile restoration of high-resolution images. Through profiling the pipeline, we pinpoint that the variational auto-encoder (VAE) is the bottleneck of latency and memory. To completely solve the problem, we leverage pixel-(un)shuffle operations to eliminate the VAE, reversing the latent-based GenDR to pixel-space GenDR-Pix. However, upscale with x8 pixelshuffle may induce artifacts of repeated patterns. To alleviate the distortion, we propose a multi-stage adversarial distillation to progressively remove the encoder and decoder. Specifically, we utilize generative features from the previous stage models to guide adversarial discrimination. Moreover, we propose random padding to augment generative features and avoid discriminator collapse. We also introduce a masked Fourier space loss to penalize the outliers of amplitude. To improve inference performance, we empirically integrate a padding-based self-ensemble with classifier-free guidance to improve inference scaling. Experimental results show that GenDR-Pix performs 2.8x acceleration and 60% memory-saving compared to GenDR with negligible visual degradation, surpassing other one-step diffusion SR. Against all odds, GenDR-Pix can restore 4K image in only 1 second and 6GB.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å°½ç®¡æ¨ç†é€Ÿåº¦æ…¢ä¸”å¯¹è®¾å¤‡çš„è¦æ±‚å¾ˆé«˜ï¼Œä½†æ‰©æ•£æ¨¡å‹åœ¨ç°å®ä¸–ç•Œçš„è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾ç€çš„çªç ´ã€‚ä¸ºäº†åŠ é€Ÿæ¨ç†ï¼ŒGenDR ç­‰æœ€è¿‘çš„å·¥ä½œé‡‡ç”¨äº†æ­¥éª¤è’¸é¦ï¼Œå°†æ­¥éª¤æ•°æœ€å°åŒ–ä¸º 1ã€‚ç„¶è€Œï¼Œå†…å­˜è¾¹ç•Œä»ç„¶é™åˆ¶æœ€å¤§å¤„ç†å¤§å°ï¼Œéœ€è¦é€å—æ¢å¤é«˜åˆ†è¾¨ç‡å›¾åƒã€‚é€šè¿‡å¯¹ç®¡é“è¿›è¡Œåˆ†æï¼Œæˆ‘ä»¬å‘ç°å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰æ˜¯å»¶è¿Ÿå’Œå†…å­˜çš„ç“¶é¢ˆã€‚ä¸ºäº†å½»åº•è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨åƒç´ ï¼ˆéï¼‰æ´—ç‰Œæ“ä½œæ¥æ¶ˆé™¤ VAEï¼Œå°†åŸºäºæ½œåœ¨çš„ GenDR åè½¬ä¸ºåƒç´ ç©ºé—´ GenDR-Pixã€‚ç„¶è€Œï¼Œx8 Pixelshuffle çš„é«˜æ¡£å¯èƒ½ä¼šå¯¼è‡´é‡å¤å›¾æ¡ˆçš„ä¼ªå½±ã€‚ä¸ºäº†å‡è½»å¤±çœŸï¼Œæˆ‘ä»¬æå‡ºäº†å¤šé˜¶æ®µå¯¹æŠ—æ€§è’¸é¦æ¥é€æ­¥åˆ é™¤ç¼–ç å™¨å’Œè§£ç å™¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨å‰ä¸€é˜¶æ®µæ¨¡å‹çš„ç”Ÿæˆç‰¹å¾æ¥æŒ‡å¯¼å¯¹æŠ—æ€§æ­§è§†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºéšæœºå¡«å……æ¥å¢å¼ºç”Ÿæˆç‰¹å¾å¹¶é¿å…é‰´åˆ«å™¨å´©æºƒã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†æ©è”½å‚…ç«‹å¶ç©ºé—´æŸå¤±æ¥æƒ©ç½šå¹…åº¦çš„å¼‚å¸¸å€¼ã€‚ä¸ºäº†æé«˜æ¨ç†æ€§èƒ½ï¼Œæˆ‘ä»¬æ ¹æ®ç»éªŒå°†åŸºäºå¡«å……çš„è‡ªé›†æˆä¸æ— åˆ†ç±»å™¨æŒ‡å¯¼ç›¸ç»“åˆï¼Œä»¥æé«˜æ¨ç†è§„æ¨¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ GenDR ç›¸æ¯”ï¼ŒGenDR-Pix çš„åŠ é€Ÿé€Ÿåº¦æé«˜äº† 2.8 å€ï¼ŒèŠ‚çœäº† 60% çš„å†…å­˜ï¼Œè§†è§‰é€€åŒ–å¯ä»¥å¿½ç•¥ä¸è®¡ï¼Œè¶…è¶Šäº†å…¶ä»–ä¸€æ­¥æ‰©æ•£ SRã€‚å°½ç®¡å›°éš¾é‡é‡ï¼ŒGenDR-Pix ä»èƒ½åœ¨ 1 ç§’å†…æ¢å¤ 4K å›¾åƒï¼Œå®¹é‡ä¸º 6GBã€‚

</details>

---

## 28. A Vision-Language Foundation Model for Zero-shot Clinical Collaboration and Automated Concept Discovery in Dermatology

**ä¸­æ–‡æ ‡é¢˜**: ç”¨äºçš®è‚¤ç§‘é›¶æ¬¡ä¸´åºŠåä½œå’Œè‡ªåŠ¨æ¦‚å¿µå‘ç°çš„è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹

**Date**: 2026-02-11 | **arXiv**: [2602.10624v1](http://arxiv.org/abs/2602.10624v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10624v1)

<details><summary><b>Abstract</b></summary>

Medical foundation models have shown promise in controlled benchmarks, yet widespread deployment remains hindered by reliance on task-specific fine-tuning. Here, we introduce DermFM-Zero, a dermatology vision-language foundation model trained via masked latent modelling and contrastive learning on over 4 million multimodal data points. We evaluated DermFM-Zero across 20 benchmarks spanning zero-shot diagnosis and multimodal retrieval, achieving state-of-the-art performance without task-specific adaptation. We further evaluated its zero-shot capabilities in three multinational reader studies involving over 1,100 clinicians. In primary care settings, AI assistance enabled general practitioners to nearly double their differential diagnostic accuracy across 98 skin conditions. In specialist settings, the model significantly outperformed board-certified dermatologists in multimodal skin cancer assessment. In collaborative workflows, AI assistance enabled non-experts to surpass unassisted experts while improving management appropriateness. Finally, we show that DermFM-Zero's latent representations are interpretable: sparse autoencoders unsupervisedly disentangle clinically meaningful concepts that outperform predefined-vocabulary approaches and enable targeted suppression of artifact-induced biases, enhancing robustness without retraining. These findings demonstrate that a foundation model can provide effective, safe, and transparent zero-shot clinical decision support.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åŒ»å­¦åŸºç¡€æ¨¡å‹åœ¨å—æ§åŸºå‡†æ–¹é¢æ˜¾ç¤ºå‡ºäº†å¸Œæœ›ï¼Œä½†å¹¿æ³›éƒ¨ç½²ä»ç„¶å› ä¾èµ–ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒè€Œå—åˆ°é˜»ç¢ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç» DermFM-Zeroï¼Œè¿™æ˜¯ä¸€ç§çš®è‚¤ç—…å­¦è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡å¯¹è¶…è¿‡ 400 ä¸‡ä¸ªå¤šæ¨¡æ€æ•°æ®ç‚¹è¿›è¡Œæ©æ¨¡æ½œåœ¨å»ºæ¨¡å’Œå¯¹æ¯”å­¦ä¹ è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬é€šè¿‡ 20 ä¸ªåŸºå‡†è¯„ä¼°äº† DermFM-Zeroï¼Œæ¶µç›–é›¶æ¬¡è¯Šæ–­å’Œå¤šæ¨¡æ€æ£€ç´¢ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œé€‚åº”å³å¯å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨æ¶‰åŠ 1,100 å¤šåä¸´åºŠåŒ»ç”Ÿçš„ä¸‰é¡¹è·¨å›½è¯»è€…ç ”ç©¶ä¸­è¿›ä¸€æ­¥è¯„ä¼°äº†å…¶é›¶æ ·æœ¬èƒ½åŠ›ã€‚åœ¨åˆçº§ä¿å¥ç¯å¢ƒä¸­ï¼Œäººå·¥æ™ºèƒ½è¾…åŠ©ä½¿å…¨ç§‘åŒ»ç”Ÿå¯¹ 98 ç§çš®è‚¤ç—…çš„é‰´åˆ«è¯Šæ–­å‡†ç¡®æ€§å‡ ä¹ç¿»äº†ä¸€ç•ªã€‚åœ¨ä¸“ä¸šç¯å¢ƒä¸­ï¼Œè¯¥æ¨¡å‹åœ¨å¤šæ¨¡å¼çš®è‚¤ç™Œè¯„ä¼°æ–¹é¢æ˜¾ç€ä¼˜äºç»è¿‡å§”å‘˜ä¼šè®¤è¯çš„çš®è‚¤ç§‘åŒ»ç”Ÿã€‚åœ¨åä½œå·¥ä½œæµç¨‹ä¸­ï¼Œäººå·¥æ™ºèƒ½è¾…åŠ©ä½¿éä¸“å®¶èƒ½å¤Ÿè¶…è¶Šæ— äººååŠ©çš„ä¸“å®¶ï¼ŒåŒæ—¶æé«˜ç®¡ç†çš„é€‚å½“æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜ DermFM-Zero çš„æ½œåœ¨è¡¨ç¤ºæ˜¯å¯è§£é‡Šçš„ï¼šç¨€ç–è‡ªåŠ¨ç¼–ç å™¨æ— ç›‘ç£åœ°è§£å¼€å…·æœ‰ä¸´åºŠæ„ä¹‰çš„æ¦‚å¿µï¼Œè¿™äº›æ¦‚å¿µä¼˜äºé¢„å®šä¹‰è¯æ±‡æ–¹æ³•ï¼Œå¹¶ä¸”èƒ½å¤Ÿæœ‰é’ˆå¯¹æ€§åœ°æŠ‘åˆ¶ä¼ªå½±å¼•èµ·çš„åå·®ï¼Œæ— éœ€é‡æ–°è®­ç»ƒå³å¯å¢å¼ºé²æ£’æ€§ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒåŸºç¡€æ¨¡å‹å¯ä»¥æä¾›æœ‰æ•ˆã€å®‰å…¨å’Œé€æ˜çš„é›¶æ ·æœ¬ä¸´åºŠå†³ç­–æ”¯æŒã€‚

</details>

---

## 29. Improving Medical Visual Reinforcement Fine-Tuning via Perception and Reasoning Augmentation

**ä¸­æ–‡æ ‡é¢˜**: é€šè¿‡æ„ŸçŸ¥å’Œæ¨ç†å¢å¼ºæ”¹å–„åŒ»å­¦è§†è§‰å¼ºåŒ–å¾®è°ƒ

**Date**: 2026-02-11 | **arXiv**: [2602.10619v1](http://arxiv.org/abs/2602.10619v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10619v1)

<details><summary><b>Abstract</b></summary>

While recent advances in Reinforcement Fine-Tuning (RFT) have shown that rule-based reward schemes can enable effective post-training for large language models, their extension to cross-modal, vision-centric domains remains largely underexplored. This limitation is especially pronounced in the medical imaging domain, where effective performance requires both robust visual perception and structured reasoning. In this work, we address this gap by proposing VRFT-Aug, a visual reinforcement fine-tuning framework tailored for the medical domain. VRFT-Aug introduces a series of training strategies designed to augment both perception and reasoning, including prior knowledge injection, perception-driven policy refinement, medically informed reward shaping, and behavioral imitation. Together, these methods aim to stabilize and improve the RFT process.   Through extensive experiments across multiple medical datasets, we show that our approaches consistently outperform both standard supervised fine-tuning and RFT baselines. Moreover, we provide empirically grounded insights and practical training heuristics that can be generalized to other medical image tasks. We hope this work contributes actionable guidance and fresh inspiration for the ongoing effort to develop reliable, reasoning-capable models for high-stakes medical applications.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è™½ç„¶å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰çš„æœ€æ–°è¿›å±•è¡¨æ˜ï¼ŒåŸºäºè§„åˆ™çš„å¥–åŠ±æ–¹æ¡ˆå¯ä»¥ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹æä¾›æœ‰æ•ˆçš„åè®­ç»ƒï¼Œä½†å®ƒä»¬å‘è·¨æ¨¡å¼ã€ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„é¢†åŸŸçš„æ‰©å±•åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚è¿™ç§é™åˆ¶åœ¨åŒ»å­¦æˆåƒé¢†åŸŸå°¤å…¶æ˜æ˜¾ï¼Œå…¶ä¸­æœ‰æ•ˆçš„æ€§èƒ½éœ€è¦å¼ºå¤§çš„è§†è§‰æ„ŸçŸ¥å’Œç»“æ„åŒ–æ¨ç†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æå‡º VRFT-Aug æ¥è§£å†³è¿™ä¸€å·®è·ï¼ŒVRFT-Aug æ˜¯ä¸€ç§ä¸ºåŒ»ç–—é¢†åŸŸé‡èº«å®šåˆ¶çš„è§†è§‰å¼ºåŒ–å¾®è°ƒæ¡†æ¶ã€‚ VRFT-Aug å¼•å…¥äº†ä¸€ç³»åˆ—æ—¨åœ¨å¢å¼ºæ„ŸçŸ¥å’Œæ¨ç†çš„è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬å…ˆéªŒçŸ¥è¯†æ³¨å…¥ã€æ„ŸçŸ¥é©±åŠ¨çš„ç­–ç•¥ç»†åŒ–ã€åŒ»å­¦çŸ¥è¯†å¥–åŠ±å¡‘é€ å’Œè¡Œä¸ºæ¨¡ä»¿ã€‚è¿™äº›æ–¹æ³•å…±åŒæ—¨åœ¨ç¨³å®šå’Œæ”¹è¿› RFT è¿‡ç¨‹ã€‚   é€šè¿‡å¯¹å¤šä¸ªåŒ»å­¦æ•°æ®é›†çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºæ ‡å‡†ç›‘ç£å¾®è°ƒå’Œ RFT åŸºçº¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›äº†åŸºäºç»éªŒçš„è§è§£å’Œå®è·µè®­ç»ƒå¯å‘æ³•ï¼Œå¯ä»¥æ¨å¹¿åˆ°å…¶ä»–åŒ»å­¦å›¾åƒä»»åŠ¡ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½å¤Ÿä¸ºæŒç»­åŠªåŠ›ä¸ºé«˜é£é™©åŒ»ç–—åº”ç”¨å¼€å‘å¯é ã€å…·æœ‰æ¨ç†èƒ½åŠ›çš„æ¨¡å‹æä¾›å¯è¡Œçš„æŒ‡å¯¼å’Œæ–°çš„çµæ„Ÿã€‚

</details>

---

## 30. Enhancing YOLOv11n for Reliable Child Detection in Noisy Surveillance Footage

**ä¸­æ–‡æ ‡é¢˜**: å¢å¼º YOLOv11n ä»¥åœ¨å˜ˆæ‚çš„ç›‘æ§å½•åƒä¸­å®ç°å¯é çš„å„¿ç«¥æ£€æµ‹

**Date**: 2026-02-11 | **arXiv**: [2602.10592v1](http://arxiv.org/abs/2602.10592v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10592v1)

**Code**: https://github.com/html-ptit/Data-Augmentation-YOLOv11n-child-detection

<details><summary><b>Abstract</b></summary>

This paper presents a practical and lightweight solution for enhancing child detection in low-quality surveillance footage, a critical component in real-world missing child alert and daycare monitoring systems. Building upon the efficient YOLOv11n architecture, we propose a deployment-ready pipeline that improves detection under challenging conditions including occlusion, small object size, low resolution, motion blur, and poor lighting commonly found in existing CCTV infrastructures. Our approach introduces a domain-specific augmentation strategy that synthesizes realistic child placements using spatial perturbations such as partial visibility, truncation, and overlaps, combined with photometric degradations including lighting variation and noise. To improve recall of small and partially occluded instances, we integrate Slicing Aided Hyper Inference (SAHI) at inference time. All components are trained and evaluated on a filtered, child-only subset of the Roboflow Daycare dataset. Compared to the baseline YOLOv11n, our enhanced system achieves a mean Average Precision at 0.5 IoU (mAP@0.5) of 0.967 and a mean Average Precision averaged over IoU thresholds from 0.5 to 0.95 (mAP@0.5:0.95) of 0.783, yielding absolute improvements of 0.7 percent and 2.3 percent, respectively, without architectural changes. Importantly, the entire pipeline maintains compatibility with low-power edge devices and supports real-time performance, making it particularly well suited for low-cost or resource-constrained industrial surveillance deployments. The example augmented dataset and the source code used to generate it are available at: https://github.com/html-ptit/Data-Augmentation-YOLOv11n-child-detection

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æœ¬æ–‡æå‡ºäº†ä¸€ç§å®ç”¨ä¸”è½»é‡çº§çš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºå¢å¼ºä½è´¨é‡ç›‘æ§å½•åƒä¸­çš„å„¿ç«¥æ£€æµ‹ï¼Œè¿™æ˜¯ç°å®ä¸–ç•Œä¸­å¤±è¸ªå„¿ç«¥è­¦æŠ¥å’Œæ—¥æ‰˜ç›‘æ§ç³»ç»Ÿçš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚åŸºäºé«˜æ•ˆçš„ YOLOv11n æ¶æ„ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯éƒ¨ç½²çš„ç®¡é“ï¼Œå¯æ”¹å–„ç°æœ‰é—­è·¯ç”µè§†åŸºç¡€è®¾æ–½ä¸­å¸¸è§çš„é®æŒ¡ã€å°ç‰©ä½“å°ºå¯¸ã€ä½åˆ†è¾¨ç‡ã€è¿åŠ¨æ¨¡ç³Šå’Œå…‰çº¿ä¸è¶³ç­‰æŒ‘æˆ˜æ€§æ¡ä»¶ä¸‹çš„æ£€æµ‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§ç‰¹å®šäºåŸŸçš„å¢å¼ºç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä½¿ç”¨ç©ºé—´æ‰°åŠ¨ï¼ˆä¾‹å¦‚éƒ¨åˆ†å¯è§æ€§ã€æˆªæ–­å’Œé‡å ï¼‰ä»¥åŠå…‰åº¦é€€åŒ–ï¼ˆåŒ…æ‹¬ç…§æ˜å˜åŒ–å’Œå™ªå£°ï¼‰æ¥åˆæˆçœŸå®çš„å­æ”¾ç½®ã€‚ä¸ºäº†æé«˜å¯¹å°å‹å’Œéƒ¨åˆ†é®æŒ¡å®ä¾‹çš„å¬å›ç‡ï¼Œæˆ‘ä»¬åœ¨æ¨ç†æ—¶é›†æˆäº†åˆ‡ç‰‡è¾…åŠ©è¶…æ¨ç†ï¼ˆSAHIï¼‰ã€‚æ‰€æœ‰ç»„ä»¶å‡åœ¨ Roboflow Daycare æ•°æ®é›†çš„ç»è¿‡ç­›é€‰ã€ä»…é™å„¿ç«¥çš„å­é›†ä¸Šè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚ä¸åŸºçº¿ YOLOv11n ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„å¢å¼ºç³»ç»Ÿåœ¨ 0.5 IoU (mAP@0.5) ä¸‹çš„å¹³å‡å¹³å‡ç²¾åº¦ä¸º 0.967ï¼Œåœ¨ IoU é˜ˆå€¼ä» 0.5 åˆ° 0.95 (mAP@0.5:0.95) ä¸Šçš„å¹³å‡å¹³å‡ç²¾åº¦ä¸º 0.783ï¼Œåœ¨æ²¡æœ‰æ¶æ„å˜åŒ–çš„æƒ…å†µä¸‹ï¼Œç»å¯¹æ”¹è¿›åˆ†åˆ«ä¸º 0.7% å’Œ 2.3%ã€‚é‡è¦çš„æ˜¯ï¼Œæ•´ä¸ªç®¡é“ä¿æŒä¸ä½åŠŸè€—è¾¹ç¼˜è®¾å¤‡çš„å…¼å®¹æ€§å¹¶æ”¯æŒå®æ—¶æ€§èƒ½ï¼Œä½¿å…¶ç‰¹åˆ«é€‚åˆä½æˆæœ¬æˆ–èµ„æºæœ‰é™çš„å·¥ä¸šç›‘æ§éƒ¨ç½²ã€‚ç¤ºä¾‹å¢å¼ºæ•°æ®é›†å’Œç”¨äºç”Ÿæˆå®ƒçš„æºä»£ç ä½äºï¼šhttps://github.com/html-ptit/Data-Augmentation-YOLOv11n-child-detection

</details>

---

## 31. MetaphorStar: Image Metaphor Understanding and Reasoning with End-to-End Visual Reinforcement Learning

**ä¸­æ–‡æ ‡é¢˜**: MetaphorStarï¼šé€šè¿‡ç«¯åˆ°ç«¯è§†è§‰å¼ºåŒ–å­¦ä¹ è¿›è¡Œå›¾åƒéšå–»ç†è§£å’Œæ¨ç†

**Date**: 2026-02-11 | **arXiv**: [2602.10575v1](http://arxiv.org/abs/2602.10575v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10575v1)

<details><summary><b>Abstract</b></summary>

Metaphorical comprehension in images remains a critical challenge for Nowadays AI systems. While Multimodal Large Language Models (MLLMs) excel at basic Visual Question Answering (VQA), they consistently struggle to grasp the nuanced cultural, emotional, and contextual implications embedded in visual content. This difficulty stems from the task's demand for sophisticated multi-hop reasoning, cultural context, and Theory of Mind (ToM) capabilities, which current models lack. To fill this gap, we propose MetaphorStar, the first end-to-end visual reinforcement learning (RL) framework for image implication tasks. Our framework includes three core components: the fine-grained dataset TFQ-Data, the visual RL method TFQ-GRPO, and the well-structured benchmark TFQ-Bench.   Our fully open-source MetaphorStar family, trained using TFQ-GRPO on TFQ-Data, significantly improves performance by an average of 82.6% on the image implication benchmarks. Compared with 20+ mainstream MLLMs, MetaphorStar-32B achieves state-of-the-art (SOTA) on Multiple-Choice Question and Open-Style Question, significantly outperforms the top closed-source model Gemini-3.0-pro on True-False Question. Crucially, our experiments reveal that learning image implication tasks improves the general understanding ability, especially the complex visual reasoning ability. We further provide a systematic analysis of model parameter scaling, training data scaling, and the impact of different model architectures and training strategies, demonstrating the broad applicability of our method. We open-sourced all model weights, datasets, and method code at https://metaphorstar.github.io.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å›¾åƒä¸­çš„éšå–»ç†è§£ä»ç„¶æ˜¯å½“ä»Šäººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ (MLLM) æ“…é•¿åŸºæœ¬çš„è§†è§‰é—®ç­” (VQA)ï¼Œä½†å®ƒä»¬å§‹ç»ˆéš¾ä»¥æŒæ¡è§†è§‰å†…å®¹ä¸­åµŒå…¥çš„å¾®å¦™æ–‡åŒ–ã€æƒ…æ„Ÿå’Œä¸Šä¸‹æ–‡å«ä¹‰ã€‚è¿™ä¸€å›°éš¾æºäºè¯¥ä»»åŠ¡å¯¹å¤æ‚çš„å¤šè·³æ¨ç†ã€æ–‡åŒ–èƒŒæ™¯å’Œå¿ƒæ™ºç†è®º (ToM) èƒ½åŠ›çš„éœ€æ±‚ï¼Œè€Œå½“å‰æ¨¡å‹ç¼ºä¹è¿™äº›èƒ½åŠ›ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº† MetaphorStarï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºå›¾åƒæš—ç¤ºä»»åŠ¡çš„ç«¯åˆ°ç«¯è§†è§‰å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šç»†ç²’åº¦æ•°æ®é›† TFQ-Dataã€è§†è§‰ RL æ–¹æ³• TFQ-GRPO å’Œç»“æ„è‰¯å¥½çš„åŸºå‡† TFQ-Benchã€‚   æˆ‘ä»¬å®Œå…¨å¼€æºçš„ MetaphorStar ç³»åˆ—åœ¨ TFQ-Data ä¸Šä½¿ç”¨ TFQ-GRPO è¿›è¡Œè®­ç»ƒï¼Œåœ¨å›¾åƒå«ä¹‰åŸºå‡†ä¸Šæ˜¾ç€æé«˜äº†å¹³å‡ 82.6% çš„æ€§èƒ½ã€‚ä¸20å¤šä¸ªä¸»æµMLLMç›¸æ¯”ï¼ŒMetaphorStar-32Båœ¨å¤šé¡¹é€‰æ‹©é¢˜å’Œå¼€æ”¾å¼é¢˜ä¸Šè¾¾åˆ°äº†state-of-the-artï¼ˆSOTAï¼‰ï¼Œåœ¨True-Falseé¢˜ä¸Šæ˜¾ç€ä¼˜äºé¡¶çº§é—­æºæ¨¡å‹Gemini-3.0-proã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå­¦ä¹ å›¾åƒæš—ç¤ºä»»åŠ¡å¯ä»¥æé«˜ä¸€èˆ¬ç†è§£èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯å¤æ‚çš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¯¹æ¨¡å‹å‚æ•°ç¼©æ”¾ã€è®­ç»ƒæ•°æ®ç¼©æ”¾ä»¥åŠä¸åŒæ¨¡å‹æ¶æ„å’Œè®­ç»ƒç­–ç•¥çš„å½±å“è¿›è¡Œäº†ç³»ç»Ÿåˆ†æï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚æˆ‘ä»¬åœ¨ https://metaphorstar.github.io å¼€æºäº†æ‰€æœ‰æ¨¡å‹æƒé‡ã€æ•°æ®é›†å’Œæ–¹æ³•ä»£ç ã€‚

</details>

---

## 32. C^2ROPE: Causal Continuous Rotary Positional Encoding for 3D Large Multimodal-Models Reasoning

**ä¸­æ–‡æ ‡é¢˜**: C^2ROPEï¼šç”¨äº 3D å¤§å‹å¤šæ¨¡æ€æ¨¡å‹æ¨ç†çš„å› æœè¿ç»­æ—‹è½¬ä½ç½®ç¼–ç 

**Date**: 2026-02-11 | **arXiv**: [2602.10551v1](http://arxiv.org/abs/2602.10551v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10551v1)

**Code**: https://github.com/ErikZ719/C2RoPE.

<details><summary><b>Abstract</b></summary>

Recent advances in 3D Large Multimodal Models (LMMs) built on Large Language Models (LLMs) have established the alignment of 3D visual features with LLM representations as the dominant paradigm. However, the inherited Rotary Position Embedding (RoPE) introduces limitations for multimodal processing. Specifically, applying 1D temporal positional indices disrupts the continuity of visual features along the column dimension, resulting in spatial locality loss. Moreover, RoPE follows the prior that temporally closer image tokens are more causally related, leading to long-term decay in attention allocation and causing the model to progressively neglect earlier visual tokens as the sequence length increases. To address these issues, we propose C^2RoPE, an improved RoPE that explicitly models local spatial Continuity and spatial Causal relationships for visual processing. C^2RoPE introduces a spatio-temporal continuous positional embedding mechanism for visual tokens. It first integrates 1D temporal positions with Cartesian-based spatial coordinates to construct a triplet hybrid positional index, and then employs a frequency allocation strategy to encode spatio-temporal positional information across the three index components. Additionally, we introduce Chebyshev Causal Masking, which determines causal dependencies by computing the Chebyshev distance of image tokens in 2D space. Evaluation results across various benchmarks, including 3D scene reasoning and 3D visual question answering, demonstrate C^2RoPE's effectiveness. The code is be available at https://github.com/ErikZ719/C2RoPE.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ (LLM) çš„ 3D å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ (LMM) çš„æœ€æ–°è¿›å±•å·²ç»å»ºç«‹äº† 3D è§†è§‰ç‰¹å¾ä¸ LLM è¡¨ç¤ºçš„ä¸€è‡´æ€§ä½œä¸ºä¸»å¯¼èŒƒå¼ã€‚ç„¶è€Œï¼Œç»§æ‰¿çš„æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰å¼•å…¥äº†å¤šæ¨¡æ€å¤„ç†çš„é™åˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œåº”ç”¨ä¸€ç»´æ—¶é—´ä½ç½®ç´¢å¼•ä¼šç ´åè§†è§‰ç‰¹å¾æ²¿åˆ—ç»´åº¦çš„è¿ç»­æ€§ï¼Œå¯¼è‡´ç©ºé—´å±€éƒ¨æ€§æŸå¤±ã€‚æ­¤å¤–ï¼ŒRoPE éµå¾ªå…ˆéªŒçŸ¥è¯†ï¼Œå³æ—¶é—´ä¸Šæ›´æ¥è¿‘çš„å›¾åƒæ ‡è®°æ›´å…·æœ‰å› æœå…³ç³»ï¼Œå¯¼è‡´æ³¨æ„åŠ›åˆ†é…çš„é•¿æœŸè¡°å‡ï¼Œå¹¶å¯¼è‡´æ¨¡å‹éšç€åºåˆ—é•¿åº¦çš„å¢åŠ é€æ¸å¿½ç•¥è¾ƒæ—©çš„è§†è§‰æ ‡è®°ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº† C^2RoPEï¼Œè¿™æ˜¯ä¸€ç§æ”¹è¿›çš„ RoPEï¼Œå®ƒå¯ä»¥æ˜¾å¼åœ°æ¨¡æ‹Ÿè§†è§‰å¤„ç†çš„å±€éƒ¨ç©ºé—´è¿ç»­æ€§å’Œç©ºé—´å› æœå…³ç³»ã€‚ C^2RoPE å¼•å…¥äº†è§†è§‰æ ‡è®°çš„æ—¶ç©ºè¿ç»­ä½ç½®åµŒå…¥æœºåˆ¶ã€‚å®ƒé¦–å…ˆå°†ä¸€ç»´æ—¶é—´ä½ç½®ä¸åŸºäºç¬›å¡å°”çš„ç©ºé—´åæ ‡ç›¸ç»“åˆä»¥æ„å»ºä¸‰å…ƒç»„æ··åˆä½ç½®ç´¢å¼•ï¼Œç„¶åé‡‡ç”¨é¢‘ç‡åˆ†é…ç­–ç•¥å¯¹ä¸‰ä¸ªç´¢å¼•åˆ†é‡ä¹‹é—´çš„æ—¶ç©ºä½ç½®ä¿¡æ¯è¿›è¡Œç¼–ç ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åˆ‡æ¯”é›ªå¤«å› æœæ©è”½ï¼Œå®ƒé€šè¿‡è®¡ç®— 2D ç©ºé—´ä¸­å›¾åƒæ ‡è®°çš„åˆ‡æ¯”é›ªå¤«è·ç¦»æ¥ç¡®å®šå› æœä¾èµ–æ€§ã€‚åŒ…æ‹¬ 3D åœºæ™¯æ¨ç†å’Œ 3D è§†è§‰é—®ç­”åœ¨å†…çš„å„ç§åŸºå‡†çš„è¯„ä¼°ç»“æœè¯æ˜äº† C^2RoPE çš„æœ‰æ•ˆæ€§ã€‚è¯¥ä»£ç å¯ä» https://github.com/ErikZ719/C2RoPE è·å–ã€‚

</details>

---

## 33. Enhancing Weakly Supervised Multimodal Video Anomaly Detection through Text Guidance

**ä¸­æ–‡æ ‡é¢˜**: é€šè¿‡æ–‡æœ¬å¼•å¯¼å¢å¼ºå¼±ç›‘ç£å¤šæ¨¡æ€è§†é¢‘å¼‚å¸¸æ£€æµ‹

**Date**: 2026-02-11 | **arXiv**: [2602.10549v1](http://arxiv.org/abs/2602.10549v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10549v1)

<details><summary><b>Abstract</b></summary>

Weakly supervised multimodal video anomaly detection has gained significant attention, yet the potential of the text modality remains under-explored. Text provides explicit semantic information that can enhance anomaly characterization and reduce false alarms. However, extracting effective text features is challenging due to the inability of general-purpose language models to capture anomaly-specific nuances and the scarcity of relevant descriptions. Furthermore, multimodal fusion often suffers from redundancy and imbalance. To address these issues, we propose a novel text-guided framework. First, we introduce an in-context learning-based multi-stage text augmentation mechanism to generate high-quality anomaly text samples for fine-tuning the text feature extractor. Second, we design a multi-scale bottleneck Transformer fusion module that uses compressed bottleneck tokens to progressively integrate information across modalities, mitigating redundancy and imbalance. Experiments on UCF-Crime and XD-Violence demonstrate state-of-the-art performance.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¼±ç›‘ç£å¤šæ¨¡æ€è§†é¢‘å¼‚å¸¸æ£€æµ‹å·²å¼•èµ·å¹¿æ³›å…³æ³¨ï¼Œä½†æ–‡æœ¬æ¨¡æ€çš„æ½œåŠ›ä»æœªå¾—åˆ°å……åˆ†å¼€å‘ã€‚æ–‡æœ¬æä¾›æ˜ç¡®çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¯ä»¥å¢å¼ºå¼‚å¸¸è¡¨å¾å¹¶å‡å°‘è¯¯æŠ¥ã€‚ç„¶è€Œï¼Œç”±äºé€šç”¨è¯­è¨€æ¨¡å‹æ— æ³•æ•è·å¼‚å¸¸ç‰¹å®šçš„ç»†å¾®å·®åˆ«ä»¥åŠç›¸å…³æè¿°çš„ç¨€ç¼ºï¼Œæå–æœ‰æ•ˆçš„æ–‡æœ¬ç‰¹å¾å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æ­¤å¤–ï¼Œå¤šæ¨¡æ€èåˆç»å¸¸é­å—å†—ä½™å’Œä¸å¹³è¡¡çš„å›°æ‰°ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–‡æœ¬å¼•å¯¼æ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥åŸºäºä¸Šä¸‹æ–‡å­¦ä¹ çš„å¤šé˜¶æ®µæ–‡æœ¬å¢å¼ºæœºåˆ¶æ¥ç”Ÿæˆé«˜è´¨é‡çš„å¼‚å¸¸æ–‡æœ¬æ ·æœ¬ï¼Œä»¥å¾®è°ƒæ–‡æœ¬ç‰¹å¾æå–å™¨ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šå°ºåº¦ç“¶é¢ˆ Transformer èåˆæ¨¡å—ï¼Œè¯¥æ¨¡å—ä½¿ç”¨å‹ç¼©çš„ç“¶é¢ˆä»¤ç‰Œé€æ­¥é›†æˆè·¨æ¨¡æ€çš„ä¿¡æ¯ï¼Œä»è€Œå‡è½»å†—ä½™å’Œä¸å¹³è¡¡ã€‚ UCF-Crime å’Œ XD-Violence ä¸Šçš„å®éªŒå±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

</details>

---

## 34. MapVerse: A Benchmark for Geospatial Question Answering on Diverse Real-World Maps

**ä¸­æ–‡æ ‡é¢˜**: MapVerseï¼šå„ç§çœŸå®ä¸–ç•Œåœ°å›¾ä¸Šçš„åœ°ç†ç©ºé—´é—®ç­”çš„åŸºå‡†

**Date**: 2026-02-11 | **arXiv**: [2602.10518v1](http://arxiv.org/abs/2602.10518v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10518v1)

<details><summary><b>Abstract</b></summary>

Maps are powerful carriers of structured and contextual knowledge, encompassing geography, demographics, infrastructure, and environmental patterns. Reasoning over such knowledge requires models to integrate spatial relationships, visual cues, real-world context, and domain-specific expertise-capabilities that current large language models (LLMs) and vision-language models (VLMs) still struggle to exhibit consistently. Yet, datasets used to benchmark VLMs on map-based reasoning remain narrow in scope, restricted to specific domains, and heavily reliant on artificially generated content (outputs from LLMs or pipeline-based methods), offering limited depth for evaluating genuine geospatial reasoning. To address this gap, we present MapVerse, a large-scale benchmark built on real-world maps. It comprises 11,837 human-authored question-answer pairs across 1,025 maps, spanning ten diverse map categories and multiple question categories for each. The dataset provides a rich setting for evaluating map reading, interpretation, and multimodal reasoning. We evaluate ten state-of-the-art models against our benchmark to establish baselines and quantify reasoning gaps. Beyond overall performance, we conduct fine-grained categorical analyses to assess model inference across multiple dimensions and investigate the visual factors shaping reasoning outcomes. Our findings reveal that while current VLMs perform competitively on classification-style tasks, both open- and closed-source models fall short on advanced tasks requiring complex spatial reasoning.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åœ°å›¾æ˜¯ç»“æ„åŒ–å’Œæƒ…å¢ƒçŸ¥è¯†çš„å¼ºå¤§è½½ä½“ï¼Œæ¶µç›–åœ°ç†ã€äººå£ç»Ÿè®¡ã€åŸºç¡€è®¾æ–½å’Œç¯å¢ƒæ¨¡å¼ã€‚å¯¹è¿™äº›çŸ¥è¯†çš„æ¨ç†éœ€è¦æ¨¡å‹æ•´åˆç©ºé—´å…³ç³»ã€è§†è§‰çº¿ç´¢ã€ç°å®ä¸–ç•ŒèƒŒæ™¯å’Œç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†èƒ½åŠ›ï¼Œè€Œå½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) å’Œè§†è§‰è¯­è¨€æ¨¡å‹ (VLM) ä»ç„¶éš¾ä»¥ä¸€è‡´åœ°å±•ç¤ºè¿™äº›èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”¨äºåŸºäºåœ°å›¾æ¨ç†å¯¹ VLM è¿›è¡ŒåŸºå‡†æµ‹è¯•çš„æ•°æ®é›†èŒƒå›´ä»ç„¶å¾ˆçª„ï¼Œä»…é™äºç‰¹å®šé¢†åŸŸï¼Œå¹¶ä¸”ä¸¥é‡ä¾èµ–äººå·¥ç”Ÿæˆçš„å†…å®¹ï¼ˆLLM æˆ–åŸºäºç®¡é“çš„æ–¹æ³•çš„è¾“å‡ºï¼‰ï¼Œä¸ºè¯„ä¼°çœŸæ­£çš„åœ°ç†ç©ºé—´æ¨ç†æä¾›çš„æ·±åº¦æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº† MapVerseï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºçœŸå®ä¸–ç•Œåœ°å›¾çš„å¤§å‹åŸºå‡†æµ‹è¯•ã€‚å®ƒåŒ…å« 1,025 ä¸ªåœ°å›¾ä¸­çš„ 11,837 ä¸ªäººå·¥ç¼–å†™çš„é—®ç­”å¯¹ï¼Œæ¶µç›– 10 ä¸ªä¸åŒçš„åœ°å›¾ç±»åˆ«ï¼Œå¹¶ä¸”æ¯ä¸ªç±»åˆ«éƒ½æœ‰å¤šä¸ªé—®é¢˜ç±»åˆ«ã€‚è¯¥æ•°æ®é›†ä¸ºè¯„ä¼°åœ°å›¾é˜…è¯»ã€è§£é‡Šå’Œå¤šæ¨¡å¼æ¨ç†æä¾›äº†ä¸°å¯Œçš„è®¾ç½®ã€‚æˆ‘ä»¬æ ¹æ®æˆ‘ä»¬çš„åŸºå‡†è¯„ä¼°åä¸ªæœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œä»¥å»ºç«‹åŸºçº¿å¹¶é‡åŒ–æ¨ç†å·®è·ã€‚é™¤äº†æ•´ä½“è¡¨ç°ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œç»†ç²’åº¦çš„åˆ†ç±»åˆ†æï¼Œä»¥è¯„ä¼°å¤šä¸ªç»´åº¦çš„æ¨¡å‹æ¨ç†ï¼Œå¹¶ç ”ç©¶å¡‘é€ æ¨ç†ç»“æœçš„è§†è§‰å› ç´ ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶å½“å‰çš„ VLM åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°å¾—å…·æœ‰ç«äº‰åŠ›ï¼Œä½†å¼€æºå’Œé—­æºæ¨¡å‹åœ¨éœ€è¦å¤æ‚ç©ºé—´æ¨ç†çš„é«˜çº§ä»»åŠ¡ä¸Šéƒ½è¡¨ç°ä¸ä½³ã€‚

</details>

---

## 35. 3DXTalker: Unifying Identity, Lip Sync, Emotion, and Spatial Dynamics in Expressive 3D Talking Avatars

**ä¸­æ–‡æ ‡é¢˜**: 3DXTalkerï¼šåœ¨å¯Œæœ‰è¡¨ç°åŠ›çš„ 3D è¯´è¯åŒ–èº«ä¸­ç»Ÿä¸€èº«ä»½ã€å£å‹åŒæ­¥ã€æƒ…æ„Ÿå’Œç©ºé—´åŠ¨æ€

**Date**: 2026-02-11 | **arXiv**: [2602.10516v1](http://arxiv.org/abs/2602.10516v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10516v1)

<details><summary><b>Abstract</b></summary>

Audio-driven 3D talking avatar generation is increasingly important in virtual communication, digital humans, and interactive media, where avatars must preserve identity, synchronize lip motion with speech, express emotion, and exhibit lifelike spatial dynamics, collectively defining a broader objective of expressivity. However, achieving this remains challenging due to insufficient training data with limited subject identities, narrow audio representations, and restricted explicit controllability. In this paper, we propose 3DXTalker, an expressive 3D talking avatar through data-curated identity modeling, audio-rich representations, and spatial dynamics controllability. 3DXTalker enables scalable identity modeling via 2D-to-3D data curation pipeline and disentangled representations, alleviating data scarcity and improving identity generalization. Then, we introduce frame-wise amplitude and emotional cues beyond standard speech embeddings, ensuring superior lip synchronization and nuanced expression modulation. These cues are unified by a flow-matching-based transformer for coherent facial dynamics. Moreover, 3DXTalker also enables natural head-pose motion generation while supporting stylized control via prompt-based conditioning. Extensive experiments show that 3DXTalker integrates lip synchronization, emotional expression, and head-pose dynamics within a unified framework, achieves superior performance in 3D talking avatar generation.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

éŸ³é¢‘é©±åŠ¨çš„ 3D è¯´è¯åŒ–èº«ç”Ÿæˆåœ¨è™šæ‹Ÿé€šä¿¡ã€æ•°å­—äººç±»å’Œäº¤äº’å¼åª’ä½“ä¸­å˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œå…¶ä¸­åŒ–èº«å¿…é¡»ä¿æŒèº«ä»½ã€ä½¿å˜´å”‡è¿åŠ¨ä¸è¯­éŸ³åŒæ­¥ã€è¡¨è¾¾æƒ…æ„Ÿå¹¶å±•ç°é€¼çœŸçš„ç©ºé—´åŠ¨æ€ï¼Œå…±åŒå®šä¹‰æ›´å¹¿æ³›çš„è¡¨ç°åŠ›ç›®æ ‡ã€‚ç„¶è€Œï¼Œç”±äºè®­ç»ƒæ•°æ®ä¸è¶³ã€å—è¯•è€…èº«ä»½æœ‰é™ã€éŸ³é¢‘è¡¨ç¤ºç‹­çª„ä»¥åŠæ˜¾å¼å¯æ§æ€§æœ‰é™ï¼Œå®ç°è¿™ä¸€ç›®æ ‡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† 3DXTalkerï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡æ•°æ®ç®¡ç†çš„èº«ä»½å»ºæ¨¡ã€ä¸°å¯Œçš„éŸ³é¢‘è¡¨ç¤ºå’Œç©ºé—´åŠ¨æ€å¯æ§æ€§å®ç°çš„å¯Œæœ‰è¡¨ç°åŠ›çš„ 3D è¯´è¯åŒ–èº«ã€‚ 3DXTalker é€šè¿‡ 2D åˆ° 3D æ•°æ®ç®¡ç†ç®¡é“å’Œè§£å¼€çš„è¡¨ç¤ºå®ç°å¯æ‰©å±•çš„èº«ä»½å»ºæ¨¡ï¼Œä»è€Œç¼“è§£æ•°æ®ç¨€ç¼ºå¹¶æé«˜èº«ä»½æ³›åŒ–ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥äº†æ ‡å‡†è¯­éŸ³åµŒå…¥ä¹‹å¤–çš„é€å¸§å¹…åº¦å’Œæƒ…æ„Ÿçº¿ç´¢ï¼Œç¡®ä¿å“è¶Šçš„å”‡å½¢åŒæ­¥å’Œç»†è‡´å…¥å¾®çš„è¡¨è¾¾è°ƒåˆ¶ã€‚è¿™äº›çº¿ç´¢é€šè¿‡åŸºäºæµåŒ¹é…çš„è½¬æ¢å™¨è¿›è¡Œç»Ÿä¸€ï¼Œä»¥å®ç°è¿è´¯çš„é¢éƒ¨åŠ¨æ€ã€‚æ­¤å¤–ï¼Œ3DXTalker è¿˜å¯ä»¥ç”Ÿæˆè‡ªç„¶çš„å¤´éƒ¨å§¿åŠ¿è¿åŠ¨ï¼ŒåŒæ—¶é€šè¿‡åŸºäºæç¤ºçš„è°ƒèŠ‚æ”¯æŒé£æ ¼åŒ–æ§åˆ¶ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œ3DXTalkerå°†å”‡å½¢åŒæ­¥ã€æƒ…ç»ªè¡¨è¾¾å’Œå¤´éƒ¨å§¿åŠ¿åŠ¨æ€é›†æˆåœ¨ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶å†…ï¼Œåœ¨3Dè¯´è¯å¤´åƒç”Ÿæˆæ–¹é¢å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚

</details>

---

## 36. 1%>100%: High-Efficiency Visual Adapter with Complex Linear Projection Optimization

**ä¸­æ–‡æ ‡é¢˜**: 1%>100%ï¼šå…·æœ‰å¤æ‚çº¿æ€§æŠ•å½±ä¼˜åŒ–çš„é«˜æ•ˆè§†è§‰é€‚é…å™¨

**Date**: 2026-02-11 | **arXiv**: [2602.10513v1](http://arxiv.org/abs/2602.10513v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10513v1)

**Code**: https://github.com/DongshuoYin/CoLin.

<details><summary><b>Abstract</b></summary>

Deploying vision foundation models typically relies on efficient adaptation strategies, whereas conventional full fine-tuning suffers from prohibitive costs and low efficiency. While delta-tuning has proven effective in boosting the performance and efficiency of LLMs during adaptation, its advantages cannot be directly transferred to the fine-tuning pipeline of vision foundation models. To push the boundaries of adaptation efficiency for vision tasks, we propose an adapter with Complex Linear Projection Optimization (CoLin). For architecture, we design a novel low-rank complex adapter that introduces only about 1% parameters to the backbone. For efficiency, we theoretically prove that low-rank composite matrices suffer from severe convergence issues during training, and address this challenge with a tailored loss. Extensive experiments on object detection, segmentation, image classification, and rotated object detection (remote sensing scenario) demonstrate that CoLin outperforms both full fine-tuning and classical delta-tuning approaches with merely 1% parameters for the first time, providing a novel and efficient solution for deployment of vision foundation models. We release the code on https://github.com/DongshuoYin/CoLin.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

éƒ¨ç½²è§†è§‰åŸºç¡€æ¨¡å‹é€šå¸¸ä¾èµ–äºé«˜æ•ˆçš„é€‚åº”ç­–ç•¥ï¼Œè€Œä¼ ç»Ÿçš„å…¨é¢å¾®è°ƒæˆæœ¬é«˜æ˜‚ä¸”æ•ˆç‡ä½ä¸‹ã€‚è™½ç„¶ Delta-tuning å·²è¢«è¯æ˜å¯ä»¥æœ‰æ•ˆæé«˜ LLM åœ¨é€‚åº”è¿‡ç¨‹ä¸­çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œä½†å…¶ä¼˜åŠ¿ä¸èƒ½ç›´æ¥è½¬ç§»åˆ°è§†è§‰åŸºç¡€æ¨¡å‹çš„å¾®è°ƒæµç¨‹ä¸­ã€‚ä¸ºäº†çªç ´è§†è§‰ä»»åŠ¡é€‚åº”æ•ˆç‡çš„ç•Œé™ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…·æœ‰å¤æ‚çº¿æ€§æŠ•å½±ä¼˜åŒ–ï¼ˆCoLinï¼‰çš„é€‚é…å™¨ã€‚å¯¹äºæ¶æ„ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„ä½é˜¶å¤æ‚é€‚é…å™¨ï¼Œä»…å‘ä¸»å¹²å¼•å…¥çº¦ 1% çš„å‚æ•°ã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜ä½ç§©å¤åˆçŸ©é˜µåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šé‡åˆ°ä¸¥é‡çš„æ”¶æ•›é—®é¢˜ï¼Œå¹¶é€šè¿‡å®šåˆ¶æŸå¤±æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚åœ¨å¯¹è±¡æ£€æµ‹ã€åˆ†å‰²ã€å›¾åƒåˆ†ç±»å’Œæ—‹è½¬å¯¹è±¡æ£€æµ‹ï¼ˆé¥æ„Ÿåœºæ™¯ï¼‰æ–¹é¢çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCoLin é¦–æ¬¡ä»…ç”¨ 1% çš„å‚æ•°å°±ä¼˜äºå®Œå…¨å¾®è°ƒå’Œç»å…¸çš„ delta-tuning æ–¹æ³•ï¼Œä¸ºè§†è§‰åŸºç¡€æ¨¡å‹çš„éƒ¨ç½²æä¾›äº†ä¸€ç§æ–°é¢–ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬åœ¨ https://github.com/DongshuoYin/CoLin ä¸Šå‘å¸ƒäº†ä»£ç ã€‚

</details>

---

## 37. The Garbage Dataset (GD): A Multi-Class Image Benchmark for Automated Waste Segregation

**ä¸­æ–‡æ ‡é¢˜**: åƒåœ¾æ•°æ®é›† (GD)ï¼šè‡ªåŠ¨åƒåœ¾åˆ†ç±»çš„å¤šç±»å›¾åƒåŸºå‡†

**Date**: 2026-02-11 | **arXiv**: [2602.10500v1](http://arxiv.org/abs/2602.10500v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10500v1)

<details><summary><b>Abstract</b></summary>

This study introduces the Garbage Dataset (GD), a publicly available image dataset designed to advance automated waste segregation through machine learning and computer vision. It's a diverse dataset covering 10 common household waste categories: metal, glass, biological, paper, battery, trash, cardboard, shoes, clothes, and plastic. The dataset comprises 13,348 labeled images collected through multiple methods, including DWaste mobile app and curated web sources. Methods included rigorous validation through checksums and outlier detection, analysis of class imbalance and visual separability via PCA/t-SNE, and assessment of background complexity using entropy and saliency measures. The dataset was benchmarked using state-of-the-art deep learning models (EfficientNetV2M, EfficientNetV2S, MobileNet, ResNet50, ResNet101) evaluated on performance metrics and operational carbon emissions. Experiment results indicate EfficientNetV2S achieved the highest performance with 96.19% accuracy and a 0.96 F1-score, though with a moderate carbon cost. Analysis revealed inherent dataset characteristics including class imbalance, a skew toward high-outlier classes (plastic, cardboard, paper), and brightness variations that require consideration. The main conclusion is that GD provides a valuable, real-world benchmark for waste classification research while highlighting important challenges such as class imbalance, background complexity, and environmental trade-offs in model selection that must be addressed for practical deployment. The dataset is publicly released to support further research in environmental sustainability applications.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è¿™é¡¹ç ”ç©¶å¼•å…¥äº†åƒåœ¾æ•°æ®é›†ï¼ˆGDï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¬å¼€çš„å›¾åƒæ•°æ®é›†ï¼Œæ—¨åœ¨é€šè¿‡æœºå™¨å­¦ä¹ å’Œè®¡ç®—æœºè§†è§‰æ¨è¿›è‡ªåŠ¨åƒåœ¾åˆ†ç±»ã€‚è¿™æ˜¯ä¸€ä¸ªå¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œæ¶µç›– 10 ç§å¸¸è§çš„å®¶åº­åºŸç‰©ç±»åˆ«ï¼šé‡‘å±ã€ç»ç’ƒã€ç”Ÿç‰©ã€çº¸å¼ ã€ç”µæ± ã€åƒåœ¾ã€çº¸æ¿ã€é‹å­ã€è¡£æœå’Œå¡‘æ–™ã€‚è¯¥æ•°æ®é›†åŒ…å«é€šè¿‡å¤šç§æ–¹æ³•æ”¶é›†çš„ 13,348 å¼ æ ‡è®°å›¾åƒï¼ŒåŒ…æ‹¬ DWaste ç§»åŠ¨åº”ç”¨ç¨‹åºå’Œç²¾é€‰çš„ç½‘ç»œèµ„æºã€‚æ–¹æ³•åŒ…æ‹¬é€šè¿‡æ ¡éªŒå’Œå’Œå¼‚å¸¸å€¼æ£€æµ‹è¿›è¡Œä¸¥æ ¼éªŒè¯ï¼Œé€šè¿‡ PCA/t-SNE åˆ†æç±»ä¸å¹³è¡¡å’Œè§†è§‰å¯åˆ†ç¦»æ€§ï¼Œä»¥åŠä½¿ç”¨ç†µå’Œæ˜¾ç€æ€§åº¦é‡è¯„ä¼°èƒŒæ™¯å¤æ‚æ€§ã€‚è¯¥æ•°æ®é›†ä½¿ç”¨æœ€å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆEfficientNetV2Mã€EfficientNetV2Sã€MobileNetã€ResNet50ã€ResNet101ï¼‰è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶æ ¹æ®æ€§èƒ½æŒ‡æ ‡å’Œè¿è¥ç¢³æ’æ”¾è¿›è¡Œè¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEfficientNetV2S å®ç°äº†æœ€é«˜æ€§èƒ½ï¼Œå‡†ç¡®ç‡è¾¾åˆ° 96.19%ï¼ŒF1 åˆ†æ•°ä¸º 0.96ï¼Œä½†ç¢³æˆæœ¬é€‚ä¸­ã€‚åˆ†ææ­ç¤ºäº†å›ºæœ‰çš„æ•°æ®é›†ç‰¹å¾ï¼ŒåŒ…æ‹¬ç±»åˆ«ä¸å¹³è¡¡ã€å‘é«˜å¼‚å¸¸å€¼ç±»åˆ«ï¼ˆå¡‘æ–™ã€çº¸æ¿ã€çº¸å¼ ï¼‰çš„å€¾æ–œä»¥åŠéœ€è¦è€ƒè™‘çš„äº®åº¦å˜åŒ–ã€‚ä¸»è¦ç»“è®ºæ˜¯ï¼ŒGD ä¸ºåºŸç‰©åˆ†ç±»ç ”ç©¶æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„ã€çœŸå®çš„åŸºå‡†ï¼ŒåŒæ—¶å¼ºè°ƒäº†å®é™…éƒ¨ç½²ä¸­å¿…é¡»è§£å†³çš„é‡è¦æŒ‘æˆ˜ï¼Œä¾‹å¦‚ç±»åˆ«ä¸å¹³è¡¡ã€èƒŒæ™¯å¤æ‚æ€§ä»¥åŠæ¨¡å‹é€‰æ‹©ä¸­çš„ç¯å¢ƒæƒè¡¡ã€‚è¯¥æ•°æ®é›†çš„å…¬å¼€å‘å¸ƒæ˜¯ä¸ºäº†æ”¯æŒç¯å¢ƒå¯æŒç»­æ€§åº”ç”¨çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚

</details>

---

## 38. Towards Remote Sensing Change Detection with Neural Memory

**ä¸­æ–‡æ ‡é¢˜**: åˆ©ç”¨ç¥ç»è®°å¿†è¿›è¡Œé¥æ„Ÿå˜åŒ–æ£€æµ‹

**Date**: 2026-02-11 | **arXiv**: [2602.10491v1](http://arxiv.org/abs/2602.10491v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10491v1)

<details><summary><b>Abstract</b></summary>

Remote sensing change detection is essential for environmental monitoring, urban planning, and related applications. However, current methods often struggle to capture long-range dependencies while maintaining computational efficiency. Although Transformers can effectively model global context, their quadratic complexity poses scalability challenges, and existing linear attention approaches frequently fail to capture intricate spatiotemporal relationships. Drawing inspiration from the recent success of Titans in language tasks, we present ChangeTitans, the Titans-based framework for remote sensing change detection. Specifically, we propose VTitans, the first Titans-based vision backbone that integrates neural memory with segmented local attention, thereby capturing long-range dependencies while mitigating computational overhead. Next, we present a hierarchical VTitans-Adapter to refine multi-scale features across different network layers. Finally, we introduce TS-CBAM, a two-stream fusion module leveraging cross-temporal attention to suppress pseudo-changes and enhance detection accuracy. Experimental evaluations on four benchmark datasets (LEVIR-CD, WHU-CD, LEVIR-CD+, and SYSU-CD) demonstrate that ChangeTitans achieves state-of-the-art results, attaining \textbf{84.36\%} IoU and \textbf{91.52\%} F1-score on LEVIR-CD, while remaining computationally competitive.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

é¥æ„Ÿå˜åŒ–æ£€æµ‹å¯¹äºç¯å¢ƒç›‘æµ‹ã€åŸå¸‚è§„åˆ’å’Œç›¸å…³åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•é€šå¸¸éš¾ä»¥åœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶æ•è·è¿œç¨‹ä¾èµ–æ€§ã€‚å°½ç®¡ Transformer å¯ä»¥æœ‰æ•ˆåœ°å¯¹å…¨å±€ä¸Šä¸‹æ–‡è¿›è¡Œå»ºæ¨¡ï¼Œä½†å…¶äºŒæ¬¡å¤æ‚æ€§å¸¦æ¥äº†å¯æ‰©å±•æ€§æŒ‘æˆ˜ï¼Œå¹¶ä¸”ç°æœ‰çš„çº¿æ€§æ³¨æ„åŠ›æ–¹æ³•ç»å¸¸æ— æ³•æ•è·å¤æ‚çš„æ—¶ç©ºå…³ç³»ã€‚ä»æœ€è¿‘ Titans åœ¨è¯­è¨€ä»»åŠ¡ä¸­å–å¾—çš„æˆåŠŸä¸­æ±²å–çµæ„Ÿï¼Œæˆ‘ä»¬æ¨å‡ºäº† ChangeTitansï¼Œè¿™æ˜¯åŸºäº Titans çš„é¥æ„Ÿå˜åŒ–æ£€æµ‹æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº† VTitansï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäº Titans çš„è§†è§‰ä¸»å¹²ï¼Œå®ƒå°†ç¥ç»è®°å¿†ä¸åˆ†æ®µå±€éƒ¨æ³¨æ„åŠ›ç›¸ç»“åˆï¼Œä»è€Œæ•è·è¿œç¨‹ä¾èµ–æ€§ï¼ŒåŒæ—¶å‡å°‘è®¡ç®—å¼€é”€ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ†å±‚çš„ VTitans-Adapter æ¥ç»†åŒ–ä¸åŒç½‘ç»œå±‚çš„å¤šå°ºåº¦ç‰¹å¾ã€‚æœ€åï¼Œæˆ‘ä»¬ä»‹ç»äº† TS-CBAMï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨è·¨æ—¶é—´æ³¨æ„åŠ›æ¥æŠ‘åˆ¶ä¼ªå˜åŒ–å¹¶æé«˜æ£€æµ‹ç²¾åº¦çš„ä¸¤æµèåˆæ¨¡å—ã€‚å¯¹å››ä¸ªåŸºå‡†æ•°æ®é›†ï¼ˆLEVIR-CDã€WHU-CDã€LEVIR-CD+ å’Œ SYSU-CDï¼‰çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒChangeTitans å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œåœ¨ LEVIR-CD ä¸Šè·å¾— \textbf{84.36\%} IoU å’Œ \textbf{91.52\%} F1 åˆ†æ•°ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—ç«äº‰åŠ›ã€‚

</details>

---

## 39. HII-DPO: Eliminate Hallucination via Accurate Hallucination-Inducing Counterfactual Images

**ä¸­æ–‡æ ‡é¢˜**: HII-DPOï¼šé€šè¿‡å‡†ç¡®çš„è¯±å‘å¹»è§‰çš„åäº‹å®å›¾åƒæ¶ˆé™¤å¹»è§‰

**Date**: 2026-02-11 | **arXiv**: [2602.10425v1](http://arxiv.org/abs/2602.10425v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10425v1)

<details><summary><b>Abstract</b></summary>

Large Vision-Language Models (VLMs) have achieved remarkable success across diverse multimodal tasks but remain vulnerable to hallucinations rooted in inherent language bias. Despite recent progress, existing hallucination mitigation methods often overlook the underlying hallucination patterns driven by language bias. In this work, we design a novel pipeline to accurately synthesize Hallucination-Inducing Images (HIIs). Using synthesized HIIs, we reveal a consistent scene-conditioned hallucination pattern: models tend to mention objects that are highly typical of the scene even when visual evidence is removed. To quantify the susceptibility of VLMs to this hallucination pattern, we establish the Masked-Object-Hallucination (MOH) benchmark to rigorously evaluate existing state-of-the-art alignment frameworks. Finally, we leverage HIIs to construct high-quality preference datasets for fine-grained alignment. Experimental results demonstrate that our approach effectively mitigates hallucinations while preserving general model capabilities. Specifically, our method achieves up to a 38% improvement over the current state-of-the-art on standard hallucination benchmarks.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å„ç§å¤šæ¨¡å¼ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾ç€çš„æˆåŠŸï¼Œä½†ä»ç„¶å®¹æ˜“å—åˆ°æºäºå›ºæœ‰è¯­è¨€åè§çš„å¹»è§‰çš„å½±å“ã€‚å°½ç®¡æœ€è¿‘å–å¾—äº†è¿›å±•ï¼Œä½†ç°æœ‰çš„å¹»è§‰ç¼“è§£æ–¹æ³•å¸¸å¸¸å¿½è§†ç”±è¯­è¨€åè§é©±åŠ¨çš„æ½œåœ¨å¹»è§‰æ¨¡å¼ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„ç®¡é“æ¥å‡†ç¡®åˆæˆå¹»è§‰è¯±å¯¼å›¾åƒï¼ˆHIIï¼‰ã€‚ä½¿ç”¨åˆæˆçš„ HIIï¼Œæˆ‘ä»¬æ­ç¤ºäº†ä¸€è‡´çš„åœºæ™¯æ¡ä»¶å¹»è§‰æ¨¡å¼ï¼šå³ä½¿è§†è§‰è¯æ®è¢«ç§»é™¤ï¼Œæ¨¡å‹ä¹Ÿå€¾å‘äºæåŠåœºæ™¯ä¸­é«˜åº¦å…¸å‹çš„ç‰©ä½“ã€‚ä¸ºäº†é‡åŒ– VLM å¯¹è¿™ç§å¹»è§‰æ¨¡å¼çš„æ•æ„Ÿæ€§ï¼Œæˆ‘ä»¬å»ºç«‹äº†è’™é¢ç‰©ä½“å¹»è§‰ (MOH) åŸºå‡†æ¥ä¸¥æ ¼è¯„ä¼°ç°æœ‰çš„æœ€å…ˆè¿›çš„å¯¹é½æ¡†æ¶ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ©ç”¨ HII æ„å»ºé«˜è´¨é‡çš„åå¥½æ•°æ®é›†ä»¥è¿›è¡Œç»†ç²’åº¦å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°å‡è½»äº†å¹»è§‰ï¼ŒåŒæ—¶ä¿ç•™äº†ä¸€èˆ¬æ¨¡å‹çš„èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ ‡å‡†å¹»è§‰åŸºå‡†ä¸Šæ¯”å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº† 38%ã€‚

</details>

---

## 40. Fine-Tuning GPT-5 for GPU Kernel Generation

**ä¸­æ–‡æ ‡é¢˜**: é’ˆå¯¹ GPU å†…æ ¸ç”Ÿæˆå¾®è°ƒ GPT-5

**Date**: 2026-02-11 | **arXiv**: [2602.11000v1](http://arxiv.org/abs/2602.11000v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11000v1)

<details><summary><b>Abstract</b></summary>

Developing efficient GPU kernels is essential for scaling modern AI systems, yet it remains a complex task due to intricate hardware architectures and the need for specialized optimization expertise. Although Large Language Models (LLMs) demonstrate strong capabilities in general sequential code generation, they face significant challenges in GPU code generation because of the scarcity of high-quality labeled training data, compiler biases when generating synthetic solutions, and limited generalization across hardware generations. This precludes supervised fine-tuning (SFT) as a scalable methodology for improving current LLMs. In contrast, reinforcement learning (RL) offers a data-efficient and adaptive alternative but requires access to relevant tools, careful selection of training problems, and a robust evaluation environment. We present Makora's environment and tools for reinforcement learning finetuning of frontier models and report our results from fine-tuning GPT-5 for Triton code generation. In the single-attempt setting, our fine-tuned model improves kernel correctness from 43.7% to 77.0% (+33.3 percentage points) and increases the fraction of problems outperforming TorchInductor from 14.8% to 21.8% (+7 percentage points) compared to baseline GPT-5, while exceeding prior state-of-the-art models on KernelBench. When integrated into a full coding agent, it is able to solve up to 97.4% of problems in an expanded KernelBench suite, outperforming the PyTorch TorchInductor compiler on 72.9% of problems with a geometric mean speedup of 2.12x. Our work demonstrates that targeted post-training with reinforcement learning can unlock LLM capabilities in highly specialized technical domains where traditional supervised learning is limited by data availability, opening new pathways for AI-assisted accelerator programming.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¼€å‘é«˜æ•ˆçš„ GPU å†…æ ¸å¯¹äºæ‰©å±•ç°ä»£äººå·¥æ™ºèƒ½ç³»ç»Ÿè‡³å…³é‡è¦ï¼Œä½†ç”±äºå¤æ‚çš„ç¡¬ä»¶æ¶æ„å’Œå¯¹ä¸“ä¸šä¼˜åŒ–ä¸“ä¸šçŸ¥è¯†çš„éœ€æ±‚ï¼Œå®ƒä»ç„¶æ˜¯ä¸€é¡¹å¤æ‚çš„ä»»åŠ¡ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) åœ¨é€šç”¨é¡ºåºä»£ç ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ç”±äºç¼ºä¹é«˜è´¨é‡çš„æ ‡è®°è®­ç»ƒæ•°æ®ã€ç”Ÿæˆç»¼åˆè§£å†³æ–¹æ¡ˆæ—¶çš„ç¼–è¯‘å™¨åå·®ä»¥åŠè·¨ç¡¬ä»¶ä»£çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œå®ƒä»¬åœ¨ GPU ä»£ç ç”Ÿæˆæ–¹é¢é¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ã€‚è¿™å°±æ’é™¤äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä½œä¸ºæ”¹è¿›å½“å‰æ³•å­¦ç¡•å£«çš„å¯æ‰©å±•æ–¹æ³•çš„å¯èƒ½æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æä¾›äº†ä¸€ç§æ•°æ®é«˜æ•ˆä¸”é€‚åº”æ€§å¼ºçš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†éœ€è¦è®¿é—®ç›¸å…³å·¥å…·ã€ä»”ç»†é€‰æ‹©è®­ç»ƒé—®é¢˜ä»¥åŠå¼ºå¤§çš„è¯„ä¼°ç¯å¢ƒã€‚æˆ‘ä»¬ä»‹ç»äº† Makora ç”¨äºå¼ºåŒ–å­¦ä¹ å¾®è°ƒå‰æ²¿æ¨¡å‹çš„ç¯å¢ƒå’Œå·¥å…·ï¼Œå¹¶æŠ¥å‘Šäº†å¾®è°ƒ GPT-5 ä»¥ç”Ÿæˆ Triton ä»£ç çš„ç»“æœã€‚åœ¨å•æ¬¡å°è¯•è®¾ç½®ä¸­ï¼Œä¸åŸºçº¿ GPT-5 ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„å¾®è°ƒæ¨¡å‹å°†å†…æ ¸æ­£ç¡®æ€§ä» 43.7% æé«˜åˆ° 77.0%ï¼ˆ+33.3 ä¸ªç™¾åˆ†ç‚¹ï¼‰ï¼Œå¹¶å°†ä¼˜äº TorchInductor çš„é—®é¢˜æ¯”ä¾‹ä» 14.8% å¢åŠ åˆ° 21.8%ï¼ˆ+7 ä¸ªç™¾åˆ†ç‚¹ï¼‰ï¼ŒåŒæ—¶è¶…è¿‡äº† KernelBench ä¸Šå…ˆå‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚å½“é›†æˆåˆ°å®Œæ•´çš„ç¼–ç ä»£ç†ä¸­æ—¶ï¼Œå®ƒèƒ½å¤Ÿè§£å†³æ‰©å±• KernelBench å¥—ä»¶ä¸­é«˜è¾¾ 97.4% çš„é—®é¢˜ï¼Œåœ¨ 72.9% çš„é—®é¢˜ä¸Šä¼˜äº PyTorch TorchInductor ç¼–è¯‘å™¨ï¼Œå‡ ä½•å¹³å‡åŠ é€Ÿç‡ä¸º 2.12 å€ã€‚æˆ‘ä»¬çš„å·¥ä½œè¡¨æ˜ï¼Œæœ‰é’ˆå¯¹æ€§çš„å¼ºåŒ–å­¦ä¹ ååŸ¹è®­å¯ä»¥åœ¨é«˜åº¦ä¸“ä¸šåŒ–çš„æŠ€æœ¯é¢†åŸŸé‡Šæ”¾æ³•å­¦ç¡•å£«çš„èƒ½åŠ›ï¼Œåœ¨è¿™äº›é¢†åŸŸï¼Œä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ å—åˆ°æ•°æ®å¯ç”¨æ€§çš„é™åˆ¶ï¼Œä¸ºäººå·¥æ™ºèƒ½è¾…åŠ©åŠ é€Ÿå™¨ç¼–ç¨‹å¼€è¾Ÿæ–°çš„é€”å¾„ã€‚

</details>

---

## 41. LoRA-Squeeze: Simple and Effective Post-Tuning and In-Tuning Compression of LoRA Modules

**ä¸­æ–‡æ ‡é¢˜**: LoRA-Squeezeï¼šLoRA æ¨¡å—çš„ç®€å•æœ‰æ•ˆçš„åè°ƒå’Œè°ƒä¸­å‹ç¼©

**Date**: 2026-02-11 | **arXiv**: [2602.10993v1](http://arxiv.org/abs/2602.10993v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10993v1)

<details><summary><b>Abstract</b></summary>

Despite its huge number of variants, standard Low-Rank Adaptation (LoRA) is still a dominant technique for parameter-efficient fine-tuning (PEFT). Nonetheless, it faces persistent challenges, including the pre-selection of an optimal rank and rank-specific hyper-parameters, as well as the deployment complexity of heterogeneous-rank modules and more sophisticated LoRA derivatives. In this work, we introduce LoRA-Squeeze, a simple and efficient methodology that aims to improve standard LoRA learning by changing LoRA module ranks either post-hoc or dynamically during training}. Our approach posits that it is better to first learn an expressive, higher-rank solution and then compress it, rather than learning a constrained, low-rank solution directly. The method involves fine-tuning with a deliberately high(er) source rank, reconstructing or efficiently approximating the reconstruction of the full weight update matrix, and then using Randomized Singular Value Decomposition (RSVD) to create a new, compressed LoRA module at a lower target rank. Extensive experiments across 13 text and 10 vision-language tasks show that post-hoc compression often produces lower-rank adapters that outperform those trained directly at the target rank, especially if a small number of fine-tuning steps at the target rank is allowed. Moreover, a gradual, in-tuning rank annealing variant of LoRA-Squeeze consistently achieves the best LoRA size-performance trade-off.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å°½ç®¡æœ‰å¤§é‡å˜ä½“ï¼Œæ ‡å‡†ä½ç§©é€‚åº” (LoRA) ä»ç„¶æ˜¯å‚æ•°é«˜æ•ˆå¾®è°ƒ (PEFT) çš„ä¸»å¯¼æŠ€æœ¯ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå®ƒä»ç„¶é¢ä¸´ç€æŒç»­çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬é¢„é€‰æ‹©æœ€ä½³ç­‰çº§å’Œç‰¹å®šäºç­‰çº§çš„è¶…å‚æ•°ï¼Œä»¥åŠå¼‚æ„ç­‰çº§æ¨¡å—å’Œæ›´å¤æ‚çš„ LoRA è¡ç”Ÿå“çš„éƒ¨ç½²å¤æ‚æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº† LoRA-Squeezeï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œé«˜æ•ˆçš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡äº‹åæˆ–åœ¨è®­ç»ƒæœŸé—´åŠ¨æ€æ›´æ”¹ LoRA æ¨¡å—æ’åæ¥æ”¹è¿›æ ‡å‡† LoRA å­¦ä¹ }ã€‚æˆ‘ä»¬çš„æ–¹æ³•å‡è®¾ï¼Œæœ€å¥½é¦–å…ˆå­¦ä¹ ä¸€ä¸ªå¯Œæœ‰è¡¨ç°åŠ›çš„é«˜ç§©è§£å†³æ–¹æ¡ˆï¼Œç„¶åå¯¹å…¶è¿›è¡Œå‹ç¼©ï¼Œè€Œä¸æ˜¯ç›´æ¥å­¦ä¹ ä¸€ä¸ªå—çº¦æŸçš„ä½ç§©è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•æ¶‰åŠä½¿ç”¨æ•…æ„è¾ƒé«˜ï¼ˆæ›´é«˜ï¼‰çš„æºç§©è¿›è¡Œå¾®è°ƒï¼Œé‡å»ºæˆ–æœ‰æ•ˆåœ°è¿‘ä¼¼é‡å»ºå…¨æƒé‡æ›´æ–°çŸ©é˜µï¼Œç„¶åä½¿ç”¨éšæœºå¥‡å¼‚å€¼åˆ†è§£ï¼ˆRSVDï¼‰ä»¥è¾ƒä½çš„ç›®æ ‡ç§©åˆ›å»ºæ–°çš„å‹ç¼© LoRA æ¨¡å—ã€‚è·¨ 13 ä¸ªæ–‡æœ¬å’Œ 10 ä¸ªè§†è§‰è¯­è¨€ä»»åŠ¡çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œäº‹åå‹ç¼©é€šå¸¸ä¼šäº§ç”Ÿè¾ƒä½ç­‰çº§çš„é€‚é…å™¨ï¼Œå…¶æ€§èƒ½ä¼˜äºç›´æ¥åœ¨ç›®æ ‡ç­‰çº§ä¸Šè®­ç»ƒçš„é€‚é…å™¨ï¼Œç‰¹åˆ«æ˜¯å¦‚æœå…è®¸åœ¨ç›®æ ‡ç­‰çº§ä¸Šè¿›è¡Œå°‘é‡å¾®è°ƒæ­¥éª¤çš„è¯ã€‚æ­¤å¤–ï¼ŒLoRA-Squeeze çš„æ¸è¿›ã€è°ƒä¼˜ç­‰çº§é€€ç«å˜ä½“å§‹ç»ˆèƒ½å¤Ÿå®ç°æœ€ä½³çš„ LoRA å°ºå¯¸ä¸æ€§èƒ½æƒè¡¡ã€‚

</details>

---

## 42. Computational Phenomenology of Temporal Experience in Autism: Quantifying the Emotional and Narrative Characteristics of Lived Unpredictability

**ä¸­æ–‡æ ‡é¢˜**: è‡ªé—­ç—‡æ—¶é—´ä½“éªŒçš„è®¡ç®—ç°è±¡å­¦ï¼šé‡åŒ–ç”Ÿæ´»ä¸å¯é¢„æµ‹æ€§çš„æƒ…æ„Ÿå’Œå™äº‹ç‰¹å¾

**Date**: 2026-02-11 | **arXiv**: [2602.10947v1](http://arxiv.org/abs/2602.10947v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10947v1)

<details><summary><b>Abstract</b></summary>

Disturbances in temporality, such as desynchronization with the social environment and its unpredictability, are considered core features of autism with a deep impact on relationships. However, limitations regarding research on this issue include: 1) the dominance of deficit-based medical models of autism, 2) sample size in qualitative research, and 3) the lack of phenomenological anchoring in computational research. To bridge the gap between phenomenological and computational approaches and overcome sample-size limitations, our research integrated three methodologies. Study A: structured phenomenological interviews with autistic individuals using the Transdiagnostic Assessment of Temporal Experience. Study B: computational analysis of an autobiographical corpus of autistic narratives built for this purpose. Study C: a replication of a computational study using narrative flow measures to assess the perceived phenomenological authenticity of autistic autobiographies. Interviews revealed that the most significant differences between the autistic and control groups concerned unpredictability of experience. Computational results mirrored these findings: the temporal lexicon in autistic narratives was significantly more negatively valenced - particularly the "Immediacy & Suddenness" category. Outlier analysis identified terms associated with perceived discontinuity (unpredictably, precipitously, and abruptly) as highly negative. The computational analysis of narrative flow found that the autistic narratives contained within the corpus quantifiably resemble autobiographical stories more than imaginary ones. Overall, the temporal challenges experienced by autistic individuals were shown to primarily concern lived unpredictability and stem from the contents of lived experience, and not from autistic narrative construction.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æ—¶é—´æ€§ç´Šä¹±ï¼Œä¾‹å¦‚ä¸ç¤¾ä¼šç¯å¢ƒçš„ä¸åŒæ­¥åŠå…¶ä¸å¯é¢„æµ‹æ€§ï¼Œè¢«è®¤ä¸ºæ˜¯è‡ªé—­ç—‡çš„æ ¸å¿ƒç‰¹å¾ï¼Œå¯¹äººé™…å…³ç³»æœ‰æ·±è¿œçš„å½±å“ã€‚ç„¶è€Œï¼Œè¯¥é—®é¢˜ç ”ç©¶çš„å±€é™æ€§åŒ…æ‹¬ï¼š1ï¼‰åŸºäºç¼ºé™·çš„è‡ªé—­ç—‡åŒ»å­¦æ¨¡å‹å ä¸»å¯¼åœ°ä½ï¼Œ2ï¼‰å®šæ€§ç ”ç©¶ä¸­çš„æ ·æœ¬é‡ï¼Œä»¥åŠ3ï¼‰è®¡ç®—ç ”ç©¶ä¸­ç¼ºä¹ç°è±¡å­¦é”šå®šã€‚ä¸ºäº†å¼¥åˆç°è±¡å­¦æ–¹æ³•å’Œè®¡ç®—æ–¹æ³•ä¹‹é—´çš„å·®è·å¹¶å…‹æœæ ·æœ¬é‡é™åˆ¶ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æ•´åˆäº†ä¸‰ç§æ–¹æ³•ã€‚ç ”ç©¶Aï¼šä½¿ç”¨æ—¶é—´ä½“éªŒè·¨è¯Šæ–­è¯„ä¼°å¯¹è‡ªé—­ç—‡æ‚£è€…è¿›è¡Œç»“æ„åŒ–ç°è±¡å­¦è®¿è°ˆã€‚ç ”ç©¶ Bï¼šä¸ºæ­¤ç›®çš„æ„å»ºçš„è‡ªä¼ ä½“å™è¿°è¯­æ–™åº“çš„è®¡ç®—åˆ†æã€‚ç ”ç©¶Cï¼šä½¿ç”¨å™äº‹æµæµ‹é‡æ¥è¯„ä¼°è‡ªé—­ç—‡è‡ªä¼ çš„ç°è±¡å­¦çœŸå®æ€§çš„è®¡ç®—ç ”ç©¶çš„å¤åˆ¶ã€‚è®¿è°ˆæ˜¾ç¤ºï¼Œè‡ªé—­ç—‡ç»„å’Œå¯¹ç…§ç»„ä¹‹é—´æœ€æ˜¾ç€çš„å·®å¼‚åœ¨äºç»å†çš„ä¸å¯é¢„æµ‹æ€§ã€‚è®¡ç®—ç»“æœåæ˜ äº†è¿™äº›å‘ç°ï¼šè‡ªé—­ç—‡å™äº‹ä¸­çš„æ—¶é—´è¯æ±‡çš„è´Ÿä»·æ˜æ˜¾æ›´é«˜â€”â€”å°¤å…¶æ˜¯â€œå³æ—¶æ€§å’Œçªç„¶æ€§â€ç±»åˆ«ã€‚å¼‚å¸¸å€¼åˆ†æå°†ä¸æ„ŸçŸ¥ä¸è¿ç»­æ€§ï¼ˆä¸å¯é¢„æµ‹çš„ã€çªç„¶çš„å’Œçªç„¶çš„ï¼‰ç›¸å…³çš„æœ¯è¯­è¯†åˆ«ä¸ºé«˜åº¦è´Ÿé¢çš„ã€‚å¯¹å™äº‹æµçš„è®¡ç®—åˆ†æå‘ç°ï¼Œè¯­æ–™åº“ä¸­åŒ…å«çš„è‡ªé—­ç—‡å™äº‹åœ¨æ•°é‡ä¸Šæ›´ç±»ä¼¼äºè‡ªä¼ æ•…äº‹ï¼Œè€Œä¸æ˜¯æƒ³è±¡çš„æ•…äº‹ã€‚æ€»ä½“è€Œè¨€ï¼Œè‡ªé—­ç—‡æ‚£è€…ç»å†çš„æ—¶é—´æŒ‘æˆ˜ä¸»è¦ä¸ç”Ÿæ´»çš„ä¸å¯é¢„æµ‹æ€§æœ‰å…³ï¼Œå¹¶ä¸”æºäºç”Ÿæ´»ç»å†çš„å†…å®¹ï¼Œè€Œä¸æ˜¯æ¥è‡ªè‡ªé—­ç—‡çš„å™äº‹ç»“æ„ã€‚

</details>

---

## 43. Blind Gods and Broken Screens: Architecting a Secure, Intent-Centric Mobile Agent Operating System

**ä¸­æ–‡æ ‡é¢˜**: ç›²ç›®çš„ä¸Šå¸å’Œç ´ç¢çš„å±å¹•ï¼šæ„å»ºä¸€ä¸ªå®‰å…¨çš„ã€ä»¥æ„å›¾ä¸ºä¸­å¿ƒçš„ç§»åŠ¨ä»£ç†æ“ä½œç³»ç»Ÿ

**Date**: 2026-02-11 | **arXiv**: [2602.10915v1](http://arxiv.org/abs/2602.10915v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10915v1)

<details><summary><b>Abstract</b></summary>

The evolution of Large Language Models (LLMs) has shifted mobile computing from App-centric interactions to system-level autonomous agents. Current implementations predominantly rely on a "Screen-as-Interface" paradigm, which inherits structural vulnerabilities and conflicts with the mobile ecosystem's economic foundations. In this paper, we conduct a systematic security analysis of state-of-the-art mobile agents using Doubao Mobile Assistant as a representative case. We decompose the threat landscape into four dimensions - Agent Identity, External Interface, Internal Reasoning, and Action Execution - revealing critical flaws such as fake App identity, visual spoofing, indirect prompt injection, and unauthorized privilege escalation stemming from a reliance on unstructured visual data.   To address these challenges, we propose Aura, an Agent Universal Runtime Architecture for a clean-slate secure agent OS. Aura replaces brittle GUI scraping with a structured, agent-native interaction model. It adopts a Hub-and-Spoke topology where a privileged System Agent orchestrates intent, sandboxed App Agents execute domain-specific tasks, and the Agent Kernel mediates all communication. The Agent Kernel enforces four defense pillars: (i) cryptographic identity binding via a Global Agent Registry; (ii) semantic input sanitization through a multilayer Semantic Firewall; (iii) cognitive integrity via taint-aware memory and plan-trajectory alignment; and (iv) granular access control with non-deniable auditing. Evaluation on MobileSafetyBench shows that, compared to Doubao, Aura improves low-risk Task Success Rate from roughly 75% to 94.3%, reduces high-risk Attack Success Rate from roughly 40% to 4.4%, and achieves near-order-of-magnitude latency gains. These results demonstrate Aura as a viable, secure alternative to the "Screen-as-Interface" paradigm.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) çš„å‘å±•å·²å°†ç§»åŠ¨è®¡ç®—ä»ä»¥åº”ç”¨ç¨‹åºä¸ºä¸­å¿ƒçš„äº¤äº’è½¬å˜ä¸ºç³»ç»Ÿçº§è‡ªä¸»ä»£ç†ã€‚å½“å‰çš„å®ç°ä¸»è¦ä¾èµ–äºâ€œå±å¹•å³ç•Œé¢â€èŒƒå¼ï¼Œè¯¥èŒƒå¼ç»§æ‰¿äº†ç»“æ„æ€§æ¼æ´å¹¶ä¸ç§»åŠ¨ç”Ÿæ€ç³»ç»Ÿçš„ç»æµåŸºç¡€å‘ç”Ÿå†²çªã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»¥è±†å®æ‰‹æœºåŠ©æ‰‹ä¸ºä»£è¡¨æ¡ˆä¾‹ï¼Œå¯¹æœ€å…ˆè¿›çš„ç§»åŠ¨ä»£ç†è¿›è¡Œäº†ç³»ç»Ÿçš„å®‰å…¨åˆ†æã€‚æˆ‘ä»¬å°†å¨èƒæ€åŠ¿åˆ†è§£ä¸ºå››ä¸ªç»´åº¦â€”â€”ä»£ç†èº«ä»½ã€å¤–éƒ¨æ¥å£ã€å†…éƒ¨æ¨ç†å’Œæ“ä½œæ‰§è¡Œâ€”â€”æ­ç¤ºäº†ç”±äºä¾èµ–éç»“æ„åŒ–è§†è§‰æ•°æ®è€Œäº§ç”Ÿçš„è™šå‡åº”ç”¨ç¨‹åºèº«ä»½ã€è§†è§‰æ¬ºéª—ã€é—´æ¥æç¤ºæ³¨å…¥å’Œæœªç»æˆæƒçš„æƒé™å‡çº§ç­‰å…³é”®ç¼ºé™·ã€‚   ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº† Auraï¼Œä¸€ç§ç”¨äºå…¨æ–°å®‰å…¨ä»£ç†æ“ä½œç³»ç»Ÿçš„ä»£ç†é€šç”¨è¿è¡Œæ—¶æ¶æ„ã€‚ Aura ç”¨ç»“æ„åŒ–çš„ä»£ç†æœ¬æœºäº¤äº’æ¨¡å‹å–ä»£äº†è„†å¼±çš„ GUI æŠ“å–ã€‚å®ƒé‡‡ç”¨ä¸­å¿ƒè¾å°„å‹æ‹“æ‰‘ï¼Œå…¶ä¸­ç‰¹æƒç³»ç»Ÿä»£ç†åè°ƒæ„å›¾ï¼Œæ²™ç›’åº”ç”¨ç¨‹åºä»£ç†æ‰§è¡Œç‰¹å®šäºåŸŸçš„ä»»åŠ¡ï¼Œä»£ç†å†…æ ¸åè°ƒæ‰€æœ‰é€šä¿¡ã€‚ä»£ç†å†…æ ¸å¼ºåˆ¶æ‰§è¡Œå››ä¸ªé˜²å¾¡æ”¯æŸ±ï¼š(i) é€šè¿‡å…¨å±€ä»£ç†æ³¨å†Œè¡¨è¿›è¡ŒåŠ å¯†èº«ä»½ç»‘å®šï¼› (ii) é€šè¿‡å¤šå±‚è¯­ä¹‰é˜²ç«å¢™è¿›è¡Œè¯­ä¹‰è¾“å…¥æ¸…ç†ï¼› (iii) é€šè¿‡æ±¡ç‚¹æ„ŸçŸ¥è®°å¿†å’Œè®¡åˆ’è½¨è¿¹å¯¹é½å®ç°è®¤çŸ¥å®Œæ•´æ€§ï¼› (iv) å…·æœ‰ä¸å¯å¦è®¤å®¡è®¡çš„ç²¾ç»†è®¿é—®æ§åˆ¶ã€‚ MobileSafetyBenchè¯„æµ‹æ˜¾ç¤ºï¼Œä¸è±†å®ç›¸æ¯”ï¼ŒAuraå°†ä½é£é™©ä»»åŠ¡æˆåŠŸç‡ä»å¤§çº¦75%æé«˜åˆ°94.3%ï¼Œå°†é«˜é£é™©æ”»å‡»æˆåŠŸç‡ä»å¤§çº¦40%é™ä½åˆ°4.4%ï¼Œå¹¶å®ç°äº†æ¥è¿‘æ•°é‡çº§çš„å»¶è¿Ÿå¢ç›Šã€‚è¿™äº›ç»“æœè¡¨æ˜ Aura æ˜¯â€œå±å¹•å³ç•Œé¢â€èŒƒä¾‹çš„å¯è¡Œã€å®‰å…¨çš„æ›¿ä»£æ–¹æ¡ˆã€‚

</details>

---

## 44. Interactive LLM-assisted Curriculum Learning for Multi-Task Evolutionary Policy Search

**ä¸­æ–‡æ ‡é¢˜**: ç”¨äºå¤šä»»åŠ¡è¿›åŒ–æ”¿ç­–æœç´¢çš„äº¤äº’å¼æ³•å­¦ç¡•å£«è¾…åŠ©è¯¾ç¨‹å­¦ä¹ 

**Date**: 2026-02-11 | **arXiv**: [2602.10891v1](http://arxiv.org/abs/2602.10891v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10891v1)

<details><summary><b>Abstract</b></summary>

Multi-task policy search is a challenging problem because policies are required to generalize beyond training cases. Curriculum learning has proven to be effective in this setting, as it introduces complexity progressively. However, designing effective curricula is labor-intensive and requires extensive domain expertise. LLM-based curriculum generation has only recently emerged as a potential solution, but was limited to operate in static, offline modes without leveraging real-time feedback from the optimizer. Here we propose an interactive LLM-assisted framework for online curriculum generation, where the LLM adaptively designs training cases based on real-time feedback from the evolutionary optimization process. We investigate how different feedback modalities, ranging from numeric metrics alone to combinations with plots and behavior visualizations, influence the LLM ability to generate meaningful curricula. Through a 2D robot navigation case study, tackled with genetic programming as optimizer, we evaluate our approach against static LLM-generated curricula and expert-designed baselines. We show that interactive curriculum generation outperforms static approaches, with multimodal feedback incorporating both progression plots and behavior visualizations yielding performance competitive with expert-designed curricula. This work contributes to understanding how LLMs can serve as interactive curriculum designers for embodied AI systems, with potential extensions to broader evolutionary robotics applications.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¤šä»»åŠ¡ç­–ç•¥æœç´¢æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œå› ä¸ºç­–ç•¥éœ€è¦æ³›åŒ–åˆ°è®­ç»ƒæ¡ˆä¾‹ä¹‹å¤–ã€‚äº‹å®è¯æ˜ï¼Œè¯¾ç¨‹å­¦ä¹ åœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯æœ‰æ•ˆçš„ï¼Œå› ä¸ºå®ƒé€æ¸å¼•å…¥äº†å¤æ‚æ€§ã€‚ç„¶è€Œï¼Œè®¾è®¡æœ‰æ•ˆçš„è¯¾ç¨‹æ˜¯åŠ³åŠ¨å¯†é›†å‹çš„ï¼Œå¹¶ä¸”éœ€è¦å¹¿æ³›çš„é¢†åŸŸä¸“ä¸šçŸ¥è¯†ã€‚åŸºäº LLM çš„è¯¾ç¨‹ç”Ÿæˆæœ€è¿‘æ‰ä½œä¸ºä¸€ç§æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆå‡ºç°ï¼Œä½†ä»…é™äºåœ¨é™æ€ã€ç¦»çº¿æ¨¡å¼ä¸‹è¿è¡Œï¼Œæ— æ³•åˆ©ç”¨ä¼˜åŒ–å™¨çš„å®æ—¶åé¦ˆã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç”¨äºåœ¨çº¿è¯¾ç¨‹ç”Ÿæˆçš„äº¤äº’å¼æ³•å­¦ç¡•å£«è¾…åŠ©æ¡†æ¶ï¼Œå…¶ä¸­æ³•å­¦ç¡•å£«æ ¹æ®è¿›åŒ–ä¼˜åŒ–è¿‡ç¨‹çš„å®æ—¶åé¦ˆè‡ªé€‚åº”åœ°è®¾è®¡è®­ç»ƒæ¡ˆä¾‹ã€‚æˆ‘ä»¬ç ”ç©¶ä¸åŒçš„åé¦ˆæ–¹å¼ï¼ˆä»å•ç‹¬çš„æ•°å­—æŒ‡æ ‡åˆ°ä¸ç»˜å›¾å’Œè¡Œä¸ºå¯è§†åŒ–çš„ç»„åˆï¼‰å¦‚ä½•å½±å“æ³•å­¦ç¡•å£«ç”Ÿæˆæœ‰æ„ä¹‰çš„è¯¾ç¨‹çš„èƒ½åŠ›ã€‚é€šè¿‡ 2D æœºå™¨äººå¯¼èˆªæ¡ˆä¾‹ç ”ç©¶ï¼Œå¹¶ä»¥é—ä¼ ç¼–ç¨‹ä½œä¸ºä¼˜åŒ–å™¨è¿›è¡Œå¤„ç†ï¼Œæˆ‘ä»¬æ ¹æ®é™æ€ LLM ç”Ÿæˆçš„è¯¾ç¨‹å’Œä¸“å®¶è®¾è®¡çš„åŸºçº¿è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œäº¤äº’å¼è¯¾ç¨‹ç”Ÿæˆä¼˜äºé™æ€æ–¹æ³•ï¼Œå¤šæ¨¡å¼åé¦ˆç»“åˆäº†è¿›åº¦å›¾å’Œè¡Œä¸ºå¯è§†åŒ–ï¼Œäº§ç”Ÿçš„æ€§èƒ½å¯ä¸ä¸“å®¶è®¾è®¡çš„è¯¾ç¨‹ç›¸åª²ç¾ã€‚è¿™é¡¹å·¥ä½œæœ‰åŠ©äºç†è§£æ³•å­¦ç¡•å£«å¦‚ä½•å……å½“å…·ä½“äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„äº¤äº’å¼è¯¾ç¨‹è®¾è®¡è€…ï¼Œå¹¶æœ‰å¯èƒ½æ‰©å±•åˆ°æ›´å¹¿æ³›çš„è¿›åŒ–æœºå™¨äººåº”ç”¨ã€‚

</details>

---

## 45. The CLEF-2026 FinMMEval Lab: Multilingual and Multimodal Evaluation of Financial AI Systems

**ä¸­æ–‡æ ‡é¢˜**: CLEF-2026 FinMMEval å®éªŒå®¤ï¼šé‡‘èäººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å¤šè¯­è¨€å’Œå¤šæ¨¡å¼è¯„ä¼°

**Date**: 2026-02-11 | **arXiv**: [2602.10886v1](http://arxiv.org/abs/2602.10886v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10886v1)

<details><summary><b>Abstract</b></summary>

We present the setup and the tasks of the FinMMEval Lab at CLEF 2026, which introduces the first multilingual and multimodal evaluation framework for financial Large Language Models (LLMs). While recent advances in financial natural language processing have enabled automated analysis of market reports, regulatory documents, and investor communications, existing benchmarks remain largely monolingual, text-only, and limited to narrow subtasks. FinMMEval 2026 addresses this gap by offering three interconnected tasks that span financial understanding, reasoning, and decision-making: Financial Exam Question Answering, Multilingual Financial Question Answering (PolyFiQA), and Financial Decision Making. Together, these tasks provide a comprehensive evaluation suite that measures models' ability to reason, generalize, and act across diverse languages and modalities. The lab aims to promote the development of robust, transparent, and globally inclusive financial AI systems, with datasets and evaluation resources publicly released to support reproducible research.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æˆ‘ä»¬åœ¨ CLEF 2026 ä¸Šä»‹ç»äº† FinMMEval å®éªŒå®¤çš„è®¾ç½®å’Œä»»åŠ¡ï¼Œè¯¥å®éªŒå®¤å¼•å…¥äº†ç¬¬ä¸€ä¸ªé’ˆå¯¹é‡‘èå¤§è¯­è¨€æ¨¡å‹ (LLM) çš„å¤šè¯­è¨€å’Œå¤šæ¨¡å¼è¯„ä¼°æ¡†æ¶ã€‚å°½ç®¡é‡‘èè‡ªç„¶è¯­è¨€å¤„ç†çš„æœ€æ–°è¿›å±•å·²ç»å®ç°äº†å¯¹å¸‚åœºæŠ¥å‘Šã€ç›‘ç®¡æ–‡ä»¶å’ŒæŠ•èµ„è€…æ²Ÿé€šçš„è‡ªåŠ¨åˆ†æï¼Œä½†ç°æœ‰åŸºå‡†ä»ç„¶ä¸»è¦æ˜¯å•è¯­è¨€ã€çº¯æ–‡æœ¬ï¼Œå¹¶ä¸”ä»…é™äºç‹­çª„çš„å­ä»»åŠ¡ã€‚ FinMMEval 2026 é€šè¿‡æä¾›ä¸‰ä¸ªæ¶µç›–é‡‘èç†è§£ã€æ¨ç†å’Œå†³ç­–çš„ç›¸äº’å…³è”çš„ä»»åŠ¡æ¥è§£å†³è¿™ä¸€å·®è·ï¼šé‡‘èè€ƒè¯•é—®ç­”ã€å¤šè¯­è¨€é‡‘èé—®ç­” (PolyFiQA) å’Œé‡‘èå†³ç­–ã€‚è¿™äº›ä»»åŠ¡å…±åŒæä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°å¥—ä»¶ï¼Œç”¨äºè¡¡é‡æ¨¡å‹è·¨ä¸åŒè¯­è¨€å’Œæ¨¡å¼è¿›è¡Œæ¨ç†ã€æ³›åŒ–å’Œè¡ŒåŠ¨çš„èƒ½åŠ›ã€‚è¯¥å®éªŒå®¤æ—¨åœ¨ä¿ƒè¿›ç¨³å¥ã€é€æ˜å’Œå…¨çƒåŒ…å®¹çš„é‡‘èäººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å‘å±•ï¼Œå…¬å¼€å‘å¸ƒæ•°æ®é›†å’Œè¯„ä¼°èµ„æºä»¥æ”¯æŒå¯é‡å¤çš„ç ”ç©¶ã€‚

</details>

---

## 46. Reinforcing Chain-of-Thought Reasoning with Self-Evolving Rubrics

**ä¸­æ–‡æ ‡é¢˜**: ç”¨è‡ªæˆ‘è¿›åŒ–çš„æ ‡å‡†å¼ºåŒ–æ€ç»´é“¾æ¨ç†

**Date**: 2026-02-11 | **arXiv**: [2602.10885v1](http://arxiv.org/abs/2602.10885v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10885v1)

<details><summary><b>Abstract</b></summary>

Despite chain-of-thought (CoT) playing crucial roles in LLM reasoning, directly rewarding it is difficult: training a reward model demands heavy human labeling efforts, and static RMs struggle with evolving CoT distributions and reward hacking. These challenges motivate us to seek an autonomous CoT rewarding approach that requires no human annotation efforts and can evolve gradually. Inspired by recent self-evolving training methods, we propose \textbf{RLCER} (\textbf{R}einforcement \textbf{L}earning with \textbf{C}oT Supervision via Self-\textbf{E}volving \textbf{R}ubrics), which enhances the outcome-centric RLVR by rewarding CoTs with self-proposed and self-evolving rubrics. We show that self-proposed and self-evolving rubrics provide reliable CoT supervision signals even without outcome rewards, enabling RLCER to outperform outcome-centric RLVR. Moreover, when used as in-prompt hints, these self-proposed rubrics further improve inference-time performance.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å°½ç®¡æ€æƒ³é“¾ (CoT) åœ¨ LLM æ¨ç†ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œä½†ç›´æ¥å¥–åŠ±å®ƒå¾ˆå›°éš¾ï¼šè®­ç»ƒå¥–åŠ±æ¨¡å‹éœ€è¦å¤§é‡çš„äººå·¥æ ‡è®°å·¥ä½œï¼Œè€Œé™æ€ RM åˆ™éš¾ä»¥åº”å¯¹ä¸æ–­å˜åŒ–çš„ CoT åˆ†å¸ƒå’Œå¥–åŠ±é»‘å®¢æ”»å‡»ã€‚è¿™äº›æŒ‘æˆ˜ä¿ƒä½¿æˆ‘ä»¬å¯»æ±‚ä¸€ç§è‡ªä¸»çš„ CoT å¥–åŠ±æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦äººå·¥æ³¨é‡Šå·¥ä½œå¹¶ä¸”å¯ä»¥é€æ¸å‘å±•ã€‚å—æœ€è¿‘è‡ªæˆ‘è¿›åŒ–è®­ç»ƒæ–¹æ³•çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº† \textbf{RLCER} ï¼ˆ\textbf{R}einforcement \textbf{L}earning with \textbf{C}oT Supervision via Self-\textbf{E}volving \textbf{R}ubricsï¼‰ï¼Œå®ƒé€šè¿‡ç”¨è‡ªæˆ‘æå‡ºå’Œè‡ªæˆ‘è¿›åŒ–çš„è§„åˆ™å¥–åŠ± CoT æ¥å¢å¼ºä»¥ç»“æœä¸ºä¸­å¿ƒçš„ RLVRã€‚æˆ‘ä»¬è¯æ˜ï¼Œå³ä½¿æ²¡æœ‰ç»“æœå¥–åŠ±ï¼Œè‡ªæˆ‘æå‡ºå’Œè‡ªæˆ‘æ¼”åŒ–çš„è§„åˆ™ä¹Ÿèƒ½æä¾›å¯é çš„ CoT ç›‘ç£ä¿¡å·ï¼Œä½¿ RLCER çš„è¡¨ç°ä¼˜äºä»¥ç»“æœä¸ºä¸­å¿ƒçš„ RLVRã€‚æ­¤å¤–ï¼Œå½“ç”¨ä½œæç¤ºæ—¶ï¼Œè¿™äº›è‡ªè¡Œæå‡ºçš„è§„åˆ™è¿›ä¸€æ­¥æé«˜äº†æ¨ç†æ—¶é—´æ€§èƒ½ã€‚

</details>

---

## 47. ICA: Information-Aware Credit Assignment for Visually Grounded Long-Horizon Information-Seeking Agents

**ä¸­æ–‡æ ‡é¢˜**: ICAï¼šè§†è§‰åŸºç¡€çš„é•¿è§†é‡ä¿¡æ¯æœç´¢ä»£ç†çš„ä¿¡æ¯æ„ŸçŸ¥ä¿¡ç”¨åˆ†é…

**Date**: 2026-02-11 | **arXiv**: [2602.10863v1](http://arxiv.org/abs/2602.10863v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10863v1)

**Code**: https://github.com/pc-inno/ICA_MM_deepsearch.git.

<details><summary><b>Abstract</b></summary>

Despite the strong performance achieved by reinforcement learning-trained information-seeking agents, learning in open-ended web environments remains severely constrained by low signal-to-noise feedback. Text-based parsers often discard layout semantics and introduce unstructured noise, while long-horizon training typically relies on sparse outcome rewards that obscure which retrieval actions actually matter. We propose a visual-native search framework that represents webpages as visual snapshots, allowing agents to leverage layout cues to quickly localize salient evidence and suppress distractors. To learn effectively from these high-dimensional observations, we introduce Information-Aware Credit Assignment (ICA), a post-hoc method that estimates each retrieved snapshot's contribution to the final outcome via posterior analysis and propagates dense learning signals back to key search turns. Integrated with a GRPO-based training pipeline, our approach consistently outperforms text-based baselines on diverse information-seeking benchmarks, providing evidence that visual snapshot grounding with information-level credit assignment alleviates the credit-assignment bottleneck in open-ended web environments. The code and datasets will be released in https://github.com/pc-inno/ICA_MM_deepsearch.git.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å°½ç®¡ç»è¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„ä¿¡æ¯æœç´¢ä»£ç†å–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œä½†å¼€æ”¾å¼ç½‘ç»œç¯å¢ƒä¸­çš„å­¦ä¹ ä»ç„¶å—åˆ°ä½ä¿¡å™ªæ¯”åé¦ˆçš„ä¸¥é‡é™åˆ¶ã€‚åŸºäºæ–‡æœ¬çš„è§£æå™¨é€šå¸¸ä¼šä¸¢å¼ƒå¸ƒå±€è¯­ä¹‰å¹¶å¼•å…¥éç»“æ„åŒ–å™ªå£°ï¼Œè€Œé•¿æœŸè®­ç»ƒé€šå¸¸ä¾èµ–äºç¨€ç–çš„ç»“æœå¥–åŠ±ï¼Œä»è€Œæ¨¡ç³Šäº†å“ªäº›æ£€ç´¢æ“ä½œçœŸæ­£é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è§†è§‰åŸç”Ÿæœç´¢æ¡†æ¶ï¼Œå°†ç½‘é¡µè¡¨ç¤ºä¸ºè§†è§‰å¿«ç…§ï¼Œå…è®¸ä»£ç†åˆ©ç”¨å¸ƒå±€çº¿ç´¢å¿«é€Ÿå®šä½æ˜¾ç€è¯æ®å¹¶æŠ‘åˆ¶å¹²æ‰°å› ç´ ã€‚ä¸ºäº†æœ‰æ•ˆåœ°ä»è¿™äº›é«˜ç»´è§‚å¯Ÿä¸­å­¦ä¹ ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¿¡æ¯æ„ŸçŸ¥ä¿¡ç”¨åˆ†é…ï¼ˆICAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§äº‹åæ–¹æ³•ï¼Œé€šè¿‡åéªŒåˆ†æä¼°è®¡æ¯ä¸ªæ£€ç´¢åˆ°çš„å¿«ç…§å¯¹æœ€ç»ˆç»“æœçš„è´¡çŒ®ï¼Œå¹¶å°†å¯†é›†çš„å­¦ä¹ ä¿¡å·ä¼ æ’­å›å…³é”®æœç´¢å›åˆã€‚ä¸åŸºäº GRPO çš„è®­ç»ƒç®¡é“ç›¸ç»“åˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§ä¿¡æ¯æœç´¢åŸºå‡†ä¸Šå§‹ç»ˆä¼˜äºåŸºäºæ–‡æœ¬çš„åŸºçº¿ï¼Œæä¾›äº†è¯æ®è¡¨æ˜å…·æœ‰ä¿¡æ¯çº§ä¿¡ç”¨åˆ†é…çš„è§†è§‰å¿«ç…§åŸºç¡€å¯ä»¥ç¼“è§£å¼€æ”¾å¼ç½‘ç»œç¯å¢ƒä¸­çš„ä¿¡ç”¨åˆ†é…ç“¶é¢ˆã€‚ä»£ç å’Œæ•°æ®é›†å°†åœ¨ https://github.com/pc-inno/ICA_MM_deepsearch.git ä¸­å‘å¸ƒã€‚

</details>

---

## 48. See, Plan, Snap: Evaluating Multimodal GUI Agents in Scratch

**ä¸­æ–‡æ ‡é¢˜**: æŸ¥çœ‹ã€è®¡åˆ’ã€æ•æ‰ï¼šåœ¨ Scratch ä¸­è¯„ä¼°å¤šæ¨¡å¼ GUI ä»£ç†

**Date**: 2026-02-11 | **arXiv**: [2602.10814v1](http://arxiv.org/abs/2602.10814v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10814v1)

<details><summary><b>Abstract</b></summary>

Block-based programming environments such as Scratch play a central role in low-code education, yet evaluating the capabilities of AI agents to construct programs through Graphical User Interfaces (GUIs) remains underexplored. We introduce ScratchWorld, a benchmark for evaluating multimodal GUI agents on program-by-construction tasks in Scratch. Grounded in the Use-Modify-Create pedagogical framework, ScratchWorld comprises 83 curated tasks spanning four distinct problem categories: Create, Debug, Extend, and Compute. To rigorously diagnose the source of agent failures, the benchmark employs two complementary interaction modes: primitive mode requires fine-grained drag-and-drop manipulation to directly assess visuomotor control, while composite mode uses high-level semantic APIs to disentangle program reasoning from GUI execution. To ensure reliable assessment, we propose an execution-based evaluation protocol that validates the functional correctness of the constructed Scratch programs through runtime tests within the browser environment. Extensive experiments across state-of-the-art multimodal language models and GUI agents reveal a substantial reasoning--acting gap, highlighting persistent challenges in fine-grained GUI manipulation despite strong planning capabilities.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

Scratch ç­‰åŸºäºå—çš„ç¼–ç¨‹ç¯å¢ƒåœ¨ä½ä»£ç æ•™è‚²ä¸­å‘æŒ¥ç€æ ¸å¿ƒä½œç”¨ï¼Œä½†è¯„ä¼°äººå·¥æ™ºèƒ½ä»£ç†é€šè¿‡å›¾å½¢ç”¨æˆ·ç•Œé¢ (GUI) æ„å»ºç¨‹åºçš„èƒ½åŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬å¼•å…¥äº† ScratchWorldï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼° Scratch ä¸­çš„ç¨‹åºæ„å»ºä»»åŠ¡çš„å¤šæ¨¡å¼ GUI ä»£ç†çš„åŸºå‡†ã€‚ ScratchWorld ä»¥â€œä½¿ç”¨-ä¿®æ”¹-åˆ›å»ºâ€æ•™å­¦æ¡†æ¶ä¸ºåŸºç¡€ï¼ŒåŒ…å« 83 ä¸ªç²¾å¿ƒç­–åˆ’çš„ä»»åŠ¡ï¼Œæ¶µç›–å››ä¸ªä¸åŒçš„é—®é¢˜ç±»åˆ«ï¼šåˆ›å»ºã€è°ƒè¯•ã€æ‰©å±•å’Œè®¡ç®—ã€‚ä¸ºäº†ä¸¥æ ¼è¯Šæ–­ä»£ç†æ•…éšœçš„æ ¹æºï¼Œè¯¥åŸºå‡†æµ‹è¯•é‡‡ç”¨äº†ä¸¤ç§äº’è¡¥çš„äº¤äº’æ¨¡å¼ï¼šåŸå§‹æ¨¡å¼éœ€è¦ç»†ç²’åº¦çš„æ‹–æ”¾æ“ä½œæ¥ç›´æ¥è¯„ä¼°è§†è§‰è¿åŠ¨æ§åˆ¶ï¼Œè€Œå¤åˆæ¨¡å¼åˆ™ä½¿ç”¨é«˜çº§è¯­ä¹‰ API å°†ç¨‹åºæ¨ç†ä¸ GUI æ‰§è¡Œåˆ†å¼€ã€‚ä¸ºäº†ç¡®ä¿è¯„ä¼°çš„å¯é æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰§è¡Œçš„è¯„ä¼°åè®®ï¼Œé€šè¿‡åœ¨æµè§ˆå™¨ç¯å¢ƒä¸­çš„è¿è¡Œæ—¶æµ‹è¯•æ¥éªŒè¯æ‰€æ„å»ºçš„ Scratch ç¨‹åºçš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚è·¨æœ€å…ˆè¿›çš„å¤šæ¨¡å¼è¯­è¨€æ¨¡å‹å’Œ GUI ä»£ç†çš„å¹¿æ³›å®éªŒæ­ç¤ºäº†å·¨å¤§çš„æ¨ç†-æ‰§è¡Œå·®è·ï¼Œå‡¸æ˜¾äº†å°½ç®¡è§„åˆ’èƒ½åŠ›å¼ºå¤§ï¼Œä½†ç»†ç²’åº¦ GUI æ“ä½œæ–¹é¢æŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ã€‚

</details>

---

## 49. VulReaD: Knowledge-Graph-guided Software Vulnerability Reasoning and Detection

**ä¸­æ–‡æ ‡é¢˜**: VulReaDï¼šçŸ¥è¯†å›¾å¼•å¯¼çš„è½¯ä»¶æ¼æ´æ¨ç†ä¸æ£€æµ‹

**Date**: 2026-02-11 | **arXiv**: [2602.10787v1](http://arxiv.org/abs/2602.10787v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10787v1)

<details><summary><b>Abstract</b></summary>

Software vulnerability detection (SVD) is a critical challenge in modern systems. Large language models (LLMs) offer natural-language explanations alongside predictions, but most work focuses on binary evaluation, and explanations often lack semantic consistency with Common Weakness Enumeration (CWE) categories. We propose VulReaD, a knowledge-graph-guided approach for vulnerability reasoning and detection that moves beyond binary classification toward CWE-level reasoning. VulReaD leverages a security knowledge graph (KG) as a semantic backbone and uses a strong teacher LLM to generate CWE-consistent contrastive reasoning supervision, enabling student model training without manual annotations. Students are fine-tuned with Odds Ratio Preference Optimization (ORPO) to encourage taxonomy-aligned reasoning while suppressing unsupported explanations. Across three real-world datasets, VulReaD improves binary F1 by 8-10% and multi-class classification by 30% Macro-F1 and 18% Micro-F1 compared to state-of-the-art baselines. Results show that LLMs outperform deep learning baselines in binary detection and that KG-guided reasoning enhances CWE coverage and interpretability.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è½¯ä»¶æ¼æ´æ£€æµ‹ï¼ˆSVDï¼‰æ˜¯ç°ä»£ç³»ç»Ÿä¸­çš„ä¸€ä¸ªä¸¥å³»æŒ‘æˆ˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) æä¾›è‡ªç„¶è¯­è¨€è§£é‡Šå’Œé¢„æµ‹ï¼Œä½†å¤§å¤šæ•°å·¥ä½œä¾§é‡äºäºŒå…ƒè¯„ä¼°ï¼Œå¹¶ä¸”è§£é‡Šé€šå¸¸ç¼ºä¹ä¸å¸¸è§å¼±ç‚¹æšä¸¾ (CWE) ç±»åˆ«çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚æˆ‘ä»¬æå‡ºäº† VulReaDï¼Œä¸€ç§ç”¨äºæ¼æ´æ¨ç†å’Œæ£€æµ‹çš„çŸ¥è¯†å›¾å¼•å¯¼æ–¹æ³•ï¼Œå®ƒè¶…è¶Šäº†äºŒè¿›åˆ¶åˆ†ç±»ï¼Œè½¬å‘äº† CWE çº§åˆ«çš„æ¨ç†ã€‚ VulReaDåˆ©ç”¨å®‰å…¨çŸ¥è¯†å›¾ï¼ˆKGï¼‰ä½œä¸ºè¯­ä¹‰ä¸»å¹²ï¼Œå¹¶ä½¿ç”¨å¼ºå¤§çš„LLMæ•™å¸ˆæ¥ç”ŸæˆCWEä¸€è‡´çš„å¯¹æ¯”æ¨ç†ç›‘ç£ï¼Œä»è€Œæ— éœ€æ‰‹åŠ¨æ³¨é‡Šå³å¯å®ç°å­¦ç”Ÿæ¨¡å‹è®­ç»ƒã€‚å­¦ç”Ÿä½¿ç”¨ä¼˜åŠ¿æ¯”åå¥½ä¼˜åŒ– (ORPO) è¿›è¡Œå¾®è°ƒï¼Œä»¥é¼“åŠ±åˆ†ç±»å­¦ä¸€è‡´çš„æ¨ç†ï¼ŒåŒæ—¶æŠ‘åˆ¶ä¸å—æ”¯æŒçš„è§£é‡Šã€‚åœ¨ä¸‰ä¸ªçœŸå®æ•°æ®é›†ä¸Šï¼Œä¸æœ€å…ˆè¿›çš„åŸºçº¿ç›¸æ¯”ï¼ŒVulReaD å°†äºŒè¿›åˆ¶ F1 æé«˜äº† 8-10%ï¼Œå°†å¤šç±»åˆ†ç±»æé«˜äº† 30% Macro-F1 å’Œ 18% Micro-F1ã€‚ç»“æœè¡¨æ˜ï¼ŒLLM åœ¨äºŒè¿›åˆ¶æ£€æµ‹æ–¹é¢ä¼˜äºæ·±åº¦å­¦ä¹ åŸºçº¿ï¼Œå¹¶ä¸”çŸ¥è¯†å›¾è°±å¼•å¯¼çš„æ¨ç†å¢å¼ºäº† CWE çš„è¦†ç›–èŒƒå›´å’Œå¯è§£é‡Šæ€§ã€‚

</details>

---

## 50. Cross-Sectional Asset Retrieval via Future-Aligned Soft Contrastive Learning

**ä¸­æ–‡æ ‡é¢˜**: é€šè¿‡é¢å‘æœªæ¥çš„è½¯å¯¹æ¯”å­¦ä¹ è¿›è¡Œæ¨ªæˆªé¢èµ„äº§æ£€ç´¢

**Date**: 2026-02-11 | **arXiv**: [2602.10711v1](http://arxiv.org/abs/2602.10711v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10711v1)

<details><summary><b>Abstract</b></summary>

Asset retrieval--finding similar assets in a financial universe--is central to quantitative investment decision-making. Existing approaches define similarity through historical price patterns or sector classifications, but such backward-looking criteria provide no guarantee about future behavior. We argue that effective asset retrieval should be future-aligned: the retrieved assets should be those most likely to exhibit correlated future returns. To this end, we propose Future-Aligned Soft Contrastive Learning (FASCL), a representation learning framework whose soft contrastive loss uses pairwise future return correlations as continuous supervision targets. We further introduce an evaluation protocol designed to directly assess whether retrieved assets share similar future trajectories. Experiments on 4,229 US equities demonstrate that FASCL consistently outperforms 13 baselines across all future-behavior metrics. The source code will be available soon.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

èµ„äº§æ£€ç´¢â€”â€”åœ¨é‡‘èé¢†åŸŸä¸­å¯»æ‰¾ç›¸ä¼¼èµ„äº§â€”â€”æ˜¯é‡åŒ–æŠ•èµ„å†³ç­–çš„æ ¸å¿ƒã€‚ç°æœ‰æ–¹æ³•é€šè¿‡å†å²ä»·æ ¼æ¨¡å¼æˆ–è¡Œä¸šåˆ†ç±»æ¥å®šä¹‰ç›¸ä¼¼æ€§ï¼Œä½†è¿™ç§å‘åçœ‹çš„æ ‡å‡†å¹¶ä¸èƒ½ä¿è¯æœªæ¥çš„è¡Œä¸ºã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œæœ‰æ•ˆçš„èµ„äº§æ£€ç´¢åº”è¯¥ä¸æœªæ¥ä¿æŒä¸€è‡´ï¼šæ£€ç´¢åˆ°çš„èµ„äº§åº”è¯¥æ˜¯é‚£äº›æœ€æœ‰å¯èƒ½è¡¨ç°å‡ºç›¸å…³çš„æœªæ¥å›æŠ¥çš„èµ„äº§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æœªæ¥å¯¹é½è½¯å¯¹æ¯”å­¦ä¹ ï¼ˆFASCLï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œå…¶è½¯å¯¹æ¯”æŸå¤±ä½¿ç”¨æˆå¯¹çš„æœªæ¥å›æŠ¥ç›¸å…³æ€§ä½œä¸ºè¿ç»­ç›‘ç£ç›®æ ‡ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ç§è¯„ä¼°åè®®ï¼Œæ—¨åœ¨ç›´æ¥è¯„ä¼°æ£€ç´¢åˆ°çš„èµ„äº§æ˜¯å¦å…·æœ‰ç›¸ä¼¼çš„æœªæ¥è½¨è¿¹ã€‚å¯¹ 4,229 åªç¾å›½è‚¡ç¥¨çš„å®éªŒè¡¨æ˜ï¼ŒFASCL åœ¨æ‰€æœ‰æœªæ¥è¡Œä¸ºæŒ‡æ ‡ä¸­å§‹ç»ˆä¼˜äº 13 ä¸ªåŸºçº¿ã€‚æºä»£ç å¾ˆå¿«å°±ä¼šæä¾›ã€‚

</details>

---

## 51. Interpretable Graph-Level Anomaly Detection via Contrast with Normal Prototypes

**ä¸­æ–‡æ ‡é¢˜**: é€šè¿‡ä¸æ­£å¸¸åŸå‹å¯¹æ¯”è¿›è¡Œå¯è§£é‡Šçš„å›¾çº§å¼‚å¸¸æ£€æµ‹

**Date**: 2026-02-11 | **arXiv**: [2602.10708v1](http://arxiv.org/abs/2602.10708v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10708v1)

<details><summary><b>Abstract</b></summary>

The task of graph-level anomaly detection (GLAD) is to identify anomalous graphs that deviate significantly from the majority of graphs in a dataset. While deep GLAD methods have shown promising performance, their black-box nature limits their reliability and deployment in real-world applications. Although some recent methods have made attempts to provide explanations for anomaly detection results, they either provide explanations without referencing normal graphs, or rely on abstract latent vectors as prototypes rather than concrete graphs from the dataset. To address these limitations, we propose Prototype-based Graph-Level Anomaly Detection (ProtoGLAD), an interpretable unsupervised framework that provides explanation for each detected anomaly by explicitly contrasting with its nearest normal prototype graph. It employs a point-set kernel to iteratively discover multiple normal prototype graphs and their associated clusters from the dataset, then identifying graphs distant from all discovered normal clusters as anomalies. Extensive experiments on multiple real-world datasets demonstrate that ProtoGLAD achieves competitive anomaly detection performance compared to state-of-the-art GLAD methods while providing better human-interpretable prototype-based explanations.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å›¾çº§å¼‚å¸¸æ£€æµ‹ï¼ˆGLADï¼‰çš„ä»»åŠ¡æ˜¯è¯†åˆ«ä¸æ•°æ®é›†ä¸­çš„å¤§å¤šæ•°å›¾æ˜¾ç€åå·®çš„å¼‚å¸¸å›¾ã€‚è™½ç„¶æ·±åº¦ GLAD æ–¹æ³•è¡¨ç°å‡ºäº†è‰¯å¥½çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬çš„é»‘ç›’æ€§è´¨é™åˆ¶äº†å®ƒä»¬åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§å’Œéƒ¨ç½²ã€‚å°½ç®¡æœ€è¿‘çš„ä¸€äº›æ–¹æ³•å°è¯•ä¸ºå¼‚å¸¸æ£€æµ‹ç»“æœæä¾›è§£é‡Šï¼Œä½†å®ƒä»¬è¦ä¹ˆåœ¨ä¸å¼•ç”¨æ­£å¸¸å›¾çš„æƒ…å†µä¸‹æä¾›è§£é‡Šï¼Œè¦ä¹ˆä¾èµ–æŠ½è±¡æ½œåœ¨å‘é‡ä½œä¸ºåŸå‹è€Œä¸æ˜¯æ•°æ®é›†ä¸­çš„å…·ä½“å›¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºåŸå‹çš„å›¾çº§å¼‚å¸¸æ£€æµ‹ï¼ˆProtoGLADï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¯è§£é‡Šçš„æ— ç›‘ç£æ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¸æœ€æ¥è¿‘çš„æ­£å¸¸åŸå‹å›¾æ˜ç¡®å¯¹æ¯”æ¥ä¸ºæ¯ä¸ªæ£€æµ‹åˆ°çš„å¼‚å¸¸æä¾›è§£é‡Šã€‚å®ƒé‡‡ç”¨ç‚¹é›†å†…æ ¸è¿­ä»£åœ°ä»æ•°æ®é›†ä¸­å‘ç°å¤šä¸ªæ­£å¸¸åŸå‹å›¾åŠå…¶å…³è”çš„ç°‡ï¼Œç„¶åå°†è¿œç¦»æ‰€æœ‰å‘ç°çš„æ­£å¸¸ç°‡çš„å›¾è¯†åˆ«ä¸ºå¼‚å¸¸ã€‚å¯¹å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„ GLAD æ–¹æ³•ç›¸æ¯”ï¼ŒProtoGLAD å®ç°äº†æœ‰ç«äº‰åŠ›çš„å¼‚å¸¸æ£€æµ‹æ€§èƒ½ï¼ŒåŒæ—¶æä¾›äº†æ›´å¥½çš„äººç±»å¯è§£é‡Šçš„åŸºäºåŸå‹çš„è§£é‡Šã€‚

</details>

---

## 52. Neuro-symbolic Action Masking for Deep Reinforcement Learning

**ä¸­æ–‡æ ‡é¢˜**: ç”¨äºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ç¥ç»ç¬¦å·åŠ¨ä½œæ©è”½

**Date**: 2026-02-11 | **arXiv**: [2602.10598v1](http://arxiv.org/abs/2602.10598v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10598v1)

<details><summary><b>Abstract</b></summary>

Deep reinforcement learning (DRL) may explore infeasible actions during training and execution. Existing approaches assume a symbol grounding function that maps high-dimensional states to consistent symbolic representations and a manually specified action masking techniques to constrain actions. In this paper, we propose Neuro-symbolic Action Masking (NSAM), a novel framework that automatically learn symbolic models, which are consistent with given domain constraints of high-dimensional states, in a minimally supervised manner during the DRL process. Based on the learned symbolic model of states, NSAM learns action masks that rules out infeasible actions. NSAM enables end-to-end integration of symbolic reasoning and deep policy optimization, where improvements in symbolic grounding and policy learning mutually reinforce each other. We evaluate NSAM on multiple domains with constraints, and experimental results demonstrate that NSAM significantly improves sample efficiency of DRL agent while substantially reducing constraint violations.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰å¯èƒ½ä¼šæ¢ç´¢è®­ç»ƒå’Œæ‰§è¡Œè¿‡ç¨‹ä¸­ä¸å¯è¡Œçš„åŠ¨ä½œã€‚ç°æœ‰æ–¹æ³•å‡è®¾ç¬¦å·åŸºç¡€å‡½æ•°å°†é«˜ç»´çŠ¶æ€æ˜ å°„åˆ°ä¸€è‡´çš„ç¬¦å·è¡¨ç¤ºï¼Œå¹¶é‡‡ç”¨æ‰‹åŠ¨æŒ‡å®šçš„åŠ¨ä½œå±è”½æŠ€æœ¯æ¥çº¦æŸåŠ¨ä½œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç¥ç»ç¬¦å·åŠ¨ä½œæ©è”½ï¼ˆNSAMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå¯ä»¥åœ¨ DRL è¿‡ç¨‹ä¸­ä»¥æœ€å°ç›‘ç£çš„æ–¹å¼è‡ªåŠ¨å­¦ä¹ ç¬¦å·æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹ä¸é«˜ç»´çŠ¶æ€çš„ç»™å®šåŸŸçº¦æŸä¸€è‡´ã€‚åŸºäºå­¦ä¹ åˆ°çš„çŠ¶æ€ç¬¦å·æ¨¡å‹ï¼ŒNSAM å­¦ä¹ æ’é™¤ä¸å¯è¡ŒåŠ¨ä½œçš„åŠ¨ä½œæ©ç ã€‚ NSAM èƒ½å¤Ÿå®ç°ç¬¦å·æ¨ç†å’Œæ·±åº¦ç­–ç•¥ä¼˜åŒ–çš„ç«¯åˆ°ç«¯é›†æˆï¼Œå…¶ä¸­ç¬¦å·åŸºç¡€å’Œç­–ç•¥å­¦ä¹ çš„æ”¹è¿›ç›¸è¾…ç›¸æˆã€‚æˆ‘ä»¬åœ¨å…·æœ‰çº¦æŸçš„å¤šä¸ªåŸŸä¸Šè¯„ä¼° NSAMï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒNSAM æ˜¾ç€æé«˜äº† DRL ä»£ç†çš„æ ·æœ¬æ•ˆç‡ï¼ŒåŒæ—¶å¤§å¤§å‡å°‘äº†çº¦æŸè¿è§„ã€‚

</details>

---

## 53. LAP: Language-Action Pre-Training Enables Zero-shot Cross-Embodiment Transfer

**ä¸­æ–‡æ ‡é¢˜**: LAPï¼šè¯­è¨€-åŠ¨ä½œé¢„è®­ç»ƒå®ç°é›¶æ ·æœ¬è·¨å®æ–½ä¾‹è¿ç§»

**Date**: 2026-02-11 | **arXiv**: [2602.10556v1](http://arxiv.org/abs/2602.10556v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10556v1)

<details><summary><b>Abstract</b></summary>

A long-standing goal in robotics is a generalist policy that can be deployed zero-shot on new robot embodiments without per-embodiment adaptation. Despite large-scale multi-embodiment pre-training, existing Vision-Language-Action models (VLAs) remain tightly coupled to their training embodiments and typically require costly fine-tuning. We introduce Language-Action Pre-training (LAP), a simple recipe that represents low-level robot actions directly in natural language, aligning action supervision with the pre-trained vision-language model's input-output distribution. LAP requires no learned tokenizer, no costly annotation, and no embodiment-specific architectural design. Based on LAP, we present LAP-3B, which to the best of our knowledge is the first VLA to achieve substantial zero-shot transfer to previously unseen robot embodiments without any embodiment-specific fine-tuning. Across multiple novel robots and manipulation tasks, LAP-3B attains over 50% average zero-shot success, delivering roughly a 2x improvement over the strongest prior VLAs. We further show that LAP enables efficient adaptation and favorable scaling, while unifying action prediction and VQA in a shared language-action format that yields additional gains through co-training.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æœºå™¨äººæŠ€æœ¯çš„ä¸€ä¸ªé•¿æœŸç›®æ ‡æ˜¯ä¸€ç§é€šç”¨ç­–ç•¥ï¼Œå¯ä»¥åœ¨æ–°çš„æœºå™¨äººå®æ–½ä¾‹ä¸Šè¿›è¡Œé›¶å°„å‡»éƒ¨ç½²ï¼Œè€Œæ— éœ€é’ˆå¯¹æ¯ä¸ªå®æ–½ä¾‹è¿›è¡Œè°ƒæ•´ã€‚å°½ç®¡è¿›è¡Œäº†å¤§è§„æ¨¡çš„å¤šå®æ–½ä¾‹é¢„è®­ç»ƒï¼Œç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰ä»ç„¶ä¸å…¶è®­ç»ƒå®æ–½ä¾‹ç´§å¯†è€¦åˆï¼Œå¹¶ä¸”é€šå¸¸éœ€è¦æ˜‚è´µçš„å¾®è°ƒã€‚æˆ‘ä»¬å¼•å…¥äº†è¯­è¨€åŠ¨ä½œé¢„è®­ç»ƒï¼ˆLAPï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œç›´æ¥ç”¨è‡ªç„¶è¯­è¨€è¡¨ç¤ºä½çº§æœºå™¨äººåŠ¨ä½œï¼Œä½¿åŠ¨ä½œç›‘ç£ä¸é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹çš„è¾“å…¥è¾“å‡ºåˆ†å¸ƒä¿æŒä¸€è‡´ã€‚ LAP ä¸éœ€è¦å­¦ä¹ åˆ†è¯å™¨ï¼Œä¸éœ€è¦æ˜‚è´µçš„æ³¨é‡Šï¼Œä¹Ÿä¸éœ€è¦ç‰¹å®šäºå®æ–½ä¾‹çš„æ¶æ„è®¾è®¡ã€‚åŸºäº LAPï¼Œæˆ‘ä»¬æå‡ºäº† LAP-3Bï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªå®ç°åˆ°ä»¥å‰æœªè§è¿‡çš„æœºå™¨äººå®æ–½ä¾‹çš„åŸºæœ¬é›¶æ ·æœ¬è½¬ç§»çš„ VLAï¼Œè€Œæ— éœ€ä»»ä½•ç‰¹å®šäºå®æ–½ä¾‹çš„å¾®è°ƒã€‚åœ¨å¤šä¸ªæ–°é¢–çš„æœºå™¨äººå’Œæ“çºµä»»åŠ¡ä¸­ï¼ŒLAP-3B è·å¾—äº†è¶…è¿‡ 50% çš„å¹³å‡é›¶å°„å‡»æˆåŠŸç‡ï¼Œæ¯”ä¹‹å‰æœ€å¼ºçš„ VLA å¤§çº¦æé«˜äº† 2 å€ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼ŒLAP èƒ½å¤Ÿå®ç°é«˜æ•ˆçš„é€‚åº”å’Œæœ‰åˆ©çš„æ‰©å±•ï¼ŒåŒæ—¶ä»¥å…±äº«çš„è¯­è¨€-åŠ¨ä½œæ ¼å¼ç»Ÿä¸€åŠ¨ä½œé¢„æµ‹å’Œ VQAï¼Œä»è€Œé€šè¿‡ååŒè®­ç»ƒäº§ç”Ÿé¢å¤–çš„æ”¶ç›Šã€‚

</details>

---

## 54. $Î¼$pscaling small models: Principled warm starts and hyperparameter transfer

**ä¸­æ–‡æ ‡é¢˜**: $Î¼$pscaling å°æ¨¡å‹ï¼šæœ‰åŸåˆ™çš„çƒ­å¯åŠ¨å’Œè¶…å‚æ•°ä¼ è¾“

**Date**: 2026-02-11 | **arXiv**: [2602.10545v1](http://arxiv.org/abs/2602.10545v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10545v1)

<details><summary><b>Abstract</b></summary>

Modern large-scale neural networks are often trained and released in multiple sizes to accommodate diverse inference budgets. To improve efficiency, recent work has explored model upscaling: initializing larger models from trained smaller ones in order to transfer knowledge and accelerate convergence. However, this method can be sensitive to hyperparameters that need to be tuned at the target upscaled model size, which is prohibitively costly to do directly. It remains unclear whether the most common workaround -- tuning on smaller models and extrapolating via hyperparameter scaling laws -- is still sound when using upscaling. We address this with principled approaches to upscaling with respect to model widths and efficiently tuning hyperparameters in this setting. First, motivated by $Î¼$P and any-dimensional architectures, we introduce a general upscaling method applicable to a broad range of architectures and optimizers, backed by theory guaranteeing that models are equivalent to their widened versions and allowing for rigorous analysis of infinite-width limits. Second, we extend the theory of $Î¼$Transfer to a hyperparameter transfer technique for models upscaled using our method and empirically demonstrate that this method is effective on realistic datasets and architectures.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç°ä»£å¤§è§„æ¨¡ç¥ç»ç½‘ç»œé€šå¸¸ä»¥å¤šç§è§„æ¨¡è¿›è¡Œè®­ç»ƒå’Œå‘å¸ƒï¼Œä»¥é€‚åº”ä¸åŒçš„æ¨ç†é¢„ç®—ã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼Œæœ€è¿‘çš„å·¥ä½œæ¢ç´¢äº†æ¨¡å‹å‡çº§ï¼šä»ç»è¿‡è®­ç»ƒçš„è¾ƒå°æ¨¡å‹åˆå§‹åŒ–è¾ƒå¤§æ¨¡å‹ï¼Œä»¥è½¬ç§»çŸ¥è¯†å¹¶åŠ é€Ÿæ”¶æ•›ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¯èƒ½å¯¹éœ€è¦åœ¨ç›®æ ‡æ”¾å¤§æ¨¡å‹å¤§å°ä¸Šè¿›è¡Œè°ƒæ•´çš„è¶…å‚æ•°æ•æ„Ÿï¼Œç›´æ¥è¿™æ ·åšæˆæœ¬é«˜æ˜‚ã€‚ç›®å‰å°šä¸æ¸…æ¥šæœ€å¸¸è§çš„è§£å†³æ–¹æ³•â€”â€”è°ƒæ•´è¾ƒå°çš„æ¨¡å‹å¹¶é€šè¿‡è¶…å‚æ•°ç¼©æ”¾å®šå¾‹è¿›è¡Œæ¨æ–­â€”â€”åœ¨ä½¿ç”¨å‡çº§æ—¶æ˜¯å¦ä»ç„¶æœ‰æ•ˆã€‚æˆ‘ä»¬é€šè¿‡åŸåˆ™æ€§çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œåœ¨æ¨¡å‹å®½åº¦æ–¹é¢è¿›è¡Œæ”¾å¤§ï¼Œå¹¶åœ¨æ­¤è®¾ç½®ä¸­æœ‰æ•ˆåœ°è°ƒæ•´è¶…å‚æ•°ã€‚é¦–å…ˆï¼Œåœ¨ $Î¼$P å’Œä»»æ„ç»´æ¶æ„çš„æ¨åŠ¨ä¸‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é€‚ç”¨äºå„ç§æ¶æ„å’Œä¼˜åŒ–å™¨çš„é€šç”¨å‡çº§æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»¥ç†è®ºä¸ºæ”¯æŒï¼Œä¿è¯æ¨¡å‹ä¸å…¶æ‰©å±•ç‰ˆæœ¬ç­‰æ•ˆï¼Œå¹¶å…è®¸å¯¹æ— é™å®½åº¦é™åˆ¶è¿›è¡Œä¸¥æ ¼åˆ†æã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°† $Î¼$Transfer çš„ç†è®ºæ‰©å±•åˆ°ä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•æ”¾å¤§çš„æ¨¡å‹çš„è¶…å‚æ•°ä¼ è¾“æŠ€æœ¯ï¼Œå¹¶å‡­ç»éªŒè¯æ˜è¯¥æ–¹æ³•å¯¹äºç°å®æ•°æ®é›†å’Œæ¶æ„æ˜¯æœ‰æ•ˆçš„ã€‚

</details>

---

## 55. Co-jump: Cooperative Jumping with Quadrupedal Robots via Multi-Agent Reinforcement Learning

**ä¸­æ–‡æ ‡é¢˜**: ååŒè·³è·ƒï¼šé€šè¿‡å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸å››è¶³æœºå™¨äººååŒè·³è·ƒ

**Date**: 2026-02-11 | **arXiv**: [2602.10514v1](http://arxiv.org/abs/2602.10514v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10514v1)

<details><summary><b>Abstract</b></summary>

While single-agent legged locomotion has witnessed remarkable progress, individual robots remain fundamentally constrained by physical actuation limits. To transcend these boundaries, we introduce Co-jump, a cooperative task where two quadrupedal robots synchronize to execute jumps far beyond their solo capabilities. We tackle the high-impulse contact dynamics of this task under a decentralized setting, achieving synchronization without explicit communication or pre-specified motion primitives. Our framework leverages Multi-Agent Proximal Policy Optimization (MAPPO) enhanced by a progressive curriculum strategy, which effectively overcomes the sparse-reward exploration challenges inherent in mechanically coupled systems. We demonstrate robust performance in simulation and successful transfer to physical hardware, executing multi-directional jumps onto platforms up to 1.5 m in height. Specifically, one of the robots achieves a foot-end elevation of 1.1 m, which represents a 144% improvement over the 0.45 m jump height of a standalone quadrupedal robot, demonstrating superior vertical performance. Notably, this precise coordination is achieved solely through proprioceptive feedback, establishing a foundation for communication-free collaborative locomotion in constrained environments.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è™½ç„¶å•ä»£ç†è…¿è¿åŠ¨å·²ç»å–å¾—äº†æ˜¾ç€çš„è¿›æ­¥ï¼Œä½†å•ä¸ªæœºå™¨äººä»ç„¶ä»æ ¹æœ¬ä¸Šå—åˆ°ç‰©ç†é©±åŠ¨é™åˆ¶çš„é™åˆ¶ã€‚ä¸ºäº†è¶…è¶Šè¿™äº›ç•Œé™ï¼Œæˆ‘ä»¬å¼•å…¥äº†ååŒè·³è·ƒï¼Œè¿™æ˜¯ä¸€ç§åˆä½œä»»åŠ¡ï¼Œå…¶ä¸­ä¸¤ä¸ªå››è¶³æœºå™¨äººåŒæ­¥æ‰§è¡Œè¿œè¿œè¶…å‡ºå…¶å•ç‹¬èƒ½åŠ›çš„è·³è·ƒã€‚æˆ‘ä»¬åœ¨åˆ†æ•£çš„è®¾ç½®ä¸‹å¤„ç†è¯¥ä»»åŠ¡çš„é«˜è„‰å†²æ¥è§¦åŠ¨åŠ›å­¦ï¼Œæ— éœ€æ˜¾å¼é€šä¿¡æˆ–é¢„å…ˆæŒ‡å®šçš„è¿åŠ¨åŸºå…ƒå³å¯å®ç°åŒæ­¥ã€‚æˆ‘ä»¬çš„æ¡†æ¶åˆ©ç”¨æ¸è¿›å¼è¯¾ç¨‹ç­–ç•¥å¢å¼ºçš„å¤šæ™ºèƒ½ä½“è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆMAPPOï¼‰ï¼Œæœ‰æ•ˆå…‹æœäº†æœºæ¢°è€¦åˆç³»ç»Ÿå›ºæœ‰çš„ç¨€ç–å¥–åŠ±æ¢ç´¢æŒ‘æˆ˜ã€‚æˆ‘ä»¬åœ¨æ¨¡æ‹Ÿä¸­å±•ç¤ºäº†å¼ºå¤§çš„æ€§èƒ½ï¼Œå¹¶æˆåŠŸè½¬ç§»åˆ°ç‰©ç†ç¡¬ä»¶ï¼Œåœ¨é«˜è¾¾ 1.5 m çš„å¹³å°ä¸Šæ‰§è¡Œå¤šå‘è·³è·ƒã€‚å…·ä½“æ¥è¯´ï¼Œå…¶ä¸­ä¸€å°æœºå™¨äººçš„è¶³ç«¯é«˜åº¦è¾¾åˆ°äº† 1.1 mï¼Œè¿™æ¯”ç‹¬ç«‹å››è¶³æœºå™¨äººçš„ 0.45 m è·³è·ƒé«˜åº¦æé«˜äº† 144%ï¼Œå±•ç¤ºäº†å“è¶Šçš„å‚ç›´æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ç§ç²¾ç¡®çš„åè°ƒä»…é€šè¿‡æœ¬ä½“æ„Ÿè§‰åé¦ˆæ¥å®ç°ï¼Œä¸ºå—é™ç¯å¢ƒä¸­çš„æ— é€šä¿¡åä½œè¿åŠ¨å¥ å®šäº†åŸºç¡€ã€‚

</details>

---

## 56. Driving Reaction Trajectories via Latent Flow Matching

**ä¸­æ–‡æ ‡é¢˜**: é€šè¿‡æ½œåœ¨æµåŒ¹é…é©±åŠ¨ååº”è½¨è¿¹

**Date**: 2026-02-11 | **arXiv**: [2602.10476v1](http://arxiv.org/abs/2602.10476v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10476v1)

<details><summary><b>Abstract</b></summary>

Recent advances in reaction prediction have achieved near-saturated accuracy on standard benchmarks (e.g., USPTO), yet most state-of-the-art models formulate the task as a one-shot mapping from reactants to products, offering limited insight into the underlying reaction process. Procedural alternatives introduce stepwise generation but often rely on mechanism-specific supervision, discrete symbolic edits, and computationally expensive inference. In this work, we propose LatentRxnFlow, a new reaction prediction paradigm that models reactions as continuous latent trajectories anchored at the thermodynamic product state. Built on Conditional Flow Matching, our approach learns time-dependent latent dynamics directly from standard reactant-product pairs, without requiring mechanistic annotations or curated intermediate labels. While LatentRxnFlow achieves state-of-the-art performance on USPTO benchmarks, more importantly, the continuous formulation exposes the full generative trajectory, enabling trajectory-level diagnostics that are difficult to realize with discrete or one-shot models. We show that latent trajectory analysis allows us to localize and characterize failure modes and to mitigate certain errors via gated inference. Furthermore, geometric properties of the learned trajectories provide an intrinsic signal of epistemic uncertainty, helping prioritize reliably predictable reaction outcomes and flag ambiguous cases for additional validation. Overall, LatentRxnFlow combines strong predictive accuracy with improved transparency, diagnosability, and uncertainty awareness, moving reaction prediction toward more trustworthy deployment in high-throughput discovery workflows.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ååº”é¢„æµ‹çš„æœ€æ–°è¿›å±•å·²åœ¨æ ‡å‡†åŸºå‡†ï¼ˆä¾‹å¦‚ç¾å›½ä¸“åˆ©å•†æ ‡å±€ï¼‰ä¸Šå®ç°äº†æ¥è¿‘é¥±å’Œçš„å‡†ç¡®æ€§ï¼Œä½†å¤§å¤šæ•°æœ€å…ˆè¿›çš„æ¨¡å‹å°†ä»»åŠ¡åˆ¶å®šä¸ºä»ååº”ç‰©åˆ°äº§ç‰©çš„ä¸€æ¬¡æ€§æ˜ å°„ï¼Œå¯¹æ½œåœ¨ååº”è¿‡ç¨‹çš„äº†è§£æœ‰é™ã€‚ç¨‹åºæ›¿ä»£æ–¹æ¡ˆå¼•å…¥äº†é€æ­¥ç”Ÿæˆï¼Œä½†é€šå¸¸ä¾èµ–äºç‰¹å®šäºæœºåˆ¶çš„ç›‘ç£ã€ç¦»æ•£ç¬¦å·ç¼–è¾‘å’Œè®¡ç®—æˆæœ¬é«˜æ˜‚çš„æ¨ç†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† LatentRxnFlowï¼Œä¸€ç§æ–°çš„ååº”é¢„æµ‹èŒƒå¼ï¼Œå®ƒå°†ååº”å»ºæ¨¡ä¸ºé”šå®šäºçƒ­åŠ›å­¦äº§ç‰©çŠ¶æ€çš„è¿ç»­æ½œåœ¨è½¨è¿¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºæ¡ä»¶æµåŒ¹é…ï¼Œç›´æ¥ä»æ ‡å‡†ååº”ç‰©-äº§ç‰©å¯¹å­¦ä¹ æ—¶é—´ç›¸å…³çš„æ½œåœ¨åŠ¨æ€ï¼Œæ— éœ€æœºæ¢°æ³¨é‡Šæˆ–ç­–åˆ’çš„ä¸­é—´æ ‡ç­¾ã€‚è™½ç„¶ LatentRxnFlow åœ¨ USPTO åŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½†æ›´é‡è¦çš„æ˜¯ï¼Œè¿ç»­å…¬å¼æš´éœ²äº†å®Œæ•´çš„ç”Ÿæˆè½¨è¿¹ï¼Œä»è€Œå®ç°äº†ç¦»æ•£æˆ–ä¸€æ¬¡æ€§æ¨¡å‹éš¾ä»¥å®ç°çš„è½¨è¿¹çº§è¯Šæ–­ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œæ½œåœ¨è½¨è¿¹åˆ†æä½¿æˆ‘ä»¬èƒ½å¤Ÿå®šä½å’Œè¡¨å¾æ•…éšœæ¨¡å¼ï¼Œå¹¶é€šè¿‡é—¨æ§æ¨ç†æ¥å‡è½»æŸäº›é”™è¯¯ã€‚æ­¤å¤–ï¼Œå­¦ä¹ è½¨è¿¹çš„å‡ ä½•ç‰¹æ€§æä¾›äº†è®¤çŸ¥ä¸ç¡®å®šæ€§çš„å†…åœ¨ä¿¡å·ï¼Œæœ‰åŠ©äºä¼˜å…ˆè€ƒè™‘å¯é é¢„æµ‹çš„ååº”ç»“æœï¼Œå¹¶æ ‡è®°ä¸æ˜ç¡®çš„æƒ…å†µä»¥è¿›è¡Œé¢å¤–éªŒè¯ã€‚æ€»ä½“è€Œè¨€ï¼ŒLatentRxnFlow å°†å¼ºå¤§çš„é¢„æµ‹å‡†ç¡®æ€§ä¸æ”¹è¿›çš„é€æ˜åº¦ã€å¯è¯Šæ–­æ€§å’Œä¸ç¡®å®šæ€§æ„è¯†ç›¸ç»“åˆï¼Œå°†ååº”é¢„æµ‹æ¨å‘é«˜é€šé‡å‘ç°å·¥ä½œæµç¨‹ä¸­æ›´å€¼å¾—ä¿¡èµ–çš„éƒ¨ç½²ã€‚

</details>

---

## 57. Found-RL: foundation model-enhanced reinforcement learning for autonomous driving

**ä¸­æ–‡æ ‡é¢˜**: Found-RLï¼šè‡ªåŠ¨é©¾é©¶çš„åŸºç¡€æ¨¡å‹å¢å¼ºå¼ºåŒ–å­¦ä¹ 

**Date**: 2026-02-11 | **arXiv**: [2602.10458v1](http://arxiv.org/abs/2602.10458v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10458v1)

**Code**: https://github.com/ys-qu/found-rl.

<details><summary><b>Abstract</b></summary>

Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering rich, context-aware knowledge, yet their high inference latency hinders deployment in high-frequency RL training loops. To bridge this gap, we present Found-RL, a platform tailored to efficiently enhance RL for AD using foundation models. A core innovation is the asynchronous batch inference framework, which decouples heavy VLM reasoning from the simulation loop, effectively resolving latency bottlenecks to support real-time learning. We introduce diverse supervision mechanisms: Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy. Additionally, we adopt high-throughput CLIP for dense reward shaping. We address CLIP's dynamic blindness via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields a normalized, margin-based bonus from context-specific action-anchor scoring. Found-RL provides an end-to-end pipeline for fine-tuned VLM integration and shows that a lightweight RL model can achieve near-VLM performance compared with billion-parameter VLMs while sustaining real-time inference (approx. 500 FPS). Code, data, and models will be publicly available at https://github.com/ys-qu/found-rl.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰çš„ä¸»å¯¼èŒƒä¾‹ã€‚ç„¶è€Œï¼Œå¼ºåŒ–å­¦ä¹ é¢ä¸´ç€æ ·æœ¬æ•ˆç‡ä½ä¸‹ä»¥åŠå¤æ‚åœºæ™¯ä¸‹ç¼ºä¹è¯­ä¹‰å¯è§£é‡Šæ€§çš„é—®é¢˜ã€‚åŸºç¡€æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯è§†è§‰è¯­è¨€æ¨¡å‹ (VLM)ï¼Œå¯ä»¥é€šè¿‡æä¾›ä¸°å¯Œçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥çŸ¥è¯†æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†å…¶é«˜æ¨ç†å»¶è¿Ÿé˜»ç¢äº†é«˜é¢‘ RL è®­ç»ƒå¾ªç¯ä¸­çš„éƒ¨ç½²ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº† Found-RLï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºä½¿ç”¨åŸºç¡€æ¨¡å‹æœ‰æ•ˆå¢å¼º AD å¼ºåŒ–å­¦ä¹ è€Œå®šåˆ¶çš„å¹³å°ã€‚æ ¸å¿ƒåˆ›æ–°æ˜¯å¼‚æ­¥æ‰¹é‡æ¨ç†æ¡†æ¶ï¼Œå°†ç¹é‡çš„VLMæ¨ç†ä¸ä»¿çœŸå¾ªç¯è§£è€¦ï¼Œæœ‰æ•ˆè§£å†³å»¶è¿Ÿç“¶é¢ˆï¼Œæ”¯æŒå®æ—¶å­¦ä¹ ã€‚ We introduce diverse supervision mechanisms: Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy.æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨é«˜é€šé‡ CLIP è¿›è¡Œå¯†é›†å¥–åŠ±å¡‘é€ ã€‚ We address CLIP's dynamic blindness via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields a normalized, margin-based bonus from context-specific action-anchor scoring. Found-RL provides an end-to-end pipeline for fine-tuned VLM integration and shows that a lightweight RL model can achieve near-VLM performance compared with billion-parameter VLMs while sustaining real-time inference (approx. 500 FPS).ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å°†åœ¨ https://github.com/ys-qu/found-rl ä¸Šå…¬å¼€æä¾›ã€‚

</details>

---

## 58. Constructing Industrial-Scale Optimization Modeling Benchmark

**ä¸­æ–‡æ ‡é¢˜**: æ„å»ºå·¥ä¸šè§„æ¨¡ä¼˜åŒ–å»ºæ¨¡åŸºå‡†

**Date**: 2026-02-11 | **arXiv**: [2602.10450v1](http://arxiv.org/abs/2602.10450v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10450v1)

<details><summary><b>Abstract</b></summary>

Optimization modeling underpins decision-making in logistics, manufacturing, energy, and finance, yet translating natural-language requirements into correct optimization formulations and solver-executable code remains labor-intensive. Although large language models (LLMs) have been explored for this task, evaluation is still dominated by toy-sized or synthetic benchmarks, masking the difficulty of industrial problems with $10^{3}$--$10^{6}$ (or more) variables and constraints. A key bottleneck is the lack of benchmarks that align natural-language specifications with reference formulations/solver code grounded in real optimization models. To fill in this gap, we introduce MIPLIB-NL, built via a structure-aware reverse construction methodology from real mixed-integer linear programs in MIPLIB~2017. Our pipeline (i) recovers compact, reusable model structure from flat solver formulations, (ii) reverse-generates natural-language specifications explicitly tied to this recovered structure under a unified model--data separation format, and (iii) performs iterative semantic validation through expert review and human--LLM interaction with independent reconstruction checks. This yields 223 one-to-one reconstructions that preserve the mathematical content of the original instances while enabling realistic natural-language-to-optimization evaluation. Experiments show substantial performance degradation on MIPLIB-NL for systems that perform strongly on existing benchmarks, exposing failure modes invisible at toy scale.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ä¼˜åŒ–å»ºæ¨¡æ”¯æ’‘ç€ç‰©æµã€åˆ¶é€ ã€èƒ½æºå’Œé‡‘èé¢†åŸŸçš„å†³ç­–ï¼Œä½†å°†è‡ªç„¶è¯­è¨€éœ€æ±‚è½¬åŒ–ä¸ºæ­£ç¡®çš„ä¼˜åŒ–å…¬å¼å’Œæ±‚è§£å™¨å¯æ‰§è¡Œä»£ç ä»ç„¶æ˜¯åŠ³åŠ¨å¯†é›†å‹çš„ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) å·²é’ˆå¯¹æ­¤ä»»åŠ¡è¿›è¡Œäº†æ¢ç´¢ï¼Œä½†è¯„ä¼°ä»ç„¶ä»¥ç©å…·å¤§å°æˆ–ç»¼åˆåŸºå‡†ä¸ºä¸»ï¼Œæ©ç›–äº†å…·æœ‰ 10^{3}$--$10^{6}$ï¼ˆæˆ–æ›´å¤šï¼‰å˜é‡å’Œçº¦æŸçš„å·¥ä¸šé—®é¢˜çš„éš¾åº¦ã€‚ä¸€ä¸ªå…³é”®ç“¶é¢ˆæ˜¯ç¼ºä¹å°†è‡ªç„¶è¯­è¨€è§„èŒƒä¸åŸºäºå®é™…ä¼˜åŒ–æ¨¡å‹çš„å‚è€ƒå…¬å¼/æ±‚è§£å™¨ä»£ç ä¿æŒä¸€è‡´çš„åŸºå‡†ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº† MIPLIB-NLï¼Œå®ƒæ˜¯é€šè¿‡ MIPLIB~2017 ä¸­çœŸå®æ··åˆæ•´æ•°çº¿æ€§ç¨‹åºçš„ç»“æ„æ„ŸçŸ¥é€†å‘æ„é€ æ–¹æ³•æ„å»ºçš„ã€‚æˆ‘ä»¬çš„æµç¨‹ï¼ˆiï¼‰ä»å¹³é¢æ±‚è§£å™¨å…¬å¼ä¸­æ¢å¤ç´§å‡‘ã€å¯é‡ç”¨çš„æ¨¡å‹ç»“æ„ï¼Œï¼ˆiiï¼‰åœ¨ç»Ÿä¸€æ¨¡å‹æ•°æ®åˆ†ç¦»æ ¼å¼ä¸‹åå‘ç”Ÿæˆä¸è¯¥æ¢å¤ç»“æ„æ˜ç¡®ç›¸å…³çš„è‡ªç„¶è¯­è¨€è§„èŒƒï¼Œä»¥åŠï¼ˆiiiï¼‰é€šè¿‡ä¸“å®¶è¯„å®¡å’Œäººä¸æ³•å­¦ç¡•å£«äº¤äº’ä»¥åŠç‹¬ç«‹é‡å»ºæ£€æŸ¥æ¥æ‰§è¡Œè¿­ä»£è¯­ä¹‰éªŒè¯ã€‚è¿™ä¼šäº§ç”Ÿ 223 ä¸ªä¸€å¯¹ä¸€çš„é‡å»ºï¼Œä¿ç•™åŸå§‹å®ä¾‹çš„æ•°å­¦å†…å®¹ï¼ŒåŒæ—¶å®ç°ç°å®çš„è‡ªç„¶è¯­è¨€ä¼˜åŒ–è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼Œå¯¹äºåœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²çš„ç³»ç»Ÿï¼ŒMIPLIB-NL çš„æ€§èƒ½ä¼šå¤§å¹…ä¸‹é™ï¼Œä»è€Œæš´éœ²å‡ºç©å…·è§„æ¨¡ä¸Šä¸å¯è§çš„æ•…éšœæ¨¡å¼ã€‚

</details>

---

## 59. A Unified Theory of Random Projection for Influence Functions

**ä¸­æ–‡æ ‡é¢˜**: å½±å“å‡½æ•°éšæœºæŠ•å½±çš„ç»Ÿä¸€ç†è®º

**Date**: 2026-02-11 | **arXiv**: [2602.10449v1](http://arxiv.org/abs/2602.10449v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10449v1)

<details><summary><b>Abstract</b></summary>

Influence functions and related data attribution scores take the form of $g^{\top}F^{-1}g^{\prime}$, where $F\succeq 0$ is a curvature operator. In modern overparameterized models, forming or inverting $F\in\mathbb{R}^{d\times d}$ is prohibitive, motivating scalable influence computation via random projection with a sketch $P \in \mathbb{R}^{m\times d}$. This practice is commonly justified via the Johnson--Lindenstrauss (JL) lemma, which ensures approximate preservation of Euclidean geometry for a fixed dataset. However, JL does not address how sketching behaves under inversion. Furthermore, there is no existing theory that explains how sketching interacts with other widely-used techniques, such as ridge regularization and structured curvature approximations.   We develop a unified theory characterizing when projection provably preserves influence functions. When $g,g^{\prime}\in\text{range}(F)$, we show that: 1) Unregularized projection: exact preservation holds iff $P$ is injective on $\text{range}(F)$, which necessitates $m\geq \text{rank}(F)$; 2) Regularized projection: ridge regularization fundamentally alters the sketching barrier, with approximation guarantees governed by the effective dimension of $F$ at the regularization scale; 3) Factorized influence: for Kronecker-factored curvatures $F=A\otimes E$, the guarantees continue to hold for decoupled sketches $P=P_A\otimes P_E$, even though such sketches exhibit row correlations that violate i.i.d. assumptions. Beyond this range-restricted setting, we analyze out-of-range test gradients and quantify a \emph{leakage} term that arises when test gradients have components in $\ker(F)$. This yields guarantees for influence queries on general test points.   Overall, this work develops a novel theory that characterizes when projection provably preserves influence and provides principled guidance for choosing the sketch size in practice.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å½±å“å‡½æ•°å’Œç›¸å…³æ•°æ®å½’å› åˆ†æ•°é‡‡ç”¨ $g^{\top}F^{-1}g^{\prime}$ çš„å½¢å¼ï¼Œå…¶ä¸­ $F\succeq 0$ æ˜¯æ›²ç‡ç®—å­ã€‚åœ¨ç°ä»£è¶…å‚æ•°åŒ–æ¨¡å‹ä¸­ï¼Œå½¢æˆæˆ–åè½¬ $F\in\mathbb{R}^{d\times d}$ æ˜¯ç¦æ­¢çš„ï¼Œé€šè¿‡ä½¿ç”¨è‰å›¾ $P \in \mathbb{R}^{m\times d}$ è¿›è¡ŒéšæœºæŠ•å½±æ¥æ¿€å‘å¯æ‰©å±•çš„å½±å“è®¡ç®—ã€‚è¿™ç§åšæ³•é€šå¸¸é€šè¿‡ Johnson-Lindenstrauss (JL) å¼•ç†æ¥è¯æ˜ï¼Œè¯¥å¼•ç†ç¡®ä¿å›ºå®šæ•°æ®é›†çš„æ¬§å‡ é‡Œå¾—å‡ ä½•çš„è¿‘ä¼¼ä¿ç•™ã€‚ç„¶è€Œï¼ŒJL æ²¡æœ‰è§£å†³è‰å›¾åœ¨åè½¬ä¸‹çš„è¡Œä¸ºæ–¹å¼ã€‚æ­¤å¤–ï¼Œæ²¡æœ‰ç°æœ‰çš„ç†è®ºå¯ä»¥è§£é‡Šè‰å›¾ç»˜åˆ¶å¦‚ä½•ä¸å…¶ä»–å¹¿æ³›ä½¿ç”¨çš„æŠ€æœ¯ï¼ˆä¾‹å¦‚è„Šæ­£åˆ™åŒ–å’Œç»“æ„åŒ–æ›²ç‡è¿‘ä¼¼ï¼‰ç›¸äº’ä½œç”¨ã€‚   æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç»Ÿä¸€çš„ç†è®ºæ¥æè¿°æŠ•å½±ä½•æ—¶å¯è¯æ˜ä¿ç•™å½±å“å‡½æ•°ã€‚å½“ $g,g^{\prime}\in\text{range}(F)$ æ—¶ï¼Œæˆ‘ä»¬è¯æ˜ï¼š 1) éæ­£åˆ™åŒ–æŠ•å½±ï¼šç²¾ç¡®ä¿ç•™æˆç«‹ï¼Œå½“ä¸”ä»…å½“ $P$ åœ¨ $\text{range}(F)$ ä¸Šå•å°„ï¼Œè¿™éœ€è¦ $m\geq \text{rank}(F)$ï¼› 2) æ­£åˆ™åŒ–æŠ•å½±ï¼šå²­æ­£åˆ™åŒ–ä»æ ¹æœ¬ä¸Šæ”¹å˜äº†è‰å›¾éšœç¢ï¼Œå¹¶ç”±æ­£åˆ™åŒ–å°ºåº¦ä¸‹ $F$ çš„æœ‰æ•ˆç»´åº¦æ§åˆ¶è¿‘ä¼¼ä¿è¯ï¼› 3) å› å­å½±å“ï¼šå¯¹äºå…‹ç½—å†…å…‹å› å­æ›²ç‡ $F=A\otimes E$ï¼Œè§£è€¦è‰å›¾ $P=P_A\otimes P_E$ çš„ä¿è¯ç»§ç»­æˆç«‹ï¼Œå³ä½¿æ­¤ç±»è‰å›¾è¡¨ç°å‡ºè¿åç‹¬ç«‹åŒåˆ†å¸ƒçš„è¡Œç›¸å…³æ€§ã€‚å‡è®¾ã€‚é™¤äº†è¿™ä¸ªèŒƒå›´é™åˆ¶è®¾ç½®ä¹‹å¤–ï¼Œæˆ‘ä»¬åˆ†æè¶…å‡ºèŒƒå›´çš„æµ‹è¯•æ¢¯åº¦å¹¶é‡åŒ–å½“æµ‹è¯•æ¢¯åº¦åœ¨ $\ker(F)$ ä¸­åŒ…å«ç»„ä»¶æ—¶å‡ºç°çš„ \emph{leakage} é¡¹ã€‚è¿™ä¸ºä¸€èˆ¬æµ‹è¯•ç‚¹çš„å½±å“æŸ¥è¯¢æä¾›äº†ä¿è¯ã€‚   æ€»ä½“è€Œè¨€ï¼Œè¿™é¡¹å·¥ä½œå¼€å‘äº†ä¸€ç§æ–°é¢–çš„ç†è®ºï¼Œè¯¥ç†è®ºæè¿°äº†æŠ•å½±ä½•æ—¶å¯è¯æ˜ä¿ç•™å½±å“åŠ›ï¼Œå¹¶ä¸ºå®è·µä¸­é€‰æ‹©è‰å›¾å°ºå¯¸æä¾›äº†åŸåˆ™æ€§æŒ‡å¯¼ã€‚

</details>

---

## 60. A Dual-Stream Physics-Augmented Unsupervised Architecture for Runtime Embedded Vehicle Health Monitoring

**ä¸­æ–‡æ ‡é¢˜**: ç”¨äºè¿è¡Œæ—¶åµŒå…¥å¼è½¦è¾†å¥åº·ç›‘æµ‹çš„åŒæµç‰©ç†å¢å¼ºæ— ç›‘ç£æ¶æ„

**Date**: 2026-02-11 | **arXiv**: [2602.10432v1](http://arxiv.org/abs/2602.10432v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10432v1)

<details><summary><b>Abstract</b></summary>

Runtime quantification of vehicle operational intensity is essential for predictive maintenance and condition monitoring in commercial and heavy-duty fleets. Traditional metrics like mileage fail to capture mechanical burden, while unsupervised deep learning models detect statistical anomalies, typically transient surface shocks, but often conflate statistical stability with mechanical rest. We identify this as a critical blind spot: high-load steady states, such as hill climbing with heavy payloads, appear statistically normal yet impose significant drivetrain fatigue. To resolve this, we propose a Dual-Stream Architecture that fuses unsupervised learning for surface anomaly detection with macroscopic physics proxies for cumulative load estimation. This approach leverages low-frequency sensor data to generate a multi-dimensional health vector, distinguishing between dynamic hazards and sustained mechanical effort. Validated on a RISC-V embedded platform, the architecture demonstrates low computational overhead, enabling comprehensive, edge-based health monitoring on resource-constrained ECUs without the latency or bandwidth costs of cloud-based monitoring.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è½¦è¾†è¿è¡Œå¼ºåº¦çš„è¿è¡Œæ—¶é‡åŒ–å¯¹äºå•†ä¸šå’Œé‡å‹è½¦é˜Ÿçš„é¢„æµ‹æ€§ç»´æŠ¤å’ŒçŠ¶æ€ç›‘æµ‹è‡³å…³é‡è¦ã€‚é‡Œç¨‹ç­‰ä¼ ç»ŸæŒ‡æ ‡æ— æ³•æ•æ‰æœºæ¢°è´Ÿè·ï¼Œè€Œæ— ç›‘ç£æ·±åº¦å­¦ä¹ æ¨¡å‹å¯ä»¥æ£€æµ‹ç»Ÿè®¡å¼‚å¸¸ï¼Œé€šå¸¸æ˜¯ç¬æ€è¡¨é¢å†²å‡»ï¼Œä½†å¸¸å¸¸å°†ç»Ÿè®¡ç¨³å®šæ€§ä¸æœºæ¢°ä¼‘æ¯æ··ä¸ºä¸€è°ˆã€‚æˆ‘ä»¬è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªå…³é”®çš„ç›²ç‚¹ï¼šé«˜è´Ÿè½½ç¨³æ€ï¼Œä¾‹å¦‚è´Ÿè½½è¾ƒé‡çš„çˆ¬å±±ï¼Œåœ¨ç»Ÿè®¡ä¸Šçœ‹èµ·æ¥æ˜¯æ­£å¸¸çš„ï¼Œä½†ä¼šé€ æˆä¸¥é‡çš„ä¼ åŠ¨ç³»ç»Ÿç–²åŠ³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒæµæ¶æ„ï¼Œå®ƒå°†ç”¨äºè¡¨é¢å¼‚å¸¸æ£€æµ‹çš„æ— ç›‘ç£å­¦ä¹ ä¸ç”¨äºç´¯ç§¯è´Ÿè½½ä¼°è®¡çš„å®è§‚ç‰©ç†ä»£ç†ç›¸èåˆã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨ä½é¢‘ä¼ æ„Ÿå™¨æ•°æ®ç”Ÿæˆå¤šç»´å¥åº·å‘é‡ï¼ŒåŒºåˆ†åŠ¨æ€å±é™©å’ŒæŒç»­çš„æœºæ¢°ä½œç”¨ã€‚è¯¥æ¶æ„åœ¨ RISC-V åµŒå…¥å¼å¹³å°ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œæ˜¾ç¤ºå‡ºè¾ƒä½çš„è®¡ç®—å¼€é”€ï¼Œèƒ½å¤Ÿå¯¹èµ„æºå—é™çš„ ECU è¿›è¡Œå…¨é¢çš„ã€åŸºäºè¾¹ç¼˜çš„è¿è¡ŒçŠ¶å†µç›‘æ§ï¼Œè€Œä¸ä¼šäº§ç”ŸåŸºäºäº‘çš„ç›‘æ§çš„å»¶è¿Ÿæˆ–å¸¦å®½æˆæœ¬ã€‚

</details>

---

## 61. Multi-Task Reinforcement Learning of Drone Aerobatics by Exploiting Geometric Symmetries

**ä¸­æ–‡æ ‡é¢˜**: åˆ©ç”¨å‡ ä½•å¯¹ç§°æ€§è¿›è¡Œæ— äººæœºç‰¹æŠ€é£è¡Œçš„å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ 

**Date**: 2026-02-11 | **arXiv**: [2602.10997v1](http://arxiv.org/abs/2602.10997v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10997v1)

<details><summary><b>Abstract</b></summary>

Flight control for autonomous micro aerial vehicles (MAVs) is evolving from steady flight near equilibrium points toward more aggressive aerobatic maneuvers, such as flips, rolls, and Power Loop. Although reinforcement learning (RL) has shown great potential in these tasks, conventional RL methods often suffer from low data efficiency and limited generalization. This challenge becomes more pronounced in multi-task scenarios where a single policy is required to master multiple maneuvers. In this paper, we propose a novel end-to-end multi-task reinforcement learning framework, called GEAR (Geometric Equivariant Aerobatics Reinforcement), which fully exploits the inherent SO(2) rotational symmetry in MAV dynamics and explicitly incorporates this property into the policy network architecture. By integrating an equivariant actor network, FiLM-based task modulation, and a multi-head critic, GEAR achieves both efficiency and flexibility in learning diverse aerobatic maneuvers, enabling a data-efficient, robust, and unified framework for aerobatic control. GEAR attains a 98.85\% success rate across various aerobatic tasks, significantly outperforming baseline methods. In real-world experiments, GEAR demonstrates stable execution of multiple maneuvers and the capability to combine basic motion primitives to complete complex aerobatics.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è‡ªä¸»å¾®å‹é£è¡Œå™¨ (MAV) çš„é£è¡Œæ§åˆ¶æ­£åœ¨ä»å¹³è¡¡ç‚¹é™„è¿‘çš„ç¨³å®šé£è¡Œå‘å±•åˆ°æ›´æ¿€è¿›çš„ç‰¹æŠ€é£è¡Œï¼Œä¾‹å¦‚ç¿»è½¬ã€æ»šè½¬å’ŒåŠ¨åŠ›å¾ªç¯ã€‚å°½ç®¡å¼ºåŒ–å­¦ä¹  (RL) åœ¨è¿™äº›ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†ä¼ ç»Ÿçš„ RL æ–¹æ³•å¾€å¾€å­˜åœ¨æ•°æ®æ•ˆç‡ä½å’Œæ³›åŒ–èƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚åœ¨éœ€è¦å•ä¸€ç­–ç•¥æ¥æŒæ¡å¤šç§æ“ä½œçš„å¤šä»»åŠ¡åœºæ™¯ä¸­ï¼Œè¿™ä¸€æŒ‘æˆ˜å˜å¾—æ›´åŠ æ˜æ˜¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç«¯åˆ°ç«¯å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç§°ä¸º GEARï¼ˆå‡ ä½•ç­‰å˜ç‰¹æŠ€é£è¡Œå¼ºåŒ–ï¼‰ï¼Œå®ƒå……åˆ†åˆ©ç”¨äº† MAV åŠ¨åŠ›å­¦ä¸­å›ºæœ‰çš„ SO(2) æ—‹è½¬å¯¹ç§°æ€§ï¼Œå¹¶æ˜ç¡®åœ°å°†è¿™ä¸€å±æ€§åˆå¹¶åˆ°ç­–ç•¥ç½‘ç»œæ¶æ„ä¸­ã€‚é€šè¿‡é›†æˆç­‰å˜å‚ä¸è€…ç½‘ç»œã€åŸºäº FiLM çš„ä»»åŠ¡è°ƒåˆ¶å’Œå¤šå¤´æ‰¹è¯„å™¨ï¼ŒGEAR åœ¨å­¦ä¹ å„ç§ç‰¹æŠ€é£è¡Œæ–¹é¢å®ç°äº†æ•ˆç‡å’Œçµæ´»æ€§ï¼Œä»è€Œä¸ºç‰¹æŠ€é£è¡Œæ§åˆ¶æä¾›äº†æ•°æ®é«˜æ•ˆã€ç¨³å¥ä¸”ç»Ÿä¸€çš„æ¡†æ¶ã€‚ GEAR åœ¨å„ç§ç‰¹æŠ€é£è¡Œä»»åŠ¡ä¸­å–å¾—äº† 98.85% çš„æˆåŠŸç‡ï¼Œæ˜¾ç€ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚åœ¨ç°å®ä¸–ç•Œçš„å®éªŒä¸­ï¼ŒGEAR å±•ç¤ºäº†å¤šç§æœºåŠ¨çš„ç¨³å®šæ‰§è¡Œä»¥åŠç»“åˆåŸºæœ¬è¿åŠ¨åŸè¯­æ¥å®Œæˆå¤æ‚ç‰¹æŠ€é£è¡Œçš„èƒ½åŠ›ã€‚

</details>

---

## 62. Scaling World Model for Hierarchical Manipulation Policies

**ä¸­æ–‡æ ‡é¢˜**: åˆ†çº§æ“çºµç­–ç•¥çš„æ‰©å±•ä¸–ç•Œæ¨¡å‹

**Date**: 2026-02-11 | **arXiv**: [2602.10983v1](http://arxiv.org/abs/2602.10983v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10983v1)

<details><summary><b>Abstract</b></summary>

Vision-Language-Action (VLA) models are promising for generalist robot manipulation but remain brittle in out-of-distribution (OOD) settings, especially with limited real-robot data. To resolve the generalization bottleneck, we introduce a hierarchical Vision-Language-Action framework \our{} that leverages the generalization of large-scale pre-trained world model for robust and generalizable VIsual Subgoal TAsk decomposition VISTA. Our hierarchical framework \our{} consists of a world model as the high-level planner and a VLA as the low-level executor. The high-level world model first divides manipulation tasks into subtask sequences with goal images, and the low-level policy follows the textual and visual guidance to generate action sequences. Compared to raw textual goal specification, these synthesized goal images provide visually and physically grounded details for low-level policies, making it feasible to generalize across unseen objects and novel scenarios. We validate both visual goal synthesis and our hierarchical VLA policies in massive out-of-distribution scenarios, and the performance of the same-structured VLA in novel scenarios could boost from 14% to 69% with the guidance generated by the world model. Results demonstrate that our method outperforms previous baselines with a clear margin, particularly in out-of-distribution scenarios. Project page: \href{https://vista-wm.github.io/}{https://vista-wm.github.io}

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹å¯¹äºé€šç”¨æœºå™¨äººæ“ä½œå¾ˆæœ‰å¸Œæœ›ï¼Œä½†åœ¨åˆ†å¸ƒå¤–ï¼ˆOODï¼‰è®¾ç½®ä¸­ä»ç„¶å¾ˆè„†å¼±ï¼Œå°¤å…¶æ˜¯åœ¨çœŸå®æœºå™¨äººæ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ã€‚ä¸ºäº†è§£å†³æ³›åŒ–ç“¶é¢ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†åˆ†å±‚è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¡†æ¶ \our{}ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒä¸–ç•Œæ¨¡å‹çš„æ³›åŒ–æ¥å®ç°ç¨³å¥ä¸”å¯æ³›åŒ–çš„è§†è§‰å­ç›®æ ‡ä»»åŠ¡åˆ†è§£ VISTAã€‚æˆ‘ä»¬çš„åˆ†å±‚æ¡†æ¶ \our{} ç”±ä½œä¸ºé«˜çº§è§„åˆ’å™¨çš„ä¸–ç•Œæ¨¡å‹å’Œä½œä¸ºä½çº§æ‰§è¡Œå™¨çš„ VLA ç»„æˆã€‚é«˜å±‚ä¸–ç•Œæ¨¡å‹é¦–å…ˆå°†æ“ä½œä»»åŠ¡åˆ’åˆ†ä¸ºå…·æœ‰ç›®æ ‡å›¾åƒçš„å­ä»»åŠ¡åºåˆ—ï¼Œä½å±‚ç­–ç•¥éµå¾ªæ–‡æœ¬å’Œè§†è§‰æŒ‡å¯¼æ¥ç”ŸæˆåŠ¨ä½œåºåˆ—ã€‚ä¸åŸå§‹æ–‡æœ¬ç›®æ ‡è§„èŒƒç›¸æ¯”ï¼Œè¿™äº›åˆæˆçš„ç›®æ ‡å›¾åƒä¸ºä½çº§ç­–ç•¥æä¾›äº†è§†è§‰å’Œç‰©ç†åŸºç¡€çš„ç»†èŠ‚ï¼Œä½¿å¾—åœ¨æœªè§è¿‡çš„ç‰©ä½“å’Œæ–°åœºæ™¯ä¸­è¿›è¡Œæ³›åŒ–æˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬åœ¨å¤§è§„æ¨¡åˆ†å¸ƒå¤–åœºæ™¯ä¸­éªŒè¯äº†è§†è§‰ç›®æ ‡åˆæˆå’Œåˆ†å±‚ VLA ç­–ç•¥ï¼Œå¹¶ä¸”åœ¨ä¸–ç•Œæ¨¡å‹ç”Ÿæˆçš„æŒ‡å¯¼ä¸‹ï¼Œç›¸åŒç»“æ„çš„ VLA åœ¨æ–°é¢–åœºæ™¯ä¸­çš„æ€§èƒ½å¯ä»¥ä» 14% æé«˜åˆ° 69%ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜æ˜¾ä¼˜äºä»¥å‰çš„åŸºçº¿ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†å¸ƒå¤–çš„æƒ…å†µä¸‹ã€‚é¡¹ç›®é¡µé¢ï¼š\href{https://vista-wm.github.io/}{https://vista-wm.github.io}

</details>

---

## 63. RADAR: Benchmarking Vision-Language-Action Generalization via Real-World Dynamics, Spatial-Physical Intelligence, and Autonomous Evaluation

**ä¸­æ–‡æ ‡é¢˜**: RADARï¼šé€šè¿‡ç°å®ä¸–ç•ŒåŠ¨åŠ›å­¦ã€ç©ºé—´ç‰©ç†æ™ºèƒ½å’Œè‡ªä¸»è¯„ä¼°å¯¹è§†è§‰-è¯­è¨€-åŠ¨ä½œæ³›åŒ–è¿›è¡ŒåŸºå‡†æµ‹è¯•

**Date**: 2026-02-11 | **arXiv**: [2602.10980v1](http://arxiv.org/abs/2602.10980v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10980v1)

<details><summary><b>Abstract</b></summary>

VLA models have achieved remarkable progress in embodied intelligence; however, their evaluation remains largely confined to simulations or highly constrained real-world settings. This mismatch creates a substantial reality gap, where strong benchmark performance often masks poor generalization in diverse physical environments. We identify three systemic shortcomings in current benchmarking practices that hinder fair and reliable model comparison. (1) Existing benchmarks fail to model real-world dynamics, overlooking critical factors such as dynamic object configurations, robot initial states, lighting changes, and sensor noise. (2) Current protocols neglect spatial--physical intelligence, reducing evaluation to rote manipulation tasks that do not probe geometric reasoning. (3) The field lacks scalable fully autonomous evaluation, instead relying on simplistic 2D metrics that miss 3D spatial structure or on human-in-the-loop systems that are costly, biased, and unscalable. To address these limitations, we introduce RADAR (Real-world Autonomous Dynamics And Reasoning), a benchmark designed to systematically evaluate VLA generalization under realistic conditions. RADAR integrates three core components: (1) a principled suite of physical dynamics; (2) dedicated tasks that explicitly test spatial reasoning and physical understanding; and (3) a fully autonomous evaluation pipeline based on 3D metrics, eliminating the need for human supervision. We apply RADAR to audit multiple state-of-the-art VLA models and uncover severe fragility beneath their apparent competence. Performance drops precipitously under modest physical dynamics, with the expectation of 3D IoU declining from 0.261 to 0.068 under sensor noise. Moreover, models exhibit limited spatial reasoning capability. These findings position RADAR as a necessary bench toward reliable and generalizable real-world evaluation of VLA models.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

VLAæ¨¡å‹åœ¨å…·èº«æ™ºèƒ½æ–¹é¢å–å¾—äº†æ˜¾ç€çš„è¿›æ­¥ï¼›ç„¶è€Œï¼Œä»–ä»¬çš„è¯„ä¼°ä»ç„¶ä¸»è¦å±€é™äºæ¨¡æ‹Ÿæˆ–é«˜åº¦å—é™çš„ç°å®ç¯å¢ƒã€‚è¿™ç§ä¸åŒ¹é…é€ æˆäº†å·¨å¤§çš„ç°å®å·®è·ï¼Œå¼ºå¤§çš„åŸºå‡†æ€§èƒ½å¾€å¾€æ©ç›–äº†ä¸åŒç‰©ç†ç¯å¢ƒä¸­è¾ƒå·®çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬å‘ç°å½“å‰åŸºå‡†æµ‹è¯•å®è·µä¸­çš„ä¸‰ä¸ªç³»ç»Ÿæ€§ç¼ºé™·é˜»ç¢äº†å…¬å¹³å’Œå¯é çš„æ¨¡å‹æ¯”è¾ƒã€‚ (1) ç°æœ‰åŸºå‡†æ— æ³•å¯¹ç°å®ä¸–ç•Œçš„åŠ¨æ€è¿›è¡Œå»ºæ¨¡ï¼Œå¿½ç•¥äº†åŠ¨æ€å¯¹è±¡é…ç½®ã€æœºå™¨äººåˆå§‹çŠ¶æ€ã€ç…§æ˜å˜åŒ–å’Œä¼ æ„Ÿå™¨å™ªå£°ç­‰å…³é”®å› ç´ ã€‚ ï¼ˆ2ï¼‰å½“å‰åè®®å¿½è§†äº†ç©ºé—´ç‰©ç†æ™ºèƒ½ï¼Œå‡å°‘äº†å¯¹ä¸æ¢ç©¶å‡ ä½•æ¨ç†çš„æ­»è®°ç¡¬èƒŒæ“ä½œä»»åŠ¡çš„è¯„ä¼°ã€‚ (3) è¯¥é¢†åŸŸç¼ºä¹å¯æ‰©å±•çš„å®Œå…¨è‡ªä¸»è¯„ä¼°ï¼Œè€Œæ˜¯ä¾èµ–äºå¿½ç•¥ 3D ç©ºé—´ç»“æ„çš„ç®€å• 2D æŒ‡æ ‡ï¼Œæˆ–è€…ä¾èµ–äºæˆæœ¬é«˜æ˜‚ã€å­˜åœ¨åè§ä¸”ä¸å¯æ‰©å±•çš„äººæœºäº¤äº’ç³»ç»Ÿã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº† RADARï¼ˆçœŸå®ä¸–ç•Œè‡ªä¸»åŠ¨åŠ›å­¦å’Œæ¨ç†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°ç°å®æ¡ä»¶ä¸‹ VLA æ³›åŒ–èƒ½åŠ›çš„åŸºå‡†ã€‚ RADAR é›†æˆäº†ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š(1) ä¸€å¥—åŸåˆ™æ€§çš„ç‰©ç†åŠ¨åŠ›å­¦ï¼› (2) æ˜ç¡®æµ‹è¯•ç©ºé—´æ¨ç†å’Œç‰©ç†ç†è§£çš„ä¸“é—¨ä»»åŠ¡ï¼› (3) åŸºäº 3D æŒ‡æ ‡çš„å®Œå…¨è‡ªä¸»çš„è¯„ä¼°æµç¨‹ï¼Œæ— éœ€äººå·¥ç›‘ç£ã€‚æˆ‘ä»¬åº”ç”¨ RADAR æ¥å®¡æ ¸å¤šä¸ªæœ€å…ˆè¿›çš„ VLA æ¨¡å‹ï¼Œå¹¶å‘ç°å…¶è¡¨é¢èƒ½åŠ›ä¹‹ä¸‹çš„ä¸¥é‡è„†å¼±æ€§ã€‚åœ¨é€‚åº¦çš„ç‰©ç†åŠ¨åŠ›å­¦æ¡ä»¶ä¸‹ï¼Œæ€§èƒ½æ€¥å‰§ä¸‹é™ï¼Œåœ¨ä¼ æ„Ÿå™¨å™ªå£°ä¸‹ï¼Œ3D IoU çš„é¢„æœŸä» 0.261 ä¸‹é™åˆ° 0.068ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¡¨ç°å‡ºæœ‰é™çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚è¿™äº›å‘ç°ä½¿ RADAR æˆä¸ºå¯¹ VLA æ¨¡å‹è¿›è¡Œå¯é ä¸”å¯æ¨å¹¿çš„ç°å®ä¸–ç•Œè¯„ä¼°çš„å¿…è¦åŸºç¡€ã€‚

</details>

---

## 64. Lie Group Variational Integrator for the Geometrically Exact Rod with Circular Cross-Section Incorporating Cross-Sectional Deformation

**ä¸­æ–‡æ ‡é¢˜**: ç»“åˆæˆªé¢å˜å½¢çš„åœ†å½¢æˆªé¢å‡ ä½•ç²¾ç¡®æ†çš„æç¾¤å˜åˆ†ç§¯åˆ†å™¨

**Date**: 2026-02-11 | **arXiv**: [2602.10963v1](http://arxiv.org/abs/2602.10963v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10963v1)

<details><summary><b>Abstract</b></summary>

In this paper, we derive the continuous space-time equations of motion of a three-dimensional geometrically exact rod, or the Cosserat rod, incorporating planar cross-sectional deformation. We then adopt the Lie group variational integrator technique to obtain a discrete model of the rod incorporating both rotational motion and cross-sectional deformation as well. The resulting discrete model possesses several desirable features: it ensures volume conservation of the discrete elements by considering cross-sectional deformation through a local dilatation factor, it demonstrates the beneficial properties associated with the variational integrator technique, such as the preservation of the rotational configuration, and energy conservation with a bounded error. An exhaustive set of numerical results under various initial conditions of the rod demonstrates the efficacy of the model in replicating the physics of the system.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¨å¯¼äº†ä¸‰ç»´å‡ ä½•ç²¾ç¡®æ†ï¼ˆæˆ– Cosserat æ†ï¼‰çš„è¿ç»­æ—¶ç©ºè¿åŠ¨æ–¹ç¨‹ï¼Œå…¶ä¸­åŒ…å«å¹³é¢æ¨ªæˆªé¢å˜å½¢ã€‚ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨æç¾¤å˜åˆ†ç§¯åˆ†å™¨æŠ€æœ¯æ¥è·å¾—åŒæ—¶åŒ…å«æ—‹è½¬è¿åŠ¨å’Œæ¨ªæˆªé¢å˜å½¢çš„æ†çš„ç¦»æ•£æ¨¡å‹ã€‚ç”±æ­¤äº§ç”Ÿçš„ç¦»æ•£æ¨¡å‹å…·æœ‰å‡ ä¸ªç†æƒ³çš„ç‰¹å¾ï¼šå®ƒé€šè¿‡å±€éƒ¨è†¨èƒ€å› å­è€ƒè™‘æ¨ªæˆªé¢å˜å½¢æ¥ç¡®ä¿ç¦»æ•£å•å…ƒçš„ä½“ç§¯å®ˆæ’ï¼Œå®ƒå±•ç¤ºäº†ä¸å˜åˆ†ç§¯åˆ†å™¨æŠ€æœ¯ç›¸å…³çš„æœ‰ç›Šç‰¹æ€§ï¼Œä¾‹å¦‚æ—‹è½¬é…ç½®çš„ä¿å­˜å’Œæœ‰ç•Œè¯¯å·®çš„èƒ½é‡å®ˆæ’ã€‚åœ¨æ†çš„å„ç§åˆå§‹æ¡ä»¶ä¸‹çš„ä¸€ç»„è¯¦å°½çš„æ•°å€¼ç»“æœè¯æ˜äº†è¯¥æ¨¡å‹åœ¨å¤åˆ¶ç³»ç»Ÿç‰©ç†ç‰¹æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

</details>

---

## 65. Developing Neural Network-Based Gaze Control Systems for Social Robots

**ä¸­æ–‡æ ‡é¢˜**: ä¸ºç¤¾äº¤æœºå™¨äººå¼€å‘åŸºäºç¥ç»ç½‘ç»œçš„æ³¨è§†æ§åˆ¶ç³»ç»Ÿ

**Date**: 2026-02-11 | **arXiv**: [2602.10946v1](http://arxiv.org/abs/2602.10946v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10946v1)

<details><summary><b>Abstract</b></summary>

During multi-party interactions, gaze direction is a key indicator of interest and intent, making it essential for social robots to direct their attention appropriately. Understanding the social context is crucial for robots to engage effectively, predict human intentions, and navigate interactions smoothly. This study aims to develop an empirical motion-time pattern for human gaze behavior in various social situations (e.g., entering, leaving, waving, talking, and pointing) using deep neural networks based on participants' data. We created two video clips-one for a computer screen and another for a virtual reality headset-depicting different social scenarios. Data were collected from 30 participants: 15 using an eye-tracker and 15 using an Oculus Quest 1 headset. Deep learning models, specifically Long Short-Term Memory (LSTM) and Transformers, were used to analyze and predict gaze patterns. Our models achieved 60% accuracy in predicting gaze direction in a 2D animation and 65% accuracy in a 3D animation. Then, the best model was implemented onto the Nao robot; and 36 new participants evaluated its performance. The feedback indicated overall satisfaction, with those experienced in robotics rating the models more favorably.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åœ¨å¤šæ–¹äº’åŠ¨è¿‡ç¨‹ä¸­ï¼Œæ³¨è§†æ–¹å‘æ˜¯å…´è¶£å’Œæ„å›¾çš„å…³é”®æŒ‡æ ‡ï¼Œå› æ­¤ç¤¾äº¤æœºå™¨äººå¿…é¡»é€‚å½“åœ°å¼•å¯¼ä»–ä»¬çš„æ³¨æ„åŠ›ã€‚äº†è§£ç¤¾ä¼šèƒŒæ™¯å¯¹äºæœºå™¨äººæœ‰æ•ˆå‚ä¸ã€é¢„æµ‹äººç±»æ„å›¾å’Œé¡ºåˆ©è¿›è¡Œäº¤äº’è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æ—¨åœ¨ä½¿ç”¨åŸºäºå‚ä¸è€…æ•°æ®çš„æ·±åº¦ç¥ç»ç½‘ç»œï¼Œå¼€å‘å„ç§ç¤¾äº¤æƒ…å¢ƒï¼ˆä¾‹å¦‚è¿›å…¥ã€ç¦»å¼€ã€æŒ¥æ‰‹ã€è¯´è¯å’ŒæŒ‡å‘ï¼‰ä¸­äººç±»æ³¨è§†è¡Œä¸ºçš„ç»éªŒè¿åŠ¨æ—¶é—´æ¨¡å¼ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸¤ä¸ªè§†é¢‘å‰ªè¾‘ï¼ˆä¸€ä¸ªç”¨äºè®¡ç®—æœºå±å¹•ï¼Œå¦ä¸€ä¸ªç”¨äºè™šæ‹Ÿç°å®è€³æœºï¼‰ï¼Œæç»˜äº†ä¸åŒçš„ç¤¾äº¤åœºæ™¯ã€‚æ•°æ®æ”¶é›†è‡ª 30 åå‚ä¸è€…ï¼š15 åä½¿ç”¨çœ¼åŠ¨ä»ªï¼Œ15 åä½¿ç”¨ Oculus Quest 1 è€³æœºã€‚æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯é•¿çŸ­æœŸè®°å¿† (LSTM) å’Œ Transformerï¼Œç”¨äºåˆ†æå’Œé¢„æµ‹æ³¨è§†æ¨¡å¼ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨ 2D åŠ¨ç”»ä¸­é¢„æµ‹æ³¨è§†æ–¹å‘çš„å‡†ç¡®ç‡è¾¾åˆ° 60%ï¼Œåœ¨ 3D åŠ¨ç”»ä¸­é¢„æµ‹æ³¨è§†æ–¹å‘çš„å‡†ç¡®ç‡è¾¾åˆ° 65%ã€‚ç„¶åï¼Œå°†æœ€ä½³æ¨¡å‹åº”ç”¨åˆ°Naoæœºå™¨äººä¸Šï¼› 36 ä½æ–°å‚ä¸è€…è¯„ä¼°äº†å…¶è¡¨ç°ã€‚åé¦ˆè¡¨æ˜æ€»ä½“æ»¡æ„åº¦ï¼Œæœºå™¨äººé¢†åŸŸç»éªŒä¸°å¯Œçš„äººå¯¹æ¨¡å‹çš„è¯„ä»·æ›´é«˜ã€‚

</details>

---

## 66. Design, Development, and Use of Maya Robot as an Assistant for the Therapy/Education of Children with Cancer: a Pilot Study

**ä¸­æ–‡æ ‡é¢˜**: ç›é›…æœºå™¨äººçš„è®¾è®¡ã€å¼€å‘å’Œä½¿ç”¨ä½œä¸ºç™Œç—‡å„¿ç«¥æ²»ç–—/æ•™è‚²çš„åŠ©æ‰‹ï¼šä¸€é¡¹è¯•ç‚¹ç ”ç©¶

**Date**: 2026-02-11 | **arXiv**: [2602.10942v1](http://arxiv.org/abs/2602.10942v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10942v1)

<details><summary><b>Abstract</b></summary>

This study centers around the design and implementation of the Maya Robot, a portable elephant-shaped social robot, intended to engage with children undergoing cancer treatment. Initial efforts were devoted to enhancing the robot's facial expression recognition accuracy, achieving a 98% accuracy through deep neural networks. Two subsequent preliminary exploratory experiments were designed to advance the study's objectives. The first experiment aimed to compare pain levels experienced by children during the injection process, with and without the presence of the Maya robot. Twenty-five children, aged 4 to 9, undergoing cancer treatment participated in this counterbalanced study. The paired T-test results revealed a significant reduction in perceived pain when the robot was actively present in the injection room. The second experiment sought to assess perspectives of hospitalized children and their mothers during engagement with Maya through a game. Forty participants, including 20 children aged 4 to 9 and their mothers, were involved. Post Human-Maya Interactions, UTAUT questionnaire results indicated that children experienced significantly less anxiety than their parents during the interaction and game play. Notably, children exhibited higher trust levels in both the robot and the games, presenting a statistically significant difference in trust levels compared to their parents (P-value < 0.05). This preliminary exploratory study highlights the positive impact of utilizing Maya as an assistant for therapy/education in a clinical setting, particularly benefiting children undergoing cancer treatment. The findings underscore the potential of social robots in pediatric healthcare contexts, emphasizing improved pain management and emotional well-being among young patients.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è¿™é¡¹ç ”ç©¶çš„é‡ç‚¹æ˜¯ç›é›…æœºå™¨äººçš„è®¾è®¡å’Œå®ç°ï¼Œç›é›…æœºå™¨äººæ˜¯ä¸€ç§ä¾¿æºå¼è±¡å½¢ç¤¾äº¤æœºå™¨äººï¼Œæ—¨åœ¨å¸®åŠ©æ¥å—ç™Œç—‡æ²»ç–—çš„å„¿ç«¥ã€‚æœ€åˆçš„åŠªåŠ›è‡´åŠ›äºæé«˜æœºå™¨äººçš„é¢éƒ¨è¡¨æƒ…è¯†åˆ«å‡†ç¡®ç‡ï¼Œé€šè¿‡æ·±åº¦ç¥ç»ç½‘ç»œå®ç°äº† 98% çš„å‡†ç¡®ç‡ã€‚éšåè®¾è®¡äº†ä¸¤é¡¹åˆæ­¥æ¢ç´¢æ€§å®éªŒæ¥æ¨è¿›ç ”ç©¶ç›®æ ‡ã€‚ç¬¬ä¸€ä¸ªå®éªŒæ—¨åœ¨æ¯”è¾ƒå„¿ç«¥åœ¨æ³¨å°„è¿‡ç¨‹ä¸­æ‰€ç»å†çš„ç–¼ç—›ç¨‹åº¦ï¼Œæ— è®ºæ˜¯å¦æœ‰ç›é›…æœºå™¨äººåœ¨åœºã€‚ 25 å 4 è‡³ 9 å²æ­£åœ¨æ¥å—ç™Œç—‡æ²»ç–—çš„å„¿ç«¥å‚ä¸äº†è¿™é¡¹å¹³è¡¡ç ”ç©¶ã€‚é…å¯¹ T æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œå½“æœºå™¨äººä¸»åŠ¨å‡ºç°åœ¨æ³¨å°„å®¤æ—¶ï¼Œæ„ŸçŸ¥åˆ°çš„ç–¼ç—›æ˜¾ç€å‡å°‘ã€‚ç¬¬äºŒä¸ªå®éªŒè¯•å›¾è¯„ä¼°ä½é™¢å„¿ç«¥åŠå…¶æ¯äº²åœ¨é€šè¿‡æ¸¸æˆä¸ç›é›…äº’åŠ¨æ—¶çš„çœ‹æ³•ã€‚å…±æœ‰ 40 åå‚ä¸è€…å‚ä¸å…¶ä¸­ï¼Œå…¶ä¸­åŒ…æ‹¬ 20 å 4 è‡³ 9 å²çš„å„¿ç«¥åŠå…¶æ¯äº²ã€‚äººç±»ä¸ç›é›…äººäº’åŠ¨åï¼ŒUTAUT é—®å·ç»“æœè¡¨æ˜ï¼Œå­©å­ä»¬åœ¨äº’åŠ¨å’Œæ¸¸æˆè¿‡ç¨‹ä¸­æ‰€ç»å†çš„ç„¦è™‘æ˜æ˜¾ä½äºçˆ¶æ¯ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå­©å­ä»¬å¯¹æœºå™¨äººå’Œæ¸¸æˆéƒ½è¡¨ç°å‡ºè¾ƒé«˜çš„ä¿¡ä»»æ°´å¹³ï¼Œä¸çˆ¶æ¯ç›¸æ¯”ï¼Œä¿¡ä»»æ°´å¹³å­˜åœ¨ç»Ÿè®¡ä¸Šçš„æ˜¾ç€å·®å¼‚ï¼ˆP å€¼ < 0.05ï¼‰ã€‚è¿™é¡¹åˆæ­¥æ¢ç´¢æ€§ç ”ç©¶å¼ºè°ƒäº†åˆ©ç”¨ Maya ä½œä¸ºä¸´åºŠç¯å¢ƒä¸­æ²»ç–—/æ•™è‚²åŠ©æ‰‹çš„ç§¯æå½±å“ï¼Œç‰¹åˆ«æ˜¯æœ‰åˆ©äºæ¥å—ç™Œç—‡æ²»ç–—çš„å„¿ç«¥ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†ç¤¾äº¤æœºå™¨äººåœ¨å„¿ç§‘åŒ»ç–—ä¿å¥ç¯å¢ƒä¸­çš„æ½œåŠ›ï¼Œå¼ºè°ƒæ”¹å–„å¹´è½»æ‚£è€…çš„ç–¼ç—›ç®¡ç†å’Œæƒ…ç»ªå¥åº·ã€‚

</details>

---

## 67. Safe mobility support system using crowd mapping and avoidance route planning using VLM

**ä¸­æ–‡æ ‡é¢˜**: ä½¿ç”¨äººç¾¤æµ‹ç»˜çš„å®‰å…¨ç§»åŠ¨æ”¯æŒç³»ç»Ÿå’Œä½¿ç”¨ VLM çš„é¿è®©è·¯çº¿è§„åˆ’

**Date**: 2026-02-11 | **arXiv**: [2602.10910v1](http://arxiv.org/abs/2602.10910v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10910v1)

<details><summary><b>Abstract</b></summary>

Autonomous mobile robots offer promising solutions for labor shortages and increased operational efficiency. However, navigating safely and effectively in dynamic environments, particularly crowded areas, remains challenging. This paper proposes a novel framework that integrates Vision-Language Models (VLM) and Gaussian Process Regression (GPR) to generate dynamic crowd-density maps (``Abstraction Maps'') for autonomous robot navigation. Our approach utilizes VLM's capability to recognize abstract environmental concepts, such as crowd densities, and represents them probabilistically via GPR. Experimental results from real-world trials on a university campus demonstrated that robots successfully generated routes avoiding both static obstacles and dynamic crowds, enhancing navigation safety and adaptability.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è‡ªä¸»ç§»åŠ¨æœºå™¨äººä¸ºåŠ³åŠ¨åŠ›çŸ­ç¼ºå’Œæé«˜è¿è¥æ•ˆç‡æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œåœ¨åŠ¨æ€ç¯å¢ƒï¼ˆå°¤å…¶æ˜¯æ‹¥æŒ¤çš„åŒºåŸŸï¼‰ä¸­å®‰å…¨æœ‰æ•ˆåœ°å¯¼èˆªä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œé«˜æ–¯è¿‡ç¨‹å›å½’ï¼ˆGPRï¼‰æ¥ç”Ÿæˆç”¨äºè‡ªä¸»æœºå™¨äººå¯¼èˆªçš„åŠ¨æ€äººç¾¤å¯†åº¦å›¾ï¼ˆâ€œæŠ½è±¡å›¾â€ï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ VLM çš„èƒ½åŠ›æ¥è¯†åˆ«æŠ½è±¡çš„ç¯å¢ƒæ¦‚å¿µï¼Œä¾‹å¦‚äººç¾¤å¯†åº¦ï¼Œå¹¶é€šè¿‡æ¢åœ°é›·è¾¾ä»¥æ¦‚ç‡æ–¹å¼è¡¨ç¤ºå®ƒä»¬ã€‚åœ¨å¤§å­¦æ ¡å›­è¿›è¡Œçš„å®é™…è¯•éªŒç»“æœè¡¨æ˜ï¼Œæœºå™¨äººæˆåŠŸç”Ÿæˆäº†é¿å¼€é™æ€éšœç¢ç‰©å’ŒåŠ¨æ€äººç¾¤çš„è·¯çº¿ï¼Œå¢å¼ºäº†å¯¼èˆªå®‰å…¨æ€§å’Œé€‚åº”æ€§ã€‚

</details>

---

## 68. Biomimetic Mantaray robot toward the underwater autonomous -- Experimental verification of swimming and diving by flapping motion -

**ä¸­æ–‡æ ‡é¢˜**: ä»¿ç”ŸMantarayæœºå™¨äººèµ°å‘æ°´ä¸‹è‡ªä¸»â€”â€”æ‰‘åŠ¨æ¸¸æ³³å’Œæ½œæ°´çš„å®éªŒéªŒè¯â€”â€”

**Date**: 2026-02-11 | **arXiv**: [2602.10904v1](http://arxiv.org/abs/2602.10904v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10904v1)

<details><summary><b>Abstract</b></summary>

This study presents the development and experimental verification of a biomimetic manta ray robot for underwater autonomous exploration. Inspired by manta rays, the robot uses flapping motion for propulsion to minimize seabed disturbance and enhance efficiency compared to traditional screw propulsion. The robot features pectoral fins driven by servo motors and a streamlined control box to reduce fluid resistance. The control system, powered by a Raspberry Pi 3B, includes an IMU and pressure sensor for real-time monitoring and control. Experiments in a pool assessed the robot's swimming and diving capabilities. Results show stable swimming and diving motions with PD control. The robot is suitable for applications in environments like aquariums and fish nurseries, requiring minimal disturbance and efficient maneuverability. Our findings demonstrate the potential of bio-inspired robotic designs to improve ecological monitoring and underwater exploration.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æœ¬ç ”ç©¶ä»‹ç»äº†ç”¨äºæ°´ä¸‹è‡ªä¸»æ¢ç´¢çš„ä»¿ç”Ÿè é²¼æœºå™¨äººçš„å¼€å‘å’Œå®éªŒéªŒè¯ã€‚å—è é²¼çš„å¯å‘ï¼Œè¯¥æœºå™¨äººä½¿ç”¨æ‰‘åŠ¨è¿åŠ¨è¿›è¡Œæ¨è¿›ï¼Œä¸ä¼ ç»Ÿçš„èºæ—‹æ¨è¿›ç›¸æ¯”ï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘æµ·åº•æ‰°åŠ¨å¹¶æé«˜æ•ˆç‡ã€‚è¯¥æœºå™¨äººå…·æœ‰ç”±ä¼ºæœç”µæœºé©±åŠ¨çš„èƒ¸é³å’Œæµçº¿å‹æ§åˆ¶ç®±ï¼Œä»¥å‡å°‘æµä½“é˜»åŠ›ã€‚è¯¥æ§åˆ¶ç³»ç»Ÿç”± Raspberry Pi 3B ä¾›ç”µï¼ŒåŒ…æ‹¬ IMU å’Œå‹åŠ›ä¼ æ„Ÿå™¨ï¼Œç”¨äºå®æ—¶ç›‘æ§ã€‚åœ¨æ³³æ± ä¸­è¿›è¡Œçš„å®éªŒè¯„ä¼°äº†æœºå™¨äººçš„æ¸¸æ³³å’Œæ½œæ°´èƒ½åŠ›ã€‚ç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡ PD æ§åˆ¶å¯ä»¥å®ç°ç¨³å®šçš„æ¸¸æ³³å’Œæ½œæ°´åŠ¨ä½œã€‚è¯¥æœºå™¨äººé€‚ç”¨äºæ°´æ—é¦†å’Œå…»é±¼åœºç­‰ç¯å¢ƒä¸­çš„åº”ç”¨ï¼Œéœ€è¦æœ€å°çš„å¹²æ‰°å’Œé«˜æ•ˆçš„æœºåŠ¨æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¯æ˜äº†ä»¿ç”Ÿæœºå™¨äººè®¾è®¡åœ¨æ”¹å–„ç”Ÿæ€ç›‘æµ‹å’Œæ°´ä¸‹æ¢ç´¢æ–¹é¢çš„æ½œåŠ›ã€‚

</details>

---

## 69. Semi-Supervised Cross-Domain Imitation Learning

**ä¸­æ–‡æ ‡é¢˜**: åŠç›‘ç£è·¨é¢†åŸŸæ¨¡ä»¿å­¦ä¹ 

**Date**: 2026-02-11 | **arXiv**: [2602.10793v1](http://arxiv.org/abs/2602.10793v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10793v1)

**Code**: https://github.com/NYCU-RL-Bandits-Lab/CDIL.

<details><summary><b>Abstract</b></summary>

Cross-domain imitation learning (CDIL) accelerates policy learning by transferring expert knowledge across domains, which is valuable in applications where the collection of expert data is costly. Existing methods are either supervised, relying on proxy tasks and explicit alignment, or unsupervised, aligning distributions without paired data, but often unstable. We introduce the Semi-Supervised CDIL (SS-CDIL) setting and propose the first algorithm for SS-CDIL with theoretical justification. Our method uses only offline data, including a small number of target expert demonstrations and some unlabeled imperfect trajectories. To handle domain discrepancy, we propose a novel cross-domain loss function for learning inter-domain state-action mappings and design an adaptive weight function to balance the source and target knowledge. Experiments on MuJoCo and Robosuite show consistent gains over the baselines, demonstrating that our approach achieves stable and data-efficient policy learning with minimal supervision. Our code is available at~ https://github.com/NYCU-RL-Bandits-Lab/CDIL.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è·¨é¢†åŸŸæ¨¡ä»¿å­¦ä¹ ï¼ˆCDILï¼‰é€šè¿‡è·¨é¢†åŸŸè½¬ç§»ä¸“å®¶çŸ¥è¯†æ¥åŠ é€Ÿæ”¿ç­–å­¦ä¹ ï¼Œè¿™åœ¨æ”¶é›†ä¸“å®¶æ•°æ®æˆæœ¬é«˜æ˜‚çš„åº”ç”¨ç¨‹åºä¸­éå¸¸æœ‰ä»·å€¼ã€‚ç°æœ‰çš„æ–¹æ³•è¦ä¹ˆæ˜¯æœ‰ç›‘ç£çš„ï¼Œä¾èµ–äºä»£ç†ä»»åŠ¡å’Œæ˜¾å¼å¯¹é½ï¼Œè¦ä¹ˆæ˜¯æ— ç›‘ç£çš„ï¼Œåœ¨æ²¡æœ‰é…å¯¹æ•°æ®çš„æƒ…å†µä¸‹å¯¹é½åˆ†å¸ƒï¼Œä½†é€šå¸¸ä¸ç¨³å®šã€‚æˆ‘ä»¬å¼•å…¥äº†åŠç›‘ç£ CDIL (SS-CDIL) è®¾ç½®ï¼Œå¹¶æå‡ºäº†ç¬¬ä¸€ä¸ªå…·æœ‰ç†è®ºä¾æ®çš„ SS-CDIL ç®—æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»…ä½¿ç”¨ç¦»çº¿æ•°æ®ï¼ŒåŒ…æ‹¬å°‘é‡ç›®æ ‡ä¸“å®¶æ¼”ç¤ºå’Œä¸€äº›æœªæ ‡è®°çš„ä¸å®Œç¾è½¨è¿¹ã€‚ä¸ºäº†å¤„ç†åŸŸå·®å¼‚ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è·¨åŸŸæŸå¤±å‡½æ•°æ¥å­¦ä¹ åŸŸé—´çŠ¶æ€åŠ¨ä½œæ˜ å°„ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªè‡ªé€‚åº”æƒé‡å‡½æ•°æ¥å¹³è¡¡æºçŸ¥è¯†å’Œç›®æ ‡çŸ¥è¯†ã€‚ MuJoCo å’Œ Robosuite ä¸Šçš„å®éªŒæ˜¾ç¤ºå‡ºç›¸å¯¹äºåŸºçº¿çš„ä¸€è‡´å¢ç›Šï¼Œè¿™è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨æœ€å°‘çš„ç›‘ç£ä¸‹å®ç°ç¨³å®šä¸”æ•°æ®é«˜æ•ˆçš„æ”¿ç­–å­¦ä¹ ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ã€œhttps://github.com/NYCU-RL-Bandits-Lab/CDIL è·å–ã€‚

</details>

---

## 70. Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation

**ä¸­æ–‡æ ‡é¢˜**: è¯´ã€æ¢¦æƒ³å’Œè¡ŒåŠ¨ï¼šå­¦ä¹ æŒ‡ä»¤é©±åŠ¨æœºå™¨äººæ“ä½œçš„è§†é¢‘ä¸–ç•Œæ¨¡å‹

**Date**: 2026-02-11 | **arXiv**: [2602.10717v1](http://arxiv.org/abs/2602.10717v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10717v1)

<details><summary><b>Abstract</b></summary>

Robotic manipulation requires anticipating how the environment evolves in response to actions, yet most existing systems lack this predictive capability, often resulting in errors and inefficiency. While Vision-Language Models (VLMs) provide high-level guidance, they cannot explicitly forecast future states, and existing world models either predict only short horizons or produce spatially inconsistent frames. To address these challenges, we propose a framework for fast and predictive video-conditioned action. Our approach first selects and adapts a robust video generation model to ensure reliable future predictions, then applies adversarial distillation for fast, few-step video generation, and finally trains an action model that leverages both generated videos and real observations to correct spatial errors. Extensive experiments show that our method produces temporally coherent, spatially accurate video predictions that directly support precise manipulation, achieving significant improvements in embodiment consistency, spatial referring ability, and task completion over existing baselines. Codes & Models will be released.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æœºå™¨äººæ“çºµéœ€è¦é¢„æµ‹ç¯å¢ƒå¦‚ä½•å“åº”è¡ŒåŠ¨è€Œæ¼”å˜ï¼Œä½†å¤§å¤šæ•°ç°æœ‰ç³»ç»Ÿç¼ºä¹è¿™ç§é¢„æµ‹èƒ½åŠ›ï¼Œå¸¸å¸¸å¯¼è‡´é”™è¯¯å’Œä½æ•ˆç‡ã€‚è™½ç„¶è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æä¾›é«˜çº§æŒ‡å¯¼ï¼Œä½†å®ƒä»¬æ— æ³•æ˜ç¡®é¢„æµ‹æœªæ¥çŠ¶æ€ï¼Œå¹¶ä¸”ç°æœ‰çš„ä¸–ç•Œæ¨¡å‹è¦ä¹ˆä»…é¢„æµ‹çŸ­æœŸæƒ…å†µï¼Œè¦ä¹ˆäº§ç”Ÿç©ºé—´ä¸ä¸€è‡´çš„æ¡†æ¶ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¿«é€Ÿã€é¢„æµ‹æ€§è§†é¢‘æ¡ä»¶åŠ¨ä½œæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆé€‰æ‹©å¹¶è°ƒæ•´ä¸€ä¸ªå¼ºå¤§çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œä»¥ç¡®ä¿å¯é çš„æœªæ¥é¢„æµ‹ï¼Œç„¶ååº”ç”¨å¯¹æŠ—æ€§è’¸é¦æ¥å¿«é€Ÿã€å‡ æ­¥è§†é¢‘ç”Ÿæˆï¼Œæœ€åè®­ç»ƒä¸€ä¸ªåŠ¨ä½œæ¨¡å‹ï¼Œåˆ©ç”¨ç”Ÿæˆçš„è§†é¢‘å’ŒçœŸå®è§‚å¯Ÿæ¥çº æ­£ç©ºé—´é”™è¯¯ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•äº§ç”Ÿæ—¶é—´ä¸Šä¸€è‡´ã€ç©ºé—´ä¸Šå‡†ç¡®çš„è§†é¢‘é¢„æµ‹ï¼Œç›´æ¥æ”¯æŒç²¾ç¡®æ“ä½œï¼Œåœ¨ç°æœ‰åŸºçº¿çš„åŸºç¡€ä¸Šå®ç°äº†å®æ–½ä¾‹ä¸€è‡´æ€§ã€ç©ºé—´å‚è€ƒèƒ½åŠ›å’Œä»»åŠ¡å®Œæˆåº¦çš„æ˜¾ç€æ”¹è¿›ã€‚ä»£ç å’Œå‹å·å°†è¢«å‘å¸ƒã€‚

</details>

---

## 71. Omnidirectional Dual-Arm Aerial Manipulator with Proprioceptive Contact Localization for Landing on Slanted Roofs

**ä¸­æ–‡æ ‡é¢˜**: å…·æœ‰æœ¬ä½“æ„Ÿè§‰æ¥è§¦å®šä½åŠŸèƒ½çš„å…¨å‘åŒè‡‚ç©ºä¸­æœºæ¢°æ‰‹ï¼Œç”¨äºåœ¨å€¾æ–œå±‹é¡¶ä¸Šç€é™†

**Date**: 2026-02-11 | **arXiv**: [2602.10703v1](http://arxiv.org/abs/2602.10703v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10703v1)

<details><summary><b>Abstract</b></summary>

Operating drones in urban environments often means they need to land on rooftops, which can have different geometries and surface irregularities. Accurately detecting roof inclination using conventional sensing methods, such as vision-based or acoustic techniques, can be unreliable, as measurement quality is strongly influenced by external factors including weather conditions and surface materials. To overcome these challenges, we propose a novel unmanned aerial manipulator morphology featuring a dual-arm aerial manipulator with an omnidirectional 3D workspace and extended reach. Building on this design, we develop a proprioceptive contact detection and contact localization strategy based on a momentum-based torque observer. This enables the UAM to infer the inclination of slanted surfaces blindly - through physical interaction - prior to touchdown. We validate the approach in flight experiments, demonstrating robust landings on surfaces with inclinations of up to 30.5 degrees and achieving an average surface inclination estimation error of 2.87 degrees over 9 experiments at different incline angles.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åœ¨åŸå¸‚ç¯å¢ƒä¸­æ“ä½œæ— äººæœºé€šå¸¸æ„å‘³ç€å®ƒä»¬éœ€è¦é™è½åœ¨å±‹é¡¶ä¸Šï¼Œå±‹é¡¶å¯èƒ½å…·æœ‰ä¸åŒçš„å‡ ä½•å½¢çŠ¶å’Œè¡¨é¢ä¸è§„åˆ™æ€§ã€‚ä½¿ç”¨åŸºäºè§†è§‰æˆ–å£°å­¦æŠ€æœ¯ç­‰ä¼ ç»Ÿä¼ æ„Ÿæ–¹æ³•å‡†ç¡®æ£€æµ‹å±‹é¡¶å€¾æ–œåº¦å¯èƒ½ä¸å¯é ï¼Œå› ä¸ºæµ‹é‡è´¨é‡å—åˆ°å¤©æ°”æ¡ä»¶å’Œè¡¨é¢ææ–™ç­‰å¤–éƒ¨å› ç´ çš„å¼ºçƒˆå½±å“ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ— äººæœºå½¢æ€ï¼Œå…¶ç‰¹ç‚¹æ˜¯å…·æœ‰å…¨å‘ 3D å·¥ä½œç©ºé—´å’Œæ‰©å±•èŒƒå›´çš„åŒè‡‚ç©ºä¸­æ“çºµå™¨ã€‚åœ¨æ­¤è®¾è®¡çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºåŠ¨é‡æ‰­çŸ©è§‚æµ‹å™¨çš„æœ¬ä½“æ„Ÿå—æ¥è§¦æ£€æµ‹å’Œæ¥è§¦å®šä½ç­–ç•¥ã€‚è¿™ä½¿å¾— UAM èƒ½å¤Ÿåœ¨ç€é™†ä¹‹å‰é€šè¿‡ç‰©ç†äº¤äº’ç›²ç›®åœ°æ¨æ–­å€¾æ–œè¡¨é¢çš„å€¾æ–œåº¦ã€‚æˆ‘ä»¬åœ¨é£è¡Œå®éªŒä¸­éªŒè¯äº†è¯¥æ–¹æ³•ï¼Œå±•ç¤ºäº†åœ¨å€¾æ–œåº¦é«˜è¾¾ 30.5 åº¦çš„è¡¨é¢ä¸Šçš„ç¨³å¥ç€é™†ï¼Œå¹¶åœ¨ä¸åŒå€¾æ–œè§’åº¦çš„ 9 æ¬¡å®éªŒä¸­å®ç°äº† 2.87 åº¦çš„å¹³å‡è¡¨é¢å€¾æ–œåº¦ä¼°è®¡è¯¯å·®ã€‚

</details>

---

## 72. 3D-Printed Anisotropic Soft Magnetic Coating for Directional Rolling of a Magnetically Actuated Capsule Robot

**ä¸­æ–‡æ ‡é¢˜**: ç”¨äºç£é©±åŠ¨èƒ¶å›Šæœºå™¨äººå®šå‘æ»šåŠ¨çš„ 3D æ‰“å°å„å‘å¼‚æ€§è½¯ç£æ¶‚å±‚

**Date**: 2026-02-11 | **arXiv**: [2602.10688v1](http://arxiv.org/abs/2602.10688v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10688v1)

<details><summary><b>Abstract</b></summary>

Capsule robots are promising tools for minimally invasive diagnostics and therapy, with applications from gastrointestinal endoscopy to targeted drug delivery and biopsy sampling. Conventional magnetic capsule robots embed bulky permanent magnets at both ends, reducing the usable cavity by about 10-20 mm and limiting integration of functional modules. We propose a compact, 3D-printed soft capsule robot with a magnetic coating that replaces internal magnets, enabling locomotion via a thin, functional shell while preserving the entire interior cavity as a continuous volume for medical payloads. The compliant silicone-magnetic composite also improves swallowability, even with a slightly larger capsule size. Magnetostatic simulations and experiments confirm that programmed NSSN/SNNS pole distributions provide strong anisotropy and reliable torque generation, enabling stable bidirectional rolling, omnidirectional steering, climbing on 7.5 degree inclines, and traversal of 5 mm protrusions. Rolling motion is sustained when the magnetic field at the capsule reaches at least 0.3 mT, corresponding to an effective actuation depth of 30 mm in our setup. Future work will optimize material composition, coating thickness, and magnetic layouts to enhance force output and durability, while next-generation robotic-arm-based field generators with closed-loop feedback will address nonlinearities and expand maneuverability. Together, these advances aim to transition coating-based capsule robots toward reliable clinical deployment and broaden their applications in minimally invasive diagnostics and therapy.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

èƒ¶å›Šæœºå™¨äººæ˜¯å¾®åˆ›è¯Šæ–­å’Œæ²»ç–—çš„æœ‰å‰é€”çš„å·¥å…·ï¼Œå…¶åº”ç”¨èŒƒå›´ä»èƒƒè‚ å†…çª¥é•œæ£€æŸ¥åˆ°é¶å‘è¯ç‰©è¾“é€å’Œæ´»æ£€å–æ ·ã€‚ä¼ ç»Ÿçš„ç£åŠ›èƒ¶å›Šæœºå™¨äººåœ¨ä¸¤ç«¯åµŒå…¥äº†ç¬¨é‡çš„æ°¸ç£ä½“ï¼Œä½¿å¯ç”¨è…”å‡å°‘äº†çº¦10-20æ¯«ç±³ï¼Œå¹¶é™åˆ¶äº†åŠŸèƒ½æ¨¡å—çš„é›†æˆã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç´§å‡‘çš„ 3D æ‰“å°è½¯èƒ¶å›Šæœºå™¨äººï¼Œå…¶ç£æ€§æ¶‚å±‚å–ä»£äº†å†…éƒ¨ç£é“ï¼Œèƒ½å¤Ÿé€šè¿‡è–„è€ŒåŠŸèƒ½æ€§çš„å¤–å£³è¿›è¡Œè¿åŠ¨ï¼ŒåŒæ—¶ä¿ç•™æ•´ä¸ªå†…éƒ¨ç©ºè…”ä½œä¸ºåŒ»ç–—æœ‰æ•ˆè´Ÿè½½çš„è¿ç»­ç©ºé—´ã€‚å³ä½¿èƒ¶å›Šå°ºå¯¸ç¨å¤§ï¼Œé¡ºåº”æ€§çš„ç¡…é…®ç£æ€§å¤åˆææ–™ä¹Ÿèƒ½æé«˜åå’½æ€§ã€‚é™ç£ä»¿çœŸå’Œå®éªŒè¯å®ï¼Œç¼–ç¨‹çš„ NSSN/SNNS ç£æåˆ†å¸ƒå¯æä¾›å¼ºå¤§çš„å„å‘å¼‚æ€§å’Œå¯é çš„æ‰­çŸ©ç”Ÿæˆï¼Œä»è€Œå®ç°ç¨³å®šçš„åŒå‘æ»šåŠ¨ã€å…¨å‘è½¬å‘ã€åœ¨ 7.5 åº¦å€¾æ–œä¸Šæ”€çˆ¬ä»¥åŠç©¿è¶Š 5 æ¯«ç±³çªèµ·ã€‚å½“èƒ¶å›Šå¤„çš„ç£åœºè¾¾åˆ°è‡³å°‘ 0.3 mTï¼ˆç›¸å½“äºæˆ‘ä»¬è®¾ç½®ä¸­ 30 æ¯«ç±³çš„æœ‰æ•ˆé©±åŠ¨æ·±åº¦ï¼‰æ—¶ï¼Œæ»šåŠ¨è¿åŠ¨å°±ä¼šæŒç»­ã€‚æœªæ¥çš„å·¥ä½œå°†ä¼˜åŒ–ææ–™æˆåˆ†ã€æ¶‚å±‚åšåº¦å’Œç£æ€§å¸ƒå±€ï¼Œä»¥å¢å¼ºåŠ›è¾“å‡ºå’Œè€ç”¨æ€§ï¼Œè€Œå…·æœ‰é—­ç¯åé¦ˆçš„ä¸‹ä¸€ä»£åŸºäºæœºæ¢°è‡‚çš„åœºå‘ç”Ÿå™¨å°†è§£å†³éçº¿æ€§é—®é¢˜å¹¶æ‰©å¤§å¯æ“ä½œæ€§ã€‚æ€»ä¹‹ï¼Œè¿™äº›è¿›æ­¥æ—¨åœ¨å°†åŸºäºæ¶‚å±‚çš„èƒ¶å›Šæœºå™¨äººè½¬å‘å¯é çš„ä¸´åºŠéƒ¨ç½²ï¼Œå¹¶æ‰©å¤§å…¶åœ¨å¾®åˆ›è¯Šæ–­å’Œæ²»ç–—ä¸­çš„åº”ç”¨ã€‚

</details>

---

## 73. Assessing Vision-Language Models for Perception in Autonomous Underwater Robotic Software

**ä¸­æ–‡æ ‡é¢˜**: è¯„ä¼°è‡ªä¸»æ°´ä¸‹æœºå™¨äººè½¯ä»¶ä¸­æ„ŸçŸ¥çš„è§†è§‰è¯­è¨€æ¨¡å‹

**Date**: 2026-02-11 | **arXiv**: [2602.10655v1](http://arxiv.org/abs/2602.10655v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10655v1)

<details><summary><b>Abstract</b></summary>

Autonomous Underwater Robots (AURs) operate in challenging underwater environments, including low visibility and harsh water conditions. Such conditions present challenges for software engineers developing perception modules for the AUR software. To successfully carry out these tasks, deep learning has been incorporated into the AUR software to support its operations. However, the unique challenges of underwater environments pose difficulties for deep learning models, which often rely on labeled data that is scarce and noisy. This may undermine the trustworthiness of AUR software that relies on perception modules. Vision-Language Models (VLMs) offer promising solutions for AUR software as they generalize to unseen objects and remain robust in noisy conditions by inferring information from contextual cues. Despite this potential, their performance and uncertainty in underwater environments remain understudied from a software engineering perspective. Motivated by the needs of an industrial partner in assurance and risk management for maritime systems to assess the potential use of VLMs in this context, we present an empirical evaluation of VLM-based perception modules within the AUR software. We assess their ability to detect underwater trash by computing performance, uncertainty, and their relationship, to enable software engineers to select appropriate VLMs for their AUR software.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è‡ªä¸»æ°´ä¸‹æœºå™¨äºº (AUR) åœ¨å……æ»¡æŒ‘æˆ˜çš„æ°´ä¸‹ç¯å¢ƒä¸­è¿è¡Œï¼ŒåŒ…æ‹¬èƒ½è§åº¦ä½å’Œæ¶åŠ£çš„æ°´æ¡ä»¶ã€‚è¿™ç§æƒ…å†µç»™å¼€å‘ AUR è½¯ä»¶æ„ŸçŸ¥æ¨¡å—çš„è½¯ä»¶å·¥ç¨‹å¸ˆå¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¸ºäº†æˆåŠŸæ‰§è¡Œè¿™äº›ä»»åŠ¡ï¼Œæ·±åº¦å­¦ä¹ å·²è¢«çº³å…¥ AUR è½¯ä»¶ä¸­ä»¥æ”¯æŒå…¶æ“ä½œã€‚ç„¶è€Œï¼Œæ°´ä¸‹ç¯å¢ƒçš„ç‹¬ç‰¹æŒ‘æˆ˜ç»™æ·±åº¦å­¦ä¹ æ¨¡å‹å¸¦æ¥äº†å›°éš¾ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹é€šå¸¸ä¾èµ–äºç¨€ç¼ºä¸”å˜ˆæ‚çš„æ ‡è®°æ•°æ®ã€‚è¿™å¯èƒ½ä¼šç ´åä¾èµ–æ„ŸçŸ¥æ¨¡å—çš„ AUR è½¯ä»¶çš„å¯ä¿¡åº¦ã€‚è§†è§‰è¯­è¨€æ¨¡å‹ (VLM) ä¸º AUR è½¯ä»¶æä¾›äº†æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥æ³›åŒ–åˆ°çœ‹ä¸è§çš„å¯¹è±¡ï¼Œå¹¶é€šè¿‡ä»ä¸Šä¸‹æ–‡çº¿ç´¢æ¨æ–­ä¿¡æ¯åœ¨å˜ˆæ‚çš„æ¡ä»¶ä¸‹ä¿æŒé²æ£’æ€§ã€‚å°½ç®¡æœ‰è¿™ç§æ½œåŠ›ï¼Œä½†ä»è½¯ä»¶å·¥ç¨‹çš„è§’åº¦æ¥çœ‹ï¼Œå®ƒä»¬åœ¨æ°´ä¸‹ç¯å¢ƒä¸­çš„æ€§èƒ½å’Œä¸ç¡®å®šæ€§ä»ç„¶æ²¡æœ‰å¾—åˆ°å……åˆ†ç ”ç©¶ã€‚å‡ºäºæµ·äº‹ç³»ç»Ÿä¿éšœå’Œé£é™©ç®¡ç†æ–¹é¢çš„å·¥ä¸šåˆä½œä¼™ä¼´éœ€è¦è¯„ä¼° VLM åœ¨æ­¤èƒŒæ™¯ä¸‹çš„æ½œåœ¨ç”¨é€”çš„éœ€æ±‚ï¼Œæˆ‘ä»¬å¯¹ AUR è½¯ä»¶ä¸­åŸºäº VLM çš„æ„ŸçŸ¥æ¨¡å—è¿›è¡Œäº†å®è¯è¯„ä¼°ã€‚æˆ‘ä»¬é€šè¿‡è®¡ç®—æ€§èƒ½ã€ä¸ç¡®å®šæ€§åŠå…¶å…³ç³»æ¥è¯„ä¼°ä»–ä»¬æ£€æµ‹æ°´ä¸‹åƒåœ¾çš„èƒ½åŠ›ï¼Œä»¥ä½¿è½¯ä»¶å·¥ç¨‹å¸ˆèƒ½å¤Ÿä¸ºå…¶ AUR è½¯ä»¶é€‰æ‹©åˆé€‚çš„ VLMã€‚

</details>

---

## 74. Pitch Angle Control of a Magnetically Actuated Capsule Robot with Nonlinear FEA-based MPC and EKF Multisensory Fusion

**ä¸­æ–‡æ ‡é¢˜**: é‡‡ç”¨åŸºäºéçº¿æ€§ FEA çš„ MPC å’Œ EKF å¤šä¼ æ„Ÿå™¨èåˆçš„ç£é©±åŠ¨èƒ¶å›Šæœºå™¨äººçš„ä¿¯ä»°è§’æ§åˆ¶

**Date**: 2026-02-11 | **arXiv**: [2602.10610v1](http://arxiv.org/abs/2602.10610v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10610v1)

<details><summary><b>Abstract</b></summary>

Magnetically actuated capsule robots promise minimally invasive diagnosis and therapy in the gastrointestinal (GI) tract, but existing systems largely neglect control of capsule pitch, a degree of freedom critical for contact-rich interaction with inclined gastric walls. This paper presents a nonlinear, model-based framework for magnetic pitch control of an ingestible capsule robot actuated by a four-coil electromagnetic array. Angle-dependent magnetic forces and torques acting on embedded permanent magnets are characterized using three-dimensional finite-element simulations and embedded as lookup tables in a control-oriented rigid-body pitching model with rolling contact and actuator dynamics. A constrained model predictive controller (MPC) is designed to regulate pitch while respecting hardware-imposed current and slew-rate limits. Experiments on a compliant stomach-inspired surface demonstrate robust pitch reorientation from both horizontal and upright configurations, achieving about three to five times faster settling and reduced oscillatory motion than on-off control. Furthermore, an extended Kalman filter (EKF) fusing inertial sensing with intermittent visual measurements enables stable closed-loop control when the camera update rate is reduced from 30 Hz to 1 Hz, emulating clinically realistic imaging constraints. These results establish finite-element-informed MPC with sensor fusion as a scalable strategy for pitch regulation, controlled docking, and future multi-degree-of-freedom capsule locomotion.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç£é©±åŠ¨èƒ¶å›Šæœºå™¨äººæœ‰æœ›åœ¨èƒƒè‚ é“ä¸­è¿›è¡Œå¾®åˆ›è¯Šæ–­å’Œæ²»ç–—ï¼Œä½†ç°æœ‰ç³»ç»Ÿåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¿½è§†äº†èƒ¶å›Šé—´è·çš„æ§åˆ¶ï¼Œè€Œèƒ¶å›Šé—´è·å¯¹äºä¸å€¾æ–œèƒƒå£çš„ä¸°å¯Œæ¥è§¦ç›¸äº’ä½œç”¨è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¨¡å‹çš„éçº¿æ€§æ¡†æ¶ï¼Œç”¨äºç”±å››çº¿åœˆç”µç£é˜µåˆ—é©±åŠ¨çš„å¯æ‘„å–èƒ¶å›Šæœºå™¨äººçš„ç£æ€§ä¿¯ä»°æ§åˆ¶ã€‚ä½¿ç”¨ä¸‰ç»´æœ‰é™å…ƒæ¨¡æ‹Ÿæ¥è¡¨å¾ä½œç”¨åœ¨åµŒå…¥å¼æ°¸ç£ä½“ä¸Šçš„ä¸è§’åº¦ç›¸å…³çš„ç£åŠ›å’Œæ‰­çŸ©ï¼Œå¹¶å°†å…¶ä½œä¸ºæŸ¥æ‰¾è¡¨åµŒå…¥åˆ°å…·æœ‰æ»šåŠ¨æ¥è§¦å’Œæ‰§è¡Œå™¨åŠ¨åŠ›å­¦çš„é¢å‘æ§åˆ¶çš„åˆšä½“ä¿¯ä»°æ¨¡å‹ä¸­ã€‚çº¦æŸæ¨¡å‹é¢„æµ‹æ§åˆ¶å™¨ (MPC) æ—¨åœ¨è°ƒèŠ‚æ¡¨è·ï¼ŒåŒæ—¶éµå®ˆç¡¬ä»¶æ–½åŠ çš„ç”µæµå’Œè½¬æ¢é€Ÿç‡é™åˆ¶ã€‚åœ¨å—èƒƒå¯å‘çš„é¡ºåº”è¡¨é¢ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæ°´å¹³å’Œç›´ç«‹é…ç½®å‡å¯å®ç°ç¨³å¥çš„ä¿¯ä»°é‡æ–°å®šå‘ï¼Œä¸å¼€å…³æ§åˆ¶ç›¸æ¯”ï¼Œå¯å®ç°çº¦ä¸‰åˆ°äº”å€çš„æ²‰é™é€Ÿåº¦å’Œå‡å°‘çš„æŒ¯è¡è¿åŠ¨ã€‚æ­¤å¤–ï¼Œå½“ç›¸æœºæ›´æ–°ç‡ä» 30 Hz é™ä½åˆ° 1 Hz æ—¶ï¼Œå°†æƒ¯æ€§ä¼ æ„Ÿä¸é—´æ­‡è§†è§‰æµ‹é‡èåˆåœ¨ä¸€èµ·çš„æ‰©å±•å¡å°”æ›¼æ»¤æ³¢å™¨ (EKF) å¯ä»¥å®ç°ç¨³å®šçš„é—­ç¯æ§åˆ¶ï¼Œä»è€Œæ¨¡æ‹Ÿä¸´åºŠå®é™…æˆåƒçº¦æŸã€‚è¿™äº›ç»“æœå»ºç«‹äº†åŸºäºæœ‰é™å…ƒçš„ MPC ä¸ä¼ æ„Ÿå™¨èåˆï¼Œä½œä¸ºä¿¯ä»°è°ƒèŠ‚ã€å—æ§å¯¹æ¥å’Œæœªæ¥å¤šè‡ªç”±åº¦èƒ¶å›Šè¿åŠ¨çš„å¯æ‰©å±•ç­–ç•¥ã€‚

</details>

---

## 75. Flow-Enabled Generalization to Human Demonstrations in Few-Shot Imitation Learning

**ä¸­æ–‡æ ‡é¢˜**: åœ¨å°‘æ ·æœ¬æ¨¡ä»¿å­¦ä¹ ä¸­å¯¹äººç±»æ¼”ç¤ºçš„æµç¨‹å¯ç”¨æ³›åŒ–

**Date**: 2026-02-11 | **arXiv**: [2602.10594v1](http://arxiv.org/abs/2602.10594v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10594v1)

<details><summary><b>Abstract</b></summary>

Imitation Learning (IL) enables robots to learn complex skills from demonstrations without explicit task modeling, but it typically requires large amounts of demonstrations, creating significant collection costs. Prior work has investigated using flow as an intermediate representation to enable the use of human videos as a substitute, thereby reducing the amount of required robot demonstrations. However, most prior work has focused on the flow, either on the object or on specific points of the robot/hand, which cannot describe the motion of interaction. Meanwhile, relying on flow to achieve generalization to scenarios observed only in human videos remains limited, as flow alone cannot capture precise motion details. Furthermore, conditioning on scene observation to produce precise actions may cause the flow-conditioned policy to overfit to training tasks and weaken the generalization indicated by the flow. To address these gaps, we propose SFCrP, which includes a Scene Flow prediction model for Cross-embodiment learning (SFCr) and a Flow and Cropped point cloud conditioned Policy (FCrP). SFCr learns from both robot and human videos and predicts any point trajectories. FCrP follows the general flow motion and adjusts the action based on observations for precision tasks. Our method outperforms SOTA baselines across various real-world task settings, while also exhibiting strong spatial and instance generalization to scenarios seen only in human videos.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰ä½¿æœºå™¨äººèƒ½å¤Ÿä»æ¼”ç¤ºä¸­å­¦ä¹ å¤æ‚çš„æŠ€èƒ½ï¼Œè€Œæ— éœ€æ˜ç¡®çš„ä»»åŠ¡å»ºæ¨¡ï¼Œä½†å®ƒé€šå¸¸éœ€è¦å¤§é‡çš„æ¼”ç¤ºï¼Œä»è€Œäº§ç”Ÿå¤§é‡çš„æ”¶é›†æˆæœ¬ã€‚ä¹‹å‰çš„å·¥ä½œå·²ç»ç ”ç©¶äº†ä½¿ç”¨æµä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œä»¥å…è®¸ä½¿ç”¨äººç±»è§†é¢‘ä½œä¸ºæ›¿ä»£å“ï¼Œä»è€Œå‡å°‘æ‰€éœ€çš„æœºå™¨äººæ¼”ç¤ºçš„æ•°é‡ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å…ˆå‰çš„å·¥ä½œéƒ½é›†ä¸­åœ¨æµä¸Šï¼Œæ— è®ºæ˜¯åœ¨ç‰©ä½“ä¸Šè¿˜æ˜¯åœ¨æœºå™¨äºº/æ‰‹çš„ç‰¹å®šç‚¹ä¸Šï¼Œè¿™æ— æ³•æè¿°äº¤äº’çš„è¿åŠ¨ã€‚ä¸æ­¤åŒæ—¶ï¼Œä¾é æµæ¥å®ç°å¯¹ä»…åœ¨äººç±»è§†é¢‘ä¸­è§‚å¯Ÿåˆ°çš„åœºæ™¯çš„æ³›åŒ–ä»ç„¶æœ‰é™ï¼Œå› ä¸ºå•ç‹¬çš„æµæ— æ³•æ•è·ç²¾ç¡®çš„è¿åŠ¨ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œå¯¹åœºæ™¯è§‚å¯Ÿè¿›è¡Œè°ƒèŠ‚ä»¥äº§ç”Ÿç²¾ç¡®çš„åŠ¨ä½œå¯èƒ½ä¼šå¯¼è‡´æµè°ƒèŠ‚ç­–ç•¥è¿‡åº¦é€‚åº”è®­ç»ƒä»»åŠ¡å¹¶å‰Šå¼±æµæ‰€æŒ‡ç¤ºçš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº† SFCrPï¼Œå…¶ä¸­åŒ…æ‹¬ç”¨äºè·¨å®æ–½ä¾‹å­¦ä¹ çš„åœºæ™¯æµé¢„æµ‹æ¨¡å‹ï¼ˆSFCrï¼‰ä»¥åŠæµå’Œè£å‰ªç‚¹äº‘æ¡ä»¶ç­–ç•¥ï¼ˆFCrPï¼‰ã€‚ SFCr ä»æœºå™¨äººå’Œäººç±»è§†é¢‘ä¸­å­¦ä¹ å¹¶é¢„æµ‹ä»»ä½•ç‚¹è½¨è¿¹ã€‚ FCrP éµå¾ªä¸€èˆ¬æµåŠ¨è¿åŠ¨ï¼Œå¹¶æ ¹æ®ç²¾ç¡®ä»»åŠ¡çš„è§‚å¯Ÿæ¥è°ƒæ•´åŠ¨ä½œã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§ç°å®ä¸–ç•Œä»»åŠ¡è®¾ç½®ä¸­éƒ½ä¼˜äº SOTA åŸºçº¿ï¼ŒåŒæ—¶è¿˜å¯¹ä»…åœ¨äººç±»è§†é¢‘ä¸­çœ‹åˆ°çš„åœºæ™¯è¡¨ç°å‡ºå¼ºå¤§çš„ç©ºé—´å’Œå®ä¾‹æ³›åŒ–èƒ½åŠ›ã€‚

</details>

---

## 76. Morphogenetic Assembly and Adaptive Control for Heterogeneous Modular Robots

**ä¸­æ–‡æ ‡é¢˜**: å¼‚æ„æ¨¡å—åŒ–æœºå™¨äººçš„å½¢æ€ç»„è£…å’Œè‡ªé€‚åº”æ§åˆ¶

**Date**: 2026-02-11 | **arXiv**: [2602.10561v1](http://arxiv.org/abs/2602.10561v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10561v1)

<details><summary><b>Abstract</b></summary>

This paper presents a closed-loop automation framework for heterogeneous modular robots, covering the full pipeline from morphological construction to adaptive control. In this framework, a mobile manipulator handles heterogeneous functional modules including structural, joint, and wheeled modules to dynamically assemble diverse robot configurations and provide them with immediate locomotion capability. To address the state-space explosion in large-scale heterogeneous reconfiguration, we propose a hierarchical planner: the high-level planner uses a bidirectional heuristic search with type-penalty terms to generate module-handling sequences, while the low level planner employs A* search to compute optimal execution trajectories. This design effectively decouples discrete configuration planning from continuous motion execution. For adaptive motion generation of unknown assembled configurations, we introduce a GPU accelerated Annealing-Variance Model Predictive Path Integral (MPPI) controller. By incorporating a multi stage variance annealing strategy to balance global exploration and local convergence, the controller enables configuration-agnostic, real-time motion control. Large scale simulations show that the type-penalty term is critical for planning robustness in heterogeneous scenarios. Moreover, the greedy heuristic produces plans with lower physical execution costs than the Hungarian heuristic. The proposed annealing-variance MPPI significantly outperforms standard MPPI in both velocity tracking accuracy and control frequency, achieving real time control at 50 Hz. The framework validates the full-cycle process, including module assembly, robot merging and splitting, and dynamic motion generation.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æœ¬æ–‡æå‡ºäº†ä¸€ç§å¼‚æ„æ¨¡å—åŒ–æœºå™¨äººçš„é—­ç¯è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œæ¶µç›–äº†ä»å½¢æ€æ„å»ºåˆ°è‡ªé€‚åº”æ§åˆ¶çš„å®Œæ•´æµç¨‹ã€‚åœ¨æ­¤æ¡†æ¶ä¸­ï¼Œç§»åŠ¨æœºæ¢°æ‰‹å¤„ç†å¼‚æ„åŠŸèƒ½æ¨¡å—ï¼ŒåŒ…æ‹¬ç»“æ„ã€å…³èŠ‚å’Œè½®å¼æ¨¡å—ï¼Œä»¥åŠ¨æ€ç»„è£…ä¸åŒçš„æœºå™¨äººé…ç½®å¹¶ä¸ºå®ƒä»¬æä¾›å³æ—¶è¿åŠ¨èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³å¤§è§„æ¨¡å¼‚æ„é‡é…ç½®ä¸­çš„çŠ¶æ€ç©ºé—´çˆ†ç‚¸é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†å±‚è§„åˆ’å™¨ï¼šé«˜å±‚è§„åˆ’å™¨ä½¿ç”¨å¸¦æœ‰ç±»å‹æƒ©ç½šé¡¹çš„åŒå‘å¯å‘å¼æœç´¢æ¥ç”Ÿæˆæ¨¡å—å¤„ç†åºåˆ—ï¼Œè€Œä½å±‚è§„åˆ’å™¨åˆ™é‡‡ç”¨ A* æœç´¢æ¥è®¡ç®—æœ€ä½³æ‰§è¡Œè½¨è¿¹ã€‚è¯¥è®¾è®¡æœ‰æ•ˆåœ°å°†ç¦»æ•£é…ç½®è§„åˆ’ä¸è¿ç»­è¿åŠ¨æ‰§è¡Œåˆ†ç¦»ã€‚å¯¹äºæœªçŸ¥ç»„è£…é…ç½®çš„è‡ªé€‚åº”è¿åŠ¨ç”Ÿæˆï¼Œæˆ‘ä»¬å¼•å…¥äº† GPU åŠ é€Ÿçš„é€€ç«æ–¹å·®æ¨¡å‹é¢„æµ‹è·¯å¾„ç§¯åˆ† (MPPI) æ§åˆ¶å™¨ã€‚é€šè¿‡ç»“åˆå¤šé˜¶æ®µæ–¹å·®é€€ç«ç­–ç•¥æ¥å¹³è¡¡å…¨å±€æ¢ç´¢å’Œå±€éƒ¨æ”¶æ•›ï¼Œæ§åˆ¶å™¨å¯å®ç°ä¸é…ç½®æ— å…³çš„å®æ—¶è¿åŠ¨æ§åˆ¶ã€‚å¤§è§„æ¨¡æ¨¡æ‹Ÿè¡¨æ˜ï¼Œç±»å‹æƒ©ç½šé¡¹å¯¹äºå¼‚æ„åœºæ™¯ä¸­çš„è§„åˆ’é²æ£’æ€§è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œè´ªå©ªå¯å‘å¼äº§ç”Ÿçš„è®¡åˆ’çš„ç‰©ç†æ‰§è¡Œæˆæœ¬æ¯”åŒˆç‰™åˆ©å¯å‘å¼æ›´ä½ã€‚æ‰€æå‡ºçš„é€€ç«æ–¹å·® MPPI åœ¨é€Ÿåº¦è·Ÿè¸ªç²¾åº¦å’Œæ§åˆ¶é¢‘ç‡æ–¹é¢éƒ½æ˜¾ç€ä¼˜äºæ ‡å‡† MPPIï¼Œå®ç°äº† 50 Hz çš„å®æ—¶æ§åˆ¶ã€‚è¯¥æ¡†æ¶éªŒè¯äº†å…¨å‘¨æœŸè¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ¨¡å—ç»„è£…ã€æœºå™¨äººåˆå¹¶å’Œåˆ†è£‚ä»¥åŠåŠ¨æ€è¿åŠ¨ç”Ÿæˆã€‚

</details>

---

## 77. Towards Long-Lived Robots: Continual Learning VLA Models via Reinforcement Fine-Tuning

**ä¸­æ–‡æ ‡é¢˜**: è¿ˆå‘é•¿å¯¿æœºå™¨äººï¼šé€šè¿‡å¼ºåŒ–å¾®è°ƒæŒç»­å­¦ä¹  VLA æ¨¡å‹

**Date**: 2026-02-11 | **arXiv**: [2602.10503v1](http://arxiv.org/abs/2602.10503v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10503v1)

<details><summary><b>Abstract</b></summary>

Pretrained on large-scale and diverse datasets, VLA models demonstrate strong generalization and adaptability as general-purpose robotic policies. However, Supervised Fine-Tuning (SFT), which serves as the primary mechanism for adapting VLAs to downstream domains, requires substantial amounts of task-specific data and is prone to catastrophic forgetting. To address these limitations, we propose LifeLong-RFT, a simple yet effective Reinforcement Fine-Tuning (RFT) strategy for VLA models independent of online environmental feedback and pre-trained reward models. By integrating chunking-level on-policy reinforcement learning with the proposed Multi-Dimensional Process Reward (MDPR) mechanism, LifeLong-RFT quantifies the heterogeneous contributions of intermediate action chunks across three dimensions to facilitate policy optimization. Specifically, (1) the Quantized Action Consistency Reward (QACR) ensures accurate action prediction within the discrete action space; (2) the Continuous Trajectory Alignment Reward (CTAR) aligns decoded continuous action chunks with reference trajectories to ensure precise control; (3) the Format Compliance Reward (FCR) guarantees the structural validity of outputs. Comprehensive experiments across SimplerEnv, LIBERO, and real-world tasks demonstrate that LifeLong-RFT exhibits strong performance in multi-task learning. Furthermore, for continual learning on the LIBERO benchmark, our method achieves a 22% gain in average success rate over SFT, while effectively adapting to new tasks using only 20% of the training data. Overall, our method provides a promising post-training paradigm for VLAs.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

VLA æ¨¡å‹åœ¨å¤§è§„æ¨¡å’Œå¤šæ ·åŒ–çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œè¡¨ç°å‡ºä½œä¸ºé€šç”¨æœºå™¨äººç­–ç•¥çš„å¼ºå¤§æ³›åŒ–æ€§å’Œé€‚åº”æ€§ã€‚ç„¶è€Œï¼Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä½œä¸ºä½¿ VLA é€‚åº”ä¸‹æ¸¸åŸŸçš„ä¸»è¦æœºåˆ¶ï¼Œéœ€è¦å¤§é‡ç‰¹å®šäºä»»åŠ¡çš„æ•°æ®ï¼Œå¹¶ä¸”å®¹æ˜“å‘ç”Ÿç¾éš¾æ€§é—å¿˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº† LifeLong-RFTï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰ç­–ç•¥ï¼Œé€‚ç”¨äºç‹¬ç«‹äºåœ¨çº¿ç¯å¢ƒåé¦ˆå’Œé¢„è®­ç»ƒå¥–åŠ±æ¨¡å‹çš„ VLA æ¨¡å‹ã€‚é€šè¿‡å°†åˆ†å—çº§åˆ«çš„ç­–ç•¥å¼ºåŒ–å­¦ä¹ ä¸æ‰€æå‡ºçš„å¤šç»´è¿‡ç¨‹å¥–åŠ±ï¼ˆMDPRï¼‰æœºåˆ¶ç›¸ç»“åˆï¼ŒLifeLong-RFT é‡åŒ–äº†ä¸‰ä¸ªç»´åº¦ä¸Šçš„ä¸­é—´åŠ¨ä½œå—çš„å¼‚æ„è´¡çŒ®ï¼Œä»¥ä¿ƒè¿›ç­–ç•¥ä¼˜åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œï¼ˆ1ï¼‰é‡åŒ–åŠ¨ä½œä¸€è‡´æ€§å¥–åŠ±ï¼ˆQACRï¼‰ç¡®ä¿ç¦»æ•£åŠ¨ä½œç©ºé—´å†…å‡†ç¡®çš„åŠ¨ä½œé¢„æµ‹ï¼› ï¼ˆ2ï¼‰è¿ç»­è½¨è¿¹å¯¹é½å¥–åŠ±ï¼ˆCTARï¼‰å°†è§£ç çš„è¿ç»­åŠ¨ä½œå—ä¸å‚è€ƒè½¨è¿¹å¯¹é½ï¼Œä»¥ç¡®ä¿ç²¾ç¡®æ§åˆ¶ï¼› ï¼ˆ3ï¼‰æ ¼å¼åˆè§„å¥–åŠ±ï¼ˆFCRï¼‰ä¿è¯è¾“å‡ºçš„ç»“æ„æœ‰æ•ˆæ€§ã€‚è·¨ SimplerEnvã€LIBERO å’Œç°å®ä¸–ç•Œä»»åŠ¡çš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒLifeLong-RFT åœ¨å¤šä»»åŠ¡å­¦ä¹ ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå¯¹äº LIBERO åŸºå‡†çš„æŒç»­å­¦ä¹ ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯” SFT çš„å¹³å‡æˆåŠŸç‡æé«˜äº† 22%ï¼ŒåŒæ—¶ä»…ä½¿ç”¨ 20% çš„è®­ç»ƒæ•°æ®å°±èƒ½æœ‰æ•ˆåœ°é€‚åº”æ–°ä»»åŠ¡ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸º VLA æä¾›äº†ä¸€ä¸ªæœ‰å‰é€”çš„è®­ç»ƒåèŒƒä¾‹ã€‚

</details>

---

## 78. RadarEye: Robust Liquid Level Tracking Using mmWave Radar in Robotic Pouring

**ä¸­æ–‡æ ‡é¢˜**: RadarEyeï¼šåœ¨æœºå™¨äººæµ‡æ³¨ä¸­ä½¿ç”¨æ¯«ç±³æ³¢é›·è¾¾è¿›è¡Œç¨³å¥çš„æ¶²ä½è·Ÿè¸ª

**Date**: 2026-02-11 | **arXiv**: [2602.10417v1](http://arxiv.org/abs/2602.10417v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10417v1)

<details><summary><b>Abstract</b></summary>

Transparent liquid manipulation in robotic pouring remains challenging for perception systems: specular/refraction effects and lighting variability degrade visual cues, undermining reliable level estimation. To address this challenge, we introduce RadarEye, a real-time mmWave radar signal processing pipeline for robust liquid level estimation and tracking during the whole pouring process. RadarEye integrates (i) a high-resolution range-angle beamforming module for liquid level sensing and (ii) a physics-informed mid-pour tracker that suppresses multipath to maintain lock on the liquid surface despite stream-induced clutter and source container reflections. The pipeline delivers sub-millisecond latency. In real-robot water-pouring experiments, RadarEye achieves a 0.35 cm median absolute height error at 0.62 ms per update, substantially outperforming vision and ultrasound baselines.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æœºå™¨äººæµ‡æ³¨ä¸­çš„é€æ˜æ¶²ä½“æ“ä½œå¯¹äºæ„ŸçŸ¥ç³»ç»Ÿæ¥è¯´ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼šé•œé¢/æŠ˜å°„æ•ˆåº”å’Œç…§æ˜å˜åŒ–ä¼šé™ä½è§†è§‰çº¿ç´¢ï¼Œç ´åå¯é çš„æ¶²ä½ä¼°è®¡ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº† RadarEyeï¼Œè¿™æ˜¯ä¸€ç§å®æ—¶æ¯«ç±³æ³¢é›·è¾¾ä¿¡å·å¤„ç†ç®¡é“ï¼Œå¯åœ¨æ•´ä¸ªæµ‡æ³¨è¿‡ç¨‹ä¸­è¿›è¡Œå¯é çš„æ¶²ä½ä¼°è®¡å’Œè·Ÿè¸ªã€‚ RadarEye é›†æˆäº† (i) ç”¨äºæ¶²ä½ä¼ æ„Ÿçš„é«˜åˆ†è¾¨ç‡è·ç¦»è§’æ³¢æŸå½¢æˆæ¨¡å—å’Œ (ii) ç‰©ç†ä¿¡æ¯çš„ä¸­å€¾è·Ÿè¸ªå™¨ï¼Œè¯¥è·Ÿè¸ªå™¨å¯æŠ‘åˆ¶å¤šè·¯å¾„ï¼Œä»¥ä¿æŒå¯¹æ¶²é¢çš„é”å®šï¼Œå°½ç®¡å­˜åœ¨æµå¼•èµ·çš„æ‚æ³¢å’Œæºå®¹å™¨åå°„ã€‚è¯¥ç®¡é“æä¾›äºšæ¯«ç§’çº§å»¶è¿Ÿã€‚åœ¨çœŸå®çš„æœºå™¨äººå€’æ°´å®éªŒä¸­ï¼ŒRadarEye åœ¨æ¯æ¬¡æ›´æ–° 0.62 æ¯«ç§’æ—¶å®ç°äº† 0.35 å˜ç±³çš„ä¸­å€¼ç»å¯¹é«˜åº¦è¯¯å·®ï¼Œå¤§å¤§ä¼˜äºè§†è§‰å’Œè¶…å£°åŸºçº¿ã€‚

</details>

---

## 79. LocoVLM: Grounding Vision and Language for Adapting Versatile Legged Locomotion Policies

**ä¸­æ–‡æ ‡é¢˜**: LocoVLMï¼šé€‚åº”å¤šåŠŸèƒ½è…¿å¼è¿åŠ¨ç­–ç•¥çš„åŸºç¡€æ„¿æ™¯å’Œè¯­è¨€

**Date**: 2026-02-11 | **arXiv**: [2602.10399v1](http://arxiv.org/abs/2602.10399v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10399v1)

<details><summary><b>Abstract</b></summary>

Recent advances in legged locomotion learning are still dominated by the utilization of geometric representations of the environment, limiting the robot's capability to respond to higher-level semantics such as human instructions. To address this limitation, we propose a novel approach that integrates high-level commonsense reasoning from foundation models into the process of legged locomotion adaptation. Specifically, our method utilizes a pre-trained large language model to synthesize an instruction-grounded skill database tailored for legged robots. A pre-trained vision-language model is employed to extract high-level environmental semantics and ground them within the skill database, enabling real-time skill advisories for the robot. To facilitate versatile skill control, we train a style-conditioned policy capable of generating diverse and robust locomotion skills with high fidelity to specified styles. To the best of our knowledge, this is the first work to demonstrate real-time adaptation of legged locomotion using high-level reasoning from environmental semantics and instructions with instruction-following accuracy of up to 87% without the need for online query to on-the-cloud foundation models.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è…¿å¼è¿åŠ¨å­¦ä¹ çš„æœ€æ–°è¿›å±•ä»ç„¶ä»¥ç¯å¢ƒå‡ ä½•è¡¨ç¤ºçš„åˆ©ç”¨ä¸ºä¸»ï¼Œé™åˆ¶äº†æœºå™¨äººå“åº”æ›´é«˜çº§åˆ«è¯­ä¹‰ï¼ˆä¾‹å¦‚äººç±»æŒ‡ä»¤ï¼‰çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå°†åŸºç¡€æ¨¡å‹çš„é«˜çº§å¸¸è¯†æ¨ç†é›†æˆåˆ°è…¿å¼è¿åŠ¨é€‚åº”è¿‡ç¨‹ä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨é¢„å…ˆè®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥åˆæˆä¸ºè…¿å¼æœºå™¨äººé‡èº«å®šåˆ¶çš„åŸºäºæŒ‡ä»¤çš„æŠ€èƒ½æ•°æ®åº“ã€‚é‡‡ç”¨é¢„å…ˆè®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹æ¥æå–é«˜çº§ç¯å¢ƒè¯­ä¹‰å¹¶å°†å…¶åµŒå…¥æŠ€èƒ½æ•°æ®åº“ä¸­ï¼Œä»è€Œä¸ºæœºå™¨äººæä¾›å®æ—¶æŠ€èƒ½å»ºè®®ã€‚ä¸ºäº†ä¿ƒè¿›å¤šåŠŸèƒ½æŠ€èƒ½æ§åˆ¶ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ç§ä»¥é£æ ¼ä¸ºæ¡ä»¶çš„ç­–ç•¥ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–ä¸”å¼ºå¤§çš„è¿åŠ¨æŠ€èƒ½ï¼Œå¹¶ä¸”å¯¹æŒ‡å®šé£æ ¼å…·æœ‰é«˜ä¿çœŸåº¦ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåˆ©ç”¨ç¯å¢ƒè¯­ä¹‰å’ŒæŒ‡ä»¤è¿›è¡Œé«˜çº§æ¨ç†æ¥æ¼”ç¤ºè…¿å¼è¿åŠ¨å®æ—¶é€‚åº”çš„å·¥ä½œï¼ŒæŒ‡ä»¤è·Ÿè¸ªå‡†ç¡®åº¦é«˜è¾¾ 87%ï¼Œè€Œæ— éœ€åœ¨çº¿æŸ¥è¯¢äº‘åŸºç¡€æ¨¡å‹ã€‚

</details>

---

## 80. TVCACHE: A Stateful Tool-Value Cache for Post-Training LLM Agents

**ä¸­æ–‡æ ‡é¢˜**: TVCACHEï¼šç”¨äºè®­ç»ƒå LLM ä»£ç†çš„çŠ¶æ€å·¥å…·å€¼ç¼“å­˜

**Date**: 2026-02-11 | **arXiv**: [2602.10986v1](http://arxiv.org/abs/2602.10986v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10986v1)

<details><summary><b>Abstract</b></summary>

In RL post-training of LLM agents, calls to external tools take several seconds or even minutes, leaving allocated GPUs idle and inflating post-training time and cost. While many tool invocations repeat across parallel rollouts and could in principle be cached, naively caching their outputs for reuse is incorrect since tool outputs depend on the environment state induced by prior agent interactions. We present TVCACHE, a stateful tool-value cache for LLM agent post-training. TVCACHE maintains a tree of observed tool-call sequences and performs longest-prefix matching for cache lookups: a hit occurs only when the agent's full tool history matches a previously executed sequence, guaranteeing identical environment state. On three diverse workloads-terminal-based tasks, SQL generation, and video understanding. TVCACHE achieves cache hit rates of up to 70% and reduces median tool call execution time by up to 6.9X, with no degradation in post-training reward accumulation.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åœ¨ LLM ä»£ç†çš„å¼ºåŒ–å­¦ä¹ åæœŸè®­ç»ƒä¸­ï¼Œè°ƒç”¨å¤–éƒ¨å·¥å…·éœ€è¦å‡ ç§’é’Ÿç”šè‡³å‡ åˆ†é’Ÿçš„æ—¶é—´ï¼Œå¯¼è‡´åˆ†é…çš„ GPU é—²ç½®ï¼Œå¹¶å¢åŠ è®­ç»ƒåçš„æ—¶é—´å’Œæˆæœ¬ã€‚è™½ç„¶è®¸å¤šå·¥å…·è°ƒç”¨åœ¨å¹¶è¡Œéƒ¨ç½²ä¸­é‡å¤ï¼Œå¹¶ä¸”åŸåˆ™ä¸Šå¯ä»¥ç¼“å­˜ï¼Œä½†å¤©çœŸåœ°ç¼“å­˜å…¶è¾“å‡ºä»¥ä¾›é‡ç”¨æ˜¯ä¸æ­£ç¡®çš„ï¼Œå› ä¸ºå·¥å…·è¾“å‡ºå–å†³äºå…ˆå‰ä»£ç†äº¤äº’å¼•èµ·çš„ç¯å¢ƒçŠ¶æ€ã€‚æˆ‘ä»¬æ¨å‡ºäº† TVCACHEï¼Œè¿™æ˜¯ä¸€ç§ç”¨äº LLM ä»£ç†åæœŸè®­ç»ƒçš„æœ‰çŠ¶æ€å·¥å…·å€¼ç¼“å­˜ã€‚ TVCACHE ç»´æŠ¤è§‚å¯Ÿåˆ°çš„å·¥å…·è°ƒç”¨åºåˆ—æ ‘ï¼Œå¹¶æ‰§è¡Œç¼“å­˜æŸ¥æ‰¾çš„æœ€é•¿å‰ç¼€åŒ¹é…ï¼šä»…å½“ä»£ç†çš„å®Œæ•´å·¥å…·å†å²è®°å½•ä¸å…ˆå‰æ‰§è¡Œçš„åºåˆ—åŒ¹é…æ—¶æ‰ä¼šå‘ç”Ÿå‘½ä¸­ï¼Œä»è€Œä¿è¯ç›¸åŒçš„ç¯å¢ƒçŠ¶æ€ã€‚å…³äºä¸‰ç§ä¸åŒçš„å·¥ä½œè´Ÿè½½â€”â€”åŸºäºç»ˆç«¯çš„ä»»åŠ¡ã€SQL ç”Ÿæˆå’Œè§†é¢‘ç†è§£ã€‚ TVCACHE å®ç°äº†é«˜è¾¾ 70% çš„ç¼“å­˜å‘½ä¸­ç‡ï¼Œå¹¶å°†å·¥å…·è°ƒç”¨æ‰§è¡Œæ—¶é—´ä¸­ä½æ•°å‡å°‘äº† 6.9 å€ï¼Œå¹¶ä¸”è®­ç»ƒåå¥–åŠ±ç§¯ç´¯æ²¡æœ‰ä¸‹é™ã€‚

</details>

---

## 81. Stochastic Parroting in Temporal Attention -- Regulating the Diagonal Sink

**ä¸­æ–‡æ ‡é¢˜**: æ—¶é—´æ³¨æ„åŠ›ä¸­çš„éšæœºé¹¦é¹‰å­¦èˆŒâ€”â€”è°ƒèŠ‚å¯¹è§’ä¸‹æ²‰

**Date**: 2026-02-11 | **arXiv**: [2602.10956v1](http://arxiv.org/abs/2602.10956v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10956v1)

<details><summary><b>Abstract</b></summary>

Spatio-temporal models analyze spatial structures and temporal dynamics, which makes them prone to information degeneration among space and time. Prior literature has demonstrated that over-squashing in causal attention or temporal convolutions creates a bias on the first tokens. To analyze whether such a bias is present in temporal attention mechanisms, we derive sensitivity bounds on the expected value of the Jacobian of a temporal attention layer. We theoretically show how off-diagonal attention scores depend on the sequence length, and that temporal attention matrices suffer a diagonal attention sink. We suggest regularization methods, and experimentally demonstrate their effectiveness.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æ—¶ç©ºæ¨¡å‹åˆ†æç©ºé—´ç»“æ„å’Œæ—¶é—´åŠ¨æ€ï¼Œè¿™ä½¿å¾—å®ƒä»¬å®¹æ˜“å‡ºç°ç©ºé—´å’Œæ—¶é—´ä¹‹é—´çš„ä¿¡æ¯é€€åŒ–ã€‚å…ˆå‰çš„æ–‡çŒ®å·²ç»è¯æ˜ï¼Œå› æœæ³¨æ„åŠ›æˆ–æ—¶é—´å·ç§¯çš„è¿‡åº¦å‹ç¼©ä¼šå¯¹ç¬¬ä¸€ä¸ªæ ‡è®°äº§ç”Ÿåå·®ã€‚ä¸ºäº†åˆ†ææ—¶é—´æ³¨æ„æœºåˆ¶ä¸­æ˜¯å¦å­˜åœ¨è¿™ç§åå·®ï¼Œæˆ‘ä»¬æ¨å¯¼äº†æ—¶é—´æ³¨æ„å±‚çš„é›…å¯æ¯”è¡Œåˆ—å¼çš„æœŸæœ›å€¼çš„æ•æ„Ÿæ€§ç•Œé™ã€‚æˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜äº†éå¯¹è§’æ³¨æ„åŠ›åˆ†æ•°å¦‚ä½•å–å†³äºåºåˆ—é•¿åº¦ï¼Œå¹¶ä¸”æ—¶é—´æ³¨æ„åŠ›çŸ©é˜µé­å—å¯¹è§’æ³¨æ„åŠ›ä¸‹æ²‰ã€‚æˆ‘ä»¬æå‡ºæ­£åˆ™åŒ–æ–¹æ³•ï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜å…¶æœ‰æ•ˆæ€§ã€‚

</details>

---

## 82. Anomaly Detection with Machine Learning Algorithms in Large-Scale Power Grids

**ä¸­æ–‡æ ‡é¢˜**: å¤§è§„æ¨¡ç”µç½‘ä¸­æœºå™¨å­¦ä¹ ç®—æ³•çš„å¼‚å¸¸æ£€æµ‹

**Date**: 2026-02-11 | **arXiv**: [2602.10888v1](http://arxiv.org/abs/2602.10888v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10888v1)

<details><summary><b>Abstract</b></summary>

We apply several machine learning algorithms to the problem of anomaly detection in operational data for large-scale, high-voltage electric power grids. We observe important differences in the performance of the algorithms. Neural networks typically outperform classical algorithms such as k-nearest neighbors and support vector machines, which we explain by the strong contextual nature of the anomalies. We show that unsupervised learning algorithm work remarkably well and that their predictions are robust against simultaneous, concurring anomalies.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æˆ‘ä»¬å°†å¤šç§æœºå™¨å­¦ä¹ ç®—æ³•åº”ç”¨äºå¤§è§„æ¨¡é«˜å‹ç”µç½‘è¿è¡Œæ•°æ®çš„å¼‚å¸¸æ£€æµ‹é—®é¢˜ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ç®—æ³•æ€§èƒ½çš„é‡è¦å·®å¼‚ã€‚ç¥ç»ç½‘ç»œé€šå¸¸ä¼˜äºç»å…¸ç®—æ³•ï¼Œä¾‹å¦‚ k æœ€è¿‘é‚»å’Œæ”¯æŒå‘é‡æœºï¼Œæˆ‘ä»¬é€šè¿‡å¼‚å¸¸çš„å¼ºçƒˆä¸Šä¸‹æ–‡æ€§è´¨æ¥è§£é‡Šè¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬è¯æ˜æ— ç›‘ç£å­¦ä¹ ç®—æ³•å·¥ä½œå¾—éå¸¸å¥½ï¼Œå¹¶ä¸”å®ƒä»¬çš„é¢„æµ‹å¯¹äºåŒæ—¶å‘ç”Ÿçš„å¼‚å¸¸æƒ…å†µæ˜¯ç¨³å¥çš„ã€‚

</details>

---

## 83. SimuScene: Training and Benchmarking Code Generation to Simulate Physical Scenarios

**ä¸­æ–‡æ ‡é¢˜**: SimuSceneï¼šæ¨¡æ‹Ÿç‰©ç†åœºæ™¯çš„è®­ç»ƒå’ŒåŸºå‡†æµ‹è¯•ä»£ç ç”Ÿæˆ

**Date**: 2026-02-11 | **arXiv**: [2602.10840v1](http://arxiv.org/abs/2602.10840v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10840v1)

<details><summary><b>Abstract</b></summary>

Large language models (LLMs) have been extensively studied for tasks like math competitions, complex coding, and scientific reasoning, yet their ability to accurately represent and simulate physical scenarios via code remains underexplored. We propose SimuScene, the first systematic study that trains and evaluates LLMs on simulating physical scenarios across five physics domains and 52 physical concepts. We build an automatic pipeline to collect data, with human verification to ensure quality. The final dataset contains 7,659 physical scenarios with 334 human-verified examples as the test set. We evaluated 10 contemporary LLMs and found that even the strongest model achieves only a 21.5% pass rate, demonstrating the difficulty of the task. Finally, we introduce a reinforcement learning pipeline with visual rewards that uses a vision-language model as a judge to train textual models. Experiments show that training with our data improves physical simulation via code while substantially enhancing general code generation performance.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) å·²é’ˆå¯¹æ•°å­¦ç«èµ›ã€å¤æ‚ç¼–ç å’Œç§‘å­¦æ¨ç†ç­‰ä»»åŠ¡è¿›è¡Œäº†å¹¿æ³›ç ”ç©¶ï¼Œä½†å®ƒä»¬é€šè¿‡ä»£ç å‡†ç¡®è¡¨ç¤ºå’Œæ¨¡æ‹Ÿç‰©ç†åœºæ™¯çš„èƒ½åŠ›ä»æœªå¾—åˆ°å……åˆ†å¼€å‘ã€‚æˆ‘ä»¬æå‡ºäº† SimuSceneï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç³»ç»Ÿç ”ç©¶ï¼Œæ—¨åœ¨è®­ç»ƒå’Œè¯„ä¼°æ³•å­¦ç¡•å£«æ¨¡æ‹Ÿè·¨äº”ä¸ªç‰©ç†é¢†åŸŸå’Œ 52 ä¸ªç‰©ç†æ¦‚å¿µçš„ç‰©ç†åœºæ™¯ã€‚æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªè‡ªåŠ¨ç®¡é“æ¥æ”¶é›†æ•°æ®ï¼Œå¹¶é€šè¿‡äººå·¥éªŒè¯æ¥ç¡®ä¿è´¨é‡ã€‚æœ€ç»ˆæ•°æ®é›†åŒ…å« 7,659 ä¸ªç‰©ç†åœºæ™¯ï¼Œå…¶ä¸­æœ‰ 334 ä¸ªç»è¿‡äººå·¥éªŒè¯çš„ç¤ºä¾‹ä½œä¸ºæµ‹è¯•é›†ã€‚æˆ‘ä»¬è¯„ä¼°äº† 10 ä½å½“ä»£æ³•å­¦ç¡•å£«ï¼Œå‘ç°å³ä½¿æ˜¯æœ€å¼ºçš„æ¨¡å‹ä¹Ÿåªèƒ½è¾¾åˆ° 21.5% çš„é€šè¿‡ç‡ï¼Œå¯è§ä»»åŠ¡çš„éš¾åº¦ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†å¸¦æœ‰è§†è§‰å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ç®¡é“ï¼Œå®ƒä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ä½œä¸ºåˆ¤æ–­æ¥è®­ç»ƒæ–‡æœ¬æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®è¿›è¡Œè®­ç»ƒå¯ä»¥é€šè¿‡ä»£ç æ”¹è¿›ç‰©ç†æ¨¡æ‹Ÿï¼ŒåŒæ—¶æ˜¾ç€æé«˜ä¸€èˆ¬ä»£ç ç”Ÿæˆæ€§èƒ½ã€‚

</details>

---

## 84. Self-Supervised Learning for Speaker Recognition: A study and review

**ä¸­æ–‡æ ‡é¢˜**: è¯´è¯äººè¯†åˆ«çš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼šç ”ç©¶ä¸å›é¡¾

**Date**: 2026-02-11 | **arXiv**: [2602.10829v1](http://arxiv.org/abs/2602.10829v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10829v1)

<details><summary><b>Abstract</b></summary>

Deep learning models trained in a supervised setting have revolutionized audio and speech processing. However, their performance inherently depends on the quantity of human-annotated data, making them costly to scale and prone to poor generalization under unseen conditions. To address these challenges, Self-Supervised Learning (SSL) has emerged as a promising paradigm, leveraging vast amounts of unlabeled data to learn relevant representations. The application of SSL for Automatic Speech Recognition (ASR) has been extensively studied, but research on other downstream tasks, notably Speaker Recognition (SR), remains in its early stages. This work describes major SSL instance-invariance frameworks (e.g., SimCLR, MoCo, and DINO), initially developed for computer vision, along with their adaptation to SR. Various SSL methods for SR, proposed in the literature and built upon these frameworks, are also presented. An extensive review of these approaches is then conducted: (1) the effect of the main hyperparameters of SSL frameworks is investigated; (2) the role of SSL components is studied (e.g., data-augmentation, projector, positive sampling); and (3) SSL frameworks are evaluated on SR with in-domain and out-of-domain data, using a consistent experimental setup, and a comprehensive comparison of SSL methods from the literature is provided. Specifically, DINO achieves the best downstream performance and effectively models intra-speaker variability, although it is highly sensitive to hyperparameters and training conditions, while SimCLR and MoCo provide robust alternatives that effectively capture inter-speaker variability and are less prone to collapse. This work aims to highlight recent trends and advancements, identifying current challenges in the field.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åœ¨ç›‘ç£ç¯å¢ƒä¸­è®­ç»ƒçš„æ·±åº¦å­¦ä¹ æ¨¡å‹å½»åº•æ”¹å˜äº†éŸ³é¢‘å’Œè¯­éŸ³å¤„ç†ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ€§èƒ½æœ¬è´¨ä¸Šå–å†³äºäººå·¥æ³¨é‡Šæ•°æ®çš„æ•°é‡ï¼Œè¿™ä½¿å¾—å®ƒä»¬çš„æ‰©å±•æˆæœ¬é«˜æ˜‚ï¼Œå¹¶ä¸”åœ¨æœªè§çš„æ¡ä»¶ä¸‹å®¹æ˜“å‡ºç°æ³›åŒ–ä¸è‰¯çš„æƒ…å†µã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œè‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰å·²æˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„èŒƒä¾‹ï¼Œå®ƒåˆ©ç”¨å¤§é‡æœªæ ‡è®°çš„æ•°æ®æ¥å­¦ä¹ ç›¸å…³çš„è¡¨ç¤ºã€‚ SSL åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ« (ASR) ä¸­çš„åº”ç”¨å·²å¾—åˆ°å¹¿æ³›ç ”ç©¶ï¼Œä½†å¯¹å…¶ä»–ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå°¤å…¶æ˜¯è¯´è¯äººè¯†åˆ« (SR)ï¼‰çš„ç ”ç©¶ä»å¤„äºæ—©æœŸé˜¶æ®µã€‚è¿™é¡¹å·¥ä½œæè¿°äº†æœ€åˆä¸ºè®¡ç®—æœºè§†è§‰å¼€å‘çš„ä¸»è¦ SSL å®ä¾‹ä¸å˜æ€§æ¡†æ¶ï¼ˆä¾‹å¦‚ SimCLRã€MoCo å’Œ DINOï¼‰ï¼Œä»¥åŠå®ƒä»¬å¯¹ SR çš„é€‚åº”ã€‚è¿˜ä»‹ç»äº†æ–‡çŒ®ä¸­æå‡ºçš„å¹¶åŸºäºè¿™äº›æ¡†æ¶æ„å»ºçš„å„ç§ç”¨äº SR çš„ SSL æ–¹æ³•ã€‚ç„¶åå¯¹è¿™äº›æ–¹æ³•è¿›è¡Œäº†å¹¿æ³›çš„å®¡æŸ¥ï¼šï¼ˆ1ï¼‰ç ”ç©¶äº† SSL æ¡†æ¶ä¸»è¦è¶…å‚æ•°çš„å½±å“ï¼› (2) ç ”ç©¶ SSL ç»„ä»¶çš„ä½œç”¨ï¼ˆä¾‹å¦‚æ•°æ®å¢å¼ºã€æŠ•å½±ä»ªã€æ­£é‡‡æ ·ï¼‰ï¼› (3) ä½¿ç”¨ä¸€è‡´çš„å®éªŒè®¾ç½®ï¼Œä½¿ç”¨åŸŸå†…å’ŒåŸŸå¤–æ•°æ®åœ¨ SR ä¸Šè¯„ä¼° SSL æ¡†æ¶ï¼Œå¹¶å¯¹æ–‡çŒ®ä¸­çš„ SSL æ–¹æ³•è¿›è¡Œå…¨é¢æ¯”è¾ƒã€‚å…·ä½“æ¥è¯´ï¼Œå°½ç®¡ DINO å¯¹è¶…å‚æ•°å’Œè®­ç»ƒæ¡ä»¶é«˜åº¦æ•æ„Ÿï¼Œä½†å®ƒå®ç°äº†æœ€ä½³çš„ä¸‹æ¸¸æ€§èƒ½å¹¶æœ‰æ•ˆåœ°æ¨¡æ‹Ÿäº†è¯´è¯äººå†…éƒ¨çš„å˜å¼‚æ€§ï¼Œè€Œ SimCLR å’Œ MoCo æä¾›äº†å¼ºå¤§çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¯ä»¥æœ‰æ•ˆæ•è·è¯´è¯äººä¹‹é—´çš„å˜å¼‚æ€§å¹¶ä¸”ä¸æ˜“å´©æºƒã€‚è¿™é¡¹å·¥ä½œæ—¨åœ¨çªå‡ºæœ€æ–°è¶‹åŠ¿å’Œè¿›å±•ï¼Œç¡®å®šè¯¥é¢†åŸŸå½“å‰çš„æŒ‘æˆ˜ã€‚

</details>

---

## 85. RePO: Bridging On-Policy Learning and Off-Policy Knowledge through Rephrasing Policy Optimization

**ä¸­æ–‡æ ‡é¢˜**: RePOï¼šé€šè¿‡é‡æ–°è¡¨è¿°ç­–ç•¥ä¼˜åŒ–æ¥æ¡¥æ¥ç­–ç•¥å†…å­¦ä¹ å’Œç­–ç•¥å¤–çŸ¥è¯†

**Date**: 2026-02-11 | **arXiv**: [2602.10819v1](http://arxiv.org/abs/2602.10819v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10819v1)

<details><summary><b>Abstract</b></summary>

Aligning large language models (LLMs) on domain-specific data remains a fundamental challenge. Supervised fine-tuning (SFT) offers a straightforward way to inject domain knowledge but often degrades the model's generality. In contrast, on-policy reinforcement learning (RL) preserves generality but fails to effectively assimilate hard samples that exceed the model's current reasoning level. Recent off-policy RL attempts improve hard sample utilization, yet they suffer from severe training instability due to the forced distribution shift toward off-policy knowledge. To reconcile effective off-policy knowledge absorption with the stability of on-policy RL, we propose Rephrasing Policy Optimization (RePO). In RePO, the policy model is prompted to first comprehend off-policy knowledge and then rephrase it into trajectories that conform to its own stylistic and parametric distribution. RePO dynamically replaces low-reward rollouts with these rephrased, high-quality trajectories. This strategy guides the model toward correct reasoning paths while strictly preserving on-policy training dynamics. Experiments on several benchmarks demonstrate that RePO improves hard-sample utilization and outperforms existing baselines, achieving state-of-the-art performance.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å°†å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) ä¸ç‰¹å®šé¢†åŸŸçš„æ•°æ®ä¿æŒä¸€è‡´ä»ç„¶æ˜¯ä¸€é¡¹åŸºæœ¬æŒ‘æˆ˜ã€‚æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æä¾›äº†ä¸€ç§æ³¨å…¥é¢†åŸŸçŸ¥è¯†çš„ç›´æ¥æ–¹æ³•ï¼Œä½†é€šå¸¸ä¼šé™ä½æ¨¡å‹çš„é€šç”¨æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåœ¨ç­–ç•¥å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¿ç•™äº†é€šç”¨æ€§ï¼Œä½†æ— æ³•æœ‰æ•ˆåœ°åŒåŒ–è¶…å‡ºæ¨¡å‹å½“å‰æ¨ç†æ°´å¹³çš„ç¡¬æ ·æœ¬ã€‚æœ€è¿‘çš„ç¦»ç­–ç•¥å¼ºåŒ–å­¦ä¹ å°è¯•æé«˜äº†ç¡¬æ ·æœ¬åˆ©ç”¨ç‡ï¼Œä½†ç”±äºå¼ºåˆ¶åˆ†å¸ƒè½¬å‘ç¦»ç­–ç•¥çŸ¥è¯†ï¼Œå®ƒä»¬é­å—äº†ä¸¥é‡çš„è®­ç»ƒä¸ç¨³å®šé—®é¢˜ã€‚ä¸ºäº†åè°ƒæœ‰æ•ˆçš„ç¦»ç­–ç•¥çŸ¥è¯†å¸æ”¶ä¸åœ¨ç­–ç•¥å¼ºåŒ–å­¦ä¹ çš„ç¨³å®šæ€§ï¼Œæˆ‘ä»¬æå‡ºäº†æ”¹å†™ç­–ç•¥ä¼˜åŒ–ï¼ˆRePOï¼‰ã€‚åœ¨RePOä¸­ï¼Œç­–ç•¥æ¨¡å‹è¢«æç¤ºé¦–å…ˆç†è§£éç­–ç•¥çŸ¥è¯†ï¼Œç„¶åå°†å…¶é‡æ–°è¡¨è¿°ä¸ºç¬¦åˆå…¶è‡ªèº«é£æ ¼å’Œå‚æ•°åˆ†å¸ƒçš„è½¨è¿¹ã€‚ RePO ç”¨è¿™äº›é‡æ–°è¡¨è¿°çš„é«˜è´¨é‡è½¨è¿¹åŠ¨æ€åœ°å–ä»£äº†ä½å¥–åŠ±çš„éƒ¨ç½²ã€‚è¯¥ç­–ç•¥å¼•å¯¼æ¨¡å‹èµ°å‘æ­£ç¡®çš„æ¨ç†è·¯å¾„ï¼ŒåŒæ—¶ä¸¥æ ¼ä¿ç•™ç­–ç•¥è®­ç»ƒåŠ¨æ€ã€‚å¤šä¸ªåŸºå‡†æµ‹è¯•çš„å®éªŒè¡¨æ˜ï¼ŒRePO æé«˜äº†ç¡¬æ ·æœ¬åˆ©ç”¨ç‡å¹¶ä¼˜äºç°æœ‰åŸºå‡†ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

</details>

---

## 86. Deep Learning-based Method for Expressing Knowledge Boundary of Black-Box LLM

**ä¸­æ–‡æ ‡é¢˜**: åŸºäºæ·±åº¦å­¦ä¹ çš„Black-Box LLMçŸ¥è¯†è¾¹ç•Œè¡¨è¾¾æ–¹æ³•

**Date**: 2026-02-11 | **arXiv**: [2602.10801v1](http://arxiv.org/abs/2602.10801v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10801v1)

<details><summary><b>Abstract</b></summary>

Large Language Models (LLMs) have achieved remarkable success, however, the emergence of content generation distortion (hallucination) limits their practical applications. The core cause of hallucination lies in LLMs' lack of awareness regarding their stored internal knowledge, preventing them from expressing their knowledge state on questions beyond their internal knowledge boundaries, as humans do. However, existing research on knowledge boundary expression primarily focuses on white-box LLMs, leaving methods suitable for black-box LLMs which offer only API access without revealing internal parameters-largely unexplored. Against this backdrop, this paper proposes LSCL (LLM-Supervised Confidence Learning), a deep learning-based method for expressing the knowledge boundaries of black-box LLMs. Based on the knowledge distillation framework, this method designs a deep learning model. Taking the input question, output answer, and token probability from a black-box LLM as inputs, it constructs a mapping between the inputs and the model' internal knowledge state, enabling the quantification and expression of the black-box LLM' knowledge boundaries. Experiments conducted on diverse public datasets and with multiple prominent black-box LLMs demonstrate that LSCL effectively assists black-box LLMs in accurately expressing their knowledge boundaries. It significantly outperforms existing baseline models on metrics such as accuracy and recall rate. Furthermore, considering scenarios where some black-box LLMs do not support access to token probability, an adaptive alternative method is proposed. The performance of this alternative approach is close to that of LSCL and surpasses baseline models.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å–å¾—äº†æ˜¾ç€çš„æˆåŠŸï¼Œç„¶è€Œï¼Œå†…å®¹ç”Ÿæˆå¤±çœŸï¼ˆå¹»è§‰ï¼‰çš„å‡ºç°é™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚äº§ç”Ÿå¹»è§‰çš„æ ¸å¿ƒåŸå› åœ¨äºæ³•å­¦ç¡•å£«å¯¹è‡ªå·±å‚¨å­˜çš„å†…éƒ¨çŸ¥è¯†ç¼ºä¹è®¤è¯†ï¼Œå¯¼è‡´ä»–ä»¬æ— æ³•åƒäººç±»ä¸€æ ·åœ¨è¶…å‡ºå…¶å†…éƒ¨çŸ¥è¯†è¾¹ç•Œçš„é—®é¢˜ä¸Šè¡¨è¾¾è‡ªå·±çš„çŸ¥è¯†çŠ¶æ€ã€‚ç„¶è€Œï¼Œç°æœ‰çš„çŸ¥è¯†è¾¹ç•Œè¡¨è¾¾ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç™½ç›’ LLM ä¸Šï¼Œè€Œé€‚ç”¨äºé»‘ç›’ LLM çš„æ–¹æ³•ä»…æä¾› API è®¿é—®è€Œä¸é€éœ²å†…éƒ¨å‚æ•°â€”â€”å¾ˆå¤§ç¨‹åº¦ä¸Šå°šæœªæ¢ç´¢ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œæœ¬æ–‡æå‡ºäº†LSCLï¼ˆLLM-Supervised Confidence Learningï¼‰ï¼Œä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ï¼Œç”¨äºè¡¨è¾¾é»‘ç›’LLMçš„çŸ¥è¯†è¾¹ç•Œã€‚è¯¥æ–¹æ³•åŸºäºçŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œè®¾è®¡äº†æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚ä»¥é»‘ç›’LLMçš„è¾“å…¥é—®é¢˜ã€è¾“å‡ºç­”æ¡ˆå’Œtokenæ¦‚ç‡ä½œä¸ºè¾“å…¥ï¼Œæ„å»ºè¾“å…¥ä¸æ¨¡å‹å†…éƒ¨çŸ¥è¯†çŠ¶æ€ä¹‹é—´çš„æ˜ å°„ï¼Œå®ç°é»‘ç›’LLMçŸ¥è¯†è¾¹ç•Œçš„é‡åŒ–å’Œè¡¨è¾¾ã€‚åœ¨ä¸åŒçš„å…¬å…±æ•°æ®é›†å’Œå¤šä¸ªè‘—åçš„é»‘ç›’æ³•å­¦ç¡•å£«ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒLSCL æœ‰æ•ˆåœ°å¸®åŠ©é»‘ç›’æ³•å­¦ç¡•å£«å‡†ç¡®è¡¨è¾¾å…¶çŸ¥è¯†è¾¹ç•Œã€‚å®ƒåœ¨å‡†ç¡®æ€§å’Œå¬å›ç‡ç­‰æŒ‡æ ‡ä¸Šæ˜¾ç€ä¼˜äºç°æœ‰çš„åŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè€ƒè™‘åˆ°ä¸€äº›é»‘ç›’LLMä¸æ”¯æŒè®¿é—®ä»¤ç‰Œæ¦‚ç‡çš„åœºæ™¯ï¼Œæå‡ºäº†ä¸€ç§è‡ªé€‚åº”æ›¿ä»£æ–¹æ³•ã€‚è¿™ç§æ›¿ä»£æ–¹æ³•çš„æ€§èƒ½æ¥è¿‘ LSCL å¹¶è¶…è¿‡åŸºçº¿æ¨¡å‹ã€‚

</details>

---

## 87. SnapMLA: Efficient Long-Context MLA Decoding via Hardware-Aware FP8 Quantized Pipelining

**ä¸­æ–‡æ ‡é¢˜**: SnapMLAï¼šé€šè¿‡ç¡¬ä»¶æ„ŸçŸ¥ FP8 é‡åŒ–æµæ°´çº¿è¿›è¡Œé«˜æ•ˆçš„é•¿ä¸Šä¸‹æ–‡ MLA è§£ç 

**Date**: 2026-02-11 | **arXiv**: [2602.10718v1](http://arxiv.org/abs/2602.10718v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10718v1)

**Code**: https://github.com/meituan-longcat/SGLang-FluentLLM.

<details><summary><b>Abstract</b></summary>

While FP8 attention has shown substantial promise in innovations like FlashAttention-3, its integration into the decoding phase of the DeepSeek Multi-head Latent Attention (MLA) architecture presents notable challenges. These challenges include numerical heterogeneity arising from the decoupling of positional embeddings, misalignment of quantization scales in FP8 PV GEMM, and the need for optimized system-level support. In this paper, we introduce SnapMLA, an FP8 MLA decoding framework optimized to improve long-context efficiency through the following hardware-aware algorithm-kernel co-optimization techniques: (i) RoPE-Aware Per-Token KV Quantization, where the RoPE part is maintained in high precision, motivated by our comprehensive analysis of the heterogeneous quantization sensitivity inherent to the MLA KV cache. Furthermore, per-token granularity is employed to align with the autoregressive decoding process and maintain quantization accuracy. (ii) Quantized PV Computation Pipeline Reconstruction, which resolves the misalignment of quantization scale in FP8 PV computation stemming from the shared KV structure of the MLA KV cache. (iii) End-to-End Dataflow Optimization, where we establish an efficient data read-and-write workflow using specialized kernels, ensuring efficient data flow and performance gains. Extensive experiments on state-of-the-art MLA LLMs show that SnapMLA achieves up to a 1.91x improvement in throughput, with negligible risk of performance degradation in challenging long-context tasks, including mathematical reasoning and code generation benchmarks. Code is available at https://github.com/meituan-longcat/SGLang-FluentLLM.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è™½ç„¶ FP8 æ³¨æ„åŠ›åœ¨ FlashAttention-3 ç­‰åˆ›æ–°ä¸­æ˜¾ç¤ºå‡ºäº†å·¨å¤§çš„å‰æ™¯ï¼Œä½†å°†å…¶é›†æˆåˆ° DeepSeek å¤šå¤´æ½œåœ¨æ³¨æ„åŠ› (MLA) æ¶æ„çš„è§£ç é˜¶æ®µå´å¸¦æ¥äº†æ˜¾ç€çš„æŒ‘æˆ˜ã€‚è¿™äº›æŒ‘æˆ˜åŒ…æ‹¬ä½ç½®åµŒå…¥è§£è€¦äº§ç”Ÿçš„æ•°å€¼å¼‚è´¨æ€§ã€FP8 PV GEMM ä¸­é‡åŒ–å°ºåº¦çš„é”™ä½ä»¥åŠä¼˜åŒ–ç³»ç»Ÿçº§æ”¯æŒçš„éœ€æ±‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† SnapMLAï¼Œè¿™æ˜¯ä¸€ç§ç»è¿‡ä¼˜åŒ–çš„ FP8 MLA è§£ç æ¡†æ¶ï¼Œé€šè¿‡ä»¥ä¸‹ç¡¬ä»¶æ„ŸçŸ¥ç®—æ³•-å†…æ ¸ååŒä¼˜åŒ–æŠ€æœ¯æ¥æé«˜é•¿ä¸Šä¸‹æ–‡æ•ˆç‡ï¼šï¼ˆiï¼‰RoPE æ„ŸçŸ¥æ¯ä»¤ç‰Œ KV é‡åŒ–ï¼Œå…¶ä¸­ RoPE éƒ¨åˆ†ä¿æŒé«˜ç²¾åº¦ï¼Œå…¶åŠ¨æœºæ˜¯æˆ‘ä»¬å¯¹ MLA KV ç¼“å­˜å›ºæœ‰çš„å¼‚æ„é‡åŒ–æ•æ„Ÿæ€§çš„å…¨é¢åˆ†æã€‚æ­¤å¤–ï¼Œæ¯ä¸ªä»¤ç‰Œçš„ç²’åº¦ç”¨äºä¸è‡ªå›å½’è§£ç è¿‡ç¨‹ä¿æŒä¸€è‡´å¹¶ä¿æŒé‡åŒ–ç²¾åº¦ã€‚ (ii)é‡åŒ–PVè®¡ç®—ç®¡é“é‡æ„ï¼Œè§£å†³äº†ç”±äºMLA KVç¼“å­˜å…±äº«KVç»“æ„å¯¼è‡´çš„FP8 PVè®¡ç®—ä¸­é‡åŒ–å°ºåº¦é”™ä½çš„é—®é¢˜ã€‚ (iii) ç«¯åˆ°ç«¯æ•°æ®æµä¼˜åŒ–ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸“ç”¨å†…æ ¸å»ºç«‹é«˜æ•ˆçš„æ•°æ®è¯»å†™å·¥ä½œæµç¨‹ï¼Œç¡®ä¿é«˜æ•ˆçš„æ•°æ®æµå’Œæ€§èƒ½æå‡ã€‚å¯¹æœ€å…ˆè¿›çš„ MLA LLM è¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSnapMLA çš„ååé‡æé«˜äº† 1.91 å€ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ï¼ˆåŒ…æ‹¬æ•°å­¦æ¨ç†å’Œä»£ç ç”ŸæˆåŸºå‡†ï¼‰ä¸­ï¼Œæ€§èƒ½ä¸‹é™çš„é£é™©å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚ä»£ç å¯åœ¨ https://github.com/meituan-longcat/SGLang-FluentLLM è·å–ã€‚

</details>

---

## 88. Robust Assortment Optimization from Observational Data

**ä¸­æ–‡æ ‡é¢˜**: æ ¹æ®è§‚æµ‹æ•°æ®è¿›è¡Œç¨³å¥çš„åˆ†ç±»ä¼˜åŒ–

**Date**: 2026-02-11 | **arXiv**: [2602.10696v1](http://arxiv.org/abs/2602.10696v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10696v1)

<details><summary><b>Abstract</b></summary>

Assortment optimization is a fundamental challenge in modern retail and recommendation systems, where the goal is to select a subset of products that maximizes expected revenue under complex customer choice behaviors. While recent advances in data-driven methods have leveraged historical data to learn and optimize assortments, these approaches typically rely on strong assumptions -- namely, the stability of customer preferences and the correctness of the underlying choice models. However, such assumptions frequently break in real-world scenarios due to preference shifts and model misspecification, leading to poor generalization and revenue loss. Motivated by this limitation, we propose a robust framework for data-driven assortment optimization that accounts for potential distributional shifts in customer choice behavior. Our approach models potential preference shift from a nominal choice model that generates data and seeks to maximize worst-case expected revenue. We first establish the computational tractability of robust assortment planning when the nominal model is known, then advance to the data-driven setting, where we design statistically optimal algorithms that minimize the data requirements while maintaining robustness. Our theoretical analysis provides both upper bounds and matching lower bounds on the sample complexity, offering theoretical guarantees for robust generalization. Notably, we uncover and identify the notion of ``robust item-wise coverage'' as the minimal data requirement to enable sample-efficient robust assortment learning. Our work bridges the gap between robustness and statistical efficiency in assortment learning, contributing new insights and tools for reliable assortment optimization under uncertainty.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åˆ†ç±»ä¼˜åŒ–æ˜¯ç°ä»£é›¶å”®å’Œæ¨èç³»ç»Ÿä¸­çš„ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼Œå…¶ç›®æ ‡æ˜¯é€‰æ‹©åœ¨å¤æ‚çš„å®¢æˆ·é€‰æ‹©è¡Œä¸ºä¸‹æœ€å¤§åŒ–é¢„æœŸæ”¶å…¥çš„äº§å“å­é›†ã€‚è™½ç„¶æ•°æ®é©±åŠ¨æ–¹æ³•çš„æœ€æ–°è¿›å±•åˆ©ç”¨å†å²æ•°æ®æ¥å­¦ä¹ å’Œä¼˜åŒ–åˆ†ç±»ï¼Œä½†è¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–äºå¼ºæœ‰åŠ›çš„å‡è®¾ï¼Œå³å®¢æˆ·åå¥½çš„ç¨³å®šæ€§å’ŒåŸºç¡€é€‰æ‹©æ¨¡å‹çš„æ­£ç¡®æ€§ã€‚ç„¶è€Œï¼Œç”±äºåå¥½è½¬å˜å’Œæ¨¡å‹é”™è¯¯æŒ‡å®šï¼Œæ­¤ç±»å‡è®¾åœ¨ç°å®åœºæ™¯ä¸­ç»å¸¸è¢«æ‰“ç ´ï¼Œå¯¼è‡´æ¦‚æ‹¬æ€§å·®å’Œæ”¶å…¥æŸå¤±ã€‚å—æ­¤é™åˆ¶çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¼ºå¤§çš„æ•°æ®é©±åŠ¨åˆ†ç±»ä¼˜åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è€ƒè™‘äº†å®¢æˆ·é€‰æ‹©è¡Œä¸ºçš„æ½œåœ¨åˆ†å¸ƒå˜åŒ–ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯¹åä¹‰é€‰æ‹©æ¨¡å‹çš„æ½œåœ¨åå¥½è½¬å˜è¿›è¡Œå»ºæ¨¡ï¼Œè¯¥æ¨¡å‹ç”Ÿæˆæ•°æ®å¹¶å¯»æ±‚æœ€å¤§åŒ–æœ€åæƒ…å†µçš„é¢„æœŸæ”¶å…¥ã€‚æˆ‘ä»¬é¦–å…ˆåœ¨æ ‡ç§°æ¨¡å‹å·²çŸ¥çš„æƒ…å†µä¸‹å»ºç«‹é²æ£’åˆ†ç±»è§„åˆ’çš„è®¡ç®—æ˜“å¤„ç†æ€§ï¼Œç„¶åè¿›å…¥æ•°æ®é©±åŠ¨è®¾ç½®ï¼Œåœ¨è¯¥è®¾ç½®ä¸­æˆ‘ä»¬è®¾è®¡ç»Ÿè®¡æœ€ä¼˜ç®—æ³•ï¼Œåœ¨ä¿æŒé²æ£’æ€§çš„åŒæ—¶æœ€å¤§é™åº¦åœ°å‡å°‘æ•°æ®éœ€æ±‚ã€‚æˆ‘ä»¬çš„ç†è®ºåˆ†ææä¾›äº†æ ·æœ¬å¤æ‚æ€§çš„ä¸Šé™å’ŒåŒ¹é…çš„ä¸‹é™ï¼Œä¸ºç¨³å¥çš„æ³›åŒ–æä¾›äº†ç†è®ºä¿è¯ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°å¹¶ç¡®å®šäº†â€œé²æ£’é€é¡¹è¦†ç›–â€çš„æ¦‚å¿µï¼Œä½œä¸ºå®ç°æ ·æœ¬é«˜æ•ˆé²æ£’åˆ†ç±»å­¦ä¹ çš„æœ€ä½æ•°æ®è¦æ±‚ã€‚æˆ‘ä»¬çš„å·¥ä½œå¼¥åˆäº†åˆ†ç±»å­¦ä¹ çš„ç¨³å¥æ€§å’Œç»Ÿè®¡æ•ˆç‡ä¹‹é—´çš„å·®è·ï¼Œä¸ºä¸ç¡®å®šæ€§ä¸‹å¯é çš„åˆ†ç±»ä¼˜åŒ–æä¾›äº†æ–°çš„è§è§£å’Œå·¥å…·ã€‚

</details>

---

## 89. A solvable high-dimensional model where nonlinear autoencoders learn structure invisible to PCA while test loss misaligns with generalization

**ä¸­æ–‡æ ‡é¢˜**: ä¸€ç§å¯è§£çš„é«˜ç»´æ¨¡å‹ï¼Œå…¶ä¸­éçº¿æ€§è‡ªåŠ¨ç¼–ç å™¨å­¦ä¹  PCA ä¸å¯è§çš„ç»“æ„ï¼Œè€Œæµ‹è¯•æŸå¤±ä¸æ³›åŒ–ä¸ä¸€è‡´

**Date**: 2026-02-11 | **arXiv**: [2602.10680v1](http://arxiv.org/abs/2602.10680v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10680v1)

<details><summary><b>Abstract</b></summary>

Many real-world datasets contain hidden structure that cannot be detected by simple linear correlations between input features. For example, latent factors may influence the data in a coordinated way, even though their effect is invisible to covariance-based methods such as PCA. In practice, nonlinear neural networks often succeed in extracting such hidden structure in unsupervised and self-supervised learning. However, constructing a minimal high-dimensional model where this advantage can be rigorously analyzed has remained an open theoretical challenge. We introduce a tractable high-dimensional spiked model with two latent factors: one visible to covariance, and one statistically dependent yet uncorrelated, appearing only in higher-order moments. PCA and linear autoencoders fail to recover the latter, while a minimal nonlinear autoencoder provably extracts both. We analyze both the population risk, and empirical risk minimization. Our model also provides a tractable example where self-supervised test loss is poorly aligned with representation quality: nonlinear autoencoders recover latent structure that linear methods miss, even though their reconstruction loss is higher.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è®¸å¤šç°å®ä¸–ç•Œçš„æ•°æ®é›†åŒ…å«éšè—ç»“æ„ï¼Œæ— æ³•é€šè¿‡è¾“å…¥ç‰¹å¾ä¹‹é—´çš„ç®€å•çº¿æ€§ç›¸å…³æ€§æ¥æ£€æµ‹ã€‚ä¾‹å¦‚ï¼Œæ½œåœ¨å› ç´ å¯èƒ½ä¼šä»¥åè°ƒçš„æ–¹å¼å½±å“æ•°æ®ï¼Œå°½ç®¡å®ƒä»¬çš„å½±å“å¯¹äºåŸºäºåæ–¹å·®çš„æ–¹æ³•ï¼ˆä¾‹å¦‚ PCAï¼‰æ˜¯ä¸å¯è§çš„ã€‚åœ¨å®è·µä¸­ï¼Œéçº¿æ€§ç¥ç»ç½‘ç»œé€šå¸¸èƒ½å¤Ÿåœ¨æ— ç›‘ç£å’Œè‡ªç›‘ç£å­¦ä¹ ä¸­æˆåŠŸæå–æ­¤ç±»éšè—ç»“æ„ã€‚ç„¶è€Œï¼Œæ„å»ºä¸€ä¸ªå¯ä»¥ä¸¥æ ¼åˆ†æè¿™ç§ä¼˜åŠ¿çš„æœ€å°é«˜ç»´æ¨¡å‹ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„ç†è®ºæŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ˜“äºå¤„ç†çš„é«˜ç»´å°–å³°æ¨¡å‹ï¼Œå…·æœ‰ä¸¤ä¸ªæ½œåœ¨å› ç´ ï¼šä¸€ä¸ªå¯¹åæ–¹å·®å¯è§ï¼Œå¦ä¸€ä¸ªåœ¨ç»Ÿè®¡ä¸Šä¾èµ–ä½†ä¸ç›¸å…³ï¼Œä»…å‡ºç°åœ¨é«˜é˜¶çŸ©ä¸­ã€‚ PCA å’Œçº¿æ€§è‡ªåŠ¨ç¼–ç å™¨æ— æ³•æ¢å¤åè€…ï¼Œè€Œæœ€å°éçº¿æ€§è‡ªåŠ¨ç¼–ç å™¨å¯ä»¥è¯æ˜ä¸¤è€…éƒ½å¯ä»¥æå–ã€‚æˆ‘ä»¬åˆ†æäº†äººå£é£é™©å’Œç»éªŒé£é™©æœ€å°åŒ–ã€‚æˆ‘ä»¬çš„æ¨¡å‹è¿˜æä¾›äº†ä¸€ä¸ªæ˜“äºå¤„ç†çš„ç¤ºä¾‹ï¼Œå…¶ä¸­è‡ªç›‘ç£æµ‹è¯•æŸå¤±ä¸è¡¨ç¤ºè´¨é‡ä¸ä¸€è‡´ï¼šéçº¿æ€§è‡ªåŠ¨ç¼–ç å™¨æ¢å¤çº¿æ€§æ–¹æ³•é”™è¿‡çš„æ½œåœ¨ç»“æ„ï¼Œå³ä½¿å®ƒä»¬çš„é‡å»ºæŸå¤±æ›´é«˜ã€‚

</details>

---

## 90. On the Role of Consistency Between Physics and Data in Physics-Informed Neural Networks

**ä¸­æ–‡æ ‡é¢˜**: å…³äºç‰©ç†ä¸æ•°æ®ä¸€è‡´æ€§åœ¨ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œä¸­çš„ä½œç”¨

**Date**: 2026-02-11 | **arXiv**: [2602.10611v1](http://arxiv.org/abs/2602.10611v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10611v1)

<details><summary><b>Abstract</b></summary>

Physics-informed neural networks (PINNs) have gained significant attention as a surrogate modeling strategy for partial differential equations (PDEs), particularly in regimes where labeled data are scarce and physical constraints can be leveraged to regularize the learning process. In practice, however, PINNs are frequently trained using experimental or numerical data that are not fully consistent with the governing equations due to measurement noise, discretization errors, or modeling assumptions. The implications of such data-to-PDE inconsistencies on the accuracy and convergence of PINNs remain insufficiently understood. In this work, we systematically analyze how data inconsistency fundamentally limits the attainable accuracy of PINNs. We introduce the concept of a consistency barrier, defined as an intrinsic lower bound on the error that arises from mismatches between the fidelity of the data and the exact enforcement of the PDE residual. To isolate and quantify this effect, we consider the 1D viscous Burgers equation with a manufactured analytical solution, which enables full control over data fidelity and residual errors. PINNs are trained using datasets of progressively increasing numerical accuracy, as well as perfectly consistent analytical data. Results show that while the inclusion of the PDE residual allows PINNs to partially mitigate low-fidelity data and recover the dominant physical structure, the training process ultimately saturates at an error level dictated by the data inconsistency. When high-fidelity numerical data are employed, PINN solutions become indistinguishable from those trained on analytical data, indicating that the consistency barrier is effectively removed. These findings clarify the interplay between data quality and physics enforcement in PINNs providing practical guidance for the construction and interpretation of physics-informed surrogate models.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ (PINN) ä½œä¸ºåå¾®åˆ†æ–¹ç¨‹ (PDE) çš„æ›¿ä»£å»ºæ¨¡ç­–ç•¥è€Œå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡è®°æ•°æ®ç¨€ç¼ºä¸”å¯ä»¥åˆ©ç”¨ç‰©ç†çº¦æŸæ¥è§„èŒƒå­¦ä¹ è¿‡ç¨‹çš„æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼Œåœ¨å®è·µä¸­ï¼ŒPINN ç»å¸¸ä½¿ç”¨å®éªŒæˆ–æ•°å€¼æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œç”±äºæµ‹é‡å™ªå£°ã€ç¦»æ•£åŒ–è¯¯å·®æˆ–å»ºæ¨¡å‡è®¾ï¼Œè¿™äº›æ•°æ®ä¸æ§åˆ¶æ–¹ç¨‹å¹¶ä¸å®Œå…¨ä¸€è‡´ã€‚è¿™ç§æ•°æ®ä¸åå¾®åˆ†æ–¹ç¨‹çš„ä¸ä¸€è‡´å¯¹ PINN å‡†ç¡®æ€§å’Œæ”¶æ•›æ€§çš„å½±å“ä»æœªå¾—åˆ°å……åˆ†ç†è§£ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°åˆ†æäº†æ•°æ®ä¸ä¸€è‡´å¦‚ä½•ä»æ ¹æœ¬ä¸Šé™åˆ¶äº† PINN å¯è¾¾åˆ°çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€è‡´æ€§å±éšœçš„æ¦‚å¿µï¼Œå®ƒè¢«å®šä¹‰ä¸ºç”±äºæ•°æ®ä¿çœŸåº¦ä¸åå¾®åˆ†æ–¹ç¨‹æ®‹å·®çš„ç²¾ç¡®æ‰§è¡Œä¹‹é—´ä¸åŒ¹é…è€Œäº§ç”Ÿçš„è¯¯å·®çš„å†…åœ¨ä¸‹é™ã€‚ä¸ºäº†éš”ç¦»å’Œé‡åŒ–è¿™ç§å½±å“ï¼Œæˆ‘ä»¬è€ƒè™‘ä½¿ç”¨åˆ¶é€ çš„è§£æè§£çš„ä¸€ç»´ç²˜æ€§ä¼¯æ ¼æ–¯æ–¹ç¨‹ï¼Œè¿™ä½¿å¾—èƒ½å¤Ÿå®Œå…¨æ§åˆ¶æ•°æ®ä¿çœŸåº¦å’Œæ®‹å·®ã€‚ PINN ä½¿ç”¨æ•°å€¼ç²¾åº¦é€æ¸æé«˜çš„æ•°æ®é›†ä»¥åŠå®Œå…¨ä¸€è‡´çš„åˆ†ææ•°æ®è¿›è¡Œè®­ç»ƒã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶åŒ…å« PDE æ®‹å·®å…è®¸ PINN éƒ¨åˆ†å‡è½»ä½ä¿çœŸåº¦æ•°æ®å¹¶æ¢å¤ä¸»è¦ç‰©ç†ç»“æ„ï¼Œä½†è®­ç»ƒè¿‡ç¨‹æœ€ç»ˆä¼šåœ¨æ•°æ®ä¸ä¸€è‡´æ‰€å†³å®šçš„é”™è¯¯æ°´å¹³ä¸Šé¥±å’Œã€‚å½“é‡‡ç”¨é«˜ä¿çœŸæ•°å€¼æ•°æ®æ—¶ï¼ŒPINN è§£å†³æ–¹æ¡ˆä¸åˆ†ææ•°æ®è®­ç»ƒçš„è§£å†³æ–¹æ¡ˆå˜å¾—æ— æ³•åŒºåˆ†ï¼Œè¿™è¡¨æ˜ä¸€è‡´æ€§éšœç¢å·²è¢«æœ‰æ•ˆæ¶ˆé™¤ã€‚è¿™äº›å‘ç°é˜æ˜äº† PINN ä¸­æ•°æ®è´¨é‡å’Œç‰©ç†æ‰§è¡Œä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œä¸ºæ„å»ºå’Œè§£é‡ŠåŸºäºç‰©ç†çš„æ›¿ä»£æ¨¡å‹æä¾›äº†å®ç”¨æŒ‡å¯¼ã€‚

</details>

---

## 91. dnaHNet: A Scalable and Hierarchical Foundation Model for Genomic Sequence Learning

**ä¸­æ–‡æ ‡é¢˜**: dnaHNetï¼šåŸºå› ç»„åºåˆ—å­¦ä¹ çš„å¯æ‰©å±•å’Œåˆ†å±‚åŸºç¡€æ¨¡å‹

**Date**: 2026-02-11 | **arXiv**: [2602.10603v1](http://arxiv.org/abs/2602.10603v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10603v1)

<details><summary><b>Abstract</b></summary>

Genomic foundation models have the potential to decode DNA syntax, yet face a fundamental tradeoff in their input representation. Standard fixed-vocabulary tokenizers fragment biologically meaningful motifs such as codons and regulatory elements, while nucleotide-level models preserve biological coherence but incur prohibitive computational costs for long contexts. We introduce dnaHNet, a state-of-the-art tokenizer-free autoregressive model that segments and models genomic sequences end-to-end. Using a differentiable dynamic chunking mechanism, dnaHNet compresses raw nucleotides into latent tokens adaptively, balancing compression with predictive accuracy. Pretrained on prokaryotic genomes, dnaHNet outperforms leading architectures including StripedHyena2 in scaling and efficiency. This recursive chunking yields quadratic FLOP reductions, enabling $>3 \times$ inference speedup over Transformers. On zero-shot tasks, dnaHNet achieves superior performance in predicting protein variant fitness and gene essentiality, while automatically discovering hierarchical biological structures without supervision. These results establish dnaHNet as a scalable, interpretable framework for next-generation genomic modeling.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åŸºå› ç»„åŸºç¡€æ¨¡å‹å…·æœ‰è§£ç  DNA è¯­æ³•çš„æ½œåŠ›ï¼Œä½†åœ¨è¾“å…¥è¡¨ç¤ºæ–¹é¢é¢ä¸´ç€æ ¹æœ¬æ€§çš„æƒè¡¡ã€‚æ ‡å‡†çš„å›ºå®šè¯æ±‡åˆ†è¯å™¨å°†å…·æœ‰ç”Ÿç‰©å­¦æ„ä¹‰çš„åŸºåºï¼ˆä¾‹å¦‚å¯†ç å­å’Œè°ƒæ§å…ƒä»¶ï¼‰ç‰‡æ®µåŒ–ï¼Œè€Œæ ¸è‹·é…¸æ°´å¹³æ¨¡å‹ä¿ç•™äº†ç”Ÿç‰©ä¸€è‡´æ€§ï¼Œä½†åœ¨é•¿ä¸Šä¸‹æ–‡ä¸­ä¼šäº§ç”Ÿé«˜æ˜‚çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬å¼•å…¥äº† dnaHNetï¼Œè¿™æ˜¯ä¸€ç§æœ€å…ˆè¿›çš„æ— åˆ†è¯å™¨çš„è‡ªå›å½’æ¨¡å‹ï¼Œå¯ä»¥ç«¯åˆ°ç«¯åœ°å¯¹åŸºå› ç»„åºåˆ—è¿›è¡Œåˆ†æ®µå’Œå»ºæ¨¡ã€‚ dnaHNet ä½¿ç”¨å¯å¾®çš„åŠ¨æ€åˆ†å—æœºåˆ¶ï¼Œè‡ªé€‚åº”åœ°å°†åŸå§‹æ ¸è‹·é…¸å‹ç¼©ä¸ºæ½œåœ¨æ ‡è®°ï¼Œå¹³è¡¡å‹ç¼©ä¸é¢„æµ‹å‡†ç¡®æ€§ã€‚ dnaHNet åœ¨åŸæ ¸åŸºå› ç»„ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œåœ¨æ‰©å±•æ€§å’Œæ•ˆç‡æ–¹é¢ä¼˜äºåŒ…æ‹¬ StripedHyena2 åœ¨å†…çš„é¢†å…ˆæ¶æ„ã€‚è¿™ç§é€’å½’åˆ†å—å¯å‡å°‘äºŒæ¬¡æ–¹çš„ FLOPï¼Œä»è€Œä½¿æ¨ç†é€Ÿåº¦æ¯” Transformer æé«˜ >3 å€ã€‚åœ¨é›¶æ ·æœ¬ä»»åŠ¡ä¸­ï¼ŒdnaHNet åœ¨é¢„æµ‹è›‹ç™½è´¨å˜å¼‚é€‚åº”æ€§å’ŒåŸºå› å¿…è¦æ€§æ–¹é¢å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨æ— éœ€ç›‘ç£çš„æƒ…å†µä¸‹è‡ªåŠ¨å‘ç°åˆ†å±‚ç”Ÿç‰©ç»“æ„ã€‚è¿™äº›ç»“æœå°† dnaHNet ç¡®ç«‹ä¸ºä¸‹ä¸€ä»£åŸºå› ç»„å»ºæ¨¡çš„å¯æ‰©å±•ã€å¯è§£é‡Šçš„æ¡†æ¶ã€‚

</details>

---

## 92. TRACE: Theoretical Risk Attribution under Covariate-shift Effects

**ä¸­æ–‡æ ‡é¢˜**: TRACEï¼šåå˜é‡è½¬ç§»æ•ˆåº”ä¸‹çš„ç†è®ºé£é™©å½’å› 

**Date**: 2026-02-11 | **arXiv**: [2602.10588v1](http://arxiv.org/abs/2602.10588v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10588v1)

<details><summary><b>Abstract</b></summary>

When a source-trained model $Q$ is replaced by a model $\tilde{Q}$ trained on shifted data, its performance on the source domain can change unpredictably. To address this, we study the two-model risk change, $Î”R := R_P(\tilde{Q}) - R_P(Q)$, under covariate shift. We introduce TRACE (Theoretical Risk Attribution under Covariate-shift Effects), a framework that decomposes $|Î”R|$ into an interpretable upper bound. This decomposition disentangles the risk change into four actionable factors: two generalization gaps, a model change penalty, and a covariate shift penalty, transforming the bound into a powerful diagnostic tool for understanding why performance has changed. To make TRACE a fully computable diagnostic, we instantiate each term. The covariate shift penalty is estimated via a model sensitivity factor (from high-quantile input gradients) and a data-shift measure; we use feature-space Optimal Transport (OT) by default and provide a robust alternative using Maximum Mean Discrepancy (MMD). The model change penalty is controlled by the average output distance between the two models on the target sample. Generalization gaps are estimated on held-out data. We validate our framework in an idealized linear regression setting, showing the TRACE bound correctly captures the scaling of the true risk difference with the magnitude of the shift. Across synthetic and vision benchmarks, TRACE diagnostics are valid and maintain a strong monotonic relationship with the true performance degradation. Crucially, we derive a deployment gate score that correlates strongly with $|Î”R|$ and achieves high AUROC/AUPRC for gating decisions, enabling safe, label-efficient model replacement.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å½“æºè®­ç»ƒæ¨¡å‹ $Q$ æ›¿æ¢ä¸ºåŸºäºç§»ä½æ•°æ®è®­ç»ƒçš„æ¨¡å‹ $\tilde{Q}$ æ—¶ï¼Œå…¶åœ¨æºåŸŸä¸Šçš„æ€§èƒ½å¯èƒ½ä¼šå‘ç”Ÿä¸å¯é¢„æµ‹çš„å˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åå˜é‡åç§»ä¸‹çš„åŒæ¨¡å‹é£é™©å˜åŒ– $Î”R := R_P(\tilde{Q}) - R_P(Q)$ã€‚æˆ‘ä»¬å¼•å…¥ TRACEï¼ˆåå˜é‡è½¬ç§»æ•ˆåº”ä¸‹çš„ç†è®ºé£é™©å½’å› ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå°† $|Î”R|$ åˆ†è§£ä¸ºå¯è§£é‡Šä¸Šé™çš„æ¡†æ¶ã€‚è¿™ç§åˆ†è§£å°†é£é™©å˜åŒ–åˆ†è§£ä¸ºå››ä¸ªå¯æ“ä½œçš„å› ç´ ï¼šä¸¤ä¸ªæ³›åŒ–å·®è·ã€ä¸€ä¸ªæ¨¡å‹å˜åŒ–æƒ©ç½šå’Œä¸€ä¸ªåå˜é‡è½¬ç§»æƒ©ç½šï¼Œå°†ç•Œé™è½¬å˜ä¸ºä¸€ä¸ªå¼ºå¤§çš„è¯Šæ–­å·¥å…·ï¼Œç”¨äºç†è§£æ€§èƒ½å˜åŒ–çš„åŸå› ã€‚ä¸ºäº†ä½¿ TRACE æˆä¸ºå®Œå…¨å¯è®¡ç®—çš„è¯Šæ–­ï¼Œæˆ‘ä»¬å®ä¾‹åŒ–äº†æ¯ä¸ªæœ¯è¯­ã€‚åå˜é‡ç§»ä½æƒ©ç½šæ˜¯é€šè¿‡æ¨¡å‹çµæ•åº¦å› å­ï¼ˆæ¥è‡ªé«˜åˆ†ä½æ•°è¾“å…¥æ¢¯åº¦ï¼‰å’Œæ•°æ®ç§»ä½åº¦é‡æ¥ä¼°è®¡çš„ï¼›æˆ‘ä»¬é»˜è®¤ä½¿ç”¨ç‰¹å¾ç©ºé—´æœ€ä¼˜ä¼ è¾“ï¼ˆOTï¼‰ï¼Œå¹¶ä½¿ç”¨æœ€å¤§å¹³å‡å·®å¼‚ï¼ˆMMDï¼‰æä¾›å¼ºå¤§çš„æ›¿ä»£æ–¹æ¡ˆã€‚æ¨¡å‹æ”¹å˜æƒ©ç½šç”±ç›®æ ‡æ ·æœ¬ä¸Šä¸¤ä¸ªæ¨¡å‹ä¹‹é—´çš„å¹³å‡è¾“å‡ºè·ç¦»æ§åˆ¶ã€‚æ³›åŒ–å·®è·æ˜¯æ ¹æ®ä¿ç•™çš„æ•°æ®ä¼°è®¡çš„ã€‚æˆ‘ä»¬åœ¨ç†æƒ³åŒ–çš„çº¿æ€§å›å½’è®¾ç½®ä¸­éªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶ï¼Œæ˜¾ç¤º TRACE ç•Œé™æ­£ç¡®æ•è·äº†çœŸå®é£é™©å·®å¼‚ä¸å˜åŒ–å¹…åº¦çš„æ¯”ä¾‹ã€‚åœ¨ç»¼åˆå’Œè§†è§‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒTRACE è¯Šæ–­æ˜¯æœ‰æ•ˆçš„ï¼Œå¹¶ä¸”ä¸çœŸå®çš„æ€§èƒ½ä¸‹é™ä¿æŒç€å¾ˆå¼ºçš„å•è°ƒå…³ç³»ã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å¾—å‡ºäº†ä¸ $|Î”R|$ å¯†åˆ‡ç›¸å…³çš„éƒ¨ç½²é—¨å¾—åˆ†ï¼Œå¹¶ä¸ºé—¨æ§å†³ç­–å®ç°äº†é«˜ AUROC/AUPRCï¼Œä»è€Œå®ç°äº†å®‰å…¨ã€æ ‡ç­¾é«˜æ•ˆçš„æ¨¡å‹æ›¿æ¢ã€‚

</details>

---

## 93. Predictive-State Communication: Innovation Coding and Reconciliation under Delay

**ä¸­æ–‡æ ‡é¢˜**: é¢„æµ‹çŠ¶æ€é€šä¿¡ï¼šå»¶è¿Ÿä¸‹çš„åˆ›æ–°ç¼–ç ä¸åè°ƒ

**Date**: 2026-02-11 | **arXiv**: [2602.10542v1](http://arxiv.org/abs/2602.10542v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10542v1)

<details><summary><b>Abstract</b></summary>

Shannon theory models communication as the reliable transfer of symbol sequences, with performance governed by capacity and rate-distortion limits. When both endpoints possess strong predictors -- as in modern large language models and related generative priors -- literal symbol transport is no longer the only operational regime. We propose predictive-state communication (PSC), in which the transmitter and receiver maintain an explicit shared predictive state, and the physical channel is used primarily to convey innovations, i.e., corrective information that reconciles the receiver's provisional trajectory with the transmitter's realized trajectory. This viewpoint replaces entropy-rate accounting by cross-entropy accounting under model mismatch, and it introduces feasibility constraints that depend jointly on capacity, delay, and perceptual continuity requirements; the resulting operating set is typically a bounded perception-capacity band rather than a one-sided threshold. We outline the protocol and architectural implications (state identifiers, anchors, bounded rollback, and patch-based updates) and provide a stylized illustrative example to visualize the induced feasibility region and its dependence on predictive quality.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

é¦™å†œç†è®ºå°†é€šä¿¡å»ºæ¨¡ä¸ºç¬¦å·åºåˆ—çš„å¯é ä¼ è¾“ï¼Œå…¶æ€§èƒ½å—å®¹é‡å’Œé€Ÿç‡å¤±çœŸé™åˆ¶æ§åˆ¶ã€‚å½“ä¸¤ä¸ªç«¯ç‚¹éƒ½æ‹¥æœ‰å¼ºå¤§çš„é¢„æµ‹å› å­æ—¶ï¼ˆå¦‚åœ¨ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹å’Œç›¸å…³çš„ç”Ÿæˆå…ˆéªŒä¸­ï¼‰ï¼Œæ–‡å­—ç¬¦å·ä¼ è¾“ä¸å†æ˜¯å”¯ä¸€çš„æ“ä½œæœºåˆ¶ã€‚æˆ‘ä»¬æå‡ºé¢„æµ‹çŠ¶æ€é€šä¿¡ï¼ˆPSCï¼‰ï¼Œå…¶ä¸­å‘å°„å™¨å’Œæ¥æ”¶å™¨ä¿æŒæ˜ç¡®çš„å…±äº«é¢„æµ‹çŠ¶æ€ï¼Œå¹¶ä¸”ç‰©ç†ä¿¡é“ä¸»è¦ç”¨äºä¼ è¾¾åˆ›æ–°ï¼Œå³ä½¿æ¥æ”¶å™¨çš„ä¸´æ—¶è½¨è¿¹ä¸å‘å°„å™¨çš„å®ç°è½¨è¿¹ç›¸ä¸€è‡´çš„æ ¡æ­£ä¿¡æ¯ã€‚è¯¥è§‚ç‚¹åœ¨æ¨¡å‹å¤±é…çš„æƒ…å†µä¸‹ç”¨äº¤å‰ç†µæ ¸ç®—å–ä»£äº†ç†µç‡æ ¸ç®—ï¼Œå¹¶å¼•å…¥äº†å…±åŒä¾èµ–äºå®¹é‡ã€å»¶è¿Ÿå’Œæ„ŸçŸ¥è¿ç»­æ€§è¦æ±‚çš„å¯è¡Œæ€§çº¦æŸï¼›ç”±æ­¤äº§ç”Ÿçš„æ“ä½œé›†é€šå¸¸æ˜¯ä¸€ä¸ªæœ‰ç•Œçš„æ„ŸçŸ¥èƒ½åŠ›å¸¦ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå•ä¾§é˜ˆå€¼ã€‚æˆ‘ä»¬æ¦‚è¿°äº†åè®®å’Œæ¶æ„å«ä¹‰ï¼ˆçŠ¶æ€æ ‡è¯†ç¬¦ã€é”šç‚¹ã€æœ‰ç•Œå›æ»šå’ŒåŸºäºè¡¥ä¸çš„æ›´æ–°ï¼‰ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªç¨‹å¼åŒ–çš„è¯´æ˜æ€§ç¤ºä¾‹æ¥å¯è§†åŒ–è¯±å¯¼çš„å¯è¡Œæ€§åŒºåŸŸåŠå…¶å¯¹é¢„æµ‹è´¨é‡çš„ä¾èµ–æ€§ã€‚

</details>

---

## 94. Privacy-Utility Tradeoffs in Quantum Information Processing

**ä¸­æ–‡æ ‡é¢˜**: é‡å­ä¿¡æ¯å¤„ç†ä¸­çš„éšç§ä¸æ•ˆç”¨æƒè¡¡

**Date**: 2026-02-11 | **arXiv**: [2602.10510v1](http://arxiv.org/abs/2602.10510v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10510v1)

<details><summary><b>Abstract</b></summary>

When sensitive information is encoded in data, it is important to ensure the privacy of information when attempting to learn useful information from the data. There is a natural tradeoff whereby increasing privacy requirements may decrease the utility of a learning protocol. In the quantum setting of differential privacy, such tradeoffs between privacy and utility have so far remained largely unexplored. In this work, we study optimal privacy-utility tradeoffs for both generic and application-specific utility metrics when privacy is quantified by $(\varepsilon,Î´)$-quantum local differential privacy. In the generic setting, we focus on optimizing fidelity and trace distance between the original state and the privatized state. We show that the depolarizing mechanism achieves the optimal utility for given privacy requirements. We then study the specific application of learning the expectation of an observable with respect to an input state when only given access to privatized states. We derive a lower bound on the number of samples of privatized data required to achieve a fixed accuracy guarantee with high probability. To prove this result, we employ existing lower bounds on private quantum hypothesis testing, thus showcasing the first operational use of them. We also devise private mechanisms that achieve optimal sample complexity with respect to the privacy parameters and accuracy parameters, demonstrating that utility can be significantly improved for specific tasks in contrast to the generic setting. In addition, we show that the number of samples required to privately learn observable expectation values scales as $Î˜((\varepsilon Î²)^{-2})$, where $\varepsilon \in (0,1)$ is the privacy parameter and $Î²$ is the accuracy tolerance. We conclude by initiating the study of private classical shadows, which promise useful applications for private learning tasks.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å½“æ•æ„Ÿä¿¡æ¯è¢«ç¼–ç åœ¨æ•°æ®ä¸­æ—¶ï¼Œåœ¨å°è¯•ä»æ•°æ®ä¸­å­¦ä¹ æœ‰ç”¨ä¿¡æ¯æ—¶ç¡®ä¿ä¿¡æ¯çš„éšç§éå¸¸é‡è¦ã€‚å­˜åœ¨ä¸€ç§è‡ªç„¶çš„æƒè¡¡ï¼Œå³å¢åŠ éšç§è¦æ±‚å¯èƒ½ä¼šé™ä½å­¦ä¹ åè®®çš„æ•ˆç”¨ã€‚åœ¨å·®åˆ†éšç§çš„é‡å­ç¯å¢ƒä¸­ï¼Œè¿„ä»Šä¸ºæ­¢ï¼Œéšç§å’Œå®ç”¨æ€§ä¹‹é—´çš„è¿™ç§æƒè¡¡åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä»æœªå¾—åˆ°æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å½“éšç§é€šè¿‡ $(\varepsilon,Î´)$-é‡å­å±€éƒ¨å·®åˆ†éšç§è¿›è¡Œé‡åŒ–æ—¶ï¼Œé€šç”¨å’Œç‰¹å®šäºåº”ç”¨ç¨‹åºçš„æ•ˆç”¨æŒ‡æ ‡çš„æœ€ä½³éšç§-æ•ˆç”¨æƒè¡¡ã€‚åœ¨é€šç”¨è®¾ç½®ä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºä¼˜åŒ–åŸå§‹çŠ¶æ€å’Œç§æœ‰åŒ–çŠ¶æ€ä¹‹é—´çš„ä¿çœŸåº¦å’Œè·Ÿè¸ªè·ç¦»ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå»æåŒ–æœºåˆ¶å¯¹äºç»™å®šçš„éšç§è¦æ±‚å®ç°äº†æœ€ä½³æ•ˆç”¨ã€‚ç„¶åï¼Œæˆ‘ä»¬ç ”ç©¶å½“ä»…å…è®¸è®¿é—®ç§æœ‰åŒ–çŠ¶æ€æ—¶å­¦ä¹ å¯è§‚å¯Ÿå€¼ç›¸å¯¹äºè¾“å…¥çŠ¶æ€çš„æœŸæœ›çš„å…·ä½“åº”ç”¨ã€‚æˆ‘ä»¬å¾—å‡ºäº†ä»¥é«˜æ¦‚ç‡å®ç°å›ºå®šç²¾åº¦ä¿è¯æ‰€éœ€çš„ç§æœ‰åŒ–æ•°æ®æ ·æœ¬æ•°é‡çš„ä¸‹é™ã€‚ä¸ºäº†è¯æ˜è¿™ä¸ªç»“æœï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ç§äººé‡å­å‡è®¾æ£€éªŒçš„ç°æœ‰ä¸‹é™ï¼Œä»è€Œå±•ç¤ºäº†å®ƒä»¬çš„é¦–æ¬¡æ“ä½œç”¨é€”ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ç§æœ‰æœºåˆ¶ï¼Œå¯ä»¥åœ¨éšç§å‚æ•°å’Œå‡†ç¡®æ€§å‚æ•°æ–¹é¢å®ç°æœ€ä½³æ ·æœ¬å¤æ‚æ€§ï¼Œè¿™è¡¨æ˜ä¸é€šç”¨è®¾ç½®ç›¸æ¯”ï¼Œç‰¹å®šä»»åŠ¡çš„æ•ˆç”¨å¯ä»¥æ˜¾ç€æé«˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œç§ä¸‹å­¦ä¹ å¯è§‚å¯ŸæœŸæœ›å€¼æ‰€éœ€çš„æ ·æœ¬æ•°é‡ä¸º $Î¸((\varepsilon Î²)^{-2})$ï¼Œå…¶ä¸­ $\varepsilon \in (0,1)$ æ˜¯éšç§å‚æ•°ï¼Œ$Î²$ æ˜¯ç²¾åº¦å®¹å·®ã€‚æœ€åï¼Œæˆ‘ä»¬å¯åŠ¨äº†ç§äººç»å…¸é˜´å½±çš„ç ”ç©¶ï¼Œè¿™æœ‰æœ›ä¸ºç§äººå­¦ä¹ ä»»åŠ¡æä¾›æœ‰ç”¨çš„åº”ç”¨ã€‚

</details>

---

## 95. Pricing Query Complexity of Multiplicative Revenue Approximation

**ä¸­æ–‡æ ‡é¢˜**: ä¹˜æ³•æ”¶å…¥è¿‘ä¼¼çš„å®šä»·æŸ¥è¯¢å¤æ‚æ€§

**Date**: 2026-02-11 | **arXiv**: [2602.10483v1](http://arxiv.org/abs/2602.10483v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10483v1)

<details><summary><b>Abstract</b></summary>

We study the pricing query complexity of revenue maximization for a single buyer whose private valuation is drawn from an unknown distribution. In this setting, the seller must learn the optimal monopoly price by posting prices and observing only binary purchase decisions, rather than the realized valuations. Prior work has established tight query complexity bounds for learning a near-optimal price with additive error $\varepsilon$ when the valuation distribution is supported on $[0,1]$. However, our understanding of how to learn a near-optimal price that achieves at least a $(1-\varepsilon)$ fraction of the optimal revenue remains limited.   In this paper, we study the pricing query complexity of the single-buyer revenue maximization problem under such multiplicative error guarantees in several settings. Observe that when pricing queries are the only source of information about the buyer's distribution, no algorithm can achieve a non-trivial approximation, since the scale of the distribution cannot be learned from pricing queries alone. Motivated by this fundamental impossibility, we consider two natural and well-motivated models that provide "scale hints": (i) a one-sample hint, in which the algorithm observes a single realized valuation before making pricing queries; and (ii) a value-range hint, in which the valuation support is known to lie within $[1, H]$. For each type of hint, we establish pricing query complexity guarantees that are tight up to polylogarithmic factors for several classes of distributions, including monotone hazard rate (MHR) distributions, regular distributions, and general distributions.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æˆ‘ä»¬ç ”ç©¶å•ä¸ªä¹°å®¶æ”¶å…¥æœ€å¤§åŒ–çš„å®šä»·æŸ¥è¯¢å¤æ‚æ€§ï¼Œå…¶ç§äººä¼°å€¼æ¥è‡ªæœªçŸ¥åˆ†å¸ƒã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå–æ–¹å¿…é¡»é€šè¿‡å‘å¸ƒä»·æ ¼å¹¶ä»…è§‚å¯ŸäºŒå…ƒè´­ä¹°å†³ç­–è€Œä¸æ˜¯å·²å®ç°çš„ä¼°å€¼æ¥äº†è§£æœ€ä½³å„æ–­ä»·æ ¼ã€‚å…ˆå‰çš„å·¥ä½œå·²ç»å»ºç«‹äº†ä¸¥æ ¼çš„æŸ¥è¯¢å¤æ‚æ€§ç•Œé™ï¼Œç”¨äºåœ¨ $[0,1]$ æ”¯æŒä¼°å€¼åˆ†å¸ƒæ—¶å­¦ä¹ å…·æœ‰é™„åŠ è¯¯å·® $\varepsilon$ çš„æ¥è¿‘æœ€ä¼˜ä»·æ ¼ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¯¹å¦‚ä½•å­¦ä¹ æ¥è¿‘æœ€ä¼˜ä»·æ ¼ä»¥å®ç°è‡³å°‘ $(1-\varepsilon)$ éƒ¨åˆ†æœ€ä¼˜æ”¶å…¥çš„ç†è§£ä»ç„¶æœ‰é™ã€‚   åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åœ¨å¤šç§è®¾ç½®ä¸‹è¿™ç§ä¹˜æ³•è¯¯å·®ä¿è¯ä¸‹å•ä¹°å®¶æ”¶å…¥æœ€å¤§åŒ–é—®é¢˜çš„å®šä»·æŸ¥è¯¢å¤æ‚æ€§ã€‚è¯·æ³¨æ„ï¼Œå½“å®šä»·æŸ¥è¯¢æ˜¯æœ‰å…³ä¹°æ–¹åˆ†å¸ƒçš„å”¯ä¸€ä¿¡æ¯æ¥æºæ—¶ï¼Œä»»ä½•ç®—æ³•éƒ½æ— æ³•å®ç°éå¹³å‡¡çš„è¿‘ä¼¼ï¼Œå› ä¸ºåˆ†å¸ƒçš„è§„æ¨¡æ— æ³•ä»…ä»å®šä»·æŸ¥è¯¢ä¸­è·æ‚‰ã€‚å—è¿™ç§æ ¹æœ¬ä¸å¯èƒ½çš„å¯å‘ï¼Œæˆ‘ä»¬è€ƒè™‘äº†ä¸¤ç§æä¾›â€œè§„æ¨¡æç¤ºâ€çš„è‡ªç„¶ä¸”åŠ¨æœºè‰¯å¥½çš„æ¨¡å‹ï¼šï¼ˆiï¼‰å•æ ·æœ¬æç¤ºï¼Œå…¶ä¸­ç®—æ³•åœ¨è¿›è¡Œå®šä»·æŸ¥è¯¢ä¹‹å‰è§‚å¯Ÿå•ä¸ªå·²å®ç°çš„ä¼°å€¼ï¼› (ii) ä»·å€¼èŒƒå›´æç¤ºï¼Œå…¶ä¸­å·²çŸ¥ä¼°å€¼æ”¯æŒä½äº $[1, H]$ èŒƒå›´å†…ã€‚å¯¹äºæ¯ç§ç±»å‹çš„æç¤ºï¼Œæˆ‘ä»¬å»ºç«‹äº†å®šä»·æŸ¥è¯¢å¤æ‚æ€§ä¿è¯ï¼Œè¯¥ä¿è¯ä¸¥æ ¼éµå®ˆå‡ ç±»åˆ†å¸ƒçš„å¤šå¯¹æ•°å› å­ï¼ŒåŒ…æ‹¬å•è°ƒé£é™©ç‡ (MHR) åˆ†å¸ƒã€æ­£åˆ™åˆ†å¸ƒå’Œä¸€èˆ¬åˆ†å¸ƒã€‚

</details>

---

## 96. Distributed Online Convex Optimization with Nonseparable Costs and Constraints

**ä¸­æ–‡æ ‡é¢˜**: å…·æœ‰ä¸å¯åˆ†ç¦»æˆæœ¬å’Œçº¦æŸçš„åˆ†å¸ƒå¼åœ¨çº¿å‡¸ä¼˜åŒ–

**Date**: 2026-02-11 | **arXiv**: [2602.10452v1](http://arxiv.org/abs/2602.10452v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10452v1)

<details><summary><b>Abstract</b></summary>

This paper studies distributed online convex optimization with time-varying coupled constraints, motivated by distributed online control in network systems. Most prior work assumes a separability condition: the global objective and coupled constraint functions are sums of local costs and individual constraints. In contrast, we study a group of agents, networked via a communication graph, that collectively select actions to minimize a sequence of nonseparable global cost functions and to stratify nonseparable long-term constraints based on full-information feedback and intra-agent communication. We propose a distributed online primal-dual belief consensus algorithm, where each agent maintains and updates a local belief of the global collective decisions, which are repeatedly exchanged with neighboring agents. Unlike the previous consensus primal-dual algorithms under separability that ask agents to only communicate their local decisions, our belief-sharing protocol eliminates coupling between the primal consensus disagreement and the dual constraint violation, yielding sublinear regret and cumulative constraint violation (CCV) bounds, both in $O({T}^{1/2})$, where $T$ denotes the time horizon. Such a result breaks the long-standing $O(T^{3/4})$ barrier for CCV and matches the lower bound of online constrained convex optimization, indicating the online learning efficiency at the cost of communication overhead.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æœ¬æ–‡ç ”ç©¶äº†å…·æœ‰æ—¶å˜è€¦åˆçº¦æŸçš„åˆ†å¸ƒå¼åœ¨çº¿å‡¸ä¼˜åŒ–ï¼Œå…¶åŠ¨æœºæ˜¯ç½‘ç»œç³»ç»Ÿä¸­çš„åˆ†å¸ƒå¼åœ¨çº¿æ§åˆ¶ã€‚å¤§å¤šæ•°å…ˆå‰çš„å·¥ä½œéƒ½å‡è®¾å¯åˆ†ç¦»æ¡ä»¶ï¼šå…¨å±€ç›®æ ‡å‡½æ•°å’Œè€¦åˆçº¦æŸå‡½æ•°æ˜¯å±€éƒ¨æˆæœ¬å’Œä¸ªä½“çº¦æŸçš„æ€»å’Œã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬ç ”ç©¶ä¸€ç»„é€šè¿‡é€šä¿¡å›¾è”ç½‘çš„ä»£ç†ï¼Œå®ƒä»¬å…±åŒé€‰æ‹©è¡ŒåŠ¨ä»¥æœ€å°åŒ–ä¸€ç³»åˆ—ä¸å¯åˆ†ç¦»çš„å…¨å±€æˆæœ¬å‡½æ•°ï¼Œå¹¶åŸºäºå®Œæ•´ä¿¡æ¯åé¦ˆå’Œä»£ç†å†…éƒ¨é€šä¿¡å¯¹ä¸å¯åˆ†ç¦»çš„é•¿æœŸçº¦æŸè¿›è¡Œåˆ†å±‚ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†å¸ƒå¼åœ¨çº¿åŸå§‹å¯¹å¶ä¿¡å¿µå…±è¯†ç®—æ³•ï¼Œå…¶ä¸­æ¯ä¸ªæ™ºèƒ½ä½“ç»´æŠ¤å’Œæ›´æ–°å…¨å±€é›†ä½“å†³ç­–çš„å±€éƒ¨ä¿¡å¿µï¼Œå¹¶ä¸ç›¸é‚»æ™ºèƒ½ä½“é‡å¤äº¤æ¢ã€‚ä¸ä¹‹å‰çš„å¯åˆ†ç¦»æ€§ä¸‹çš„å…±è¯†åŸå§‹å¯¹å¶ç®—æ³•è¦æ±‚ä»£ç†ä»…ä¼ è¾¾å…¶æœ¬åœ°å†³ç­–ä¸åŒï¼Œæˆ‘ä»¬çš„ä¿¡å¿µå…±äº«åè®®æ¶ˆé™¤äº†åŸå§‹å…±è¯†åˆ†æ­§å’Œå¯¹å¶çº¦æŸè¿åä¹‹é—´çš„è€¦åˆï¼Œäº§ç”Ÿæ¬¡çº¿æ€§åæ‚”å’Œç´¯ç§¯çº¦æŸè¿åï¼ˆCCVï¼‰ç•Œé™ï¼Œä¸¤è€…éƒ½åœ¨ $O({T}^{1/2})$ ä¸­ï¼Œå…¶ä¸­ $T$ è¡¨ç¤ºæ—¶é—´èŒƒå›´ã€‚è¿™æ ·çš„ç»“æœæ‰“ç ´äº†CCVé•¿æœŸä»¥æ¥çš„$O(T^{3/4})$éšœç¢ï¼Œå¹¶ä¸åœ¨çº¿çº¦æŸå‡¸ä¼˜åŒ–çš„ä¸‹é™ç›¸åŒ¹é…ï¼Œè¡¨æ˜ä»¥é€šä¿¡å¼€é”€ä¸ºä»£ä»·çš„åœ¨çº¿å­¦ä¹ æ•ˆç‡ã€‚

</details>

---

## 97. Chamfer-Linkage for Hierarchical Agglomerative Clustering

**ä¸­æ–‡æ ‡é¢˜**: ç”¨äºåˆ†å±‚å‡èšèšç±»çš„å€’è§’è¿æ¥

**Date**: 2026-02-11 | **arXiv**: [2602.10444v1](http://arxiv.org/abs/2602.10444v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10444v1)

<details><summary><b>Abstract</b></summary>

Hierarchical Agglomerative Clustering (HAC) is a widely-used clustering method based on repeatedly merging the closest pair of clusters, where inter-cluster distances are determined by a linkage function. Unlike many clustering methods, HAC does not optimize a single explicit global objective; clustering quality is therefore primarily evaluated empirically, and the choice of linkage function plays a crucial role in practice. However, popular classical linkages, such as single-linkage, average-linkage and Ward's method show high variability across real-world datasets and do not consistently produce high-quality clusterings in practice.   In this paper, we propose \emph{Chamfer-linkage}, a novel linkage function that measures the distance between clusters using the Chamfer distance, a popular notion of distance between point-clouds in machine learning and computer vision. We argue that Chamfer-linkage satisfies desirable concept representation properties that other popular measures struggle to satisfy. Theoretically, we show that Chamfer-linkage HAC can be implemented in $O(n^2)$ time, matching the efficiency of classical linkage functions. Experimentally, we find that Chamfer-linkage consistently yields higher-quality clusterings than classical linkages such as average-linkage and Ward's method across a diverse collection of datasets. Our results establish Chamfer-linkage as a practical drop-in replacement for classical linkage functions, broadening the toolkit for hierarchical clustering in both theory and practice.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å±‚æ¬¡èšåˆèšç±»ï¼ˆHACï¼‰æ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„èšç±»æ–¹æ³•ï¼ŒåŸºäºé‡å¤åˆå¹¶æœ€æ¥è¿‘çš„ä¸€å¯¹èšç±»ï¼Œå…¶ä¸­èšç±»é—´çš„è·ç¦»ç”±é“¾æ¥å‡½æ•°ç¡®å®šã€‚ä¸è®¸å¤šèšç±»æ–¹æ³•ä¸åŒï¼ŒHAC ä¸ä¼šä¼˜åŒ–å•ä¸ªæ˜¾å¼å…¨å±€ç›®æ ‡ï¼›å› æ­¤ï¼Œèšç±»è´¨é‡ä¸»è¦æ ¹æ®ç»éªŒè¿›è¡Œè¯„ä¼°ï¼Œè€Œé“¾æ¥å‡½æ•°çš„é€‰æ‹©åœ¨å®è·µä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œæµè¡Œçš„ç»å…¸é“¾æ¥ï¼Œä¾‹å¦‚å•é“¾æ¥ã€å¹³å‡é“¾æ¥å’Œ Ward æ–¹æ³•ï¼Œåœ¨ç°å®ä¸–ç•Œçš„æ•°æ®é›†ä¸­è¡¨ç°å‡ºé«˜åº¦çš„å¯å˜æ€§ï¼Œå¹¶ä¸”åœ¨å®è·µä¸­å¹¶ä¸èƒ½å§‹ç»ˆå¦‚ä¸€åœ°äº§ç”Ÿé«˜è´¨é‡çš„èšç±»ã€‚   åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† \emph{Chamfer-linkage}ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„é“¾æ¥å‡½æ•°ï¼Œå®ƒä½¿ç”¨ Chamfer è·ç¦»ï¼ˆæœºå™¨å­¦ä¹ å’Œè®¡ç®—æœºè§†è§‰ä¸­ç‚¹äº‘ä¹‹é—´çš„è·ç¦»çš„æµè¡Œæ¦‚å¿µï¼‰æ¥æµ‹é‡ç°‡ä¹‹é—´çš„è·ç¦»ã€‚æˆ‘ä»¬è®¤ä¸ºå€’è§’è”åŠ¨æ»¡è¶³äº†å…¶ä»–æµè¡Œæªæ–½éš¾ä»¥æ»¡è¶³çš„ç†æƒ³æ¦‚å¿µè¡¨ç¤ºå±æ€§ã€‚ç†è®ºä¸Šï¼Œæˆ‘ä»¬è¡¨æ˜å€’è§’é“¾æ¥ HAC å¯ä»¥åœ¨ $O(n^2)$ æ—¶é—´å†…å®ç°ï¼Œä¸ç»å…¸é“¾æ¥å‡½æ•°çš„æ•ˆç‡ç›¸åŒ¹é…ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬å‘ç°åœ¨ä¸åŒçš„æ•°æ®é›†ä¸­ï¼ŒChamfer-linkage å§‹ç»ˆæ¯”ä¼ ç»Ÿçš„é“¾æ¥ï¼ˆä¾‹å¦‚å¹³å‡é“¾æ¥å’Œ Ward æ–¹æ³•ï¼‰äº§ç”Ÿæ›´é«˜è´¨é‡çš„èšç±»ã€‚æˆ‘ä»¬çš„ç»“æœå°† Chamfer-linkage ç¡®ç«‹ä¸ºç»å…¸è”åŠ¨å‡½æ•°çš„å®ç”¨æ›¿ä»£å“ï¼Œåœ¨ç†è®ºå’Œå®è·µä¸Šæ‹“å®½äº†å±‚æ¬¡èšç±»çš„å·¥å…·åŒ…ã€‚

</details>

---

## 98. Binary Flow Matching: Prediction-Loss Space Alignment for Robust Learning

**ä¸­æ–‡æ ‡é¢˜**: äºŒå…ƒæµåŒ¹é…ï¼šç”¨äºé²æ£’å­¦ä¹ çš„é¢„æµ‹æŸå¤±ç©ºé—´å¯¹é½

**Date**: 2026-02-11 | **arXiv**: [2602.10420v1](http://arxiv.org/abs/2602.10420v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10420v1)

<details><summary><b>Abstract</b></summary>

Flow matching has emerged as a powerful framework for generative modeling, with recent empirical successes highlighting the effectiveness of signal-space prediction ($x$-prediction). In this work, we investigate the transfer of this paradigm to binary manifolds, a fundamental setting for generative modeling of discrete data. While $x$-prediction remains effective, we identify a latent structural mismatch that arises when it is coupled with velocity-based objectives ($v$-loss), leading to a time-dependent singular weighting that amplifies gradient sensitivity to approximation errors. Motivated by this observation, we formalize prediction-loss alignment as a necessary condition for flow matching training. We prove that re-aligning the objective to the signal space ($x$-loss) eliminates the singular weighting, yielding uniformly bounded gradients and enabling robust training under uniform timestep sampling without reliance on heuristic schedules. Finally, with alignment secured, we examine design choices specific to binary data, revealing a topology-dependent distinction between probabilistic objectives (e.g., cross-entropy) and geometric losses (e.g., mean squared error). Together, these results provide theoretical foundations and practical guidelines for robust flow matching on binary -- and related discrete -- domains, positioning signal-space alignment as a key principle for robust diffusion learning.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æµåŒ¹é…å·²æˆä¸ºç”Ÿæˆå»ºæ¨¡çš„å¼ºå¤§æ¡†æ¶ï¼Œæœ€è¿‘çš„ç»éªŒæˆåŠŸå‡¸æ˜¾äº†ä¿¡å·ç©ºé—´é¢„æµ‹ï¼ˆ$x$-é¢„æµ‹ï¼‰çš„æœ‰æ•ˆæ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†è¿™ç§èŒƒå¼åˆ°äºŒå…ƒæµå½¢çš„è½¬ç§»ï¼Œè¿™æ˜¯ç¦»æ•£æ•°æ®ç”Ÿæˆå»ºæ¨¡çš„åŸºæœ¬è®¾ç½®ã€‚è™½ç„¶$x$-é¢„æµ‹ä»ç„¶æœ‰æ•ˆï¼Œä½†æˆ‘ä»¬å‘ç°å½“å®ƒä¸åŸºäºé€Ÿåº¦çš„ç›®æ ‡ï¼ˆ$v$-lossï¼‰ç»“åˆæ—¶ä¼šå‡ºç°æ½œåœ¨çš„ç»“æ„å¤±é…ï¼Œä»è€Œå¯¼è‡´ä¾èµ–äºæ—¶é—´çš„å¥‡å¼‚æƒé‡ï¼Œä»è€Œæ”¾å¤§äº†å¯¹è¿‘ä¼¼è¯¯å·®çš„æ¢¯åº¦æ•æ„Ÿæ€§ã€‚å—è¿™ä¸€è§‚å¯Ÿçš„å¯å‘ï¼Œæˆ‘ä»¬å°†é¢„æµ‹æŸå¤±å¯¹é½å½¢å¼åŒ–ä¸ºæµåŒ¹é…è®­ç»ƒçš„å¿…è¦æ¡ä»¶ã€‚æˆ‘ä»¬è¯æ˜ï¼Œå°†ç›®æ ‡é‡æ–°ä¸ä¿¡å·ç©ºé—´ï¼ˆ$x$-lossï¼‰å¯¹é½å¯ä»¥æ¶ˆé™¤å¥‡å¼‚æƒé‡ï¼Œäº§ç”Ÿå‡åŒ€æœ‰ç•Œçš„æ¢¯åº¦ï¼Œå¹¶åœ¨å‡åŒ€æ—¶é—´æ­¥é‡‡æ ·ä¸‹å®ç°ç¨³å¥çš„è®­ç»ƒï¼Œè€Œæ— éœ€ä¾èµ–å¯å‘å¼è®¡åˆ’ã€‚æœ€åï¼Œåœ¨ç¡®ä¿å¯¹é½çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æ£€æŸ¥ç‰¹å®šäºäºŒè¿›åˆ¶æ•°æ®çš„è®¾è®¡é€‰æ‹©ï¼Œæ­ç¤ºæ¦‚ç‡ç›®æ ‡ï¼ˆä¾‹å¦‚äº¤å‰ç†µï¼‰å’Œå‡ ä½•æŸå¤±ï¼ˆä¾‹å¦‚å‡æ–¹è¯¯å·®ï¼‰ä¹‹é—´ä¾èµ–äºæ‹“æ‰‘çš„åŒºåˆ«ã€‚æ€»ä¹‹ï¼Œè¿™äº›ç»“æœä¸ºäºŒè¿›åˆ¶åŸŸå’Œç›¸å…³ç¦»æ•£åŸŸä¸Šçš„é²æ£’æµåŒ¹é…æä¾›äº†ç†è®ºåŸºç¡€å’Œå®è·µæŒ‡å—ï¼Œå°†ä¿¡å·ç©ºé—´å¯¹é½å®šä½ä¸ºé²æ£’æ‰©æ•£å­¦ä¹ çš„å…³é”®åŸåˆ™ã€‚

</details>

---

## 99. Gated Removal of Normalization in Transformers Enables Stable Training and Efficient Inference

**ä¸­æ–‡æ ‡é¢˜**: Transformer ä¸­æ ‡å‡†åŒ–çš„é—¨æ§å»é™¤å¯å®ç°ç¨³å®šçš„è®­ç»ƒå’Œé«˜æ•ˆçš„æ¨ç†

**Date**: 2026-02-11 | **arXiv**: [2602.10408v1](http://arxiv.org/abs/2602.10408v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10408v1)

<details><summary><b>Abstract</b></summary>

Normalization is widely viewed as essential for stabilizing Transformer training. We revisit this assumption for pre-norm Transformers and ask to what extent sample-dependent normalization is needed inside Transformer blocks. We introduce TaperNorm, a drop-in replacement for RMSNorm/LayerNorm that behaves exactly like the standard normalizer early in training and then smoothly tapers to a learned sample-independent linear/affine map. A single global gate is held at $g{=}1$ during gate warmup, used to calibrate the scaling branch via EMAs, and then cosine-decayed to $g{=}0$, at which point per-token statistics vanish and the resulting fixed scalings can be folded into adjacent linear projections. Our theoretical and empirical results isolate scale anchoring as the key role played by output normalization: as a (near) $0$-homogeneous map it removes radial gradients at the output, whereas without such an anchor cross-entropy encourages unbounded logit growth (``logit chasing''). We further show that a simple fixed-target auxiliary loss on the pre-logit residual-stream scale provides an explicit alternative anchor and can aid removal of the final normalization layer. Empirically, TaperNorm matches normalized baselines under identical setups while eliminating per-token statistics and enabling these layers to be folded into adjacent linear projections at inference. On an efficiency microbenchmark, folding internal scalings yields up to $1.22\times$ higher throughput in last-token logits mode. These results take a step towards norm-free Transformers while identifying the special role output normalization plays.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æ ‡å‡†åŒ–è¢«å¹¿æ³›è®¤ä¸ºå¯¹äºç¨³å®š Transformer è®­ç»ƒè‡³å…³é‡è¦ã€‚æˆ‘ä»¬é‡æ–°å®¡è§†é¢„è§„èŒƒ Transformer çš„è¿™ä¸€å‡è®¾ï¼Œå¹¶è¯¢é—® Transformer å—å†…éœ€è¦å¤šå¤§ç¨‹åº¦çš„æ ·æœ¬ç›¸å…³è§„èŒƒåŒ–ã€‚æˆ‘ä»¬å¼•å…¥äº† TaperNormï¼Œå®ƒæ˜¯ RMSNorm/LayerNorm çš„ç›´æ¥æ›¿ä»£å“ï¼Œå…¶è¡Œä¸ºåœ¨è®­ç»ƒæ—©æœŸä¸æ ‡å‡†æ ‡å‡†åŒ–å™¨å®Œå…¨ç›¸åŒï¼Œç„¶åå¹³æ»‘åœ°é€æ¸å‡å°åˆ°å­¦ä¹ çš„ä¸æ ·æœ¬æ— å…³çš„çº¿æ€§/ä»¿å°„æ˜ å°„ã€‚åœ¨é—¨é¢„çƒ­æœŸé—´ï¼Œå•ä¸ªå…¨å±€é—¨ä¿æŒåœ¨ $g{=}1$ï¼Œç”¨äºé€šè¿‡ EMA æ ¡å‡†ç¼©æ”¾åˆ†æ”¯ï¼Œç„¶åä½™å¼¦è¡°å‡åˆ° $g{=}0$ï¼Œæ­¤æ—¶æ¯ä¸ªä»£å¸çš„ç»Ÿè®¡æ•°æ®æ¶ˆå¤±ï¼Œæ‰€å¾—çš„å›ºå®šç¼©æ”¾å¯ä»¥æŠ˜å åˆ°ç›¸é‚»çš„çº¿æ€§æŠ•å½±ä¸­ã€‚æˆ‘ä»¬çš„ç†è®ºå’Œå®è¯ç»“æœå°†å°ºåº¦é”šå®šä½œä¸ºè¾“å‡ºå½’ä¸€åŒ–æ‰€å‘æŒ¥çš„å…³é”®ä½œç”¨ï¼šä½œä¸ºï¼ˆæ¥è¿‘ï¼‰$0$åŒè´¨æ˜ å°„ï¼Œå®ƒæ¶ˆé™¤äº†è¾“å‡ºå¤„çš„å¾„å‘æ¢¯åº¦ï¼Œè€Œå¦‚æœæ²¡æœ‰è¿™æ ·çš„é”šå®šï¼Œäº¤å‰ç†µä¼šé¼“åŠ±æ— ç•Œçš„ logit å¢é•¿ï¼ˆâ€œlogit è¿½é€â€ï¼‰ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼Œåœ¨ pre-logit æ®‹å·®æµå°ºåº¦ä¸Šçš„ç®€å•å›ºå®šç›®æ ‡è¾…åŠ©æŸå¤±æä¾›äº†æ˜ç¡®çš„æ›¿ä»£é”šç‚¹ï¼Œå¹¶ä¸”å¯ä»¥å¸®åŠ©å»é™¤æœ€ç»ˆçš„å½’ä¸€åŒ–å±‚ã€‚æ ¹æ®ç»éªŒï¼ŒTaperNorm åœ¨ç›¸åŒçš„è®¾ç½®ä¸‹åŒ¹é…æ ‡å‡†åŒ–åŸºçº¿ï¼ŒåŒæ—¶æ¶ˆé™¤æ¯ä¸ªæ ‡è®°çš„ç»Ÿè®¡æ•°æ®ï¼Œå¹¶ä½¿è¿™äº›å±‚èƒ½å¤Ÿåœ¨æ¨ç†æ—¶æŠ˜å åˆ°ç›¸é‚»çš„çº¿æ€§æŠ•å½±ä¸­ã€‚åœ¨æ•ˆç‡å¾®åŸºå‡†æµ‹è¯•ä¸­ï¼ŒæŠ˜å å†…éƒ¨ç¼©æ”¾åœ¨æœ€åä»¤ç‰Œ logits æ¨¡å¼ä¸‹å¯å¸¦æ¥é«˜è¾¾ $1.22\times$ çš„ååé‡æå‡ã€‚è¿™äº›ç»“æœå‘æ— èŒƒæ•° Transformer è¿ˆå‡ºäº†ä¸€æ­¥ï¼ŒåŒæ—¶ç¡®å®šäº†è¾“å‡ºæ ‡å‡†åŒ–æ‰€æ‰®æ¼”çš„ç‰¹æ®Šè§’è‰²ã€‚

</details>

---

## 100. Deep learning outperforms traditional machine learning methods in predicting childhood malnutrition: evidence from survey data

**ä¸­æ–‡æ ‡é¢˜**: æ·±åº¦å­¦ä¹ åœ¨é¢„æµ‹å„¿ç«¥è¥å…»ä¸è‰¯æ–¹é¢ä¼˜äºä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•ï¼šè°ƒæŸ¥æ•°æ®çš„è¯æ®

**Date**: 2026-02-11 | **arXiv**: [2602.10381v1](http://arxiv.org/abs/2602.10381v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10381v1)

<details><summary><b>Abstract</b></summary>

Childhood malnutrition remains a major public health concern in Nepal and other low-resource settings, while conventional case-finding approaches are labor-intensive and frequently unavailable in remote areas. This study provides the first comprehensive assessment of machine learning and deep learning methodologies for identifying malnutrition among children under five years of age in Nepal. We systematically compared 16 algorithms spanning deep learning, gradient boosting, and traditional machine learning families, using data from the Nepal Multiple Indicator Cluster Survey (MICS) 2019. A composite malnutrition indicator was constructed by integrating stunting, wasting, and underweight status, and model performance was evaluated using ten metrics, with emphasis on F1-score and recall to account for substantial class imbalance and the high cost of failing to detect malnourished children. Among all models, TabNet demonstrated the best performance, likely attributable to its attention-based architecture, and outperformed both support vector machine and AdaBoost classifiers. A consensus feature importance analysis identified maternal education, household wealth index, and child age as the primary predictors of malnutrition, followed by geographic characteristics, vaccination status, and meal frequency. Collectively, these results demonstrate a scalable, survey-based screening framework for identifying children at elevated risk of malnutrition and for guiding targeted nutritional interventions. The proposed approach supports Nepal's progress toward the Sustainable Development Goals and offers a transferable methodological template for similar low-resource settings globally.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å„¿ç«¥è¥å…»ä¸è‰¯ä»ç„¶æ˜¯å°¼æ³Šå°”å’Œå…¶ä»–èµ„æºåŒ®ä¹åœ°åŒºçš„ä¸€ä¸ªä¸»è¦å…¬å…±å«ç”Ÿé—®é¢˜ï¼Œè€Œä¼ ç»Ÿçš„ç—…ä¾‹å‘ç°æ–¹æ³•æ˜¯åŠ³åŠ¨å¯†é›†å‹çš„ï¼Œå¹¶ä¸”åœ¨åè¿œåœ°åŒºå¸¸å¸¸æ— æ³•å®ç°ã€‚è¿™é¡¹ç ”ç©¶é¦–æ¬¡å¯¹æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œä»¥è¯†åˆ«å°¼æ³Šå°”äº”å²ä»¥ä¸‹å„¿ç«¥çš„è¥å…»ä¸è‰¯æƒ…å†µã€‚æˆ‘ä»¬ä½¿ç”¨ 2019 å¹´å°¼æ³Šå°”å¤šæŒ‡æ ‡èšç±»è°ƒæŸ¥ (MICS) çš„æ•°æ®ï¼Œç³»ç»Ÿåœ°æ¯”è¾ƒäº†æ¶µç›–æ·±åº¦å­¦ä¹ ã€æ¢¯åº¦æå‡å’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ ç³»åˆ—çš„ 16 ç§ç®—æ³•ã€‚é€šè¿‡æ•´åˆå‘è‚²è¿Ÿç¼“ã€æ¶ˆç˜¦å’Œä½“é‡ä¸è¶³çŠ¶æ€æ„å»ºäº†ç»¼åˆè¥å…»ä¸è‰¯æŒ‡æ ‡ï¼Œå¹¶ä½¿ç”¨ 10 ä¸ªæŒ‡æ ‡è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œé‡ç‚¹æ˜¯ F1 åˆ†æ•°å’Œå¬å›ç‡ï¼Œä»¥è€ƒè™‘ä¸¥é‡çš„ç±»åˆ«ä¸å¹³è¡¡å’Œæœªèƒ½æ£€æµ‹åˆ°è¥å…»ä¸è‰¯å„¿ç«¥çš„é«˜æ˜‚æˆæœ¬ã€‚åœ¨æ‰€æœ‰æ¨¡å‹ä¸­ï¼ŒTabNet è¡¨ç°å‡ºäº†æœ€å¥½çš„æ€§èƒ½ï¼Œè¿™å¯èƒ½å½’åŠŸäºå…¶åŸºäºæ³¨æ„åŠ›çš„æ¶æ„ï¼Œå¹¶ä¸”ä¼˜äºæ”¯æŒå‘é‡æœºå’Œ AdaBoost åˆ†ç±»å™¨ã€‚å…±è¯†ç‰¹å¾é‡è¦æ€§åˆ†æç¡®å®šå­•äº§å¦‡æ•™è‚²ã€å®¶åº­è´¢å¯ŒæŒ‡æ•°å’Œå„¿ç«¥å¹´é¾„æ˜¯è¥å…»ä¸è‰¯çš„ä¸»è¦é¢„æµ‹å› ç´ ï¼Œå…¶æ¬¡æ˜¯åœ°ç†ç‰¹å¾ã€ç–«è‹—æ¥ç§çŠ¶å†µå’Œè¿›é¤é¢‘ç‡ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™äº›ç»“æœå±•ç¤ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„ã€åŸºäºè°ƒæŸ¥çš„ç­›æŸ¥æ¡†æ¶ï¼Œç”¨äºè¯†åˆ«è¥å…»ä¸è‰¯é£é™©è¾ƒé«˜çš„å„¿ç«¥å¹¶æŒ‡å¯¼æœ‰é’ˆå¯¹æ€§çš„è¥å…»å¹²é¢„æªæ–½ã€‚æ‹Ÿè®®çš„æ–¹æ³•æ”¯æŒå°¼æ³Šå°”åœ¨å®ç°å¯æŒç»­å‘å±•ç›®æ ‡æ–¹é¢å–å¾—è¿›å±•ï¼Œå¹¶ä¸ºå…¨çƒç±»ä¼¼çš„èµ„æºåŒ®ä¹ç¯å¢ƒæä¾›äº†å¯è½¬ç§»çš„æ–¹æ³•æ¨¡æ¿ã€‚

</details>

---

## 101. ENIGMA: EEG-to-Image in 15 Minutes Using Less Than 1% of the Parameters

**ä¸­æ–‡æ ‡é¢˜**: ENIGMAï¼šä½¿ç”¨ä¸åˆ° 1% çš„å‚æ•°åœ¨ 15 åˆ†é’Ÿå†…å°†è„‘ç”µå›¾è½¬æ¢ä¸ºå›¾åƒ

**Date**: 2026-02-10 | **arXiv**: [2602.10361v1](http://arxiv.org/abs/2602.10361v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10361v1)

<details><summary><b>Abstract</b></summary>

To be practical for real-life applications, models for brain-computer interfaces must be easily and quickly deployable on new subjects, effective on affordable scanning hardware, and small enough to run locally on accessible computing resources. To directly address these current limitations, we introduce ENIGMA, a multi-subject electroencephalography (EEG)-to-Image decoding model that reconstructs seen images from EEG recordings and achieves state-of-the-art (SOTA) performance on the research-grade THINGS-EEG2 and consumer-grade AllJoined-1.6M benchmarks, while fine-tuning effectively on new subjects with as little as 15 minutes of data. ENIGMA boasts a simpler architecture and requires less than 1% of the trainable parameters necessary for previous approaches. Our approach integrates a subject-unified spatio-temporal backbone along with a set of multi-subject latent alignment layers and an MLP projector to map raw EEG signals to a rich visual latent space. We evaluate our approach using a broad suite of image reconstruction metrics that have been standardized in the adjacent field of fMRI-to-Image research, and we describe the first EEG-to-Image study to conduct extensive behavioral evaluations of our reconstructions using human raters. Our simple and robust architecture provides a significant performance boost across both research-grade and consumer-grade EEG hardware, and a substantial improvement in fine-tuning efficiency and inference cost. Finally, we provide extensive ablations to determine the architectural choices most responsible for our performance gains in both single and multi-subject cases across multiple benchmark datasets. Collectively, our work provides a substantial step towards the development of practical brain-computer interface applications.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ä¸ºäº†åœ¨ç°å®ç”Ÿæ´»ä¸­åº”ç”¨ï¼Œè„‘æœºæ¥å£æ¨¡å‹å¿…é¡»èƒ½å¤Ÿè½»æ¾å¿«é€Ÿåœ°éƒ¨ç½²åœ¨æ–°çš„ä¸»é¢˜ä¸Šï¼Œåœ¨ç»æµå®æƒ çš„æ‰«æç¡¬ä»¶ä¸Šæœ‰æ•ˆï¼Œå¹¶ä¸”è¶³å¤Ÿå°ä»¥åœ¨å¯è®¿é—®çš„è®¡ç®—èµ„æºä¸Šæœ¬åœ°è¿è¡Œã€‚ä¸ºäº†ç›´æ¥è§£å†³å½“å‰çš„è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº† ENIGMAï¼Œè¿™æ˜¯ä¸€ç§å¤šä¸»ä½“è„‘ç”µå›¾ (EEG) åˆ°å›¾åƒè§£ç æ¨¡å‹ï¼Œå¯ä»¥é‡å»ºä» EEG è®°å½•ä¸­çœ‹åˆ°çš„å›¾åƒï¼Œå¹¶åœ¨ç ”ç©¶çº§ THINGS-EEG2 å’Œæ¶ˆè´¹çº§ AllJoined-1.6M åŸºå‡†ä¸Šå®ç°æœ€å…ˆè¿›çš„ (SOTA) æ€§èƒ½ï¼ŒåŒæ—¶ç”¨çŸ­çŸ­ 15 åˆ†é’Ÿçš„æ•°æ®å¯¹æ–°ä¸»ä½“è¿›è¡Œæœ‰æ•ˆå¾®è°ƒã€‚ ENIGMA æ‹¥æœ‰æ›´ç®€å•çš„æ¶æ„ï¼Œæ‰€éœ€çš„å¯è®­ç»ƒå‚æ•°ä¸åˆ°ä»¥å‰æ–¹æ³•çš„ 1%ã€‚æˆ‘ä»¬çš„æ–¹æ³•é›†æˆäº†ä¸»é¢˜ç»Ÿä¸€çš„æ—¶ç©ºä¸»å¹²ä»¥åŠä¸€ç»„å¤šä¸»é¢˜æ½œåœ¨å¯¹é½å±‚å’Œ MLP æŠ•å½±ä»ªï¼Œä»¥å°†åŸå§‹ EEG ä¿¡å·æ˜ å°„åˆ°ä¸°å¯Œçš„è§†è§‰æ½œåœ¨ç©ºé—´ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ç³»åˆ—å¹¿æ³›çš„å›¾åƒé‡å»ºæŒ‡æ ‡æ¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¿™äº›æŒ‡æ ‡åœ¨åŠŸèƒ½ç£å…±æŒ¯æˆåƒåˆ°å›¾åƒç ”ç©¶çš„ç›¸é‚»é¢†åŸŸå·²ç»æ ‡å‡†åŒ–ï¼Œå¹¶ä¸”æˆ‘ä»¬æè¿°äº†ç¬¬ä¸€ä¸ªè„‘ç”µå›¾åˆ°å›¾åƒç ”ç©¶ï¼Œä»¥ä½¿ç”¨äººç±»è¯„ä¼°è€…å¯¹æˆ‘ä»¬çš„é‡å»ºè¿›è¡Œå¹¿æ³›çš„è¡Œä¸ºè¯„ä¼°ã€‚æˆ‘ä»¬ç®€å•è€Œå¼ºå¤§çš„æ¶æ„æ˜¾ç€æå‡äº†ç ”ç©¶çº§å’Œæ¶ˆè´¹çº§è„‘ç”µå›¾ç¡¬ä»¶çš„æ€§èƒ½ï¼Œå¹¶æ˜¾ç€æé«˜äº†å¾®è°ƒæ•ˆç‡å’Œæ¨ç†æˆæœ¬ã€‚æœ€åï¼Œæˆ‘ä»¬æä¾›äº†å¹¿æ³›çš„æ¶ˆèï¼Œä»¥ç¡®å®šåœ¨è·¨å¤šä¸ªåŸºå‡†æ•°æ®é›†çš„å•ä¸»é¢˜å’Œå¤šä¸»é¢˜æ¡ˆä¾‹ä¸­æœ€èƒ½æé«˜æ€§èƒ½çš„æ¶æ„é€‰æ‹©ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„å·¥ä½œä¸ºå¼€å‘å®ç”¨çš„è„‘æœºæ¥å£åº”ç”¨ç¨‹åºè¿ˆå‡ºäº†å®è´¨æ€§çš„ä¸€æ­¥ã€‚

</details>

---

## 102. Uncertainty-Aware Ordinal Deep Learning for cross-Dataset Diabetic Retinopathy Grading

**ä¸­æ–‡æ ‡é¢˜**: ç”¨äºè·¨æ•°æ®é›†ç³–å°¿ç—…è§†ç½‘è†œç—…å˜åˆ†çº§çš„ä¸ç¡®å®šæ€§æœ‰åºæ·±åº¦å­¦ä¹ 

**Date**: 2026-02-10 | **arXiv**: [2602.10315v1](http://arxiv.org/abs/2602.10315v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10315v1)

<details><summary><b>Abstract</b></summary>

Diabetes mellitus is a chronic metabolic disorder characterized by persistent hyperglycemia due to insufficient insulin production or impaired insulin utilization. One of its most severe complications is diabetic retinopathy (DR), a progressive retinal disease caused by microvascular damage, leading to hemorrhages, exudates, and potential vision loss. Early and reliable detection of DR is therefore critical for preventing irreversible blindness.   In this work, we propose an uncertainty-aware deep learning framework for automated DR severity grading that explicitly models the ordinal nature of disease progression. Our approach combines a convolutional backbone with lesion-query attention pooling and an evidential Dirichlet-based ordinal regression head, enabling both accurate severity prediction and principled estimation of predictive uncertainty. The model is trained using an ordinal evidential loss with annealed regularization to encourage calibrated confidence under domain shift.   We evaluate the proposed method on a multi-domain training setup combining APTOS, Messidor-2, and a subset of EyePACS fundus datasets. Experimental results demonstrate strong cross-dataset generalization, achieving competitive classification accuracy and high quadratic weighted kappa on held-out test sets, while providing meaningful uncertainty estimates for low-confidence cases. These results suggest that ordinal evidential learning is a promising direction for robust and clinically reliable diabetic retinopathy grading.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç³–å°¿ç—…æ˜¯ä¸€ç§æ…¢æ€§ä»£è°¢æ€§ç–¾ç—…ï¼Œå…¶ç‰¹å¾æ˜¯ç”±äºèƒ°å²›ç´ äº§ç”Ÿä¸è¶³æˆ–èƒ°å²›ç´ åˆ©ç”¨å—æŸè€Œå¯¼è‡´æŒç»­é«˜è¡€ç³–ã€‚å…¶æœ€ä¸¥é‡çš„å¹¶å‘ç—‡ä¹‹ä¸€æ˜¯ç³–å°¿ç—…è§†ç½‘è†œç—…å˜ï¼ˆDRï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”±å¾®è¡€ç®¡æŸä¼¤å¼•èµ·çš„è¿›è¡Œæ€§è§†ç½‘è†œç–¾ç—…ï¼Œå¯¼è‡´å‡ºè¡€ã€æ¸—å‡ºå’Œæ½œåœ¨çš„è§†åŠ›ä¸§å¤±ã€‚å› æ­¤ï¼Œæ—©æœŸã€å¯é åœ°æ£€æµ‹ DR å¯¹äºé¢„é˜²ä¸å¯é€†æ€§å¤±æ˜è‡³å…³é‡è¦ã€‚   åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºè‡ªåŠ¨ DR ä¸¥é‡ç¨‹åº¦åˆ†çº§çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ˜ç¡®åœ°æ¨¡æ‹Ÿäº†ç–¾ç—…è¿›å±•çš„é¡ºåºæ€§è´¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†å·ç§¯éª¨å¹²ä¸ç—…å˜æŸ¥è¯¢æ³¨æ„åŠ›æ± å’ŒåŸºäºè¯æ®ç‹„åˆ©å…‹é›·çš„åºæ•°å›å½’å¤´ç›¸ç»“åˆï¼Œä»è€Œå®ç°å‡†ç¡®çš„ä¸¥é‡æ€§é¢„æµ‹å’Œé¢„æµ‹ä¸ç¡®å®šæ€§çš„åŸåˆ™ä¼°è®¡ã€‚è¯¥æ¨¡å‹ä½¿ç”¨åºæ•°è¯æ®æŸå¤±å’Œé€€ç«æ­£åˆ™åŒ–è¿›è¡Œè®­ç»ƒï¼Œä»¥é¼“åŠ±åŸŸè½¬ç§»ä¸‹çš„æ ¡å‡†ç½®ä¿¡åº¦ã€‚   æˆ‘ä»¬åœ¨ç»“åˆ APTOSã€Messidor-2 å’Œ EyePACS çœ¼åº•æ•°æ®é›†å­é›†çš„å¤šåŸŸè®­ç»ƒè®¾ç½®ä¸Šè¯„ä¼°æ‰€æå‡ºçš„æ–¹æ³•ã€‚å®éªŒç»“æœè¯æ˜äº†å¼ºå¤§çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨ä¿ç•™çš„æµ‹è¯•é›†ä¸Šå®ç°äº†æœ‰ç«äº‰åŠ›çš„åˆ†ç±»ç²¾åº¦å’Œé«˜äºŒæ¬¡åŠ æƒ kappaï¼ŒåŒæ—¶ä¸ºä½ç½®ä¿¡åº¦æ¡ˆä¾‹æä¾›äº†æœ‰æ„ä¹‰çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæœ‰åºè¯æ®å­¦ä¹ æ˜¯ç¨³å¥ä¸”ä¸´åºŠå¯é çš„ç³–å°¿ç—…è§†ç½‘è†œç—…å˜åˆ†çº§çš„ä¸€ä¸ªæœ‰å‰é€”çš„æ–¹å‘ã€‚

</details>

---

## 103. ERGO: Excess-Risk-Guided Optimization for High-Fidelity Monocular 3D Gaussian Splatting

**ä¸­æ–‡æ ‡é¢˜**: ERGOï¼šé«˜ä¿çœŸå•ç›® 3D é«˜æ–¯æ³¼æº…çš„è¿‡åº¦é£é™©å¼•å¯¼ä¼˜åŒ–

**Date**: 2026-02-10 | **arXiv**: [2602.10278v1](http://arxiv.org/abs/2602.10278v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10278v1)

<details><summary><b>Abstract</b></summary>

Generating 3D content from a single image remains a fundamentally challenging and ill-posed problem due to the inherent absence of geometric and textural information in occluded regions. While state-of-the-art generative models can synthesize auxiliary views to provide additional supervision, these views inevitably contain geometric inconsistencies and textural misalignments that propagate and amplify artifacts during 3D reconstruction. To effectively harness these imperfect supervisory signals, we propose an adaptive optimization framework guided by excess risk decomposition, termed ERGO. Specifically, ERGO decomposes the optimization losses in 3D Gaussian splatting into two components, i.e., excess risk that quantifies the suboptimality gap between current and optimal parameters, and Bayes error that models the irreducible noise inherent in synthesized views. This decomposition enables ERGO to dynamically estimate the view-specific excess risk and adaptively adjust loss weights during optimization. Furthermore, we introduce geometry-aware and texture-aware objectives that complement the excess-risk-derived weighting mechanism, establishing a synergistic global-local optimization paradigm. Consequently, ERGO demonstrates robustness against supervision noise while consistently enhancing both geometric fidelity and textural quality of the reconstructed 3D content. Extensive experiments on the Google Scanned Objects dataset and the OmniObject3D dataset demonstrate the superiority of ERGO over existing state-of-the-art methods.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç”±äºé®æŒ¡åŒºåŸŸå›ºæœ‰åœ°ç¼ºä¹å‡ ä½•å’Œçº¹ç†ä¿¡æ¯ï¼Œä»å•ä¸ªå›¾åƒç”Ÿæˆ 3D å†…å®¹ä»ç„¶æ˜¯ä¸€ä¸ªæ ¹æœ¬æ€§çš„æŒ‘æˆ˜å’Œä¸é€‚å®šé—®é¢˜ã€‚è™½ç„¶æœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹å¯ä»¥åˆæˆè¾…åŠ©è§†å›¾æ¥æä¾›é¢å¤–çš„ç›‘ç£ï¼Œä½†è¿™äº›è§†å›¾ä¸å¯é¿å…åœ°åŒ…å«å‡ ä½•ä¸ä¸€è‡´å’Œçº¹ç†é”™ä½ï¼Œè¿™äº›ä¸ä¸€è‡´å’Œçº¹ç†é”™ä½ä¼šåœ¨ 3D é‡å»ºè¿‡ç¨‹ä¸­ä¼ æ’­å’Œæ”¾å¤§ä¼ªå½±ã€‚ä¸ºäº†æœ‰æ•ˆåœ°åˆ©ç”¨è¿™äº›ä¸å®Œå–„çš„ç›‘ç®¡ä¿¡å·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»¥è¿‡åº¦é£é™©åˆ†è§£ä¸ºæŒ‡å¯¼çš„è‡ªé€‚åº”ä¼˜åŒ–æ¡†æ¶ï¼Œç§°ä¸º ERGOã€‚å…·ä½“æ¥è¯´ï¼ŒERGO å°† 3D é«˜æ–¯åˆ†å¸ƒä¸­çš„ä¼˜åŒ–æŸå¤±åˆ†è§£ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼Œå³é‡åŒ–å½“å‰å‚æ•°å’Œæœ€ä¼˜å‚æ•°ä¹‹é—´çš„æ¬¡ä¼˜å·®è·çš„è¶…é¢é£é™©ï¼Œä»¥åŠå¯¹åˆæˆè§†å›¾ä¸­å›ºæœ‰çš„ä¸å¯çº¦å™ªå£°è¿›è¡Œå»ºæ¨¡çš„è´å¶æ–¯è¯¯å·®ã€‚è¿™ç§åˆ†è§£ä½¿ ERGO èƒ½å¤ŸåŠ¨æ€ä¼°è®¡è§†å›¾ç‰¹å®šçš„è¶…é¢é£é™©ï¼Œå¹¶åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­è‡ªé€‚åº”è°ƒæ•´æŸå¤±æƒé‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å‡ ä½•æ„ŸçŸ¥å’Œçº¹ç†æ„ŸçŸ¥ç›®æ ‡ï¼Œè¡¥å……äº†è¿‡åº¦é£é™©è¡ç”Ÿçš„åŠ æƒæœºåˆ¶ï¼Œå»ºç«‹äº†ååŒçš„å…¨å±€å±€éƒ¨ä¼˜åŒ–èŒƒä¾‹ã€‚å› æ­¤ï¼ŒERGO å±•ç¤ºäº†é’ˆå¯¹ç›‘ç£å™ªå£°çš„é²æ£’æ€§ï¼ŒåŒæ—¶æŒç»­å¢å¼ºé‡å»º 3D å†…å®¹çš„å‡ ä½•ä¿çœŸåº¦å’Œçº¹ç†è´¨é‡ã€‚å¯¹ Google Scanned Objects æ•°æ®é›†å’Œ OmniObject3D æ•°æ®é›†çš„å¤§é‡å®éªŒè¯æ˜äº† ERGO ç›¸å¯¹äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚

</details>

---

## 104. Colorimeter-Supervised Skin Tone Estimation from Dermatoscopic Images for Fairness Auditing

**ä¸­æ–‡æ ‡é¢˜**: è‰²åº¦è®¡ç›‘ç£ä¸‹çš„çš®è‚¤é•œå›¾åƒè‚¤è‰²ä¼°è®¡ï¼Œç”¨äºå…¬å¹³æ€§å®¡æ ¸

**Date**: 2026-02-10 | **arXiv**: [2602.10265v1](http://arxiv.org/abs/2602.10265v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10265v1)

<details><summary><b>Abstract</b></summary>

Neural-network-based diagnosis from dermatoscopic images is increasingly used for clinical decision support, yet studies report performance disparities across skin tones. Fairness auditing of these models is limited by the lack of reliable skin-tone annotations in public dermatoscopy datasets. We address this gap with neural networks that predict Fitzpatrick skin type via ordinal regression and the Individual Typology Angle (ITA) via color regression, using in-person Fitzpatrick labels and colorimeter measurements as targets. We further leverage extensive pretraining on synthetic and real dermatoscopic and clinical images. The Fitzpatrick model achieves agreement comparable to human crowdsourced annotations, and ITA predictions show high concordance with colorimeter-derived ITA, substantially outperforming pixel-averaging approaches. Applying these estimators to ISIC 2020 and MILK10k, we find that fewer than 1% of subjects belong to Fitzpatrick types V and VI. We release code and pretrained models as an open-source tool for rapid skin-tone annotation and bias auditing. This is, to our knowledge, the first dermatoscopic skin-tone estimation neural network validated against colorimeter measurements, and it supports growing evidence of clinically relevant performance gaps across skin-tone groups.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åŸºäºçš®è‚¤é•œå›¾åƒçš„ç¥ç»ç½‘ç»œè¯Šæ–­è¶Šæ¥è¶Šå¤šåœ°ç”¨äºä¸´åºŠå†³ç­–æ”¯æŒï¼Œä½†ç ”ç©¶æŠ¥å‘Šäº†ä¸åŒè‚¤è‰²çš„æ€§èƒ½å·®å¼‚ã€‚ç”±äºå…¬å…±çš®è‚¤é•œæ•°æ®é›†ä¸­ç¼ºä¹å¯é çš„è‚¤è‰²æ³¨é‡Šï¼Œè¿™äº›æ¨¡å‹çš„å…¬å¹³æ€§å®¡æ ¸å—åˆ°é™åˆ¶ã€‚æˆ‘ä»¬é€šè¿‡ç¥ç»ç½‘ç»œè§£å†³äº†è¿™ä¸€å·®è·ï¼Œè¯¥ç¥ç»ç½‘ç»œé€šè¿‡åºæ•°å›å½’é¢„æµ‹è²èŒ¨å¸•ç‰¹é‡Œå…‹çš®è‚¤ç±»å‹ï¼Œå¹¶é€šè¿‡é¢œè‰²å›å½’é¢„æµ‹ä¸ªä½“ç±»å‹å­¦è§’åº¦ï¼ˆITAï¼‰ï¼Œå¹¶ä½¿ç”¨ç°åœºè²èŒ¨å¸•ç‰¹é‡Œå…‹æ ‡ç­¾å’Œè‰²åº¦è®¡æµ‹é‡ä½œä¸ºç›®æ ‡ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åˆ©ç”¨å¯¹åˆæˆå’ŒçœŸå®çš®è‚¤é•œå’Œä¸´åºŠå›¾åƒçš„å¹¿æ³›é¢„è®­ç»ƒã€‚ Fitzpatrick æ¨¡å‹è¾¾åˆ°äº†ä¸äººç±»ä¼—åŒ…æ³¨é‡Šç›¸å½“çš„ä¸€è‡´æ€§ï¼ŒITA é¢„æµ‹ä¸è‰²åº¦è®¡è¡ç”Ÿçš„ ITA é«˜åº¦ä¸€è‡´ï¼Œå¤§å¤§ä¼˜äºåƒç´ å¹³å‡æ–¹æ³•ã€‚å°†è¿™äº›ä¼°è®¡é‡åº”ç”¨äº ISIC 2020 å’Œ MILK10kï¼Œæˆ‘ä»¬å‘ç°ä¸åˆ° 1% çš„å—è¯•è€…å±äº Fitzpatrick V å‹å’Œ VI å‹ã€‚æˆ‘ä»¬å‘å¸ƒä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºå¼€æºå·¥å…·ï¼Œç”¨äºå¿«é€Ÿè‚¤è‰²æ³¨é‡Šå’Œåå·®å®¡æ ¸ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹è‰²åº¦è®¡æµ‹é‡è¿›è¡ŒéªŒè¯çš„çš®è‚¤é•œè‚¤è‰²ä¼°è®¡ç¥ç»ç½‘ç»œï¼Œå¹¶ä¸”å®ƒæ”¯æŒäº†è¶Šæ¥è¶Šå¤šçš„è¯æ®è¡¨æ˜ä¸åŒè‚¤è‰²ç¾¤ä½“ä¹‹é—´çš„ä¸´åºŠç›¸å…³æ€§èƒ½å·®è·ã€‚

</details>

---

## 105. DEGMC: Denoising Diffusion Models Based on Riemannian Equivariant Group Morphological Convolutions

**ä¸­æ–‡æ ‡é¢˜**: DEGMCï¼šåŸºäºé»æ›¼ç­‰å˜ç¾¤å½¢æ€å·ç§¯çš„å»å™ªæ‰©æ•£æ¨¡å‹

**Date**: 2026-02-10 | **arXiv**: [2602.10221v1](http://arxiv.org/abs/2602.10221v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10221v1)

<details><summary><b>Abstract</b></summary>

In this work, we address two major issues in recent Denoising Diffusion Probabilistic Models (DDPM): {\bf 1)} geometric key feature extraction and {\bf 2)} network equivariance. Since the DDPM prediction network relies on the U-net architecture, which is theoretically only translation equivariant, we introduce a geometric approach combined with an equivariance property of the more general Euclidean group, which includes rotations, reflections, and permutations. We introduce the notion of group morphological convolutions in Riemannian manifolds, which are derived from the viscosity solutions of first-order Hamilton-Jacobi-type partial differential equations (PDEs) that act as morphological multiscale dilations and erosions. We add a convection term to the model and solve it using the method of characteristics. This helps us better capture nonlinearities, represent thin geometric structures, and incorporate symmetries into the learning process. Experimental results on the MNIST, RotoMNIST, and CIFAR-10 datasets show noticeable improvements compared to the baseline DDPM model.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†æœ€è¿‘çš„å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ä¸­çš„ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼š{\bf 1)}å‡ ä½•å…³é”®ç‰¹å¾æå–å’Œ{\bf 2)}ç½‘ç»œç­‰æ–¹å·®ã€‚ç”±äº DDPM é¢„æµ‹ç½‘ç»œä¾èµ–äº U-net æ¶æ„ï¼Œç†è®ºä¸Šåªæ˜¯å¹³ç§»ç­‰å˜ï¼Œå› æ­¤æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¸æ›´ä¸€èˆ¬çš„æ¬§å‡ é‡Œå¾—ç¾¤çš„ç­‰å˜å±æ€§ç›¸ç»“åˆçš„å‡ ä½•æ–¹æ³•ï¼Œå…¶ä¸­åŒ…æ‹¬æ—‹è½¬ã€åå°„å’Œæ’åˆ—ã€‚æˆ‘ä»¬åœ¨é»æ›¼æµå½¢ä¸­å¼•å…¥ç¾¤å½¢æ€å·ç§¯çš„æ¦‚å¿µï¼Œè¯¥æ¦‚å¿µæºè‡ªä¸€é˜¶ Hamilton-Jacobi å‹åå¾®åˆ†æ–¹ç¨‹ (PDE) çš„ç²˜åº¦è§£ï¼Œå……å½“å½¢æ€å¤šå°ºåº¦è†¨èƒ€å’Œè…èš€ã€‚æˆ‘ä»¬åœ¨æ¨¡å‹ä¸­æ·»åŠ å¯¹æµé¡¹å¹¶ä½¿ç”¨ç‰¹å¾æ–¹æ³•æ±‚è§£ã€‚è¿™æœ‰åŠ©äºæˆ‘ä»¬æ›´å¥½åœ°æ•æ‰éçº¿æ€§ï¼Œè¡¨ç¤ºè–„çš„å‡ ä½•ç»“æ„ï¼Œå¹¶å°†å¯¹ç§°æ€§çº³å…¥å­¦ä¹ è¿‡ç¨‹ã€‚ä¸åŸºçº¿ DDPM æ¨¡å‹ç›¸æ¯”ï¼ŒMNISTã€RotoMNIST å’Œ CIFAR-10 æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºå‡ºæ˜¾ç€çš„æ”¹è¿›ã€‚

</details>

---

## 106. When the Prompt Becomes Visual: Vision-Centric Jailbreak Attacks for Large Image Editing Models

**ä¸­æ–‡æ ‡é¢˜**: å½“æç¤ºå˜å¾—å¯è§†åŒ–æ—¶ï¼šé’ˆå¯¹å¤§å›¾åƒç¼–è¾‘æ¨¡å‹çš„ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„è¶Šç‹±æ”»å‡»

**Date**: 2026-02-10 | **arXiv**: [2602.10179v1](http://arxiv.org/abs/2602.10179v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10179v1)

<details><summary><b>Abstract</b></summary>

Recent advances in large image editing models have shifted the paradigm from text-driven instructions to vision-prompt editing, where user intent is inferred directly from visual inputs such as marks, arrows, and visual-text prompts. While this paradigm greatly expands usability, it also introduces a critical and underexplored safety risk: the attack surface itself becomes visual. In this work, we propose Vision-Centric Jailbreak Attack (VJA), the first visual-to-visual jailbreak attack that conveys malicious instructions purely through visual inputs. To systematically study this emerging threat, we introduce IESBench, a safety-oriented benchmark for image editing models. Extensive experiments on IESBench demonstrate that VJA effectively compromises state-of-the-art commercial models, achieving attack success rates of up to 80.9% on Nano Banana Pro and 70.1% on GPT-Image-1.5. To mitigate this vulnerability, we propose a training-free defense based on introspective multimodal reasoning, which substantially improves the safety of poorly aligned models to a level comparable with commercial systems, without auxiliary guard models and with negligible computational overhead. Our findings expose new vulnerabilities, provide both a benchmark and practical defense to advance safe and trustworthy modern image editing systems. Warning: This paper contains offensive images created by large image editing models.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¤§å‹å›¾åƒç¼–è¾‘æ¨¡å‹çš„æœ€æ–°è¿›å±•å·²å°†èŒƒå¼ä»æ–‡æœ¬é©±åŠ¨æŒ‡ä»¤è½¬å˜ä¸ºè§†è§‰æç¤ºç¼–è¾‘ï¼Œå…¶ä¸­ç”¨æˆ·æ„å›¾ç›´æ¥ä»è§†è§‰è¾“å…¥ï¼ˆä¾‹å¦‚æ ‡è®°ã€ç®­å¤´å’Œè§†è§‰æ–‡æœ¬æç¤ºï¼‰æ¨æ–­å‡ºæ¥ã€‚è™½ç„¶è¿™ç§èŒƒä¾‹æå¤§åœ°æ‰©å±•äº†å¯ç”¨æ€§ï¼Œä½†å®ƒä¹Ÿå¼•å…¥äº†ä¸€ä¸ªå…³é”®çš„ä¸”å°šæœªå……åˆ†æ¢ç´¢çš„å®‰å…¨é£é™©ï¼šæ”»å‡»é¢æœ¬èº«å˜å¾—å¯è§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„è¶Šç‹±æ”»å‡»ï¼ˆVJAï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªçº¯ç²¹é€šè¿‡è§†è§‰è¾“å…¥ä¼ è¾¾æ¶æ„æŒ‡ä»¤çš„è§†è§‰åˆ°è§†è§‰è¶Šç‹±æ”»å‡»ã€‚ä¸ºäº†ç³»ç»Ÿåœ°ç ”ç©¶è¿™ä¸€æ–°å…´å¨èƒï¼Œæˆ‘ä»¬å¼•å…¥äº† IESBenchï¼Œè¿™æ˜¯ä¸€ç§é¢å‘å›¾åƒç¼–è¾‘æ¨¡å‹çš„å®‰å…¨åŸºå‡†ã€‚ IESBench ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVJA æœ‰æ•ˆåœ°ç ´åäº†æœ€å…ˆè¿›çš„å•†ä¸šæ¨¡å‹ï¼Œåœ¨ Nano Banana Pro ä¸Šå®ç°äº†é«˜è¾¾ 80.9% çš„æ”»å‡»æˆåŠŸç‡ï¼Œåœ¨ GPT-Image-1.5 ä¸Šå®ç°äº†é«˜è¾¾ 70.1% çš„æ”»å‡»æˆåŠŸç‡ã€‚ä¸ºäº†å‡è½»è¿™ä¸ªæ¼æ´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå†…çœå¤šæ¨¡æ€æ¨ç†çš„å…è®­ç»ƒé˜²å¾¡ï¼Œå®ƒå¤§å¤§æé«˜äº†å¯¹é½ä¸è‰¯æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œè¾¾åˆ°ä¸å•†ä¸šç³»ç»Ÿç›¸å½“çš„æ°´å¹³ï¼Œæ— éœ€è¾…åŠ©é˜²æŠ¤æ¨¡å‹ï¼Œè®¡ç®—å¼€é”€å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæš´éœ²äº†æ–°çš„æ¼æ´ï¼Œä¸ºæ¨è¿›å®‰å…¨å¯é çš„ç°ä»£å›¾åƒç¼–è¾‘ç³»ç»Ÿæä¾›äº†åŸºå‡†å’Œå®é™…é˜²å¾¡ã€‚è­¦å‘Šï¼šæœ¬æ–‡åŒ…å«ç”±å¤§å‹å›¾åƒç¼–è¾‘æ¨¡å‹åˆ›å»ºçš„ä»¤äººåæ„Ÿçš„å›¾åƒã€‚

</details>

---

## 107. SAGE: Scalable Agentic 3D Scene Generation for Embodied AI

**ä¸­æ–‡æ ‡é¢˜**: SAGEï¼šç”¨äºåµŒå…¥å¼ AI çš„å¯æ‰©å±•ä»£ç† 3D åœºæ™¯ç”Ÿæˆ

**Date**: 2026-02-10 | **arXiv**: [2602.10116v1](http://arxiv.org/abs/2602.10116v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10116v1)

<details><summary><b>Abstract</b></summary>

Real-world data collection for embodied agents remains costly and unsafe, calling for scalable, realistic, and simulator-ready 3D environments. However, existing scene-generation systems often rely on rule-based or task-specific pipelines, yielding artifacts and physically invalid scenes. We present SAGE, an agentic framework that, given a user-specified embodied task (e.g., "pick up a bowl and place it on the table"), understands the intent and automatically generates simulation-ready environments at scale. The agent couples multiple generators for layout and object composition with critics that evaluate semantic plausibility, visual realism, and physical stability. Through iterative reasoning and adaptive tool selection, it self-refines the scenes until meeting user intent and physical validity. The resulting environments are realistic, diverse, and directly deployable in modern simulators for policy training. Policies trained purely on this data exhibit clear scaling trends and generalize to unseen objects and layouts, demonstrating the promise of simulation-driven scaling for embodied AI. Code, demos, and the SAGE-10k dataset can be found on the project page here: https://nvlabs.github.io/sage.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å®ä½“ä»£ç†çš„çœŸå®ä¸–ç•Œæ•°æ®æ”¶é›†ä»ç„¶æ˜‚è´µä¸”ä¸å®‰å…¨ï¼Œéœ€è¦å¯æ‰©å±•ã€çœŸå®ä¸”å¯ç”¨äºæ¨¡æ‹Ÿå™¨çš„ 3D ç¯å¢ƒã€‚ç„¶è€Œï¼Œç°æœ‰çš„åœºæ™¯ç”Ÿæˆç³»ç»Ÿé€šå¸¸ä¾èµ–äºåŸºäºè§„åˆ™æˆ–ç‰¹å®šäºä»»åŠ¡çš„ç®¡é“ï¼Œä»è€Œäº§ç”Ÿä¼ªåƒå’Œç‰©ç†ä¸Šæ— æ•ˆçš„åœºæ™¯ã€‚æˆ‘ä»¬æå‡ºäº† SAGEï¼Œä¸€ä¸ªä»£ç†æ¡†æ¶ï¼Œç»™å®šç”¨æˆ·æŒ‡å®šçš„å…·ä½“ä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼Œâ€œæ‹¿èµ·ä¸€ä¸ªç¢—å¹¶å°†å…¶æ”¾åœ¨æ¡Œå­ä¸Šâ€ï¼‰ï¼Œå®ƒå¯ä»¥ç†è§£æ„å›¾å¹¶è‡ªåŠ¨å¤§è§„æ¨¡ç”Ÿæˆæ¨¡æ‹Ÿå°±ç»ªç¯å¢ƒã€‚è¯¥ä»£ç†å°†å¤šä¸ªç”¨äºå¸ƒå±€å’Œå¯¹è±¡ç»„åˆçš„ç”Ÿæˆå™¨ä¸è¯„ä¼°è¯­ä¹‰åˆç†æ€§ã€è§†è§‰çœŸå®æ€§å’Œç‰©ç†ç¨³å®šæ€§çš„è¯„è®ºå®¶ç»“åˆèµ·æ¥ã€‚é€šè¿‡è¿­ä»£æ¨ç†å’Œè‡ªé€‚åº”å·¥å…·é€‰æ‹©ï¼Œå®ƒå¯ä»¥è‡ªæˆ‘å®Œå–„åœºæ™¯ï¼Œç›´åˆ°æ»¡è¶³ç”¨æˆ·æ„å›¾å’Œç‰©ç†æœ‰æ•ˆæ€§ã€‚ç”±æ­¤äº§ç”Ÿçš„ç¯å¢ƒæ˜¯çœŸå®çš„ã€å¤šæ ·åŒ–çš„ï¼Œå¹¶ä¸”å¯ä»¥ç›´æ¥éƒ¨ç½²åœ¨ç°ä»£æ¨¡æ‹Ÿå™¨ä¸­è¿›è¡Œæ”¿ç­–åŸ¹è®­ã€‚çº¯ç²¹åŸºäºè¿™äº›æ•°æ®è®­ç»ƒçš„ç­–ç•¥è¡¨ç°å‡ºæ˜æ˜¾çš„æ‰©å±•è¶‹åŠ¿ï¼Œå¹¶æ¨å¹¿åˆ°çœ‹ä¸è§çš„å¯¹è±¡å’Œå¸ƒå±€ï¼Œå±•ç¤ºäº†æ¨¡æ‹Ÿé©±åŠ¨çš„æ‰©å±•å¯¹å…·ä½“äººå·¥æ™ºèƒ½çš„å‰æ™¯ã€‚ä»£ç ã€æ¼”ç¤ºå’Œ SAGE-10k æ•°æ®é›†å¯ä»¥åœ¨é¡¹ç›®é¡µé¢ä¸Šæ‰¾åˆ°ï¼šhttps://nvlabs.github.io/sageã€‚

</details>

---

## 108. Quantum Multiple Rotation Averaging

**ä¸­æ–‡æ ‡é¢˜**: é‡å­å¤šæ¬¡æ—‹è½¬å¹³å‡

**Date**: 2026-02-10 | **arXiv**: [2602.10115v1](http://arxiv.org/abs/2602.10115v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10115v1)

<details><summary><b>Abstract</b></summary>

Multiple rotation averaging (MRA) is a fundamental optimization problem in 3D vision and robotics that aims to recover globally consistent absolute rotations from noisy relative measurements. Established classical methods, such as L1-IRLS and Shonan, face limitations including local minima susceptibility and reliance on convex relaxations that fail to preserve the exact manifold geometry, leading to reduced accuracy in high-noise scenarios. We introduce IQARS (Iterative Quantum Annealing for Rotation Synchronization), the first algorithm that reformulates MRA as a sequence of local quadratic non-convex sub-problems executable on quantum annealers after binarization, to leverage inherent hardware advantages. IQARS removes convex relaxation dependence and better preserves non-Euclidean rotation manifold geometry while leveraging quantum tunneling and parallelism for efficient solution space exploration. We evaluate IQARS's performance on synthetic and real-world datasets. While current annealers remain in their nascent phase and only support solving problems of limited scale with constrained performance, we observed that IQARS on D-Wave annealers can already achieve ca. 12% higher accuracy than Shonan, i.e., the best-performing classical method evaluated empirically.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¤šæ¬¡æ—‹è½¬å¹³å‡ (MRA) æ˜¯ 3D è§†è§‰å’Œæœºå™¨äººæŠ€æœ¯ä¸­çš„ä¸€ä¸ªåŸºæœ¬ä¼˜åŒ–é—®é¢˜ï¼Œæ—¨åœ¨ä»å˜ˆæ‚çš„ç›¸å¯¹æµ‹é‡ä¸­æ¢å¤å…¨å±€ä¸€è‡´çš„ç»å¯¹æ—‹è½¬ã€‚å·²å»ºç«‹çš„ç»å…¸æ–¹æ³•ï¼ˆä¾‹å¦‚ L1-IRLS å’Œ Shonanï¼‰é¢ä¸´å±€é™æ€§ï¼ŒåŒ…æ‹¬å±€éƒ¨æœ€å°å€¼æ•æ„Ÿæ€§å’Œå¯¹å‡¸æ¾å¼›çš„ä¾èµ–ï¼Œè€Œå‡¸æ¾å¼›æ— æ³•ä¿ç•™ç²¾ç¡®çš„æµå½¢å‡ ä½•å½¢çŠ¶ï¼Œä»è€Œå¯¼è‡´é«˜å™ªå£°åœºæ™¯ä¸­çš„ç²¾åº¦é™ä½ã€‚æˆ‘ä»¬å¼•å…¥äº† IQARSï¼ˆæ—‹è½¬åŒæ­¥è¿­ä»£é‡å­é€€ç«ï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°† MRA é‡æ–°è¡¨è¿°ä¸ºäºŒå€¼åŒ–ååœ¨é‡å­é€€ç«å™¨ä¸Šå¯æ‰§è¡Œçš„å±€éƒ¨äºŒæ¬¡éå‡¸å­é—®é¢˜åºåˆ—çš„ç®—æ³•ï¼Œä»¥åˆ©ç”¨å›ºæœ‰çš„ç¡¬ä»¶ä¼˜åŠ¿ã€‚ IQARS æ¶ˆé™¤äº†å‡¸æ¾å¼›ä¾èµ–æ€§å¹¶æ›´å¥½åœ°ä¿ç•™äº†éæ¬§å‡ é‡Œå¾·æ—‹è½¬æµå½¢å‡ ä½•å½¢çŠ¶ï¼ŒåŒæ—¶åˆ©ç”¨é‡å­éš§é“å’Œå¹¶è¡Œæ€§è¿›è¡Œæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆç©ºé—´æ¢ç´¢ã€‚æˆ‘ä»¬è¯„ä¼° IQARS åœ¨åˆæˆæ•°æ®é›†å’ŒçœŸå®æ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚è™½ç„¶å½“å‰çš„é€€ç«å™¨ä»å¤„äºèµ·æ­¥é˜¶æ®µï¼Œä»…æ”¯æŒè§£å†³è§„æ¨¡æœ‰é™ä¸”æ€§èƒ½æœ‰é™çš„é—®é¢˜ï¼Œä½†æˆ‘ä»¬è§‚å¯Ÿåˆ° D-Wave é€€ç«å™¨ä¸Šçš„ IQARS å·²ç»å¯ä»¥å®ç°çº¦ 100% çš„æ€§èƒ½ã€‚æ¯”æ¹˜å—ï¼ˆæ ¹æ®ç»éªŒè¯„ä¼°çš„è¡¨ç°æœ€å¥½çš„ç»å…¸æ–¹æ³•ï¼‰å‡†ç¡®ç‡é«˜ 12%ã€‚

</details>

---

## 109. ConsID-Gen: View-Consistent and Identity-Preserving Image-to-Video Generation

**ä¸­æ–‡æ ‡é¢˜**: ConsID-Genï¼šè§†å›¾ä¸€è‡´ä¸”ä¿ç•™èº«ä»½çš„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆ

**Date**: 2026-02-10 | **arXiv**: [2602.10113v1](http://arxiv.org/abs/2602.10113v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10113v1)

<details><summary><b>Abstract</b></summary>

Image-to-Video generation (I2V) animates a static image into a temporally coherent video sequence following textual instructions, yet preserving fine-grained object identity under changing viewpoints remains a persistent challenge. Unlike text-to-video models, existing I2V pipelines often suffer from appearance drift and geometric distortion, artifacts we attribute to the sparsity of single-view 2D observations and weak cross-modal alignment. Here we address this problem from both data and model perspectives. First, we curate ConsIDVid, a large-scale object-centric dataset built with a scalable pipeline for high-quality, temporally aligned videos, and establish ConsIDVid-Bench, where we present a novel benchmarking and evaluation framework for multi-view consistency using metrics sensitive to subtle geometric and appearance deviations. We further propose ConsID-Gen, a view-assisted I2V generation framework that augments the first frame with unposed auxiliary views and fuses semantic and structural cues via a dual-stream visual-geometric encoder as well as a text-visual connector, yielding unified conditioning for a Diffusion Transformer backbone. Experiments across ConsIDVid-Bench demonstrate that ConsID-Gen consistently outperforms in multiple metrics, with the best overall performance surpassing leading video generation models like Wan2.1 and HunyuanVideo, delivering superior identity fidelity and temporal coherence under challenging real-world scenarios. We will release our model and dataset at https://myangwu.github.io/ConsID-Gen.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆ (I2V) å°†é™æ€å›¾åƒæŒ‰ç…§æ–‡æœ¬æŒ‡ä»¤åŠ¨ç”»åŒ–ä¸ºæ—¶é—´è¿è´¯çš„è§†é¢‘åºåˆ—ï¼Œä½†åœ¨ä¸æ–­å˜åŒ–çš„è§†ç‚¹ä¸‹ä¿ç•™ç»†ç²’åº¦çš„å¯¹è±¡èº«ä»½ä»ç„¶æ˜¯ä¸€ä¸ªæŒç»­çš„æŒ‘æˆ˜ã€‚ä¸æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ä¸åŒï¼Œç°æœ‰çš„ I2V ç®¡é“ç»å¸¸é­å—å¤–è§‚æ¼‚ç§»å’Œå‡ ä½•å¤±çœŸçš„å½±å“ï¼Œæˆ‘ä»¬å°†è¿™äº›ä¼ªå½±å½’å› äºå•è§†å›¾ 2D è§‚å¯Ÿçš„ç¨€ç–æ€§å’Œå¼±çš„è·¨æ¨¡å¼å¯¹é½ã€‚è¿™é‡Œæˆ‘ä»¬ä»æ•°æ®å’Œæ¨¡å‹ä¸¤ä¸ªè§’åº¦æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ç­–åˆ’ ConsIDVidï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥å¯æ‰©å±•ç®¡é“æ„å»ºçš„å¤§è§„æ¨¡ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„æ•°æ®é›†ï¼Œç”¨äºé«˜è´¨é‡ã€æ—¶é—´å¯¹é½çš„è§†é¢‘ï¼Œå¹¶å»ºç«‹ ConsIDVid-Benchï¼Œåœ¨å…¶ä¸­æˆ‘ä»¬ä½¿ç”¨å¯¹ç»†å¾®å‡ ä½•å’Œå¤–è§‚åå·®æ•æ„Ÿçš„æŒ‡æ ‡ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šè§†å›¾ä¸€è‡´æ€§åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æ¡†æ¶ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡º ConsID-Genï¼Œä¸€ç§è§†å›¾è¾…åŠ©çš„ I2V ç”Ÿæˆæ¡†æ¶ï¼Œå®ƒä½¿ç”¨æœªè®¾ç½®çš„è¾…åŠ©è§†å›¾å¢å¼ºç¬¬ä¸€å¸§ï¼Œå¹¶é€šè¿‡åŒæµè§†è§‰å‡ ä½•ç¼–ç å™¨ä»¥åŠæ–‡æœ¬è§†è§‰è¿æ¥å™¨èåˆè¯­ä¹‰å’Œç»“æ„çº¿ç´¢ï¼Œä»è€Œä¸º Diffusion Transformer ä¸»å¹²äº§ç”Ÿç»Ÿä¸€æ¡ä»¶ã€‚ ConsIDVid-Bench çš„å®éªŒè¡¨æ˜ï¼ŒConsID-Gen åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå§‹ç»ˆè¡¨ç°å‡ºè‰²ï¼Œå…¶æœ€ä½³æ•´ä½“æ€§èƒ½è¶…è¶Šäº† Wan2.1 å’Œ HunyuanVideo ç­‰é¢†å…ˆçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç°å®åœºæ™¯ä¸‹æä¾›å“è¶Šçš„èº«ä»½ä¿çœŸåº¦å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å°†åœ¨ https://myangwu.github.io/ConsID-Gen å‘å¸ƒæˆ‘ä»¬çš„æ¨¡å‹å’Œæ•°æ®é›†ã€‚

</details>

---

## 110. Olaf-World: Orienting Latent Actions for Video World Modeling

**ä¸­æ–‡æ ‡é¢˜**: Olaf-Worldï¼šå®šå‘è§†é¢‘ä¸–ç•Œå»ºæ¨¡çš„æ½œåœ¨åŠ¨ä½œ

**Date**: 2026-02-10 | **arXiv**: [2602.10104v1](http://arxiv.org/abs/2602.10104v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10104v1)

<details><summary><b>Abstract</b></summary>

Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to extract control interfaces from unlabeled video, learned latents often fail to transfer across contexts: they entangle scene-specific cues and lack a shared coordinate system. This occurs because standard objectives operate only within each clip, providing no mechanism to align action semantics across contexts. Our key insight is that although actions are unobserved, their semantic effects are observable and can serve as a shared reference. We introduce Seq$Î”$-REPA, a sequence-level control-effect alignment objective that anchors integrated latent action to temporal feature differences from a frozen, self-supervised video encoder. Building on this, we present Olaf-World, a pipeline that pretrains action-conditioned video world models from large-scale passive video. Extensive experiments demonstrate that our method learns a more structured latent action space, leading to stronger zero-shot action transfer and more data-efficient adaptation to new control interfaces than state-of-the-art baselines.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åŠ¨ä½œå¯æ§ä¸–ç•Œæ¨¡å‹çš„æ‰©å±•å—åˆ°åŠ¨ä½œæ ‡ç­¾ç¨€ç¼ºçš„é™åˆ¶ã€‚è™½ç„¶æ½œåœ¨åŠ¨ä½œå­¦ä¹ æœ‰æœ›ä»æœªæ ‡è®°çš„è§†é¢‘ä¸­æå–æ§åˆ¶ç•Œé¢ï¼Œä½†å­¦ä¹ åˆ°çš„æ½œåœ¨åŠ¨ä½œé€šå¸¸æ— æ³•è·¨ä¸Šä¸‹æ–‡è¿ç§»ï¼šå®ƒä»¬çº ç¼ äº†ç‰¹å®šäºåœºæ™¯çš„çº¿ç´¢å¹¶ä¸”ç¼ºä¹å…±äº«çš„åæ ‡ç³»ã€‚å‘ç”Ÿè¿™ç§æƒ…å†µæ˜¯å› ä¸ºæ ‡å‡†ç›®æ ‡ä»…åœ¨æ¯ä¸ªå‰ªè¾‘å†…è¿è¡Œï¼Œæ²¡æœ‰æä¾›è·¨ä¸Šä¸‹æ–‡å¯¹é½åŠ¨ä½œè¯­ä¹‰çš„æœºåˆ¶ã€‚æˆ‘ä»¬çš„ä¸»è¦è§è§£æ˜¯ï¼Œè™½ç„¶åŠ¨ä½œæ˜¯ä¸å¯è§‚å¯Ÿçš„ï¼Œä½†å®ƒä»¬çš„è¯­ä¹‰æ•ˆæœæ˜¯å¯è§‚å¯Ÿçš„å¹¶ä¸”å¯ä»¥ä½œä¸ºå…±äº«å‚è€ƒã€‚æˆ‘ä»¬å¼•å…¥äº† Seq$Î”$-REPAï¼Œè¿™æ˜¯ä¸€ç§åºåˆ—çº§æ§åˆ¶æ•ˆæœå¯¹é½ç›®æ ‡ï¼Œå®ƒå°†é›†æˆçš„æ½œåœ¨åŠ¨ä½œé”šå®šåˆ°æ¥è‡ªå†»ç»“çš„è‡ªç›‘ç£è§†é¢‘ç¼–ç å™¨çš„æ—¶é—´ç‰¹å¾å·®å¼‚ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº† Olaf-Worldï¼Œè¿™æ˜¯ä¸€ä¸ªä»å¤§è§„æ¨¡è¢«åŠ¨è§†é¢‘ä¸­é¢„è®­ç»ƒåŠ¨ä½œæ¡ä»¶è§†é¢‘ä¸–ç•Œæ¨¡å‹çš„ç®¡é“ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å­¦ä¹ äº†ä¸€ä¸ªæ›´åŠ ç»“æ„åŒ–çš„æ½œåœ¨åŠ¨ä½œç©ºé—´ï¼Œä¸æœ€å…ˆè¿›çš„åŸºçº¿ç›¸æ¯”ï¼Œå¯ä»¥å®ç°æ›´å¼ºçš„é›¶æ ·æœ¬åŠ¨ä½œè½¬ç§»å’Œæ›´é«˜æ•ˆçš„æ•°æ®é€‚åº”æ–°çš„æ§åˆ¶ç•Œé¢ã€‚

</details>

---

## 111. VideoWorld 2: Learning Transferable Knowledge from Real-world Videos

**ä¸­æ–‡æ ‡é¢˜**: VideoWorld 2ï¼šä»ç°å®ä¸–ç•Œçš„è§†é¢‘ä¸­å­¦ä¹ å¯è½¬ç§»çš„çŸ¥è¯†

**Date**: 2026-02-10 | **arXiv**: [2602.10102v1](http://arxiv.org/abs/2602.10102v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10102v1)

<details><summary><b>Abstract</b></summary>

Learning transferable knowledge from unlabeled video data and applying it in new environments is a fundamental capability of intelligent agents. This work presents VideoWorld 2, which extends VideoWorld and offers the first investigation into learning transferable knowledge directly from raw real-world videos. At its core, VideoWorld 2 introduces a dynamic-enhanced Latent Dynamics Model (dLDM) that decouples action dynamics from visual appearance: a pretrained video diffusion model handles visual appearance modeling, enabling the dLDM to learn latent codes that focus on compact and meaningful task-related dynamics. These latent codes are then modeled autoregressively to learn task policies and support long-horizon reasoning. We evaluate VideoWorld 2 on challenging real-world handcraft making tasks, where prior video generation and latent-dynamics models struggle to operate reliably. Remarkably, VideoWorld 2 achieves up to 70% improvement in task success rate and produces coherent long execution videos. In robotics, we show that VideoWorld 2 can acquire effective manipulation knowledge from the Open-X dataset, which substantially improves task performance on CALVIN. This study reveals the potential of learning transferable world knowledge directly from raw videos, with all code, data, and models to be open-sourced for further research.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ä»æœªæ ‡è®°çš„è§†é¢‘æ•°æ®ä¸­å­¦ä¹ å¯è½¬ç§»çš„çŸ¥è¯†å¹¶å°†å…¶åº”ç”¨åˆ°æ–°ç¯å¢ƒä¸­æ˜¯æ™ºèƒ½ä»£ç†çš„åŸºæœ¬èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œæå‡ºäº† VideoWorld 2ï¼Œå®ƒæ‰©å±•äº† VideoWorldï¼Œå¹¶é¦–æ¬¡å¯¹ç›´æ¥ä»åŸå§‹ç°å®ä¸–ç•Œè§†é¢‘ä¸­å­¦ä¹ å¯è½¬ç§»çŸ¥è¯†è¿›è¡Œäº†ç ”ç©¶ã€‚ VideoWorld 2 çš„æ ¸å¿ƒå¼•å…¥äº†åŠ¨æ€å¢å¼ºçš„æ½œåœ¨åŠ¨æ€æ¨¡å‹ (dLDM)ï¼Œå®ƒå°†åŠ¨ä½œåŠ¨æ€ä¸è§†è§‰å¤–è§‚åˆ†ç¦»ï¼šé¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹å¤„ç†è§†è§‰å¤–è§‚å»ºæ¨¡ï¼Œä½¿ dLDM èƒ½å¤Ÿå­¦ä¹ ä¸“æ³¨äºç´§å‡‘ä¸”æœ‰æ„ä¹‰çš„ä»»åŠ¡ç›¸å…³åŠ¨æ€çš„æ½œåœ¨ä»£ç ã€‚ç„¶åå¯¹è¿™äº›æ½œåœ¨ä»£ç è¿›è¡Œè‡ªå›å½’å»ºæ¨¡ï¼Œä»¥å­¦ä¹ ä»»åŠ¡ç­–ç•¥å¹¶æ”¯æŒé•¿æœŸæ¨ç†ã€‚æˆ‘ä»¬åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç°å®ä¸–ç•Œæ‰‹å·¥åˆ¶ä½œä»»åŠ¡ä¸­è¯„ä¼°äº† VideoWorld 2ï¼Œå…¶ä¸­å…ˆå‰çš„è§†é¢‘ç”Ÿæˆå’Œæ½œåœ¨åŠ¨æ€æ¨¡å‹éš¾ä»¥å¯é è¿è¡Œã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒVideoWorld 2 å°†ä»»åŠ¡æˆåŠŸç‡æé«˜äº† 70%ï¼Œå¹¶ç”Ÿæˆè¿è´¯çš„é•¿æ‰§è¡Œè§†é¢‘ã€‚åœ¨æœºå™¨äººæŠ€æœ¯ä¸­ï¼Œæˆ‘ä»¬è¯æ˜ VideoWorld 2 å¯ä»¥ä» Open-X æ•°æ®é›†ä¸­è·å–æœ‰æ•ˆçš„æ“ä½œçŸ¥è¯†ï¼Œè¿™å¤§å¤§æé«˜äº† CALVIN ä¸Šçš„ä»»åŠ¡æ€§èƒ½ã€‚è¿™é¡¹ç ”ç©¶æ­ç¤ºäº†ç›´æ¥ä»åŸå§‹è§†é¢‘ä¸­å­¦ä¹ å¯è½¬ç§»çš„ä¸–ç•ŒçŸ¥è¯†çš„æ½œåŠ›ï¼Œæ‰€æœ‰ä»£ç ã€æ•°æ®å’Œæ¨¡å‹éƒ½å°†å¼€æºä»¥ä¾›è¿›ä¸€æ­¥ç ”ç©¶ã€‚

</details>

---

## 112. VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model

**ä¸­æ–‡æ ‡é¢˜**: VLA-JEPAï¼šåˆ©ç”¨æ½œåœ¨ä¸–ç•Œæ¨¡å‹å¢å¼ºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹

**Date**: 2026-02-10 | **arXiv**: [2602.10098v1](http://arxiv.org/abs/2602.10098v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10098v1)

<details><summary><b>Abstract</b></summary>

Pretraining Vision-Language-Action (VLA) policies on internet-scale video is appealing, yet current latent-action objectives often learn the wrong thing: they remain anchored to pixel variation rather than action-relevant state transitions, making them vulnerable to appearance bias, nuisance motion, and information leakage. We introduce VLA-JEPA, a JEPA-style pretraining framework that sidesteps these pitfalls by design. The key idea is \emph{leakage-free state prediction}: a target encoder produces latent representations from future frames, while the student pathway sees only the current observation -- future information is used solely as supervision targets, never as input. By predicting in latent space rather than pixel space, VLA-JEPA learns dynamics abstractions that are robust to camera motion and irrelevant background changes. This yields a simple two-stage recipe -- JEPA pretraining followed by action-head fine-tuning -- without the multi-stage complexity of prior latent-action pipelines. Experiments on LIBERO, LIBERO-Plus, SimplerEnv and real-world manipulation tasks show that VLA-JEPA achieves consistent gains in generalization and robustness over existing methods.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åœ¨äº’è”ç½‘è§„æ¨¡çš„è§†é¢‘ä¸Šé¢„è®­ç»ƒè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰ç­–ç•¥å¾ˆæœ‰å¸å¼•åŠ›ï¼Œä½†å½“å‰çš„æ½œåœ¨åŠ¨ä½œç›®æ ‡ç»å¸¸å­¦åˆ°é”™è¯¯çš„ä¸œè¥¿ï¼šå®ƒä»¬ä»ç„¶é”šå®šäºåƒç´ å˜åŒ–è€Œä¸æ˜¯ä¸åŠ¨ä½œç›¸å…³çš„çŠ¶æ€è½¬æ¢ï¼Œè¿™ä½¿å¾—å®ƒä»¬å®¹æ˜“å—åˆ°å¤–è§‚åå·®ã€ä»¤äººè®¨åŒçš„è¿åŠ¨å’Œä¿¡æ¯æ³„æ¼çš„å½±å“ã€‚æˆ‘ä»¬å¼•å…¥äº† VLA-JEPAï¼Œè¿™æ˜¯ä¸€ç§ JEPA é£æ ¼çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œå®ƒé€šè¿‡è®¾è®¡é¿å¼€äº†è¿™äº›é™·é˜±ã€‚å…³é”®æ€æƒ³æ˜¯ \emph{æ— æ³„æ¼çŠ¶æ€é¢„æµ‹}ï¼šç›®æ ‡ç¼–ç å™¨ä»æœªæ¥å¸§ç”Ÿæˆæ½œåœ¨è¡¨ç¤ºï¼Œè€Œå­¦ç”Ÿè·¯å¾„åªèƒ½çœ‹åˆ°å½“å‰çš„è§‚å¯Ÿç»“æœ - æœªæ¥ä¿¡æ¯ä»…ç”¨ä½œç›‘ç£ç›®æ ‡ï¼Œä»ä¸ç”¨ä½œè¾“å…¥ã€‚é€šè¿‡åœ¨æ½œåœ¨ç©ºé—´è€Œä¸æ˜¯åƒç´ ç©ºé—´ä¸­è¿›è¡Œé¢„æµ‹ï¼ŒVLA-JEPA å­¦ä¹ äº†å¯¹ç›¸æœºè¿åŠ¨å’Œä¸ç›¸å…³èƒŒæ™¯å˜åŒ–å…·æœ‰é²æ£’æ€§çš„åŠ¨æ€æŠ½è±¡ã€‚è¿™äº§ç”Ÿäº†ä¸€ä¸ªç®€å•çš„ä¸¤é˜¶æ®µé…æ–¹â€”â€”JEPA é¢„è®­ç»ƒï¼Œç„¶åæ˜¯åŠ¨ä½œå¤´å¾®è°ƒâ€”â€”æ²¡æœ‰å…ˆå‰æ½œåœ¨åŠ¨ä½œç®¡é“çš„å¤šé˜¶æ®µå¤æ‚æ€§ã€‚å¯¹ LIBEROã€LIBERO-Plusã€SimplerEnv å’Œç°å®ä¸–ç•Œæ“ä½œä»»åŠ¡çš„å®éªŒè¡¨æ˜ï¼ŒVLA-JEPA åœ¨æ³›åŒ–æ€§å’Œé²æ£’æ€§æ–¹é¢æ¯”ç°æœ‰æ–¹æ³•å–å¾—äº†ä¸€è‡´çš„è¿›æ­¥ã€‚

</details>

---

## 113. Causality in Video Diffusers is Separable from Denoising

**ä¸­æ–‡æ ‡é¢˜**: è§†é¢‘æ‰©æ•£å™¨ä¸­çš„å› æœå…³ç³»ä¸å»å™ªæ˜¯å¯åˆ†ç¦»çš„

**Date**: 2026-02-10 | **arXiv**: [2602.10095v1](http://arxiv.org/abs/2602.10095v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10095v1)

<details><summary><b>Abstract</b></summary>

Causality -- referring to temporal, uni-directional cause-effect relationships between components -- underlies many complex generative processes, including videos, language, and robot trajectories. Current causal diffusion models entangle temporal reasoning with iterative denoising, applying causal attention across all layers, at every denoising step, and over the entire context. In this paper, we show that the causal reasoning in these models is separable from the multi-step denoising process. Through systematic probing of autoregressive video diffusers, we uncover two key regularities: (1) early layers produce highly similar features across denoising steps, indicating redundant computation along the diffusion trajectory; and (2) deeper layers exhibit sparse cross-frame attention and primarily perform intra-frame rendering. Motivated by these findings, we introduce Separable Causal Diffusion (SCD), a new architecture that explicitly decouples once-per-frame temporal reasoning, via a causal transformer encoder, from multi-step frame-wise rendering, via a lightweight diffusion decoder. Extensive experiments on both pretraining and post-training tasks across synthetic and real benchmarks show that SCD significantly improves throughput and per-frame latency while matching or surpassing the generation quality of strong causal diffusion baselines.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å› æœå…³ç³»â€”â€”æŒ‡çš„æ˜¯ç»„ä»¶ä¹‹é—´çš„æ—¶é—´æ€§ã€å•å‘å› æœå…³ç³»â€”â€”æ˜¯è®¸å¤šå¤æ‚ç”Ÿæˆè¿‡ç¨‹çš„åŸºç¡€ï¼ŒåŒ…æ‹¬è§†é¢‘ã€è¯­è¨€å’Œæœºå™¨äººè½¨è¿¹ã€‚å½“å‰çš„å› æœæ‰©æ•£æ¨¡å‹å°†æ—¶é—´æ¨ç†ä¸è¿­ä»£å»å™ªç»“åˆèµ·æ¥ï¼Œåœ¨æ‰€æœ‰å±‚ã€æ¯ä¸ªå»å™ªæ­¥éª¤ä»¥åŠæ•´ä¸ªä¸Šä¸‹æ–‡ä¸­åº”ç”¨å› æœæ³¨æ„åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯æ˜è¿™äº›æ¨¡å‹ä¸­çš„å› æœæ¨ç†ä¸å¤šæ­¥éª¤å»å™ªè¿‡ç¨‹æ˜¯å¯åˆ†ç¦»çš„ã€‚é€šè¿‡å¯¹è‡ªå›å½’è§†é¢‘æ‰©æ•£å™¨çš„ç³»ç»Ÿæ¢æµ‹ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸¤ä¸ªå…³é”®è§„å¾‹ï¼šï¼ˆ1ï¼‰æ—©æœŸå±‚åœ¨å»å™ªæ­¥éª¤ä¸­äº§ç”Ÿé«˜åº¦ç›¸ä¼¼çš„ç‰¹å¾ï¼Œè¡¨æ˜æ²¿æ‰©æ•£è½¨è¿¹çš„å†—ä½™è®¡ç®—ï¼› ï¼ˆ2ï¼‰æ›´æ·±çš„å±‚è¡¨ç°å‡ºç¨€ç–çš„è·¨å¸§æ³¨æ„åŠ›ï¼Œå¹¶ä¸”ä¸»è¦æ‰§è¡Œå¸§å†…æ¸²æŸ“ã€‚å—è¿™äº›å‘ç°çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯åˆ†ç¦»å› æœæ‰©æ•£ï¼ˆSCDï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°æ¶æ„ï¼Œå®ƒé€šè¿‡å› æœå˜æ¢ç¼–ç å™¨å°†æ¯å¸§ä¸€æ¬¡çš„æ—¶é—´æ¨ç†ä¸é€šè¿‡è½»é‡çº§æ‰©æ•£è§£ç å™¨çš„å¤šæ­¥é€å¸§æ¸²æŸ“æ˜ç¡®è§£è€¦ã€‚å¯¹åˆæˆåŸºå‡†å’ŒçœŸå®åŸºå‡†çš„è®­ç»ƒå‰å’Œè®­ç»ƒåä»»åŠ¡è¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSCD æ˜¾ç€æé«˜äº†ååé‡å’Œæ¯å¸§å»¶è¿Ÿï¼ŒåŒæ—¶åŒ¹é…æˆ–è¶…è¶Šäº†å¼ºå› æœæ‰©æ•£åŸºå‡†çš„ç”Ÿæˆè´¨é‡ã€‚

</details>

---

## 114. 4RC: 4D Reconstruction via Conditional Querying Anytime and Anywhere

**ä¸­æ–‡æ ‡é¢˜**: 4RCï¼šéšæ—¶éšåœ°æ¡ä»¶æŸ¥è¯¢4Dé‡å»º

**Date**: 2026-02-10 | **arXiv**: [2602.10094v1](http://arxiv.org/abs/2602.10094v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10094v1)

<details><summary><b>Abstract</b></summary>

We present 4RC, a unified feed-forward framework for 4D reconstruction from monocular videos. Unlike existing approaches that typically decouple motion from geometry or produce limited 4D attributes such as sparse trajectories or two-view scene flow, 4RC learns a holistic 4D representation that jointly captures dense scene geometry and motion dynamics. At its core, 4RC introduces a novel encode-once, query-anywhere and anytime paradigm: a transformer backbone encodes the entire video into a compact spatio-temporal latent space, from which a conditional decoder can efficiently query 3D geometry and motion for any query frame at any target timestamp. To facilitate learning, we represent per-view 4D attributes in a minimally factorized form by decomposing them into base geometry and time-dependent relative motion. Extensive experiments demonstrate that 4RC outperforms prior and concurrent methods across a wide range of 4D reconstruction tasks.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æˆ‘ä»¬æå‡ºäº† 4RCï¼Œä¸€ä¸ªç”¨äºå•ç›®è§†é¢‘ 4D é‡å»ºçš„ç»Ÿä¸€å‰é¦ˆæ¡†æ¶ã€‚ä¸é€šå¸¸å°†è¿åŠ¨ä¸å‡ ä½•ä½“è§£è€¦æˆ–äº§ç”Ÿæœ‰é™çš„ 4D å±æ€§ï¼ˆä¾‹å¦‚ç¨€ç–è½¨è¿¹æˆ–åŒè§†å›¾åœºæ™¯æµï¼‰çš„ç°æœ‰æ–¹æ³•ä¸åŒï¼Œ4RC å­¦ä¹ è”åˆæ•è·å¯†é›†åœºæ™¯å‡ ä½•ä½“å’Œè¿åŠ¨åŠ¨åŠ›å­¦çš„æ•´ä½“ 4D è¡¨ç¤ºã€‚ 4RC çš„æ ¸å¿ƒå¼•å…¥äº†ä¸€ç§æ–°é¢–çš„ä¸€æ¬¡ç¼–ç ã€éšæ—¶éšåœ°æŸ¥è¯¢çš„èŒƒä¾‹ï¼šå˜å‹å™¨ä¸»å¹²å°†æ•´ä¸ªè§†é¢‘ç¼–ç åˆ°ç´§å‡‘çš„æ—¶ç©ºæ½œåœ¨ç©ºé—´ä¸­ï¼Œæ¡ä»¶è§£ç å™¨å¯ä»¥ä»ä¸­æœ‰æ•ˆåœ°æŸ¥è¯¢ä»»ä½•ç›®æ ‡æ—¶é—´æˆ³å¤„ä»»ä½•æŸ¥è¯¢å¸§çš„ 3D å‡ ä½•å’Œè¿åŠ¨ã€‚ä¸ºäº†ä¾¿äºå­¦ä¹ ï¼Œæˆ‘ä»¬é€šè¿‡å°†æ¯ä¸ªè§†å›¾çš„ 4D å±æ€§åˆ†è§£ä¸ºåŸºæœ¬å‡ ä½•å½¢çŠ¶å’Œä¸æ—¶é—´ç›¸å…³çš„ç›¸å¯¹è¿åŠ¨ï¼Œä»¥æœ€å°åˆ†è§£å½¢å¼è¡¨ç¤ºå®ƒä»¬ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œ4RC åœ¨å„ç§ 4D é‡å»ºä»»åŠ¡ä¸­å‡ä¼˜äºå…ˆå‰æ–¹æ³•å’Œå¹¶å‘æ–¹æ³•ã€‚

</details>

---

## 115. Can Image Splicing and Copy-Move Forgery Be Detected by the Same Model? Forensim: An Attention-Based State-Space Approach

**ä¸­æ–‡æ ‡é¢˜**: å›¾åƒæ‹¼æ¥å’Œå¤åˆ¶ç§»åŠ¨ä¼ªé€ å¯ä»¥ç”¨åŒä¸€æ¨¡å‹æ£€æµ‹å—ï¼Ÿ Forensimï¼šåŸºäºæ³¨æ„åŠ›çš„çŠ¶æ€ç©ºé—´æ–¹æ³•

**Date**: 2026-02-10 | **arXiv**: [2602.10079v1](http://arxiv.org/abs/2602.10079v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10079v1)

<details><summary><b>Abstract</b></summary>

We introduce Forensim, an attention-based state-space framework for image forgery detection that jointly localizes both manipulated (target) and source regions. Unlike traditional approaches that rely solely on artifact cues to detect spliced or forged areas, Forensim is designed to capture duplication patterns crucial for understanding context. In scenarios such as protest imagery, detecting only the forged region, for example a duplicated act of violence inserted into a peaceful crowd, can mislead interpretation, highlighting the need for joint source-target localization. Forensim outputs three-class masks (pristine, source, target) and supports detection of both splicing and copy-move forgeries within a unified architecture. We propose a visual state-space model that leverages normalized attention maps to identify internal similarities, paired with a region-based block attention module to distinguish manipulated regions. This design enables end-to-end training and precise localization. Forensim achieves state-of-the-art performance on standard benchmarks. We also release CMFD-Anything, a new dataset addressing limitations of existing copy-move forgery datasets.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æˆ‘ä»¬å¼•å…¥äº† Forensimï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„çŠ¶æ€ç©ºé—´æ¡†æ¶ï¼Œç”¨äºå›¾åƒä¼ªé€ æ£€æµ‹ï¼Œå¯è”åˆå®šä½æ“çºµï¼ˆç›®æ ‡ï¼‰åŒºåŸŸå’ŒæºåŒºåŸŸã€‚ä¸ä»…ä¾é å·¥ä»¶çº¿ç´¢æ¥æ£€æµ‹æ‹¼æ¥æˆ–ä¼ªé€ åŒºåŸŸçš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒForensim æ—¨åœ¨æ•è·å¯¹äºç†è§£ä¸Šä¸‹æ–‡è‡³å…³é‡è¦çš„é‡å¤æ¨¡å¼ã€‚åœ¨æŠ—è®®å›¾åƒç­‰åœºæ™¯ä¸­ï¼Œä»…æ£€æµ‹ä¼ªé€ åŒºåŸŸï¼ˆä¾‹å¦‚ï¼Œåœ¨å’Œå¹³äººç¾¤ä¸­æ’å…¥é‡å¤çš„æš´åŠ›è¡Œä¸ºï¼‰å¯èƒ½ä¼šè¯¯å¯¼è§£é‡Šï¼Œä»è€Œå‡¸æ˜¾äº†è”åˆæºç›®æ ‡å®šä½çš„å¿…è¦æ€§ã€‚ Forensim è¾“å‡ºä¸‰ç±»æ©ç ï¼ˆåŸå§‹ã€æºã€ç›®æ ‡ï¼‰ï¼Œå¹¶æ”¯æŒåœ¨ç»Ÿä¸€æ¶æ„å†…æ£€æµ‹æ‹¼æ¥å’Œå¤åˆ¶ç§»åŠ¨ä¼ªé€ ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è§†è§‰çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œåˆ©ç”¨å½’ä¸€åŒ–æ³¨æ„åŠ›å›¾æ¥è¯†åˆ«å†…éƒ¨ç›¸ä¼¼æ€§ï¼Œå¹¶ä¸åŸºäºåŒºåŸŸçš„å—æ³¨æ„åŠ›æ¨¡å—é…å¯¹æ¥åŒºåˆ†æ“çºµåŒºåŸŸã€‚è¿™ç§è®¾è®¡å¯ä»¥å®ç°ç«¯åˆ°ç«¯è®­ç»ƒå’Œç²¾ç¡®å®šä½ã€‚ Forensim åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº† CMFD-Anythingï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œè§£å†³äº†ç°æœ‰å¤åˆ¶-ç§»åŠ¨ä¼ªé€ æ•°æ®é›†çš„å±€é™æ€§ã€‚

</details>

---

## 116. Spatio-Temporal Attention for Consistent Video Semantic Segmentation in Automated Driving

**ä¸­æ–‡æ ‡é¢˜**: è‡ªåŠ¨é©¾é©¶ä¸­ä¸€è‡´è§†é¢‘è¯­ä¹‰åˆ†å‰²çš„æ—¶ç©ºæ³¨æ„åŠ›

**Date**: 2026-02-10 | **arXiv**: [2602.10052v1](http://arxiv.org/abs/2602.10052v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10052v1)

<details><summary><b>Abstract</b></summary>

Deep neural networks, especially transformer-based architectures, have achieved remarkable success in semantic segmentation for environmental perception. However, existing models process video frames independently, thus failing to leverage temporal consistency, which could significantly improve both accuracy and stability in dynamic scenes. In this work, we propose a Spatio-Temporal Attention (STA) mechanism that extends transformer attention blocks to incorporate multi-frame context, enabling robust temporal feature representations for video semantic segmentation. Our approach modifies standard self-attention to process spatio-temporal feature sequences while maintaining computational efficiency and requiring minimal changes to existing architectures. STA demonstrates broad applicability across diverse transformer architectures and remains effective across both lightweight and larger-scale models. A comprehensive evaluation on the Cityscapes and BDD100k datasets shows substantial improvements of 9.20 percentage points in temporal consistency metrics and up to 1.76 percentage points in mean intersection over union compared to single-frame baselines. These results demonstrate STA as an effective architectural enhancement for video-based semantic segmentation applications.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æ·±åº¦ç¥ç»ç½‘ç»œï¼Œç‰¹åˆ«æ˜¯åŸºäº Transformer çš„æ¶æ„ï¼Œåœ¨ç¯å¢ƒæ„ŸçŸ¥çš„è¯­ä¹‰åˆ†å‰²æ–¹é¢å–å¾—äº†æ˜¾ç€çš„æˆåŠŸã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹ç‹¬ç«‹å¤„ç†è§†é¢‘å¸§ï¼Œå› æ­¤æ— æ³•åˆ©ç”¨æ—¶é—´ä¸€è‡´æ€§ï¼Œè€Œæ—¶é—´ä¸€è‡´æ€§å¯ä»¥æ˜¾ç€æé«˜åŠ¨æ€åœºæ™¯çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ—¶ç©ºæ³¨æ„åŠ›ï¼ˆSTAï¼‰æœºåˆ¶ï¼Œè¯¥æœºåˆ¶æ‰©å±•äº†å˜å‹å™¨æ³¨æ„å—ä»¥åˆå¹¶å¤šå¸§ä¸Šä¸‹æ–‡ï¼Œä»è€Œä¸ºè§†é¢‘è¯­ä¹‰åˆ†å‰²æä¾›äº†é²æ£’çš„æ—¶é—´ç‰¹å¾è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¿®æ”¹äº†æ ‡å‡†çš„è‡ªæ³¨æ„åŠ›æ¥å¤„ç†æ—¶ç©ºç‰¹å¾åºåˆ—ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡å¹¶ä¸”éœ€è¦å¯¹ç°æœ‰æ¶æ„è¿›è¡Œæœ€å°çš„æ›´æ”¹ã€‚ STA åœ¨ä¸åŒçš„å˜å‹å™¨æ¶æ„ä¸­å±•ç¤ºäº†å¹¿æ³›çš„é€‚ç”¨æ€§ï¼Œå¹¶ä¸”åœ¨è½»é‡çº§å’Œå¤§å‹æ¨¡å‹ä¸­ä»ç„¶æœ‰æ•ˆã€‚å¯¹ Cityscapes å’Œ BDD100k æ•°æ®é›†çš„ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼Œä¸å•å¸§åŸºçº¿ç›¸æ¯”ï¼Œè”åˆçš„æ—¶é—´ä¸€è‡´æ€§æŒ‡æ ‡æ˜¾ç€æé«˜äº† 9.20 ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹³å‡äº¤é›†æé«˜äº† 1.76 ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™äº›ç»“æœè¡¨æ˜ STA æ˜¯åŸºäºè§†é¢‘çš„è¯­ä¹‰åˆ†å‰²åº”ç”¨ç¨‹åºçš„æœ‰æ•ˆæ¶æ„å¢å¼ºã€‚

</details>

---

## 117. Fake-HR1: Rethinking Reasoning of Vision Language Model for Synthetic Image Detection

**ä¸­æ–‡æ ‡é¢˜**: Fake-HR1ï¼šé‡æ–°æ€è€ƒç”¨äºåˆæˆå›¾åƒæ£€æµ‹çš„è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†

**Date**: 2026-02-10 | **arXiv**: [2602.10042v2](http://arxiv.org/abs/2602.10042v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.10042v2)

<details><summary><b>Abstract</b></summary>

Recent studies have demonstrated that incorporating Chain-of-Thought (CoT) reasoning into the detection process can enhance a model's ability to detect synthetic images. However, excessively lengthy reasoning incurs substantial resource overhead, including token consumption and latency, which is particularly redundant when handling obviously generated forgeries. To address this issue, we propose Fake-HR1, a large-scale hybrid-reasoning model that, to the best of our knowledge, is the first to adaptively determine whether reasoning is necessary based on the characteristics of the generative detection task. To achieve this, we design a two-stage training framework: we first perform Hybrid Fine-Tuning (HFT) for cold-start initialization, followed by online reinforcement learning with Hybrid-Reasoning Grouped Policy Optimization (HGRPO) to implicitly learn when to select an appropriate reasoning mode. Experimental results show that Fake-HR1 adaptively performs reasoning across different types of queries, surpassing existing LLMs in both reasoning ability and generative detection performance, while significantly improving response efficiency.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå°†æ€æƒ³é“¾ (CoT) æ¨ç†çº³å…¥æ£€æµ‹è¿‡ç¨‹å¯ä»¥å¢å¼ºæ¨¡å‹æ£€æµ‹åˆæˆå›¾åƒçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿‡é•¿çš„æ¨ç†ä¼šå¸¦æ¥å¤§é‡çš„èµ„æºå¼€é”€ï¼ŒåŒ…æ‹¬ä»¤ç‰Œæ¶ˆè€—å’Œå»¶è¿Ÿï¼Œè¿™åœ¨å¤„ç†æ˜æ˜¾ç”Ÿæˆçš„ä¼ªé€ æ—¶å°¤å…¶å¤šä½™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº† Fake-HR1ï¼Œè¿™æ˜¯ä¸€ç§å¤§è§„æ¨¡æ··åˆæ¨ç†æ¨¡å‹ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªæ ¹æ®ç”Ÿæˆæ£€æµ‹ä»»åŠ¡çš„ç‰¹å¾è‡ªé€‚åº”åœ°ç¡®å®šæ˜¯å¦éœ€è¦æ¨ç†çš„æ¨¡å‹ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è®­ç»ƒæ¡†æ¶ï¼šæˆ‘ä»¬é¦–å…ˆæ‰§è¡Œæ··åˆå¾®è°ƒï¼ˆHFTï¼‰è¿›è¡Œå†·å¯åŠ¨åˆå§‹åŒ–ï¼Œç„¶åä½¿ç”¨æ··åˆæ¨ç†åˆ†ç»„ç­–ç•¥ä¼˜åŒ–ï¼ˆHGRPOï¼‰è¿›è¡Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œä»¥éšå¼å­¦ä¹ ä½•æ—¶é€‰æ‹©åˆé€‚çš„æ¨ç†æ¨¡å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFake-HR1èƒ½å¤Ÿè‡ªé€‚åº”åœ°è·¨ä¸åŒç±»å‹çš„æŸ¥è¯¢è¿›è¡Œæ¨ç†ï¼Œåœ¨æ¨ç†èƒ½åŠ›å’Œç”Ÿæˆæ£€æµ‹æ€§èƒ½ä¸Šéƒ½è¶…è¶Šäº†ç°æœ‰çš„LLMï¼ŒåŒæ—¶æ˜¾ç€æé«˜äº†å“åº”æ•ˆç‡ã€‚

</details>

---

## 118. Faster-GS: Analyzing and Improving Gaussian Splatting Optimization

**ä¸­æ–‡æ ‡é¢˜**: Faster-GSï¼šåˆ†æå’Œæ”¹è¿›é«˜æ–¯æ³¼æº…ä¼˜åŒ–

**Date**: 2026-02-10 | **arXiv**: [2602.09999v1](http://arxiv.org/abs/2602.09999v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09999v1)

<details><summary><b>Abstract</b></summary>

Recent advances in 3D Gaussian Splatting (3DGS) have focused on accelerating optimization while preserving reconstruction quality. However, many proposed methods entangle implementation-level improvements with fundamental algorithmic modifications or trade performance for fidelity, leading to a fragmented research landscape that complicates fair comparison. In this work, we consolidate and evaluate the most effective and broadly applicable strategies from prior 3DGS research and augment them with several novel optimizations. We further investigate underexplored aspects of the framework, including numerical stability, Gaussian truncation, and gradient approximation. The resulting system, Faster-GS, provides a rigorously optimized algorithm that we evaluate across a comprehensive suite of benchmarks. Our experiments demonstrate that Faster-GS achieves up to 5$\times$ faster training while maintaining visual quality, establishing a new cost-effective and resource efficient baseline for 3DGS optimization. Furthermore, we demonstrate that optimizations can be applied to 4D Gaussian reconstruction, leading to efficient non-rigid scene optimization.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

3D é«˜æ–¯åˆ†å¸ƒ (3DGS) çš„æœ€æ–°è¿›å±•é›†ä¸­äºåŠ é€Ÿä¼˜åŒ–ï¼ŒåŒæ—¶ä¿æŒé‡å»ºè´¨é‡ã€‚ç„¶è€Œï¼Œè®¸å¤šæå‡ºçš„æ–¹æ³•å°†å®ç°çº§åˆ«çš„æ”¹è¿›ä¸åŸºæœ¬ç®—æ³•ä¿®æ”¹æˆ–ä¿çœŸåº¦æ€§èƒ½çš„äº¤æ˜“çº ç¼ åœ¨ä¸€èµ·ï¼Œå¯¼è‡´ç ”ç©¶ç¯å¢ƒåˆ†æ•£ï¼Œä½¿å…¬å¹³æ¯”è¾ƒå˜å¾—å¤æ‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ•´åˆå’Œè¯„ä¼°äº†å…ˆå‰ 3DGS ç ”ç©¶ä¸­æœ€æœ‰æ•ˆå’Œæœ€å¹¿æ³›é€‚ç”¨çš„ç­–ç•¥ï¼Œå¹¶é€šè¿‡å‡ ç§æ–°é¢–çš„ä¼˜åŒ–å¯¹å…¶è¿›è¡Œäº†å¢å¼ºã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ç ”ç©¶äº†è¯¥æ¡†æ¶å°šæœªå¼€å‘çš„æ–¹é¢ï¼ŒåŒ…æ‹¬æ•°å€¼ç¨³å®šæ€§ã€é«˜æ–¯æˆªæ–­å’Œæ¢¯åº¦è¿‘ä¼¼ã€‚ç”±æ­¤äº§ç”Ÿçš„ç³»ç»Ÿ Faster-GS æä¾›äº†ä¸¥æ ¼ä¼˜åŒ–çš„ç®—æ³•ï¼Œæˆ‘ä»¬é€šè¿‡ä¸€å¥—å…¨é¢çš„åŸºå‡†æµ‹è¯•å¯¹å…¶è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒFaster-GS åœ¨ä¿æŒè§†è§‰è´¨é‡çš„åŒæ—¶å®ç°äº†é«˜è¾¾ 5 ç¾å…ƒ\å€çš„è®­ç»ƒé€Ÿåº¦ï¼Œä¸º 3DGS ä¼˜åŒ–å»ºç«‹äº†æ–°çš„ç»æµé«˜æ•ˆä¸”èµ„æºé«˜æ•ˆçš„åŸºå‡†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜ä¼˜åŒ–å¯ä»¥åº”ç”¨äº 4D é«˜æ–¯é‡å»ºï¼Œä»è€Œå®ç°é«˜æ•ˆçš„éåˆšæ€§åœºæ™¯ä¼˜åŒ–ã€‚

</details>

---

## 119. Efficient Special Stain Classification

**ä¸­æ–‡æ ‡é¢˜**: é«˜æ•ˆçš„ç‰¹æ®ŠæŸ“è‰²åˆ†ç±»

**Date**: 2026-02-10 | **arXiv**: [2602.09989v1](http://arxiv.org/abs/2602.09989v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09989v1)

<details><summary><b>Abstract</b></summary>

Stains are essential in histopathology to visualize specific tissue characteristics, with Haematoxylin and Eosin (H&E) serving as the clinical standard. However, pathologists frequently   utilize a variety of special stains for the diagnosis of specific morphologies. Maintaining accurate metadata for these slides is critical for quality control in clinical archives and for   the integrity of computational pathology datasets. In this work, we compare two approaches for automated classification of stains using whole slide images, covering the 14 most commonly   used special stains in our institute alongside standard and frozen-section H&E. We evaluate a Multi-Instance Learning (MIL) pipeline and a proposed lightweight thumbnail-based approach.   On internal test data, MIL achieved the highest performance (macro F1: 0.941 for 16 classes; 0.969 for 14 merged classes), while the thumbnail approach remained competitive (0.897 and   0.953, respectively). On external TCGA data, the thumbnail model generalized best (weighted F1: 0.843 vs. 0.807 for MIL). The thumbnail approach also increased throughput by two orders of   magnitude (5.635 vs. 0.018 slides/s for MIL with all patches). We conclude that thumbnail-based classification provides a scalable and robust solution for routine visual quality control   in digital pathology workflows.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æŸ“è‰²å¯¹äºç»„ç»‡ç—…ç†å­¦ä¸­è§‚å¯Ÿç‰¹å®šç»„ç»‡ç‰¹å¾è‡³å…³é‡è¦ï¼Œè‹æœ¨ç²¾å’Œæ›™çº¢ (H&E) æ˜¯ä¸´åºŠæ ‡å‡†ã€‚ç„¶è€Œï¼Œç—…ç†å­¦å®¶ç»å¸¸åˆ©ç”¨å„ç§ç‰¹æ®ŠæŸ“è‰²æ¥è¯Šæ–­ç‰¹å®šå½¢æ€ã€‚ç»´æŠ¤è¿™äº›è½½ç»ç‰‡çš„å‡†ç¡®å…ƒæ•°æ®å¯¹äºä¸´åºŠæ¡£æ¡ˆçš„è´¨é‡æ§åˆ¶å’Œè®¡ç®—ç—…ç†å­¦æ•°æ®é›†çš„å®Œæ•´æ€§è‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†ä¸¤ç§ä½¿ç”¨æ•´ä¸ªè½½ç»ç‰‡å›¾åƒè‡ªåŠ¨åˆ†ç±»æŸ“è‰²å‰‚çš„æ–¹æ³•ï¼Œæ¶µç›–äº†æˆ‘ä»¬ç ”ç©¶æ‰€æœ€å¸¸ç”¨çš„ 14 ç§ç‰¹æ®ŠæŸ“è‰²å‰‚ä»¥åŠæ ‡å‡†å’Œå†°å†»åˆ‡ç‰‡ H&Eã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰ç®¡é“å’Œæå‡ºçš„è½»é‡çº§åŸºäºç¼©ç•¥å›¾çš„æ–¹æ³•ã€‚   åœ¨å†…éƒ¨æµ‹è¯•æ•°æ®ä¸Šï¼ŒMIL å®ç°äº†æœ€é«˜æ€§èƒ½ï¼ˆå®è§‚ F1ï¼š16 ä¸ªç±»åˆ«ä¸º 0.941ï¼›14 ä¸ªåˆå¹¶ç±»åˆ«ä¸º 0.969ï¼‰ï¼Œè€Œç¼©ç•¥å›¾æ–¹æ³•ä»ç„¶å…·æœ‰ç«äº‰åŠ›ï¼ˆåˆ†åˆ«ä¸º 0.897 å’Œ 0.953ï¼‰ã€‚åœ¨å¤–éƒ¨ TCGA æ•°æ®ä¸Šï¼Œç¼©ç•¥å›¾æ¨¡å‹æ¦‚æ‹¬æ€§æœ€å¥½ï¼ˆåŠ æƒ F1ï¼š0.843 å¯¹æ¯” MIL çš„ 0.807ï¼‰ã€‚ç¼©ç•¥å›¾æ–¹æ³•è¿˜å°†ååé‡æé«˜äº†ä¸¤ä¸ªæ•°é‡çº§ï¼ˆå¯¹äºå…·æœ‰æ‰€æœ‰è¡¥ä¸çš„ MILï¼Œååé‡ä¸º 5.635 vs. 0.018 å¹»ç¯ç‰‡/ç§’ï¼‰ã€‚æˆ‘ä»¬çš„ç»“è®ºæ˜¯ï¼ŒåŸºäºç¼©ç•¥å›¾çš„åˆ†ç±»ä¸ºæ•°å­—ç—…ç†å·¥ä½œæµç¨‹ä¸­çš„å¸¸è§„è§†è§‰è´¨é‡æ§åˆ¶æä¾›äº†å¯æ‰©å±•ä¸”å¼ºå¤§çš„è§£å†³æ–¹æ¡ˆã€‚

</details>

---

## 120. Online Monitoring Framework for Automotive Time Series Data using JEPA Embeddings

**ä¸­æ–‡æ ‡é¢˜**: ä½¿ç”¨ JEPA åµŒå…¥çš„æ±½è½¦æ—¶é—´åºåˆ—æ•°æ®åœ¨çº¿ç›‘æ§æ¡†æ¶

**Date**: 2026-02-10 | **arXiv**: [2602.09985v1](http://arxiv.org/abs/2602.09985v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09985v1)

<details><summary><b>Abstract</b></summary>

As autonomous vehicles are rolled out, measures must be taken to ensure their safe operation. In order to supervise a system that is already in operation, monitoring frameworks are frequently employed. These run continuously online in the background, supervising the system status and recording anomalies. This work proposes an online monitoring framework to detect anomalies in object state representations. Thereby, a key challenge is creating a framework for anomaly detection without anomaly labels, which are usually unavailable for unknown anomalies. To address this issue, this work applies a self-supervised embedding method to translate object data into a latent representation space. For this, a JEPA-based self-supervised prediction task is constructed, allowing training without anomaly labels and the creation of rich object embeddings. The resulting expressive JEPA embeddings serve as input for established anomaly detection methods, in order to identify anomalies within object state representations. This framework is particularly useful for applications in real-world environments, where new or unknown anomalies may occur during operation for which there are no labels available. Experiments performed on the publicly available, real-world nuScenes dataset illustrate the framework's capabilities.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

éšç€è‡ªåŠ¨é©¾é©¶æ±½è½¦çš„æ¨å‡ºï¼Œå¿…é¡»é‡‡å–æªæ–½ç¡®ä¿å…¶å®‰å…¨è¿è¡Œã€‚ä¸ºäº†ç›‘æ§å·²ç»è¿è¡Œçš„ç³»ç»Ÿï¼Œç»å¸¸ä½¿ç”¨ç›‘æ§æ¡†æ¶ã€‚å®ƒä»¬åœ¨åå°æŒç»­åœ¨çº¿è¿è¡Œï¼Œç›‘æ§ç³»ç»ŸçŠ¶æ€å¹¶è®°å½•å¼‚å¸¸æƒ…å†µã€‚è¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ç§åœ¨çº¿ç›‘æ§æ¡†æ¶æ¥æ£€æµ‹å¯¹è±¡çŠ¶æ€è¡¨ç¤ºä¸­çš„å¼‚å¸¸æƒ…å†µã€‚å› æ­¤ï¼Œä¸€ä¸ªå…³é”®çš„æŒ‘æˆ˜æ˜¯åˆ›å»ºä¸€ä¸ªæ²¡æœ‰å¼‚å¸¸æ ‡ç­¾çš„å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼Œè€Œå¼‚å¸¸æ ‡ç­¾é€šå¸¸æ— æ³•ç”¨äºæœªçŸ¥å¼‚å¸¸ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¿™é¡¹å·¥ä½œåº”ç”¨äº†è‡ªç›‘ç£åµŒå…¥æ–¹æ³•å°†å¯¹è±¡æ•°æ®è½¬æ¢ä¸ºæ½œåœ¨è¡¨ç¤ºç©ºé—´ã€‚ä¸ºæ­¤ï¼Œæ„å»ºäº†ä¸€ä¸ªåŸºäº JEPA çš„è‡ªç›‘ç£é¢„æµ‹ä»»åŠ¡ï¼Œå…è®¸åœ¨æ²¡æœ‰å¼‚å¸¸æ ‡ç­¾çš„æƒ…å†µä¸‹è¿›è¡Œè®­ç»ƒå¹¶åˆ›å»ºä¸°å¯Œçš„å¯¹è±¡åµŒå…¥ã€‚ç”±æ­¤äº§ç”Ÿçš„å¯Œæœ‰è¡¨ç°åŠ›çš„ JEPA åµŒå…¥å¯ä½œä¸ºå·²å»ºç«‹çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•çš„è¾“å…¥ï¼Œä»¥ä¾¿è¯†åˆ«å¯¹è±¡çŠ¶æ€è¡¨ç¤ºä¸­çš„å¼‚å¸¸ã€‚è¯¥æ¡†æ¶å¯¹äºç°å®ç¯å¢ƒä¸­çš„åº”ç”¨ç¨‹åºç‰¹åˆ«æœ‰ç”¨ï¼Œåœ¨ç°å®ç¯å¢ƒä¸­ï¼Œåœ¨æ²¡æœ‰å¯ç”¨æ ‡ç­¾çš„æ“ä½œè¿‡ç¨‹ä¸­å¯èƒ½ä¼šå‡ºç°æ–°çš„æˆ–æœªçŸ¥çš„å¼‚å¸¸æƒ…å†µã€‚åœ¨å…¬å¼€çš„çœŸå® nuScenes æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¯´æ˜äº†è¯¥æ¡†æ¶çš„åŠŸèƒ½ã€‚

</details>

---

## 121. Coupled Inference in Diffusion Models for Semantic Decomposition

**ä¸­æ–‡æ ‡é¢˜**: è¯­ä¹‰åˆ†è§£æ‰©æ•£æ¨¡å‹ä¸­çš„è€¦åˆæ¨ç†

**Date**: 2026-02-10 | **arXiv**: [2602.09983v1](http://arxiv.org/abs/2602.09983v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09983v1)

<details><summary><b>Abstract</b></summary>

Many visual scenes can be described as compositions of latent factors. Effective recognition, reasoning, and editing often require not only forming such compositional representations, but also solving the decomposition problem. One popular choice for constructing these representations is through the binding operation. Resonator networks, which can be understood as coupled Hopfield networks, were proposed as a way to perform decomposition on such bound representations. Recent works have shown notable similarities between Hopfield networks and diffusion models. Motivated by these observations, we introduce a framework for semantic decomposition using coupled inference in diffusion models. Our method frames semantic decomposition as an inverse problem and couples the diffusion processes using a reconstruction-driven guidance term that encourages the composition of factor estimates to match the bound vector. We also introduce a novel iterative sampling scheme that improves the performance of our model. Finally, we show that attention-based resonator networks are a special case of our framework. Empirically, we demonstrate that our coupled inference framework outperforms resonator networks across a range of synthetic semantic decomposition tasks.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è®¸å¤šè§†è§‰åœºæ™¯å¯ä»¥è¢«æè¿°ä¸ºæ½œåœ¨å› ç´ çš„ç»„åˆã€‚æœ‰æ•ˆçš„è¯†åˆ«ã€æ¨ç†å’Œç¼–è¾‘é€šå¸¸ä¸ä»…éœ€è¦å½¢æˆè¿™ç§ç»„åˆè¡¨ç¤ºï¼Œè¿˜éœ€è¦è§£å†³åˆ†è§£é—®é¢˜ã€‚æ„å»ºè¿™äº›è¡¨ç¤ºçš„ä¸€ç§æµè¡Œé€‰æ‹©æ˜¯é€šè¿‡ç»‘å®šæ“ä½œã€‚è°æŒ¯å™¨ç½‘ç»œå¯ä»¥ç†è§£ä¸ºè€¦åˆçš„ Hopfield ç½‘ç»œï¼Œè¢«æå‡ºä½œä¸ºå¯¹æ­¤ç±»è¾¹ç•Œè¡¨ç¤ºè¿›è¡Œåˆ†è§£çš„ä¸€ç§æ–¹æ³•ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ Hopfield ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹ä¹‹é—´å­˜åœ¨æ˜¾ç€çš„ç›¸ä¼¼ä¹‹å¤„ã€‚å—è¿™äº›è§‚å¯Ÿçš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†åœ¨æ‰©æ•£æ¨¡å‹ä¸­ä½¿ç”¨è€¦åˆæ¨ç†è¿›è¡Œè¯­ä¹‰åˆ†è§£çš„æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†è¯­ä¹‰åˆ†è§£æ„å»ºä¸ºé€†é—®é¢˜ï¼Œå¹¶ä½¿ç”¨é‡å»ºé©±åŠ¨çš„æŒ‡å¯¼é¡¹è€¦åˆæ‰©æ•£è¿‡ç¨‹ï¼Œè¯¥æŒ‡å¯¼é¡¹é¼“åŠ±å› å­ä¼°è®¡çš„ç»„åˆä»¥åŒ¹é…è¾¹ç•Œå‘é‡ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è¿­ä»£é‡‡æ ·æ–¹æ¡ˆï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬è¡¨æ˜åŸºäºæ³¨æ„åŠ›çš„è°æŒ¯å™¨ç½‘ç»œæ˜¯æˆ‘ä»¬æ¡†æ¶çš„ä¸€ä¸ªç‰¹ä¾‹ã€‚æ ¹æ®ç»éªŒï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„è€¦åˆæ¨ç†æ¡†æ¶åœ¨ä¸€ç³»åˆ—åˆæˆè¯­ä¹‰åˆ†è§£ä»»åŠ¡ä¸­ä¼˜äºè°æŒ¯å™¨ç½‘ç»œã€‚

</details>

---

## 122. Learning to Detect Baked Goods with Limited Supervision

**ä¸­æ–‡æ ‡é¢˜**: å­¦ä¹ åœ¨æœ‰é™çš„ç›‘ç£ä¸‹æ£€æµ‹çƒ˜ç„™é£Ÿå“

**Date**: 2026-02-10 | **arXiv**: [2602.09979v1](http://arxiv.org/abs/2602.09979v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09979v1)

<details><summary><b>Abstract</b></summary>

Monitoring leftover products provides valuable insights that can be used to optimize future production. This is especially important for German bakeries because freshly baked goods have a very short shelf life. Automating this process can reduce labor costs, improve accuracy, and streamline operations. We propose automating this process using an object detection model to identify baked goods from images. However, the large diversity of German baked goods makes fully supervised training prohibitively expensive and limits scalability. Although open-vocabulary detectors (e.g., OWLv2, Grounding DINO) offer lexibility, we demonstrate that they are insufficient for our task. While motivated by bakeries, our work addresses the broader challenges of deploying computer vision in industries, where tasks are specialized and annotated datasets are scarce. We compile dataset splits with varying supervision levels, covering 19 classes of baked goods. We propose two training workflows to train an object detection model with limited supervision. First, we combine OWLv2 and Grounding DINO localization with image-level supervision to train the model in a weakly supervised manner. Second, we improve viewpoint robustness by fine-tuning on video frames annotated using Segment Anything 2 as a pseudo-label propagation model. Using these workflows, we train YOLOv11 for our detection task due to its favorable speed accuracy tradeoff. Relying solely on image-level supervision, the model achieves a mean Average Precision (mAP) of 0.91. Finetuning with pseudo-labels raises model performance by 19.3% under non-ideal deployment conditions. Combining these workflows trains a model that surpasses our fully-supervised baseline model under non-ideal deployment conditions, despite relying only on image-level supervision.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç›‘æ§å‰©ä½™äº§å“â€‹â€‹å¯ä»¥æä¾›å®è´µçš„è§è§£ï¼Œå¯ç”¨äºä¼˜åŒ–æœªæ¥çš„ç”Ÿäº§ã€‚è¿™å¯¹äºå¾·å›½é¢åŒ…åº—å°¤å…¶é‡è¦ï¼Œå› ä¸ºæ–°é²œçƒ˜ç„™çš„é£Ÿå“ä¿è´¨æœŸå¾ˆçŸ­ã€‚è‡ªåŠ¨åŒ–æ­¤è¿‡ç¨‹å¯ä»¥é™ä½åŠ³åŠ¨åŠ›æˆæœ¬ã€æé«˜å‡†ç¡®æ€§å¹¶ç®€åŒ–æ“ä½œã€‚æˆ‘ä»¬å»ºè®®ä½¿ç”¨å¯¹è±¡æ£€æµ‹æ¨¡å‹æ¥è‡ªåŠ¨åŒ–æ­¤è¿‡ç¨‹ï¼Œä»¥ä»å›¾åƒä¸­è¯†åˆ«çƒ˜ç„™é£Ÿå“ã€‚ç„¶è€Œï¼Œå¾·å›½çƒ˜ç„™é£Ÿå“çš„å¤šæ ·æ€§ä½¿å¾—å®Œå…¨ç›‘ç£çš„åŸ¹è®­æˆæœ¬é«˜æ˜‚ï¼Œå¹¶ä¸”é™åˆ¶äº†å¯æ‰©å±•æ€§ã€‚å°½ç®¡å¼€æ”¾è¯æ±‡æ£€æµ‹å™¨ï¼ˆä¾‹å¦‚ OWLv2ã€Grounding DINOï¼‰æä¾›äº†çµæ´»æ€§ï¼Œä½†æˆ‘ä»¬è¯æ˜å®ƒä»¬ä¸è¶³ä»¥å®Œæˆæˆ‘ä»¬çš„ä»»åŠ¡ã€‚è™½ç„¶å—åˆ°é¢åŒ…åº—çš„æ¨åŠ¨ï¼Œä½†æˆ‘ä»¬çš„å·¥ä½œè§£å†³äº†åœ¨è¡Œä¸šä¸­éƒ¨ç½²è®¡ç®—æœºè§†è§‰çš„æ›´å¹¿æ³›çš„æŒ‘æˆ˜ï¼Œè¿™äº›è¡Œä¸šçš„ä»»åŠ¡æ˜¯ä¸“ä¸šåŒ–çš„ï¼Œå¹¶ä¸”å¸¦æ³¨é‡Šçš„æ•°æ®é›†ç¨€ç¼ºã€‚æˆ‘ä»¬ç¼–è¯‘äº†å…·æœ‰ä¸åŒç›‘ç£çº§åˆ«çš„æ•°æ®é›†åˆ†å‰²ï¼Œæ¶µç›– 19 ç±»çƒ˜ç„™é£Ÿå“ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§è®­ç»ƒå·¥ä½œæµç¨‹æ¥è®­ç»ƒå…·æœ‰æœ‰é™ç›‘ç£çš„å¯¹è±¡æ£€æµ‹æ¨¡å‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°† OWLv2 å’Œ Grounding DINO å®šä½ä¸å›¾åƒçº§ç›‘ç£ç›¸ç»“åˆï¼Œä»¥å¼±ç›‘ç£çš„æ–¹å¼è®­ç»ƒæ¨¡å‹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹ä½¿ç”¨ Segment Anything 2 ä½œä¸ºä¼ªæ ‡ç­¾ä¼ æ’­æ¨¡å‹æ³¨é‡Šçš„è§†é¢‘å¸§è¿›è¡Œå¾®è°ƒæ¥æé«˜è§†ç‚¹é²æ£’æ€§ã€‚ä½¿ç”¨è¿™äº›å·¥ä½œæµç¨‹ï¼Œæˆ‘ä»¬è®­ç»ƒ YOLOv11 æ¥å®Œæˆæˆ‘ä»¬çš„æ£€æµ‹ä»»åŠ¡ï¼Œå› ä¸ºå®ƒå…·æœ‰æœ‰åˆ©çš„é€Ÿåº¦ç²¾åº¦æƒè¡¡ã€‚ä»…ä¾é å›¾åƒçº§ç›‘ç£ï¼Œè¯¥æ¨¡å‹çš„å¹³å‡ç²¾åº¦ (mAP) ä¸º 0.91ã€‚åœ¨éç†æƒ³éƒ¨ç½²æ¡ä»¶ä¸‹ï¼Œä½¿ç”¨ä¼ªæ ‡ç­¾è¿›è¡Œå¾®è°ƒå¯å°†æ¨¡å‹æ€§èƒ½æé«˜ 19.3%ã€‚ç»“åˆè¿™äº›å·¥ä½œæµç¨‹å¯ä»¥è®­ç»ƒå‡ºä¸€ä¸ªæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨éç†æƒ³éƒ¨ç½²æ¡ä»¶ä¸‹è¶…è¶Šäº†æˆ‘ä»¬å®Œå…¨ç›‘ç£çš„åŸºçº¿æ¨¡å‹ï¼Œå°½ç®¡ä»…ä¾èµ–äºå›¾åƒçº§ç›‘ç£ã€‚

</details>

---

## 123. Bladder Vessel Segmentation using a Hybrid Attention-Convolution Framework

**ä¸­æ–‡æ ‡é¢˜**: ä½¿ç”¨æ··åˆæ³¨æ„åŠ›å·ç§¯æ¡†æ¶çš„è†€èƒ±è¡€ç®¡åˆ†å‰²

**Date**: 2026-02-10 | **arXiv**: [2602.09949v1](http://arxiv.org/abs/2602.09949v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09949v1)

<details><summary><b>Abstract</b></summary>

Urinary bladder cancer surveillance requires tracking tumor sites across repeated interventions, yet the deformable and hollow bladder lacks stable landmarks for orientation. While blood vessels visible during endoscopy offer a patient-specific "vascular fingerprint" for navigation, automated segmentation is challenged by imperfect endoscopic data, including sparse labels, artifacts like bubbles or variable lighting, continuous deformation, and mucosal folds that mimic vessels. State-of-the-art vessel segmentation methods often fail to address these domain-specific complexities. We introduce a Hybrid Attention-Convolution (HAC) architecture that combines Transformers to capture global vessel topology prior with a CNN that learns a residual refinement map to precisely recover thin-vessel details. To prioritize structural connectivity, the Transformer is trained on optimized ground truth data that exclude short and terminal branches. Furthermore, to address data scarcity, we employ a physics-aware pretraining, that is a self-supervised strategy using clinically grounded augmentations on unlabeled data. Evaluated on the BlaVeS dataset, consisting of endoscopic video frames, our approach achieves high accuracy (0.94) and superior precision (0.61) and clDice (0.66) compared to state-of-the-art medical segmentation models. Crucially, our method successfully suppresses false positives from mucosal folds that dynamically appear and vanish as the bladder fills and empties during surgery. Hence, HAC provides the reliable structural stability required for clinical navigation.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è†€èƒ±ç™Œç›‘æµ‹éœ€è¦é€šè¿‡é‡å¤å¹²é¢„æ¥è·Ÿè¸ªè‚¿ç˜¤éƒ¨ä½ï¼Œä½†å¯å˜å½¢ä¸”ä¸­ç©ºçš„è†€èƒ±ç¼ºä¹ç¨³å®šçš„å®šä½æ ‡å¿—ã€‚è™½ç„¶å†…çª¥é•œæ£€æŸ¥æœŸé—´å¯è§çš„è¡€ç®¡ä¸ºå¯¼èˆªæä¾›äº†æ‚£è€…ç‰¹å®šçš„â€œè¡€ç®¡æŒ‡çº¹â€ï¼Œä½†è‡ªåŠ¨åˆ†å‰²å—åˆ°ä¸å®Œå–„çš„å†…çª¥é•œæ•°æ®çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç¨€ç–çš„æ ‡ç­¾ã€æ°”æ³¡æˆ–å¯å˜ç…§æ˜ç­‰ä¼ªå½±ã€è¿ç»­å˜å½¢å’Œæ¨¡ä»¿è¡€ç®¡çš„ç²˜è†œè¤¶çš±ã€‚æœ€å…ˆè¿›çš„è¡€ç®¡åˆ†å‰²æ–¹æ³•é€šå¸¸æ— æ³•è§£å†³è¿™äº›ç‰¹å®šé¢†åŸŸçš„å¤æ‚æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ··åˆæ³¨æ„åŠ›å·ç§¯ (HAC) æ¶æ„ï¼Œè¯¥æ¶æ„ç»“åˆäº† Transformer æ¥æ•è·å…¨å±€è¡€ç®¡æ‹“æ‰‘ï¼Œå¹¶ä½¿ç”¨ CNN æ¥å­¦ä¹ æ®‹å·®ç»†åŒ–å›¾ä»¥ç²¾ç¡®æ¢å¤è–„è¡€ç®¡ç»†èŠ‚ã€‚ä¸ºäº†ä¼˜å…ˆè€ƒè™‘ç»“æ„è¿æ¥æ€§ï¼Œå˜å‹å™¨æ¥å—äº†ä¼˜åŒ–çš„åœ°é¢å®å†µæ•°æ®çš„è®­ç»ƒï¼Œæ’é™¤äº†çŸ­åˆ†æ”¯å’Œç»ˆç«¯åˆ†æ”¯ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ç‰©ç†æ„ŸçŸ¥é¢„è®­ç»ƒï¼Œè¿™æ˜¯ä¸€ç§è‡ªæˆ‘ç›‘ç£ç­–ç•¥ï¼Œå¯¹æœªæ ‡è®°çš„æ•°æ®ä½¿ç”¨åŸºäºä¸´åºŠçš„å¢å¼ºã€‚åœ¨ç”±å†…çª¥é•œè§†é¢‘å¸§ç»„æˆçš„ BlaVeS æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œä¸æœ€å…ˆè¿›çš„åŒ»å­¦åˆ†å‰²æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†é«˜ç²¾åº¦ (0.94) å’Œå“è¶Šçš„ç²¾åº¦ (0.61) ä»¥åŠ clDice (0.66)ã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸåœ°æŠ‘åˆ¶äº†ç²˜è†œè¤¶çš±çš„å‡é˜³æ€§ï¼Œè¿™äº›å‡é˜³æ€§åœ¨æ‰‹æœ¯æœŸé—´éšç€è†€èƒ±çš„å¡«å……å’Œæ’ç©ºè€ŒåŠ¨æ€åœ°å‡ºç°å’Œæ¶ˆå¤±ã€‚å› æ­¤ï¼ŒHAC æä¾›äº†ä¸´åºŠå¯¼èˆªæ‰€éœ€çš„å¯é çš„ç»“æ„ç¨³å®šæ€§ã€‚

</details>

---

## 124. VersaViT: Enhancing MLLM Vision Backbones via Task-Guided Optimization

**ä¸­æ–‡æ ‡é¢˜**: VersaViTï¼šé€šè¿‡ä»»åŠ¡å¼•å¯¼ä¼˜åŒ–å¢å¼º MLLM è§†è§‰éª¨å¹²

**Date**: 2026-02-10 | **arXiv**: [2602.09934v1](http://arxiv.org/abs/2602.09934v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09934v1)

<details><summary><b>Abstract</b></summary>

Multimodal Large Language Models (MLLMs) have recently achieved remarkable success in visual-language understanding, demonstrating superior high-level semantic alignment within their vision encoders. An important question thus arises: Can these encoders serve as versatile vision backbones, capable of reliably performing classic vision-centric tasks as well? To address the question, we make the following contributions: (i) we identify that the vision encoders within MLLMs exhibit deficiencies in their dense feature representations, as evidenced by their suboptimal performance on dense prediction tasks (e.g., semantic segmentation, depth estimation); (ii) we propose VersaViT, a well-rounded vision transformer that instantiates a novel multi-task framework for collaborative post-training. This framework facilitates the optimization of the vision backbone via lightweight task heads with multi-granularity supervision; (iii) extensive experiments across various downstream tasks demonstrate the effectiveness of our method, yielding a versatile vision backbone suited for both language-mediated reasoning and pixel-level understanding.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLM) æœ€è¿‘åœ¨è§†è§‰è¯­è¨€ç†è§£æ–¹é¢å–å¾—äº†æ˜¾ç€çš„æˆåŠŸï¼Œåœ¨å…¶è§†è§‰ç¼–ç å™¨ä¸­å±•ç¤ºäº†å“è¶Šçš„é«˜çº§è¯­ä¹‰å¯¹é½ã€‚å› æ­¤å‡ºç°äº†ä¸€ä¸ªé‡è¦çš„é—®é¢˜ï¼šè¿™äº›ç¼–ç å™¨èƒ½å¦ä½œä¸ºå¤šåŠŸèƒ½è§†è§‰éª¨å¹²ï¼Œèƒ½å¤Ÿå¯é åœ°æ‰§è¡Œç»å…¸çš„ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ï¼Ÿä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åšå‡ºä»¥ä¸‹è´¡çŒ®ï¼šï¼ˆiï¼‰æˆ‘ä»¬å‘ç° MLLM ä¸­çš„è§†è§‰ç¼–ç å™¨åœ¨å¯†é›†ç‰¹å¾è¡¨ç¤ºæ–¹é¢è¡¨ç°å‡ºç¼ºé™·ï¼Œæ­£å¦‚å®ƒä»¬åœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡ï¼ˆä¾‹å¦‚è¯­ä¹‰åˆ†å‰²ã€æ·±åº¦ä¼°è®¡ï¼‰ä¸Šçš„æ¬¡ä¼˜æ€§èƒ½æ‰€è¯æ˜çš„é‚£æ ·ï¼› (ii) æˆ‘ä»¬æå‡ºäº† VersaViTï¼Œè¿™æ˜¯ä¸€ç§å…¨é¢çš„è§†è§‰è½¬æ¢å™¨ï¼Œå®ƒå®ä¾‹åŒ–äº†ç”¨äºåä½œåè®­ç»ƒçš„æ–°é¢–çš„å¤šä»»åŠ¡æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å…·æœ‰å¤šç²’åº¦ç›‘ç£çš„è½»é‡çº§ä»»åŠ¡å¤´ä¿ƒè¿›è§†è§‰ä¸»å¹²çš„ä¼˜åŒ–ï¼› ï¼ˆiiiï¼‰è·¨å„ç§ä¸‹æ¸¸ä»»åŠ¡çš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œäº§ç”Ÿäº†é€‚åˆè¯­è¨€ä»‹å¯¼æ¨ç†å’Œåƒç´ çº§ç†è§£çš„å¤šåŠŸèƒ½è§†è§‰ä¸»å¹²ã€‚

</details>

---

## 125. ArtisanGS: Interactive Tools for Gaussian Splat Selection with AI and Human in the Loop

**ä¸­æ–‡æ ‡é¢˜**: ArtisanGSï¼šåˆ©ç”¨ AI å’Œ Human in the Loop è¿›è¡Œé«˜æ–¯ Splat é€‰æ‹©çš„äº¤äº’å¼å·¥å…·

**Date**: 2026-02-10 | **arXiv**: [2602.10173v1](http://arxiv.org/abs/2602.10173v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10173v1)

<details><summary><b>Abstract</b></summary>

Representation in the family of 3D Gaussian Splats (3DGS) are growing into a viable alternative to traditional graphics for an expanding number of application, including recent techniques that facilitate physics simulation and animation. However, extracting usable objects from in-the-wild captures remains challenging and controllable editing techniques for this representation are limited. Unlike the bulk of emerging techniques, focused on automatic solutions or high-level editing, we introduce an interactive suite of tools centered around versatile Gaussian Splat selection and segmentation. We propose a fast AI-driven method to propagate user-guided 2D selection masks to 3DGS selections. This technique allows for user intervention in the case of errors and is further coupled with flexible manual selection and segmentation tools. These allow a user to achieve virtually any binary segmentation of an unstructured 3DGS scene. We evaluate our toolset against the state-of-the-art for Gaussian Splat selection and demonstrate their utility for downstream applications by developing a user-guided local editing approach, leveraging a custom Video Diffusion Model. With flexible selection tools, users have direct control over the areas that the AI can modify. Our selection and editing tools can be used for any in-the-wild capture without additional optimization.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

3D é«˜æ–¯å›¾ (3DGS) ç³»åˆ—ä¸­çš„è¡¨ç¤ºæ­£åœ¨å‘å±•æˆä¸ºä¼ ç»Ÿå›¾å½¢çš„å¯è¡Œæ›¿ä»£å“ï¼Œå…¶åº”ç”¨èŒƒå›´ä¸æ–­æ‰©å¤§ï¼ŒåŒ…æ‹¬ä¿ƒè¿›ç‰©ç†æ¨¡æ‹Ÿå’ŒåŠ¨ç”»çš„æœ€æ–°æŠ€æœ¯ã€‚ç„¶è€Œï¼Œä»é‡å¤–æ•è·ä¸­æå–å¯ç”¨å¯¹è±¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå¹¶ä¸”è¿™ç§è¡¨ç¤ºçš„å¯æ§ç¼–è¾‘æŠ€æœ¯æ˜¯æœ‰é™çš„ã€‚ä¸å¤§é‡ä¸“æ³¨äºè‡ªåŠ¨è§£å†³æ–¹æ¡ˆæˆ–é«˜çº§ç¼–è¾‘çš„æ–°å…´æŠ€æœ¯ä¸åŒï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€å¥—ä»¥å¤šåŠŸèƒ½é«˜æ–¯ Splat é€‰æ‹©å’Œåˆ†å‰²ä¸ºä¸­å¿ƒçš„äº¤äº’å¼å·¥å…·ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¿«é€Ÿ AI é©±åŠ¨çš„æ–¹æ³•ï¼Œå°†ç”¨æˆ·å¼•å¯¼çš„ 2D é€‰æ‹©æ©æ¨¡ä¼ æ’­åˆ° 3DGS é€‰æ‹©ã€‚è¯¥æŠ€æœ¯å…è®¸ç”¨æˆ·åœ¨å‡ºç°é”™è¯¯æ—¶è¿›è¡Œå¹²é¢„ï¼Œå¹¶è¿›ä¸€æ­¥ä¸çµæ´»çš„æ‰‹åŠ¨é€‰æ‹©å’Œåˆ†æ®µå·¥å…·ç›¸ç»“åˆã€‚è¿™äº›å…è®¸ç”¨æˆ·å®ç°éç»“æ„åŒ– 3DGS åœºæ™¯çš„å‡ ä¹ä»»ä½•äºŒè¿›åˆ¶åˆ†å‰²ã€‚æˆ‘ä»¬æ ¹æ®æœ€å…ˆè¿›çš„é«˜æ–¯ Splat é€‰æ‹©æ¥è¯„ä¼°æˆ‘ä»¬çš„å·¥å…·é›†ï¼Œå¹¶é€šè¿‡å¼€å‘ç”¨æˆ·å¼•å¯¼çš„æœ¬åœ°ç¼–è¾‘æ–¹æ³•ï¼Œåˆ©ç”¨è‡ªå®šä¹‰è§†é¢‘æ‰©æ•£æ¨¡å‹æ¥å±•ç¤ºå®ƒä»¬å¯¹ä¸‹æ¸¸åº”ç”¨ç¨‹åºçš„å®ç”¨æ€§ã€‚å€ŸåŠ©çµæ´»çš„é€‰æ‹©å·¥å…·ï¼Œç”¨æˆ·å¯ä»¥ç›´æ¥æ§åˆ¶äººå·¥æ™ºèƒ½å¯ä»¥ä¿®æ”¹çš„åŒºåŸŸã€‚æˆ‘ä»¬çš„é€‰æ‹©å’Œç¼–è¾‘å·¥å…·å¯ç”¨äºä»»ä½•é‡å¤–æ•æ‰ï¼Œæ— éœ€é¢å¤–ä¼˜åŒ–ã€‚

</details>

---

## 126. Monocular Normal Estimation via Shading Sequence Estimation

**ä¸­æ–‡æ ‡é¢˜**: é€šè¿‡é˜´å½±åºåˆ—ä¼°è®¡è¿›è¡Œå•ç›®æ³•çº¿ä¼°è®¡

**Date**: 2026-02-10 | **arXiv**: [2602.09929v2](http://arxiv.org/abs/2602.09929v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.09929v2)

<details><summary><b>Abstract</b></summary>

Monocular normal estimation aims to estimate the normal map from a single RGB image of an object under arbitrary lights. Existing methods rely on deep models to directly predict normal maps. However, they often suffer from 3D misalignment: while the estimated normal maps may appear to have a correct appearance, the reconstructed surfaces often fail to align with the geometric details. We argue that this misalignment stems from the current paradigm: the model struggles to distinguish and reconstruct varying geometry represented in normal maps, as the differences in underlying geometry are reflected only through relatively subtle color variations. To address this issue, we propose a new paradigm that reformulates normal estimation as shading sequence estimation, where shading sequences are more sensitive to various geometric information. Building on this paradigm, we present RoSE, a method that leverages image-to-video generative models to predict shading sequences. The predicted shading sequences are then converted into normal maps by solving a simple ordinary least-squares problem. To enhance robustness and better handle complex objects, RoSE is trained on a synthetic dataset, MultiShade, with diverse shapes, materials, and light conditions. Experiments demonstrate that RoSE achieves state-of-the-art performance on real-world benchmark datasets for object-based monocular normal estimation.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å•ç›®æ³•çº¿ä¼°è®¡æ—¨åœ¨ä»ä»»æ„å…‰ç…§ä¸‹ç‰©ä½“çš„å•ä¸ª RGB å›¾åƒä¼°è®¡æ³•çº¿å›¾ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–æ·±åº¦æ¨¡å‹æ¥ç›´æ¥é¢„æµ‹æ³•çº¿è´´å›¾ã€‚ç„¶è€Œï¼Œå®ƒä»¬ç»å¸¸é­å— 3D æœªå¯¹å‡†çš„å½±å“ï¼šè™½ç„¶ä¼°è®¡çš„æ³•çº¿è´´å›¾å¯èƒ½çœ‹èµ·æ¥å…·æœ‰æ­£ç¡®çš„å¤–è§‚ï¼Œä½†é‡å»ºçš„è¡¨é¢é€šå¸¸æ— æ³•ä¸å‡ ä½•ç»†èŠ‚å¯¹é½ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™ç§é”™ä½æºäºå½“å‰çš„èŒƒå¼ï¼šæ¨¡å‹éš¾ä»¥åŒºåˆ†å’Œé‡å»ºæ³•çº¿è´´å›¾ä¸­è¡¨ç¤ºçš„ä¸åŒå‡ ä½•å½¢çŠ¶ï¼Œå› ä¸ºåº•å±‚å‡ ä½•å½¢çŠ¶çš„å·®å¼‚ä»…é€šè¿‡ç›¸å¯¹å¾®å¦™çš„é¢œè‰²å˜åŒ–åæ˜ å‡ºæ¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„èŒƒå¼ï¼Œå°†æ³•çº¿ä¼°è®¡é‡æ–°è¡¨è¿°ä¸ºç€è‰²åºåˆ—ä¼°è®¡ï¼Œå…¶ä¸­ç€è‰²åºåˆ—å¯¹å„ç§å‡ ä½•ä¿¡æ¯æ›´åŠ æ•æ„Ÿã€‚åœ¨æ­¤èŒƒä¾‹çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº† RoSEï¼Œä¸€ç§åˆ©ç”¨å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹æ¥é¢„æµ‹ç€è‰²åºåˆ—çš„æ–¹æ³•ã€‚ç„¶åé€šè¿‡è§£å†³ç®€å•çš„æ™®é€šæœ€å°äºŒä¹˜é—®é¢˜å°†é¢„æµ‹çš„ç€è‰²åºåˆ—è½¬æ¢ä¸ºæ³•çº¿è´´å›¾ã€‚ä¸ºäº†å¢å¼ºé²æ£’æ€§å¹¶æ›´å¥½åœ°å¤„ç†å¤æ‚å¯¹è±¡ï¼ŒRoSE åœ¨å…·æœ‰ä¸åŒå½¢çŠ¶ã€ææ–™å’Œå…‰ç…§æ¡ä»¶çš„åˆæˆæ•°æ®é›† MultiShade ä¸Šè¿›è¡Œè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒRoSE åœ¨åŸºäºå¯¹è±¡çš„å•ç›®æ³•çº¿ä¼°è®¡çš„çœŸå®ä¸–ç•ŒåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚

</details>

---

## 127. AdaTSQ: Pushing the Pareto Frontier of Diffusion Transformers via Temporal-Sensitivity Quantization

**ä¸­æ–‡æ ‡é¢˜**: AdaTSQï¼šé€šè¿‡æ—¶é—´æ•æ„Ÿæ€§é‡åŒ–æ¨åŠ¨æ‰©æ•£å˜å‹å™¨çš„å¸•ç´¯æ‰˜å‰æ²¿

**Date**: 2026-02-10 | **arXiv**: [2602.09883v1](http://arxiv.org/abs/2602.09883v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09883v1)

**Code**: https://github.com/Qiushao-E/AdaTSQ.

<details><summary><b>Abstract</b></summary>

Diffusion Transformers (DiTs) have emerged as the state-of-the-art backbone for high-fidelity image and video generation. However, their massive computational cost and memory footprint hinder deployment on edge devices. While post-training quantization (PTQ) has proven effective for large language models (LLMs), directly applying existing methods to DiTs yields suboptimal results due to the neglect of the unique temporal dynamics inherent in diffusion processes. In this paper, we propose AdaTSQ, a novel PTQ framework that pushes the Pareto frontier of efficiency and quality by exploiting the temporal sensitivity of DiTs. First, we propose a Pareto-aware timestep-dynamic bit-width allocation strategy. We model the quantization policy search as a constrained pathfinding problem. We utilize a beam search algorithm guided by end-to-end reconstruction error to dynamically assign layer-wise bit-widths across different timesteps. Second, we propose a Fisher-guided temporal calibration mechanism. It leverages temporal Fisher information to prioritize calibration data from highly sensitive timesteps, seamlessly integrating with Hessian-based weight optimization. Extensive experiments on four advanced DiTs (e.g., Flux-Dev, Flux-Schnell, Z-Image, and Wan2.1) demonstrate that AdaTSQ significantly outperforms state-of-the-art methods like SVDQuant and ViDiT-Q. Our code will be released at https://github.com/Qiushao-E/AdaTSQ.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æ‰©æ•£å˜å‹å™¨ (DiT) å·²æˆä¸ºé«˜ä¿çœŸå›¾åƒå’Œè§†é¢‘ç”Ÿæˆçš„æœ€å…ˆè¿›çš„æ”¯æŸ±ã€‚ç„¶è€Œï¼Œå®ƒä»¬å·¨å¤§çš„è®¡ç®—æˆæœ¬å’Œå†…å­˜å ç”¨é˜»ç¢äº†åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„éƒ¨ç½²ã€‚è™½ç„¶è®­ç»ƒåé‡åŒ– (PTQ) å·²è¢«è¯æ˜å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) æœ‰æ•ˆï¼Œä½†ç”±äºå¿½ç•¥äº†æ‰©æ•£è¿‡ç¨‹ä¸­å›ºæœ‰çš„ç‹¬ç‰¹æ—¶é—´åŠ¨æ€ï¼Œç›´æ¥å°†ç°æœ‰æ–¹æ³•åº”ç”¨äº DiT ä¼šäº§ç”Ÿæ¬¡ä¼˜ç»“æœã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† AdaTSQï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„ PTQ æ¡†æ¶ï¼Œå®ƒé€šè¿‡åˆ©ç”¨ DiT çš„æ—¶é—´æ•æ„Ÿæ€§æ¥æ¨åŠ¨æ•ˆç‡å’Œè´¨é‡çš„å¸•ç´¯æ‰˜å‰æ²¿ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†å¸•ç´¯æ‰˜æ„ŸçŸ¥æ—¶é—´æ­¥åŠ¨æ€ä½å®½åˆ†é…ç­–ç•¥ã€‚æˆ‘ä»¬å°†é‡åŒ–ç­–ç•¥æœç´¢å»ºæ¨¡ä¸ºå—é™å¯»è·¯é—®é¢˜ã€‚æˆ‘ä»¬åˆ©ç”¨ç”±ç«¯åˆ°ç«¯é‡å»ºè¯¯å·®å¼•å¯¼çš„æ³¢æŸæœç´¢ç®—æ³•æ¥è·¨ä¸åŒæ—¶é—´æ­¥åŠ¨æ€åˆ†é…åˆ†å±‚ä½å®½ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†è´¹èˆå°”å¼•å¯¼çš„æ—¶é—´æ ¡å‡†æœºåˆ¶ã€‚å®ƒåˆ©ç”¨æ—¶æ€ Fisher ä¿¡æ¯å¯¹æ¥è‡ªé«˜åº¦æ•æ„Ÿæ—¶é—´æ­¥é•¿çš„æ ¡å‡†æ•°æ®è¿›è¡Œä¼˜å…ˆçº§æ’åºï¼Œä¸åŸºäº Hessian çš„æƒé‡ä¼˜åŒ–æ— ç¼é›†æˆã€‚å¯¹å››ç§å…ˆè¿› DiTï¼ˆä¾‹å¦‚ Flux-Devã€Flux-Schnellã€Z-Image å’Œ Wan2.1ï¼‰çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAdaTSQ çš„æ€§èƒ½æ˜¾ç€ä¼˜äº SVDQuant å’Œ ViDiT-Q ç­‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å°†å‘å¸ƒåœ¨https://github.com/Qiushao-E/AdaTSQã€‚

</details>

---

## 128. MVISTA-4D: View-Consistent 4D World Model with Test-Time Action Inference for Robotic Manipulation

**ä¸­æ–‡æ ‡é¢˜**: MVISTA-4Dï¼šè§†å›¾ä¸€è‡´çš„ 4D ä¸–ç•Œæ¨¡å‹ï¼Œå…·æœ‰ç”¨äºæœºå™¨äººæ“ä½œçš„æµ‹è¯•æ—¶åŠ¨ä½œæ¨ç†

**Date**: 2026-02-10 | **arXiv**: [2602.09878v1](http://arxiv.org/abs/2602.09878v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09878v1)

<details><summary><b>Abstract</b></summary>

World-model-based imagine-then-act becomes a promising paradigm for robotic manipulation, yet existing approaches typically support either purely image-based forecasting or reasoning over partial 3D geometry, limiting their ability to predict complete 4D scene dynamics. This work proposes a novel embodied 4D world model that enables geometrically consistent, arbitrary-view RGBD generation: given only a single-view RGBD observation as input, the model imagines the remaining viewpoints, which can then be back-projected and fused to assemble a more complete 3D structure across time. To efficiently learn the multi-view, cross-modality generation, we explicitly design cross-view and cross-modality feature fusion that jointly encourage consistency between RGB and depth and enforce geometric alignment across views. Beyond prediction, converting generated futures into actions is often handled by inverse dynamics, which is ill-posed because multiple actions can explain the same transition. We address this with a test-time action optimization strategy that backpropagates through the generative model to infer a trajectory-level latent best matching the predicted future, and a residual inverse dynamics model that turns this trajectory prior into accurate executable actions. Experiments on three datasets demonstrate strong performance on both 4D scene generation and downstream manipulation, and ablations provide practical insights into the key design choices.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åŸºäºä¸–ç•Œæ¨¡å‹çš„â€œæƒ³è±¡ç„¶åè¡ŒåŠ¨â€æˆä¸ºæœºå™¨äººæ“çºµçš„ä¸€ä¸ªæœ‰å‰æ™¯çš„èŒƒä¾‹ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸æ”¯æŒçº¯ç²¹åŸºäºå›¾åƒçš„é¢„æµ‹æˆ–å¯¹éƒ¨åˆ† 3D å‡ ä½•å›¾å½¢çš„æ¨ç†ï¼Œé™åˆ¶äº†å®ƒä»¬é¢„æµ‹å®Œæ•´ 4D åœºæ™¯åŠ¨æ€çš„èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ç§æ–°é¢–çš„å…·ä½“åŒ– 4D ä¸–ç•Œæ¨¡å‹ï¼Œå¯å®ç°å‡ ä½•ä¸€è‡´çš„ä»»æ„è§†å›¾ RGBD ç”Ÿæˆï¼šä»…å°†å•è§†å›¾ RGBD è§‚å¯Ÿä½œä¸ºè¾“å…¥ï¼Œè¯¥æ¨¡å‹ä¼šæƒ³è±¡å‰©ä½™çš„è§†ç‚¹ï¼Œç„¶åå¯ä»¥å¯¹è¿™äº›è§†ç‚¹è¿›è¡Œåå‘æŠ•å½±å’Œèåˆï¼Œä»¥è·¨æ—¶é—´ç»„è£…æ›´å®Œæ•´çš„ 3D ç»“æ„ã€‚ä¸ºäº†æœ‰æ•ˆåœ°å­¦ä¹ å¤šè§†å›¾ã€è·¨æ¨¡æ€ç”Ÿæˆï¼Œæˆ‘ä»¬æ˜ç¡®è®¾è®¡äº†è·¨è§†å›¾å’Œè·¨æ¨¡æ€ç‰¹å¾èåˆï¼Œå…±åŒä¿ƒè¿› RGB å’Œæ·±åº¦ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œå¹¶å¼ºåˆ¶è·¨è§†å›¾çš„å‡ ä½•å¯¹é½ã€‚é™¤äº†é¢„æµ‹ä¹‹å¤–ï¼Œå°†ç”Ÿæˆçš„æœªæ¥è½¬æ¢ä¸ºè¡ŒåŠ¨é€šå¸¸æ˜¯é€šè¿‡é€†åŠ¨æ€æ¥å¤„ç†çš„ï¼Œè¿™æ˜¯ä¸é€‚å®šçš„ï¼Œå› ä¸ºå¤šä¸ªè¡ŒåŠ¨å¯ä»¥è§£é‡Šç›¸åŒçš„è½¬å˜ã€‚æˆ‘ä»¬é€šè¿‡æµ‹è¯•æ—¶åŠ¨ä½œä¼˜åŒ–ç­–ç•¥æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥ç­–ç•¥é€šè¿‡ç”Ÿæˆæ¨¡å‹è¿›è¡Œåå‘ä¼ æ’­ï¼Œä»¥æ¨æ–­å‡ºä¸é¢„æµ‹çš„æœªæ¥æœ€åŒ¹é…çš„è½¨è¿¹çº§æ½œåœ¨å˜é‡ï¼Œä»¥åŠæ®‹å·®é€†åŠ¨æ€æ¨¡å‹ï¼Œå°†è¯¥è½¨è¿¹å…ˆéªŒè½¬åŒ–ä¸ºå‡†ç¡®çš„å¯æ‰§è¡ŒåŠ¨ä½œã€‚å¯¹ä¸‰ä¸ªæ•°æ®é›†çš„å®éªŒè¯æ˜äº† 4D åœºæ™¯ç”Ÿæˆå’Œä¸‹æ¸¸æ“ä½œçš„å¼ºå¤§æ€§èƒ½ï¼Œå¹¶ä¸”æ¶ˆèä¸ºå…³é”®è®¾è®¡é€‰æ‹©æä¾›äº†å®ç”¨çš„è§è§£ã€‚

</details>

---

## 129. Free-GVC: Towards Training-Free Extreme Generative Video Compression with Temporal Coherence

**ä¸­æ–‡æ ‡é¢˜**: Free-GVCï¼šå®ç°å…·æœ‰æ—¶é—´ä¸€è‡´æ€§çš„å…è®­ç»ƒæç«¯ç”Ÿæˆè§†é¢‘å‹ç¼©

**Date**: 2026-02-10 | **arXiv**: [2602.09868v1](http://arxiv.org/abs/2602.09868v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09868v1)

<details><summary><b>Abstract</b></summary>

Building on recent advances in video generation, generative video compression has emerged as a new paradigm for achieving visually pleasing reconstructions. However, existing methods exhibit limited exploitation of temporal correlations, causing noticeable flicker and degraded temporal coherence at ultra-low bitrates. In this paper, we propose Free-GVC, a training-free generative video compression framework that reformulates video coding as latent trajectory compression guided by a video diffusion prior. Our method operates at the group-of-pictures (GOP) level, encoding video segments into a compact latent space and progressively compressing them along the diffusion trajectory. To ensure perceptually consistent reconstruction across GOPs, we introduce an Adaptive Quality Control module that dynamically constructs an online rate-perception surrogate model to predict the optimal diffusion step for each GOP. In addition, an Inter-GOP Alignment module establishes frame overlap and performs latent fusion between adjacent groups, thereby mitigating flicker and enhancing temporal coherence. Experiments show that Free-GVC achieves an average of 93.29% BD-Rate reduction in DISTS over the latest neural codec DCVC-RT, and a user study further confirms its superior perceptual quality and temporal coherence at ultra-low bitrates.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åŸºäºè§†é¢‘ç”Ÿæˆé¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œç”Ÿæˆè§†é¢‘å‹ç¼©å·²æˆä¸ºå®ç°è§†è§‰ä¸Šä»¤äººæ„‰æ‚¦çš„é‡å»ºçš„æ–°èŒƒä¾‹ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¯¹æ—¶é—´ç›¸å…³æ€§çš„åˆ©ç”¨æœ‰é™ï¼Œå¯¼è‡´åœ¨è¶…ä½æ¯”ç‰¹ç‡ä¸‹å‡ºç°æ˜æ˜¾çš„é—ªçƒå’Œæ—¶é—´ç›¸å¹²æ€§ä¸‹é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† Free-GVCï¼Œè¿™æ˜¯ä¸€ç§å…è®­ç»ƒçš„ç”Ÿæˆè§†é¢‘å‹ç¼©æ¡†æ¶ï¼Œå®ƒå°†è§†é¢‘ç¼–ç é‡æ–°è¡¨è¿°ä¸ºç”±è§†é¢‘æ‰©æ•£å…ˆéªŒå¼•å¯¼çš„æ½œåœ¨è½¨è¿¹å‹ç¼©ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾ç‰‡ç»„ï¼ˆGOPï¼‰çº§åˆ«ä¸Šè¿è¡Œï¼Œå°†è§†é¢‘ç‰‡æ®µç¼–ç åˆ°ç´§å‡‘çš„æ½œåœ¨ç©ºé—´ä¸­ï¼Œå¹¶æ²¿ç€æ‰©æ•£è½¨è¿¹é€æ­¥å‹ç¼©å®ƒä»¬ã€‚ä¸ºäº†ç¡®ä¿è·¨ GOP çš„æ„ŸçŸ¥ä¸€è‡´é‡å»ºï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªé€‚åº”è´¨é‡æ§åˆ¶æ¨¡å—ï¼Œè¯¥æ¨¡å—åŠ¨æ€æ„å»ºåœ¨çº¿é€Ÿç‡æ„ŸçŸ¥ä»£ç†æ¨¡å‹æ¥é¢„æµ‹æ¯ä¸ª GOP çš„æœ€ä½³æ‰©æ•£æ­¥éª¤ã€‚æ­¤å¤–ï¼ŒGOP é—´å¯¹é½æ¨¡å—å¯å»ºç«‹å¸§é‡å å¹¶åœ¨ç›¸é‚»ç»„ä¹‹é—´æ‰§è¡Œæ½œåœ¨èåˆï¼Œä»è€Œå‡è½»é—ªçƒå¹¶å¢å¼ºæ—¶é—´ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°çš„ç¥ç»ç¼–è§£ç å™¨ DCVC-RT ç›¸æ¯”ï¼ŒFree-GVC åœ¨ DISTS ä¸­å®ç°äº†å¹³å‡ 93.29% çš„ BD-Rate é™ä½ï¼Œå¹¶ä¸”ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†å…¶åœ¨è¶…ä½æ¯”ç‰¹ç‡ä¸‹çš„å“è¶Šæ„ŸçŸ¥è´¨é‡å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚

</details>

---

## 130. Code2World: A GUI World Model via Renderable Code Generation

**ä¸­æ–‡æ ‡é¢˜**: Code2Worldï¼šé€šè¿‡å¯æ¸²æŸ“ä»£ç ç”Ÿæˆçš„ GUI ä¸–ç•Œæ¨¡å‹

**Date**: 2026-02-10 | **arXiv**: [2602.09856v1](http://arxiv.org/abs/2602.09856v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09856v1)

**Code**: https://github.com/AMAP-ML/Code2World.

<details><summary><b>Abstract</b></summary>

Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è‡ªä¸» GUI ä»£ç†é€šè¿‡æ„ŸçŸ¥ç•Œé¢å¹¶æ‰§è¡Œæ“ä½œä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ã€‚ä½œä¸ºä¸€ä¸ªè™šæ‹Ÿæ²™ç®±ï¼ŒGUI World æ¨¡å‹é€šè¿‡å¯ç”¨åŠ¨ä½œæ¡ä»¶é¢„æµ‹ï¼Œä½¿ä»£ç†å…·æœ‰ç±»ä¼¼äººç±»çš„è¿œè§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºæ–‡æœ¬å’Œåƒç´ çš„æ–¹æ³•å¾ˆéš¾åŒæ—¶å®ç°é«˜è§†è§‰ä¿çœŸåº¦å’Œç»†ç²’åº¦çš„ç»“æ„å¯æ§æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº† Code2Worldï¼Œä¸€ç§è§†è§‰è¯­è¨€ç¼–ç å™¨ï¼Œå¯é€šè¿‡å¯æ¸²æŸ“ä»£ç ç”Ÿæˆæ¥æ¨¡æ‹Ÿä¸‹ä¸€ä¸ªè§†è§‰çŠ¶æ€ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæˆ‘ä»¬é€šè¿‡å°† GUI è½¨è¿¹è½¬æ¢ä¸ºé«˜ä¿çœŸ HTML å¹¶é€šè¿‡è§†è§‰åé¦ˆä¿®è®¢æœºåˆ¶å®Œå–„åˆæˆä»£ç æ¥æ„å»º AndroidCodeï¼Œä»è€Œç”Ÿæˆè¶…è¿‡ 80K é«˜è´¨é‡å±å¹•æ“ä½œå¯¹çš„è¯­æ–™åº“ã€‚ä¸ºäº†ä½¿ç°æœ‰çš„ VLM é€‚åº”ä»£ç é¢„æµ‹ï¼Œæˆ‘ä»¬é¦–å…ˆæ‰§è¡Œ SFT ä½œä¸ºæ ¼å¼å¸ƒå±€éµå¾ªçš„å†·å¯åŠ¨ï¼Œç„¶åè¿›ä¸€æ­¥åº”ç”¨æ¸²æŸ“æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡å¼ºåˆ¶è§†è§‰è¯­ä¹‰ä¿çœŸåº¦å’ŒåŠ¨ä½œä¸€è‡´æ€§ï¼Œä½¿ç”¨æ¸²æŸ“ç»“æœä½œä¸ºå¥–åŠ±ä¿¡å·ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCode2World-8B å®ç°äº†æ€§èƒ½æœ€ä½³çš„ä¸‹ä¸€ä¸ª UI é¢„æµ‹ï¼Œå¯ä¸ç«äº‰æ€§çš„ GPT-5 å’Œ Gemini-3-Pro-Image ç›¸åª²ç¾ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒCode2World ä»¥çµæ´»çš„æ–¹å¼æ˜¾ç€æé«˜äº†ä¸‹æ¸¸å¯¼èˆªçš„æˆåŠŸç‡ï¼Œä½¿ Gemini-2.5-Flash åœ¨ AndroidWorld å¯¼èˆªä¸Šæé«˜äº† 9.5%ã€‚è¯¥ä»£ç å¯ä» https://github.com/AMAP-ML/Code2World è·å–ã€‚

</details>

---

## 131. Reason-IAD: Knowledge-Guided Dynamic Latent Reasoning for Explainable Industrial Anomaly Detection

**ä¸­æ–‡æ ‡é¢˜**: Reason-IADï¼šç”¨äºå¯è§£é‡Šå·¥ä¸šå¼‚å¸¸æ£€æµ‹çš„çŸ¥è¯†å¼•å¯¼åŠ¨æ€æ½œåœ¨æ¨ç†

**Date**: 2026-02-10 | **arXiv**: [2602.09850v1](http://arxiv.org/abs/2602.09850v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09850v1)

**Code**: https://github.com/chenpeng052/Reason-IAD.

<details><summary><b>Abstract</b></summary>

Industrial anomaly detection demands precise reasoning over fine-grained defect patterns. However, existing multimodal large language models (MLLMs), pretrained on general-domain data, often struggle to capture category-specific anomalies, thereby limiting both detection accuracy and interpretability. To address these limitations, we propose Reason-IAD, a knowledge-guided dynamic latent reasoning framework for explainable industrial anomaly detection. Reason-IAD comprises two core components. First, a retrieval-augmented knowledge module incorporates category-specific textual descriptions into the model input, enabling context-aware reasoning over domain-specific defects. Second, an entropy-driven latent reasoning mechanism conducts iterative exploration within a compact latent space using optimizable latent think tokens, guided by an entropy-based reward that encourages confident and stable predictions. Furthermore, a dynamic visual injection strategy selectively incorporates the most informative image patches into the latent sequence, directing the reasoning process toward regions critical for anomaly detection. Extensive experimental results demonstrate that Reason-IAD consistently outperforms state-of-the-art methods. The code will be publicly available at https://github.com/chenpeng052/Reason-IAD.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å·¥ä¸šå¼‚å¸¸æ£€æµ‹éœ€è¦å¯¹ç»†ç²’åº¦ç¼ºé™·æ¨¡å¼è¿›è¡Œç²¾ç¡®æ¨ç†ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨é€šç”¨é¢†åŸŸæ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œé€šå¸¸éš¾ä»¥æ•è·ç‰¹å®šç±»åˆ«çš„å¼‚å¸¸ï¼Œä»è€Œé™åˆ¶äº†æ£€æµ‹çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº† Reason-IADï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¯è§£é‡Šçš„å·¥ä¸šå¼‚å¸¸æ£€æµ‹çš„çŸ¥è¯†å¼•å¯¼çš„åŠ¨æ€æ½œåœ¨æ¨ç†æ¡†æ¶ã€‚ Reason-IAD åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ã€‚é¦–å…ˆï¼Œæ£€ç´¢å¢å¼ºçŸ¥è¯†æ¨¡å—å°†ç‰¹å®šç±»åˆ«çš„æ–‡æœ¬æè¿°åˆå¹¶åˆ°æ¨¡å‹è¾“å…¥ä¸­ï¼Œä»è€Œèƒ½å¤Ÿå¯¹ç‰¹å®šé¢†åŸŸçš„ç¼ºé™·è¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨ç†ã€‚å…¶æ¬¡ï¼Œç†µé©±åŠ¨çš„æ½œåœ¨æ¨ç†æœºåˆ¶ä½¿ç”¨å¯ä¼˜åŒ–çš„æ½œåœ¨æ€è€ƒä»¤ç‰Œåœ¨ç´§å‡‘çš„æ½œåœ¨ç©ºé—´å†…è¿›è¡Œè¿­ä»£æ¢ç´¢ï¼Œå¹¶ä»¥åŸºäºç†µçš„å¥–åŠ±ä¸ºæŒ‡å¯¼ï¼Œé¼“åŠ±è‡ªä¿¡å’Œç¨³å®šçš„é¢„æµ‹ã€‚æ­¤å¤–ï¼ŒåŠ¨æ€è§†è§‰æ³¨å…¥ç­–ç•¥æœ‰é€‰æ‹©åœ°å°†ä¿¡æ¯æœ€ä¸°å¯Œçš„å›¾åƒå—åˆå¹¶åˆ°æ½œåœ¨åºåˆ—ä¸­ï¼Œå°†æ¨ç†è¿‡ç¨‹å¼•å¯¼åˆ°å¯¹å¼‚å¸¸æ£€æµ‹è‡³å…³é‡è¦çš„åŒºåŸŸã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒReason-IAD å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¯¥ä»£ç å°†åœ¨ https://github.com/chenpeng052/Reason-IAD ä¸Šå…¬å¼€æä¾›ã€‚

</details>

---

## 132. Kelix Technique Report

**ä¸­æ–‡æ ‡é¢˜**: Kelix æŠ€æœ¯æŠ¥å‘Š

**Date**: 2026-02-10 | **arXiv**: [2602.09843v2](http://arxiv.org/abs/2602.09843v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.09843v2)

<details><summary><b>Abstract</b></summary>

Autoregressive large language models (LLMs) scale well by expressing diverse tasks as sequences of discrete natural-language tokens and training with next-token prediction, which unifies comprehension and generation under self-supervision. Extending this paradigm to multimodal data requires a shared, discrete representation across modalities. However, most vision-language models (VLMs) still rely on a hybrid interface: discrete text tokens paired with continuous Vision Transformer (ViT) features. Because supervision is largely text-driven, these models are often biased toward understanding and cannot fully leverage large-scale self-supervised learning on non-text data. Recent work has explored discrete visual tokenization to enable fully autoregressive multimodal modeling, showing promising progress toward unified understanding and generation. Yet existing discrete vision tokens frequently lose information due to limited code capacity, resulting in noticeably weaker understanding than continuous-feature VLMs. We present Kelix, a fully discrete autoregressive unified model that closes the understanding gap between discrete and continuous visual representations.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è‡ªå›å½’å¤§è¯­è¨€æ¨¡å‹ (LLM) é€šè¿‡å°†ä¸åŒçš„ä»»åŠ¡è¡¨ç¤ºä¸ºç¦»æ•£çš„è‡ªç„¶è¯­è¨€æ ‡è®°åºåˆ—å¹¶é€šè¿‡ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹è¿›è¡Œè®­ç»ƒï¼Œä»è€Œåœ¨è‡ªæˆ‘ç›‘ç£ä¸‹ç»Ÿä¸€ç†è§£å’Œç”Ÿæˆï¼Œä»è€Œå¯ä»¥å¾ˆå¥½åœ°æ‰©å±•ã€‚å°†è¿™ç§èŒƒå¼æ‰©å±•åˆ°å¤šæ¨¡æ€æ•°æ®éœ€è¦è·¨æ¨¡æ€çš„å…±äº«ã€ç¦»æ•£è¡¨ç¤ºã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°è§†è§‰è¯­è¨€æ¨¡å‹ (VLM) ä»ç„¶ä¾èµ–äºæ··åˆæ¥å£ï¼šç¦»æ•£æ–‡æœ¬æ ‡è®°ä¸è¿ç»­è§†è§‰å˜æ¢å™¨ (ViT) åŠŸèƒ½é…å¯¹ã€‚ç”±äºç›‘ç£å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯æ–‡æœ¬é©±åŠ¨çš„ï¼Œè¿™äº›æ¨¡å‹å¾€å¾€åå‘äºç†è§£ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨å¯¹éæ–‡æœ¬æ•°æ®çš„å¤§è§„æ¨¡è‡ªç›‘ç£å­¦ä¹ ã€‚æœ€è¿‘çš„å·¥ä½œæ¢ç´¢äº†ç¦»æ•£è§†è§‰æ ‡è®°åŒ–ï¼Œä»¥å®ç°å®Œå…¨è‡ªå›å½’å¤šæ¨¡æ€å»ºæ¨¡ï¼Œæ˜¾ç¤ºå‡ºåœ¨ç»Ÿä¸€ç†è§£å’Œç”Ÿæˆæ–¹é¢å–å¾—çš„æœ‰å¸Œæœ›çš„è¿›å±•ã€‚ç„¶è€Œï¼Œç”±äºä»£ç å®¹é‡æœ‰é™ï¼Œç°æœ‰çš„ç¦»æ•£è§†è§‰ä»¤ç‰Œç»å¸¸ä¸¢å¤±ä¿¡æ¯ï¼Œå¯¼è‡´ç†è§£èƒ½åŠ›æ˜æ˜¾å¼±äºè¿ç»­ç‰¹å¾ VLMã€‚æˆ‘ä»¬æå‡º Kelixï¼Œä¸€ä¸ªå®Œå…¨ç¦»æ•£çš„è‡ªå›å½’ç»Ÿä¸€æ¨¡å‹ï¼Œå®ƒç¼©å°äº†ç¦»æ•£å’Œè¿ç»­è§†è§‰è¡¨ç¤ºä¹‹é—´çš„ç†è§£å·®è·ã€‚

</details>

---

## 133. ARK: A Dual-Axis Multimodal Retrieval Benchmark along Reasoning and Knowledge

**ä¸­æ–‡æ ‡é¢˜**: ARKï¼šæ²¿ç€æ¨ç†å’ŒçŸ¥è¯†çš„åŒè½´å¤šæ¨¡æ€æ£€ç´¢åŸºå‡†

**Date**: 2026-02-10 | **arXiv**: [2602.09839v1](http://arxiv.org/abs/2602.09839v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09839v1)

<details><summary><b>Abstract</b></summary>

Existing multimodal retrieval benchmarks largely emphasize semantic matching on daily-life images and offer limited diagnostics of professional knowledge and complex reasoning. To address this gap, we introduce ARK, a benchmark designed to analyze multimodal retrieval from two complementary perspectives: (i) knowledge domains (five domains with 17 subtypes), which characterize the content and expertise retrieval relies on, and (ii) reasoning skills (six categories), which characterize the type of inference over multimodal evidence required to identify the correct candidate. Specifically, ARK evaluates retrieval with both unimodal and multimodal queries and candidates, covering 16 heterogeneous visual data types. To avoid shortcut matching during evaluation, most queries are paired with targeted hard negatives that require multi-step reasoning. We evaluate 23 representative text-based and multimodal retrievers on ARK and observe a pronounced gap between knowledge-intensive and reasoning-intensive retrieval, with fine-grained visual and spatial reasoning emerging as persistent bottlenecks. We further show that simple enhancements such as re-ranking and rewriting yield consistent improvements, but substantial headroom remains.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç°æœ‰çš„å¤šæ¨¡æ€æ£€ç´¢åŸºå‡†ä¸»è¦å¼ºè°ƒæ—¥å¸¸ç”Ÿæ´»å›¾åƒçš„è¯­ä¹‰åŒ¹é…ï¼Œå¹¶æä¾›æœ‰é™çš„ä¸“ä¸šçŸ¥è¯†å’Œå¤æ‚æ¨ç†çš„è¯Šæ–­ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº† ARKï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä»ä¸¤ä¸ªäº’è¡¥è§’åº¦åˆ†æå¤šæ¨¡æ€æ£€ç´¢çš„åŸºå‡†ï¼š(i) çŸ¥è¯†é¢†åŸŸï¼ˆå…·æœ‰ 17 ä¸ªå­ç±»å‹çš„ 5 ä¸ªé¢†åŸŸï¼‰ï¼Œå®ƒæè¿°äº†æ£€ç´¢æ‰€ä¾èµ–çš„å†…å®¹å’Œä¸“ä¸šçŸ¥è¯†ï¼›(ii) æ¨ç†æŠ€èƒ½ï¼ˆå…­ä¸ªç±»åˆ«ï¼‰ï¼Œå®ƒæè¿°äº†è¯†åˆ«æ­£ç¡®å€™é€‰è€…æ‰€éœ€çš„å¤šæ¨¡æ€è¯æ®çš„æ¨ç†ç±»å‹ã€‚å…·ä½“æ¥è¯´ï¼ŒARK ä½¿ç”¨å•æ¨¡æ€å’Œå¤šæ¨¡æ€æŸ¥è¯¢å’Œå€™é€‰æ¥è¯„ä¼°æ£€ç´¢ï¼Œæ¶µç›– 16 ç§å¼‚æ„è§†è§‰æ•°æ®ç±»å‹ã€‚ä¸ºäº†é¿å…åœ¨è¯„ä¼°è¿‡ç¨‹ä¸­è¿›è¡Œå¿«æ·åŒ¹é…ï¼Œå¤§å¤šæ•°æŸ¥è¯¢éƒ½ä¸éœ€è¦å¤šæ­¥éª¤æ¨ç†çš„ç›®æ ‡ç¡¬å¦å®šé…å¯¹ã€‚æˆ‘ä»¬åœ¨ ARK ä¸Šè¯„ä¼°äº† 23 ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„åŸºäºæ–‡æœ¬å’Œå¤šæ¨¡æ€æ£€ç´¢å™¨ï¼Œå¹¶è§‚å¯Ÿåˆ°çŸ¥è¯†å¯†é›†å‹æ£€ç´¢å’Œæ¨ç†å¯†é›†å‹æ£€ç´¢ä¹‹é—´å­˜åœ¨æ˜æ˜¾å·®è·ï¼Œå…¶ä¸­ç»†ç²’åº¦è§†è§‰å’Œç©ºé—´æ¨ç†æˆä¸ºæŒç»­å­˜åœ¨çš„ç“¶é¢ˆã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼Œé‡æ–°æ’åå’Œé‡å†™ç­‰ç®€å•çš„å¢å¼ºåŠŸèƒ½å¯ä»¥å¸¦æ¥ä¸€è‡´çš„æ”¹è¿›ï¼Œä½†ä»ç„¶å­˜åœ¨å·¨å¤§çš„ç©ºé—´ã€‚

</details>

---

## 134. SAKED: Mitigating Hallucination in Large Vision-Language Models via Stability-Aware Knowledge Enhanced Decoding

**ä¸­æ–‡æ ‡é¢˜**: SAKEDï¼šé€šè¿‡ç¨³å®šæ€§æ„ŸçŸ¥çŸ¥è¯†å¢å¼ºè§£ç å‡è½»å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰

**Date**: 2026-02-10 | **arXiv**: [2602.09825v1](http://arxiv.org/abs/2602.09825v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09825v1)

<details><summary><b>Abstract</b></summary>

Hallucinations in Large Vision-Language Models (LVLMs) pose significant security and reliability risks in real-world applications. Inspired by the observation that humans are more error-prone when uncertain or hesitant, we investigate how instability in a model 's internal knowledge contributes to LVLM hallucinations. We conduct extensive empirical analyses from three perspectives, namely attention heads, model layers, and decoding tokens, and identify three key hallucination patterns: (i) visual activation drift across attention heads, (ii) pronounced knowledge fluctuations across layers, and (iii) visual focus distraction between neighboring output tokens. Building on these findings, we propose Stability-Aware Knowledge-Enhanced Decoding (SAKED), which introduces a layer-wise Knowledge Stability Score (KSS) to quantify knowledge stability throughout the model. By contrasting the most stability-aware and stability-agnostic layers, SAKED suppresses decoding noise and dynamically leverages the most reliable internal knowledge for faithful token generation. Moreover, SAKED is training-free and can be seamlessly integrated into different architectures. Extensive experiments demonstrate that SAKED achieves state-of-the-art performance for hallucination mitigation on various models, tasks, and benchmarks.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¤§è§†è§‰è¯­è¨€æ¨¡å‹ (LVLM) ä¸­çš„å¹»è§‰åœ¨ç°å®åº”ç”¨ä¸­å¸¦æ¥äº†é‡å¤§çš„å®‰å…¨å’Œå¯é æ€§é£é™©ã€‚å—äººç±»åœ¨ä¸ç¡®å®šæˆ–çŠ¹è±«æ—¶æ›´å®¹æ˜“å‡ºé”™è¿™ä¸€è§‚å¯Ÿçš„å¯å‘ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ¨¡å‹å†…éƒ¨çŸ¥è¯†çš„ä¸ç¨³å®šæ€§å¦‚ä½•å¯¼è‡´ LVLM å¹»è§‰ã€‚æˆ‘ä»¬ä»ä¸‰ä¸ªè§’åº¦ï¼ˆå³æ³¨æ„åŠ›å¤´ã€æ¨¡å‹å±‚å’Œè§£ç ä»¤ç‰Œï¼‰è¿›è¡Œäº†å¹¿æ³›çš„å®è¯åˆ†æï¼Œå¹¶ç¡®å®šäº†ä¸‰ç§å…³é”®çš„å¹»è§‰æ¨¡å¼ï¼šï¼ˆiï¼‰æ³¨æ„åŠ›å¤´ä¹‹é—´çš„è§†è§‰æ¿€æ´»æ¼‚ç§»ï¼Œï¼ˆiiï¼‰è·¨å±‚çš„æ˜æ˜¾çŸ¥è¯†æ³¢åŠ¨ï¼Œä»¥åŠï¼ˆiiiï¼‰ç›¸é‚»è¾“å‡ºä»¤ç‰Œä¹‹é—´çš„è§†è§‰ç„¦ç‚¹åˆ†æ•£ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ç¨³å®šæ€§æ„ŸçŸ¥çŸ¥è¯†å¢å¼ºè§£ç ï¼ˆSAKEDï¼‰ï¼Œå®ƒå¼•å…¥äº†åˆ†å±‚çŸ¥è¯†ç¨³å®šæ€§è¯„åˆ†ï¼ˆKSSï¼‰æ¥é‡åŒ–æ•´ä¸ªæ¨¡å‹çš„çŸ¥è¯†ç¨³å®šæ€§ã€‚é€šè¿‡å¯¹æ¯”æœ€ç¨³å®šçš„æ„ŸçŸ¥å±‚å’Œä¸ç¨³å®šæ€§æ— å…³çš„å±‚ï¼ŒSAKED æŠ‘åˆ¶è§£ç å™ªå£°å¹¶åŠ¨æ€åˆ©ç”¨æœ€å¯é çš„å†…éƒ¨çŸ¥è¯†æ¥å¿ å®åœ°ç”Ÿæˆä»¤ç‰Œã€‚æ­¤å¤–ï¼ŒSAKEDæ— éœ€åŸ¹è®­ï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°ä¸åŒçš„æ¶æ„ä¸­ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSAKED åœ¨å„ç§æ¨¡å‹ã€ä»»åŠ¡å’ŒåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„å¹»è§‰ç¼“è§£æ€§èƒ½ã€‚

</details>

---

## 135. SciFlow-Bench: Evaluating Structure-Aware Scientific Diagram Generation via Inverse Parsing

**ä¸­æ–‡æ ‡é¢˜**: SciFlow-Benchï¼šé€šè¿‡é€†å‘è§£æè¯„ä¼°ç»“æ„æ„ŸçŸ¥ç§‘å­¦å›¾ç”Ÿæˆ

**Date**: 2026-02-10 | **arXiv**: [2602.09809v1](http://arxiv.org/abs/2602.09809v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09809v1)

<details><summary><b>Abstract</b></summary>

Scientific diagrams convey explicit structural information, yet modern text-to-image models often produce visually plausible but structurally incorrect results. Existing benchmarks either rely on image-centric or subjective metrics insensitive to structure, or evaluate intermediate symbolic representations rather than final rendered images, leaving pixel-based diagram generation underexplored. We introduce SciFlow-Bench, a structure-first benchmark for evaluating scientific diagram generation directly from pixel-level outputs. Built from real scientific PDFs, SciFlow-Bench pairs each source framework figure with a canonical ground-truth graph and evaluates models as black-box image generators under a closed-loop, round-trip protocol that inverse-parses generated diagram images back into structured graphs for comparison. This design enforces evaluation by structural recoverability rather than visual similarity alone, and is enabled by a hierarchical multi-agent system that coordinates planning, perception, and structural reasoning. Experiments show that preserving structural correctness remains a fundamental challenge, particularly for diagrams with complex topology, underscoring the need for structure-aware evaluation.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç§‘å­¦å›¾è¡¨ä¼ è¾¾äº†æ˜ç¡®çš„ç»“æ„ä¿¡æ¯ï¼Œä½†ç°ä»£æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹é€šå¸¸ä¼šäº§ç”Ÿè§†è§‰ä¸Šåˆç†ä½†ç»“æ„ä¸Šä¸æ­£ç¡®çš„ç»“æœã€‚ç°æœ‰çš„åŸºå‡†è¦ä¹ˆä¾èµ–äºä»¥å›¾åƒä¸ºä¸­å¿ƒçš„æˆ–å¯¹ç»“æ„ä¸æ•æ„Ÿçš„ä¸»è§‚æŒ‡æ ‡ï¼Œè¦ä¹ˆè¯„ä¼°ä¸­é—´ç¬¦å·è¡¨ç¤ºè€Œä¸æ˜¯æœ€ç»ˆæ¸²æŸ“çš„å›¾åƒï¼Œä»è€Œå¯¼è‡´åŸºäºåƒç´ çš„å›¾è¡¨ç”Ÿæˆå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬å¼•å…¥äº† SciFlow-Benchï¼Œè¿™æ˜¯ä¸€ç§ç»“æ„ä¼˜å…ˆçš„åŸºå‡†ï¼Œç”¨äºç›´æ¥ä»åƒç´ çº§è¾“å‡ºè¯„ä¼°ç§‘å­¦å›¾è¡¨çš„ç”Ÿæˆã€‚ SciFlow-Bench ä»¥çœŸæ­£çš„ç§‘å­¦ PDF ä¸ºåŸºç¡€ï¼Œå°†æ¯ä¸ªæºæ¡†æ¶å›¾ä¸è§„èŒƒçš„åœ°é¢å®å†µå›¾é…å¯¹ï¼Œå¹¶åœ¨é—­ç¯ã€å¾€è¿”åè®®ä¸‹å°†æ¨¡å‹è¯„ä¼°ä¸ºé»‘ç›’å›¾åƒç”Ÿæˆå™¨ï¼Œè¯¥åè®®å°†ç”Ÿæˆçš„å›¾è¡¨å›¾åƒåå‘è§£æå›ç»“æ„åŒ–å›¾ä»¥è¿›è¡Œæ¯”è¾ƒã€‚è¯¥è®¾è®¡é€šè¿‡ç»“æ„å¯æ¢å¤æ€§è€Œä¸æ˜¯ä»…é€šè¿‡è§†è§‰ç›¸ä¼¼æ€§æ¥å¼ºåˆ¶è¿›è¡Œè¯„ä¼°ï¼Œå¹¶é€šè¿‡åè°ƒè§„åˆ’ã€æ„ŸçŸ¥å’Œç»“æ„æ¨ç†çš„åˆ†å±‚å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ¥å®ç°ã€‚å®éªŒè¡¨æ˜ï¼Œä¿æŒç»“æ„æ­£ç¡®æ€§ä»ç„¶æ˜¯ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå…·æœ‰å¤æ‚æ‹“æ‰‘çš„å›¾ï¼Œè¿™å¼ºè°ƒäº†ç»“æ„æ„ŸçŸ¥è¯„ä¼°çš„å¿…è¦æ€§ã€‚

</details>

---

## 136. Where Do Images Come From? Analyzing Captions to Geographically Profile Datasets

**ä¸­æ–‡æ ‡é¢˜**: å›¾åƒä»å“ªé‡Œæ¥ï¼Ÿåˆ†æè¯´æ˜ä»¥åœ°ç†å‰–ææ•°æ®é›†

**Date**: 2026-02-10 | **arXiv**: [2602.09775v1](http://arxiv.org/abs/2602.09775v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09775v1)

<details><summary><b>Abstract</b></summary>

Recent studies show that text-to-image models often fail to generate geographically representative images, raising concerns about the representativeness of their training data and motivating the question: which parts of the world do these training examples come from? We geographically profile large-scale multimodal datasets by mapping image-caption pairs to countries based on location information extracted from captions using LLMs. Studying English captions from three widely used datasets (Re-LAION, DataComp1B, and Conceptual Captions) across $20$ common entities (e.g., house, flag), we find that the United States, the United Kingdom, and Canada account for $48.0\%$ of samples, while South American and African countries are severely under-represented with only $1.8\%$ and $3.8\%$ of images, respectively. We observe a strong correlation between a country's GDP and its representation in the data ($Ï= 0.82$). Examining non-English subsets for $4$ languages from the Re-LAION dataset, we find that representation skews heavily toward countries where these languages are predominantly spoken. Additionally, we find that higher representation does not necessarily translate to greater visual or semantic diversity. Finally, analyzing country-specific images generated by Stable Diffusion v1.3 trained on Re-LAION, we show that while generations appear realistic, they are severely limited in their coverage compared to real-world images.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹é€šå¸¸æ— æ³•ç”Ÿæˆå…·æœ‰åœ°ç†ä»£è¡¨æ€§çš„å›¾åƒï¼Œè¿™å¼•èµ·äº†äººä»¬å¯¹å…¶è®­ç»ƒæ•°æ®ä»£è¡¨æ€§çš„æ‹…å¿§ï¼Œå¹¶å¼•å‘äº†ä¸€ä¸ªé—®é¢˜ï¼šè¿™äº›è®­ç»ƒç¤ºä¾‹æ¥è‡ªä¸–ç•Œçš„å“ªäº›åœ°åŒºï¼Ÿæˆ‘ä»¬æ ¹æ®ä½¿ç”¨æ³•å­¦ç¡•å£«ä»å­—å¹•ä¸­æå–çš„ä½ç½®ä¿¡æ¯ï¼Œå°†å›¾åƒå­—å¹•å¯¹æ˜ å°„åˆ°å›½å®¶/åœ°åŒºï¼Œä»è€Œå¯¹å¤§è§„æ¨¡å¤šæ¨¡å¼æ•°æ®é›†è¿›è¡Œåœ°ç†åˆ†æã€‚ç ”ç©¶ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ï¼ˆRe-LAIONã€DataComp1B å’Œ Conceptual Captionsï¼‰ä¸­æ¶‰åŠ 20 ç¾å…ƒå¸¸è§å®ä½“ï¼ˆä¾‹å¦‚æˆ¿å±‹ã€æ——å¸œï¼‰çš„è‹±æ–‡å­—å¹•ï¼Œæˆ‘ä»¬å‘ç°ç¾å›½ã€è‹±å›½å’ŒåŠ æ‹¿å¤§å æ ·æœ¬çš„ 48.0\%$ï¼Œè€Œå—ç¾å’Œéæ´²å›½å®¶çš„ä»£è¡¨æ€§ä¸¥é‡ä¸è¶³ï¼Œåˆ†åˆ«åªæœ‰ $1.8\%$ å’Œ $3.8\%$ çš„å›¾åƒã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸€ä¸ªå›½å®¶çš„ GDP ä¸å…¶åœ¨æ•°æ®ä¸­çš„è¡¨ç¤ºå½¢å¼ä¹‹é—´å­˜åœ¨å¾ˆå¼ºçš„ç›¸å…³æ€§ ($Ï= 0.82$)ã€‚æ£€æŸ¥ Re-LAION æ•°æ®é›†ä¸­ 4 ç¾å…ƒè¯­è¨€çš„éè‹±è¯­å­é›†ï¼Œæˆ‘ä»¬å‘ç°ä»£è¡¨æ€§ä¸¥é‡åå‘ä¸»è¦ä½¿ç”¨è¿™äº›è¯­è¨€çš„å›½å®¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°æ›´é«˜çš„è¡¨ç¤ºå¹¶ä¸ä¸€å®šæ„å‘³ç€æ›´å¤§çš„è§†è§‰æˆ–è¯­ä¹‰å¤šæ ·æ€§ã€‚æœ€åï¼Œé€šè¿‡åˆ†æåœ¨ Re-LAION ä¸Šè®­ç»ƒçš„ Stable Diffusion v1.3 ç”Ÿæˆçš„ç‰¹å®šå›½å®¶å›¾åƒï¼Œæˆ‘ä»¬å‘ç°è™½ç„¶å„ä»£å›¾åƒçœ‹èµ·æ¥å¾ˆçœŸå®ï¼Œä½†ä¸çœŸå®ä¸–ç•Œå›¾åƒç›¸æ¯”ï¼Œå®ƒä»¬çš„è¦†ç›–èŒƒå›´å—åˆ°ä¸¥é‡é™åˆ¶ã€‚

</details>

---

## 137. Self-Supervised Learning as Discrete Communication

**ä¸­æ–‡æ ‡é¢˜**: ä½œä¸ºç¦»æ•£æ²Ÿé€šçš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ 

**Date**: 2026-02-10 | **arXiv**: [2602.09764v1](http://arxiv.org/abs/2602.09764v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09764v1)

<details><summary><b>Abstract</b></summary>

Most self-supervised learning (SSL) methods learn continuous visual representations by aligning different views of the same input, offering limited control over how information is structured across representation dimensions. In this work, we frame visual self-supervised learning as a discrete communication process between a teacher and a student network, where semantic information is transmitted through a fixed-capacity binary channel. Rather than aligning continuous features, the student predicts multi-label binary messages produced by the teacher. Discrete agreement is enforced through an element-wise binary cross-entropy objective, while a coding-rate regularization term encourages effective utilization of the constrained channel, promoting structured representations. We further show that periodically reinitializing the projection head strengthens this effect by encouraging embeddings that remain predictive across multiple discrete encodings. Extensive experiments demonstrate consistent improvements over continuous agreement baselines on image classification, retrieval, and dense visual prediction tasks, as well as under domain shift through self-supervised adaptation. Beyond backbone representations, we analyze the learned binary codes and show that they form a compact and informative discrete language, capturing semantic factors reusable across classes.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¤§å¤šæ•°è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ–¹æ³•é€šè¿‡å¯¹é½åŒä¸€è¾“å…¥çš„ä¸åŒè§†å›¾æ¥å­¦ä¹ è¿ç»­çš„è§†è§‰è¡¨ç¤ºï¼Œä»è€Œå¯¹è·¨è¡¨ç¤ºç»´åº¦çš„ä¿¡æ¯ç»“æ„æä¾›æœ‰é™çš„æ§åˆ¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†è§†è§‰è‡ªç›‘ç£å­¦ä¹ æ„å»ºä¸ºæ•™å¸ˆå’Œå­¦ç”Ÿç½‘ç»œä¹‹é—´çš„ç¦»æ•£é€šä¿¡è¿‡ç¨‹ï¼Œå…¶ä¸­è¯­ä¹‰ä¿¡æ¯é€šè¿‡å›ºå®šå®¹é‡çš„äºŒè¿›åˆ¶é€šé“ä¼ è¾“ã€‚å­¦ç”Ÿä¸æ˜¯å¯¹é½è¿ç»­ç‰¹å¾ï¼Œè€Œæ˜¯é¢„æµ‹æ•™å¸ˆäº§ç”Ÿçš„å¤šæ ‡ç­¾äºŒè¿›åˆ¶æ¶ˆæ¯ã€‚ç¦»æ•£ä¸€è‡´æ€§æ˜¯é€šè¿‡å…ƒç´ çº§äºŒå…ƒäº¤å‰ç†µç›®æ ‡æ¥å¼ºåˆ¶æ‰§è¡Œçš„ï¼Œè€Œç¼–ç ç‡æ­£åˆ™åŒ–é¡¹åˆ™é¼“åŠ±æœ‰æ•ˆåˆ©ç”¨å—é™é€šé“ï¼Œä»è€Œä¿ƒè¿›ç»“æ„åŒ–è¡¨ç¤ºã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼Œå®šæœŸé‡æ–°åˆå§‹åŒ–æŠ•å½±å¤´å¯ä»¥é€šè¿‡é¼“åŠ±åœ¨å¤šä¸ªç¦»æ•£ç¼–ç ä¸­ä¿æŒé¢„æµ‹æ€§çš„åµŒå…¥æ¥å¢å¼ºè¿™ç§æ•ˆæœã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼Œåœ¨å›¾åƒåˆ†ç±»ã€æ£€ç´¢å’Œå¯†é›†è§†è§‰é¢„æµ‹ä»»åŠ¡ä¸Šï¼Œä»¥åŠé€šè¿‡è‡ªæˆ‘ç›‘ç£é€‚åº”è¿›è¡ŒåŸŸè½¬ç§»æ–¹é¢ï¼Œåœ¨è¿ç»­ä¸€è‡´æ€§åŸºçº¿ä¸Šå–å¾—äº†ä¸€è‡´çš„æ”¹è¿›ã€‚é™¤äº†ä¸»å¹²è¡¨ç¤ºä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ†æäº†å­¦ä¹ åˆ°çš„äºŒè¿›åˆ¶ä»£ç ï¼Œå¹¶è¡¨æ˜å®ƒä»¬å½¢æˆäº†ä¸€ç§ç´§å‡‘ä¸”ä¿¡æ¯ä¸°å¯Œçš„ç¦»æ•£è¯­è¨€ï¼Œæ•è·äº†å¯è·¨ç±»é‡ç”¨çš„è¯­ä¹‰å› ç´ ã€‚

</details>

---

## 138. Robust Vision Systems for Connected and Autonomous Vehicles: Security Challenges and Attack Vectors

**ä¸­æ–‡æ ‡é¢˜**: é€‚ç”¨äºè”ç½‘å’Œè‡ªåŠ¨é©¾é©¶è½¦è¾†çš„é²æ£’è§†è§‰ç³»ç»Ÿï¼šå®‰å…¨æŒ‘æˆ˜å’Œæ”»å‡»å‘é‡

**Date**: 2026-02-10 | **arXiv**: [2602.09740v2](http://arxiv.org/abs/2602.09740v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.09740v2)

<details><summary><b>Abstract</b></summary>

This article investigates the robustness of vision systems in Connected and Autonomous Vehicles (CAVs), which is critical for developing Level-5 autonomous driving capabilities. Safe and reliable CAV navigation undeniably depends on robust vision systems that enable accurate detection of objects, lane markings, and traffic signage. We analyze the key sensors and vision components essential for CAV navigation to derive a reference architecture for CAV vision system (CAVVS). This reference architecture provides a basis for identifying potential attack surfaces of CAVVS. Subsequently, we elaborate on identified attack vectors targeting each attack surface, rigorously evaluating their implications for confidentiality, integrity, and availability (CIA). Our study provides a comprehensive understanding of attack vector dynamics in vision systems, which is crucial for formulating robust security measures that can uphold the principles of the CIA triad.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æœ¬æ–‡ç ”ç©¶äº†è”ç½‘è‡ªåŠ¨é©¾é©¶è½¦è¾† (CAV) ä¸­è§†è§‰ç³»ç»Ÿçš„ç¨³å¥æ€§ï¼Œè¿™å¯¹äºå¼€å‘ 5 çº§è‡ªåŠ¨é©¾é©¶åŠŸèƒ½è‡³å…³é‡è¦ã€‚å®‰å…¨å¯é çš„ CAV å¯¼èˆªæ— ç–‘ä¾èµ–äºå¼ºå¤§çš„è§†è§‰ç³»ç»Ÿï¼Œèƒ½å¤Ÿå‡†ç¡®æ£€æµ‹ç‰©ä½“ã€è½¦é“æ ‡è®°å’Œäº¤é€šæ ‡å¿—ã€‚æˆ‘ä»¬åˆ†æäº† CAV å¯¼èˆªæ‰€å¿…éœ€çš„å…³é”®ä¼ æ„Ÿå™¨å’Œè§†è§‰ç»„ä»¶ï¼Œå¾—å‡º CAV è§†è§‰ç³»ç»Ÿ (CAVVS) çš„å‚è€ƒæ¶æ„ã€‚è¯¥å‚è€ƒæ¶æ„ä¸ºè¯†åˆ« CAVVS çš„æ½œåœ¨æ”»å‡»é¢æä¾›äº†åŸºç¡€ã€‚éšåï¼Œæˆ‘ä»¬è¯¦ç»†é˜è¿°é’ˆå¯¹æ¯ä¸ªæ”»å‡»é¢çš„å·²è¯†åˆ«æ”»å‡»å‘é‡ï¼Œä¸¥æ ¼è¯„ä¼°å®ƒä»¬å¯¹æœºå¯†æ€§ã€å®Œæ•´æ€§å’Œå¯ç”¨æ€§ (CIA) çš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶æä¾›äº†å¯¹è§†è§‰ç³»ç»Ÿä¸­æ”»å‡»å‘é‡åŠ¨æ€çš„å…¨é¢äº†è§£ï¼Œè¿™å¯¹äºåˆ¶å®šèƒ½å¤Ÿç»´æŠ¤ CIA ä¸‰åˆä¼šåŸåˆ™çš„å¼ºå¤§å®‰å…¨æªæ–½è‡³å…³é‡è¦ã€‚

</details>

---

## 139. Toward Fine-Grained Facial Control in 3D Talking Head Generation

**ä¸­æ–‡æ ‡é¢˜**: å®ç° 3D å¤´éƒ¨è¯´è¯ä¸­çš„ç»†ç²’åº¦é¢éƒ¨æ§åˆ¶

**Date**: 2026-02-10 | **arXiv**: [2602.09736v1](http://arxiv.org/abs/2602.09736v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09736v1)

<details><summary><b>Abstract</b></summary>

Audio-driven talking head generation is a core component of digital avatars, and 3D Gaussian Splatting has shown strong performance in real-time rendering of high-fidelity talking heads. However, achieving precise control over fine-grained facial movements remains a significant challenge, particularly due to lip-synchronization inaccuracies and facial jitter, both of which can contribute to the uncanny valley effect. To address these challenges, we propose Fine-Grained 3D Gaussian Splatting (FG-3DGS), a novel framework that enables temporally consistent and high-fidelity talking head generation. Our method introduces a frequency-aware disentanglement strategy to explicitly model facial regions based on their motion characteristics. Low-frequency regions, such as the cheeks, nose, and forehead, are jointly modeled using a standard MLP, while high-frequency regions, including the eyes and mouth, are captured separately using a dedicated network guided by facial area masks. The predicted motion dynamics, represented as Gaussian deltas, are applied to the static Gaussians to generate the final head frames, which are rendered via a rasterizer using frame-specific camera parameters. Additionally, a high-frequency-refined post-rendering alignment mechanism, learned from large-scale audio-video pairs by a pretrained model, is incorporated to enhance per-frame generation and achieve more accurate lip synchronization. Extensive experiments on widely used datasets for talking head generation demonstrate that our method outperforms recent state-of-the-art approaches in producing high-fidelity, lip-synced talking head videos.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

éŸ³é¢‘é©±åŠ¨çš„å¤´åƒç”Ÿæˆæ˜¯æ•°å­—åŒ–èº«çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼Œ3D Gaussian Splatting åœ¨é«˜ä¿çœŸå¤´åƒå®æ—¶æ¸²æŸ“æ–¹é¢è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ç°å¯¹ç»†ç²’åº¦é¢éƒ¨è¿åŠ¨çš„ç²¾ç¡®æ§åˆ¶ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯ç”±äºå£å‹åŒæ­¥ä¸å‡†ç¡®å’Œé¢éƒ¨æŠ–åŠ¨ï¼Œè¿™ä¸¤è€…éƒ½å¯èƒ½å¯¼è‡´ææ€–è°·æ•ˆåº”ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç»†ç²’åº¦ 3D é«˜æ–¯åˆ†å¸ƒ (FG-3DGS)ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œå¯ä»¥å®ç°æ—¶é—´ä¸€è‡´å’Œé«˜ä¿çœŸå¤´éƒ¨è¯´è¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†é¢‘ç‡æ„ŸçŸ¥çš„è§£ç¼ ç»“ç­–ç•¥ï¼Œä»¥æ ¹æ®è¿åŠ¨ç‰¹å¾æ˜¾å¼åœ°å»ºæ¨¡é¢éƒ¨åŒºåŸŸã€‚ä½é¢‘åŒºåŸŸï¼ˆä¾‹å¦‚è„¸é¢Šã€é¼»å­å’Œå‰é¢ï¼‰ä½¿ç”¨æ ‡å‡† MLP è”åˆå»ºæ¨¡ï¼Œè€Œé«˜é¢‘åŒºåŸŸï¼ˆåŒ…æ‹¬çœ¼ç›å’Œå˜´å·´ï¼‰åˆ™ä½¿ç”¨ç”±é¢éƒ¨åŒºåŸŸæ©æ¨¡å¼•å¯¼çš„ä¸“ç”¨ç½‘ç»œå•ç‹¬æ•è·ã€‚é¢„æµ‹çš„è¿åŠ¨åŠ¨æ€ï¼ˆè¡¨ç¤ºä¸ºé«˜æ–¯å¢é‡ï¼‰åº”ç”¨äºé™æ€é«˜æ–¯ä»¥ç”Ÿæˆæœ€ç»ˆçš„å¤´éƒ¨å¸§ï¼Œè¯¥å¤´éƒ¨å¸§é€šè¿‡å…‰æ …åŒ–å™¨ä½¿ç”¨ç‰¹å®šäºå¸§çš„ç›¸æœºå‚æ•°è¿›è¡Œæ¸²æŸ“ã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨äº†é€šè¿‡é¢„è®­ç»ƒæ¨¡å‹ä»å¤§è§„æ¨¡éŸ³é¢‘-è§†é¢‘å¯¹ä¸­å­¦ä¹ çš„é«˜é¢‘ç»†åŒ–åæ¸²æŸ“å¯¹é½æœºåˆ¶ï¼Œä»¥å¢å¼ºæ¯å¸§ç”Ÿæˆå¹¶å®ç°æ›´å‡†ç¡®çš„å”‡å½¢åŒæ­¥ã€‚å¯¹å¹¿æ³›ä½¿ç”¨çš„å¤´åƒç”Ÿæˆæ•°æ®é›†è¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆé«˜ä¿çœŸã€å£å‹åŒæ­¥çš„å¤´åƒè§†é¢‘æ–¹é¢ä¼˜äºæœ€æ–°çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚

</details>

---

## 140. Allure of Craquelure: A Variational-Generative Approach to Crack Detection in Paintings

**ä¸­æ–‡æ ‡é¢˜**: è£‚çº¹çš„é­…åŠ›ï¼šç»˜ç”»è£‚çº¹æ£€æµ‹çš„å˜åˆ†ç”Ÿæˆæ–¹æ³•

**Date**: 2026-02-10 | **arXiv**: [2602.09730v1](http://arxiv.org/abs/2602.09730v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09730v1)

<details><summary><b>Abstract</b></summary>

Recent advances in imaging technologies, deep learning and numerical performance have enabled non-invasive detailed analysis of artworks, supporting their documentation and conservation. In particular, automated detection of craquelure in digitized paintings is crucial for assessing degradation and guiding restoration, yet remains challenging due to the possibly complex scenery and the visual similarity between cracks and crack-like artistic features such as brush strokes or hair. We propose a hybrid approach that models crack detection as an inverse problem, decomposing an observed image into a crack-free painting and a crack component. A deep generative model is employed as powerful prior for the underlying artwork, while crack structures are captured using a Mumford--Shah-type variational functional together with a crack prior. Joint optimization yields a pixel-level map of crack localizations in the painting.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æˆåƒæŠ€æœ¯ã€æ·±åº¦å­¦ä¹ å’Œæ•°å€¼æ€§èƒ½çš„æœ€æ–°è¿›å±•ä½¿å¾—å¯¹è‰ºæœ¯å“è¿›è¡Œéä¾µå…¥å¼è¯¦ç»†åˆ†ææˆä¸ºå¯èƒ½ï¼Œæ”¯æŒå…¶è®°å½•å’Œä¿æŠ¤ã€‚ç‰¹åˆ«æ˜¯ï¼Œæ•°å­—åŒ–ç»˜ç”»ä¸­è£‚çº¹çš„è‡ªåŠ¨æ£€æµ‹å¯¹äºè¯„ä¼°é€€åŒ–å’ŒæŒ‡å¯¼ä¿®å¤è‡³å…³é‡è¦ï¼Œä½†ç”±äºå¯èƒ½å¤æ‚çš„åœºæ™¯ä»¥åŠè£‚çº¹å’Œç±»ä¼¼è£‚çº¹çš„è‰ºæœ¯ç‰¹å¾ï¼ˆä¾‹å¦‚ç¬”è§¦æˆ–å¤´å‘ï¼‰ä¹‹é—´çš„è§†è§‰ç›¸ä¼¼æ€§ï¼Œä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆæ–¹æ³•ï¼Œå°†è£‚çº¹æ£€æµ‹å»ºæ¨¡ä¸ºé€†é—®é¢˜ï¼Œå°†è§‚å¯Ÿåˆ°çš„å›¾åƒåˆ†è§£ä¸ºæ— è£‚çº¹çš„ç»˜ç”»å’Œè£‚çº¹ç»„ä»¶ã€‚é‡‡ç”¨æ·±åº¦ç”Ÿæˆæ¨¡å‹ä½œä¸ºåº•å±‚è‰ºæœ¯å“çš„å¼ºå¤§å…ˆéªŒï¼Œè€Œä½¿ç”¨ Mumford-Shah å‹å˜åˆ†å‡½æ•°å’Œè£‚çº¹å…ˆéªŒæ¥æ•è·è£‚çº¹ç»“æ„ã€‚è”åˆä¼˜åŒ–äº§ç”Ÿäº†ç»˜ç”»ä¸­è£‚çº¹å®šä½çš„åƒç´ çº§å›¾ã€‚

</details>

---

## 141. GenSeg-R1: RL-Driven Vision-Language Grounding for Fine-Grained Referring Segmentation

**ä¸­æ–‡æ ‡é¢˜**: GenSeg-R1ï¼šRL é©±åŠ¨çš„è§†è§‰è¯­è¨€åŸºç¡€ï¼Œç”¨äºç»†ç²’åº¦å‚è€ƒåˆ†å‰²

**Date**: 2026-02-10 | **arXiv**: [2602.09701v1](http://arxiv.org/abs/2602.09701v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09701v1)

<details><summary><b>Abstract</b></summary>

We study fine-grained referring image segmentation via a decoupled reason-then-segment pipeline. A vision-language model (VLM) receives an image and a natural-language query, reasons about the scene, and emits structured spatial prompts: a bounding box plus two interior keypoints for every referred instance. A frozen promptable segmenter (SAM 2) converts these prompts into high-quality masks.   Within our GenSeg-R1 framework we finetune Qwen3-VL models (4B and 8B parameters) using Group Relative Policy Optimization (GRPO), requiring no supervised reasoning-chain annotations. On RefCOCOg validation our best model (GenSeg-R1-8B) achieves 0.7127 cIoU and 0.7382 mIoU, substantially outperforming the corresponding Qwen3-VL Instruct baselines (+15.3 and +21.9 points, respectively) and surpassing Seg-Zero-7B [3] by +3.3 cIoU under identical evaluation.   We further introduce GenSeg-R1-G, a variant trained on GRefCOCO [9] with a SAM 2 in-the-loop reward that directly optimizes mask quality. On GRefCOCO validation GenSeg-R1-G achieves 76.69% target mIoU with 82.40% accuracy on negative (no-target) prompts, substantially outperforming Seg-R1-7B and Seg-Zero-7B, which lack no-target detection capability. On ReasonSeg test, GenSeg-R1-4B reaches 68.40% mIoU, surpassing Seg-Zero-7B by +7.0 and Seg-R1-7B by +10.7 points.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æˆ‘ä»¬é€šè¿‡è§£è€¦çš„æ¨ç†ç„¶ååˆ†æ®µç®¡é“ç ”ç©¶ç»†ç²’åº¦å‚è€ƒå›¾åƒåˆ†å‰²ã€‚è§†è§‰è¯­è¨€æ¨¡å‹ (VLM) æ¥æ”¶å›¾åƒå’Œè‡ªç„¶è¯­è¨€æŸ¥è¯¢ã€åœºæ™¯æ¨ç†ï¼Œå¹¶å‘å‡ºç»“æ„åŒ–ç©ºé—´æç¤ºï¼šä¸€ä¸ªè¾¹ç•Œæ¡†ä»¥åŠæ¯ä¸ªå¼•ç”¨å®ä¾‹çš„ä¸¤ä¸ªå†…éƒ¨å…³é”®ç‚¹ã€‚å†»ç»“çš„æç¤ºåˆ†æ®µå™¨ (SAM 2) å°†è¿™äº›æç¤ºè½¬æ¢ä¸ºé«˜è´¨é‡çš„è’™ç‰ˆã€‚   åœ¨æˆ‘ä»¬çš„ GenSeg-R1 æ¡†æ¶ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ– (GRPO) å¾®è°ƒ Qwen3-VL æ¨¡å‹ï¼ˆ4B å’Œ 8B å‚æ•°ï¼‰ï¼Œä¸éœ€è¦ç›‘ç£æ¨ç†é“¾æ³¨é‡Šã€‚åœ¨ RefCOCOg éªŒè¯ä¸­ï¼Œæˆ‘ä»¬çš„æœ€ä½³æ¨¡å‹ (GenSeg-R1-8B) è¾¾åˆ°äº† 0.7127 cIoU å’Œ 0.7382 mIoUï¼Œå¤§å¤§ä¼˜äºç›¸åº”çš„ Qwen3-VL Instruct åŸºçº¿ï¼ˆåˆ†åˆ«ä¸º +15.3 å’Œ +21.9 åˆ†ï¼‰ï¼Œå¹¶åœ¨ç›¸åŒè¯„ä¼°ä¸‹è¶…è¿‡ Seg-Zero-7B [3] +3.3 cIoUã€‚   æˆ‘ä»¬è¿›ä¸€æ­¥ä»‹ç»äº† GenSeg-R1-Gï¼Œè¿™æ˜¯ä¸€ç§åœ¨ GRefCOCO [9] ä¸Šè®­ç»ƒçš„å˜ä½“ï¼Œå…·æœ‰ SAM 2 å¾ªç¯å¥–åŠ±ï¼Œå¯ç›´æ¥ä¼˜åŒ–æ©æ¨¡è´¨é‡ã€‚åœ¨ GRefCOCO éªŒè¯ä¸­ï¼ŒGenSeg-R1-G å®ç°äº† 76.69% çš„ç›®æ ‡ mIoUï¼Œåœ¨é˜´æ€§ï¼ˆæ— ç›®æ ‡ï¼‰æç¤ºä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ° 82.40%ï¼Œå¤§å¤§ä¼˜äºç¼ºä¹æ— ç›®æ ‡æ£€æµ‹èƒ½åŠ›çš„ Seg-R1-7B å’Œ Seg-Zero-7Bã€‚åœ¨ ReasonSeg æµ‹è¯•ä¸­ï¼ŒGenSeg-R1-4B è¾¾åˆ° 68.40% mIoUï¼Œè¶…è¿‡ Seg-Zero-7B +7.0 ç‚¹ï¼Œè¶…è¿‡ Seg-R1-7B +10.7 ç‚¹ã€‚

</details>

---

## 142. Semi-supervised Liver Segmentation and Patch-based Fibrosis Staging with Registration-aided Multi-parametric MRI

**ä¸­æ–‡æ ‡é¢˜**: ä½¿ç”¨é…å‡†è¾…åŠ©å¤šå‚æ•° MRI è¿›è¡ŒåŠç›‘ç£è‚è„åˆ†å‰²å’ŒåŸºäºæ–‘å—çš„çº¤ç»´åŒ–åˆ†æœŸ

**Date**: 2026-02-10 | **arXiv**: [2602.09686v1](http://arxiv.org/abs/2602.09686v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09686v1)

**Code**: https://github.com/mileywang3061/Care-Liver

<details><summary><b>Abstract</b></summary>

Liver fibrosis poses a substantial challenge in clinical practice, emphasizing the necessity for precise liver segmentation and accurate disease staging. Based on the CARE Liver 2025 Track 4 Challenge, this study introduces a multi-task deep learning framework developed for liver segmentation (LiSeg) and liver fibrosis staging (LiFS) using multiparametric MRI. The LiSeg phase addresses the challenge of limited annotated images and the complexities of multi-parametric MRI data by employing a semi-supervised learning model that integrates image segmentation and registration. By leveraging both labeled and unlabeled data, the model overcomes the difficulties introduced by domain shifts and variations across modalities. In the LiFS phase, we employed a patchbased method which allows the visualization of liver fibrosis stages based on the classification outputs. Our approach effectively handles multimodality imaging data, limited labels, and domain shifts. The proposed method has been tested by the challenge organizer on an independent test set that includes in-distribution (ID) and out-of-distribution (OOD) cases using three-channel MRIs (T1, T2, DWI) and seven-channel MRIs (T1, T2, DWI, GED1-GED4). The code is freely available. Github link: https://github.com/mileywang3061/Care-Liver

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è‚çº¤ç»´åŒ–åœ¨ä¸´åºŠå®è·µä¸­æå‡ºäº†å·¨å¤§çš„æŒ‘æˆ˜ï¼Œå¼ºè°ƒäº†ç²¾ç¡®çš„è‚è„åˆ†å‰²å’Œå‡†ç¡®çš„ç–¾ç—…åˆ†æœŸçš„å¿…è¦æ€§ã€‚åŸºäº CARE Liver 2025 Track 4 Challengeï¼Œæœ¬ç ”ç©¶ä»‹ç»äº†ä½¿ç”¨å¤šå‚æ•° MRI ä¸ºè‚è„åˆ†å‰² (LiSeg) å’Œè‚çº¤ç»´åŒ–åˆ†æœŸ (LiFS) å¼€å‘çš„å¤šä»»åŠ¡æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚ LiSeg é˜¶æ®µé€šè¿‡é‡‡ç”¨é›†æˆå›¾åƒåˆ†å‰²å’Œé…å‡†çš„åŠç›‘ç£å­¦ä¹ æ¨¡å‹æ¥è§£å†³æœ‰é™æ³¨é‡Šå›¾åƒå’Œå¤šå‚æ•° MRI æ•°æ®å¤æ‚æ€§çš„æŒ‘æˆ˜ã€‚é€šè¿‡åˆ©ç”¨æ ‡è®°å’Œæœªæ ‡è®°æ•°æ®ï¼Œè¯¥æ¨¡å‹å…‹æœäº†åŸŸè½¬ç§»å’Œè·¨æ¨¡å¼å˜åŒ–å¸¦æ¥çš„å›°éš¾ã€‚åœ¨ LiFS é˜¶æ®µï¼Œæˆ‘ä»¬é‡‡ç”¨äº†åŸºäºè¡¥ä¸çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å…è®¸æ ¹æ®åˆ†ç±»è¾“å‡ºå¯è§†åŒ–è‚çº¤ç»´åŒ–é˜¶æ®µã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°å¤„ç†å¤šæ¨¡æ€æˆåƒæ•°æ®ã€æœ‰é™çš„æ ‡ç­¾å’ŒåŸŸè½¬ç§»ã€‚æŒ‘æˆ˜ç»„ç»‡è€…å·²åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šå¯¹æ‰€æå‡ºçš„æ–¹æ³•è¿›è¡Œäº†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•é›†åŒ…æ‹¬ä½¿ç”¨ä¸‰é€šé“ MRIï¼ˆT1ã€T2ã€DWIï¼‰å’Œä¸ƒé€šé“ MRIï¼ˆT1ã€T2ã€DWIã€GED1-GED4ï¼‰çš„åˆ†å¸ƒå†…ï¼ˆIDï¼‰å’Œåˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ¡ˆä¾‹ã€‚è¯¥ä»£ç æ˜¯å…è´¹æä¾›çš„ã€‚ Github é“¾æ¥ï¼šhttps://github.com/mileywang3061/Care-Liver

</details>

---

## 143. TreeCUA: Efficiently Scaling GUI Automation with Tree-Structured Verifiable Evolution

**ä¸­æ–‡æ ‡é¢˜**: TreeCUAï¼šé€šè¿‡æ ‘å½¢ç»“æ„çš„å¯éªŒè¯æ¼”åŒ–æœ‰æ•ˆæ‰©å±• GUI è‡ªåŠ¨åŒ–

**Date**: 2026-02-10 | **arXiv**: [2602.09662v1](http://arxiv.org/abs/2602.09662v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09662v1)

**Code**: https://github.com/UITron-hub/TreeCUA.

<details><summary><b>Abstract</b></summary>

Effectively scaling GUI automation is essential for computer-use agents (CUAs); however, existing work primarily focuses on scaling GUI grounding rather than the more crucial GUI planning, which requires more sophisticated data collection. In reality, the exploration process of a CUA across apps/desktops/web pages typically follows a tree structure, with earlier functional entry points often being explored more frequently. Thus, organizing large-scale trajectories into tree structures can reduce data cost and streamline the data scaling of GUI planning. In this work, we propose TreeCUA to efficiently scale GUI automation with tree-structured verifiable evolution. We propose a multi-agent collaborative framework to explore the environment, verify actions, summarize trajectories, and evaluate quality to generate high-quality and scalable GUI trajectories. To improve efficiency, we devise a novel tree-based topology to store and replay duplicate exploration nodes, and design an adaptive exploration algorithm to balance the depth (\emph{i.e.}, trajectory difficulty) and breadth (\emph{i.e.}, trajectory diversity). Moreover, we develop world knowledge guidance and global memory backtracking to avoid low-quality generation. Finally, we naturally extend and propose the TreeCUA-DPO method from abundant tree node information, improving GUI planning capability by referring to the branch information of adjacent trajectories. Experimental results show that TreeCUA and TreeCUA-DPO offer significant improvements, and out-of-domain (OOD) studies further demonstrate strong generalization. All trajectory node information and code will be available at https://github.com/UITron-hub/TreeCUA.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æœ‰æ•ˆæ‰©å±• GUI è‡ªåŠ¨åŒ–å¯¹äºè®¡ç®—æœºä½¿ç”¨ä»£ç† (CUA) è‡³å…³é‡è¦ï¼›ç„¶è€Œï¼Œç°æœ‰çš„å·¥ä½œä¸»è¦ä¾§é‡äºæ‰©å±• GUI åŸºç¡€ï¼Œè€Œä¸æ˜¯æ›´é‡è¦çš„ GUI è§„åˆ’ï¼Œåè€…éœ€è¦æ›´å¤æ‚çš„æ•°æ®æ”¶é›†ã€‚å®é™…ä¸Šï¼Œè·¨åº”ç”¨ç¨‹åº/æ¡Œé¢/ç½‘é¡µçš„ CUA æ¢ç´¢è¿‡ç¨‹é€šå¸¸éµå¾ªæ ‘å½¢ç»“æ„ï¼Œæ—©æœŸçš„åŠŸèƒ½å…¥å£ç‚¹é€šå¸¸ä¼šè¢«æ›´é¢‘ç¹åœ°æ¢ç´¢ã€‚å› æ­¤ï¼Œå°†å¤§è§„æ¨¡è½¨è¿¹ç»„ç»‡æˆæ ‘ç»“æ„å¯ä»¥é™ä½æ•°æ®æˆæœ¬å¹¶ç®€åŒ– GUI è§„åˆ’çš„æ•°æ®æ‰©å±•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡º TreeCUA é€šè¿‡æ ‘å½¢ç»“æ„çš„å¯éªŒè¯è¿›åŒ–æ¥æœ‰æ•ˆåœ°æ‰©å±• GUI è‡ªåŠ¨åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶æ¥æ¢ç´¢ç¯å¢ƒã€éªŒè¯åŠ¨ä½œã€æ€»ç»“è½¨è¿¹å¹¶è¯„ä¼°è´¨é‡ï¼Œä»¥ç”Ÿæˆé«˜è´¨é‡å’Œå¯æ‰©å±•çš„ GUI è½¨è¿¹ã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„åŸºäºæ ‘çš„æ‹“æ‰‘æ¥å­˜å‚¨å’Œé‡æ”¾é‡å¤çš„æ¢ç´¢èŠ‚ç‚¹ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§è‡ªé€‚åº”æ¢ç´¢ç®—æ³•æ¥å¹³è¡¡æ·±åº¦ï¼ˆ\emph{å³ï¼Œè½¨è¿¹éš¾åº¦ï¼‰å’Œå¹¿åº¦ï¼ˆ\emph{å³}ï¼Œè½¨è¿¹å¤šæ ·æ€§ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸–ç•ŒçŸ¥è¯†æŒ‡å¯¼å’Œå…¨å±€è®°å¿†å›æº¯ï¼Œä»¥é¿å…ä½è´¨é‡çš„ç”Ÿæˆã€‚æœ€åï¼Œæˆ‘ä»¬ä»ä¸°å¯Œçš„æ ‘èŠ‚ç‚¹ä¿¡æ¯ä¸­è‡ªç„¶åœ°æ‰©å±•å’Œæå‡ºäº†TreeCUA-DPOæ–¹æ³•ï¼Œé€šè¿‡å‚è€ƒç›¸é‚»è½¨è¿¹çš„åˆ†æ”¯ä¿¡æ¯æ¥æé«˜GUIè§„åˆ’èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTreeCUA å’Œ TreeCUA-DPO æä¾›äº†æ˜¾ç€çš„æ”¹è¿›ï¼ŒåŸŸå¤–ï¼ˆOODï¼‰ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æ‰€æœ‰è½¨è¿¹èŠ‚ç‚¹ä¿¡æ¯å’Œä»£ç å°†åœ¨ https://github.com/UITron-hub/TreeCUA ä¸Šæä¾›ã€‚

</details>

---

## 144. Time2General: Learning Spatiotemporal Invariant Representations for Domain-Generalization Video Semantic Segmentation

**ä¸­æ–‡æ ‡é¢˜**: Time2Generalï¼šå­¦ä¹ é¢†åŸŸæ³›åŒ–è§†é¢‘è¯­ä¹‰åˆ†å‰²çš„æ—¶ç©ºä¸å˜è¡¨ç¤º

**Date**: 2026-02-10 | **arXiv**: [2602.09648v1](http://arxiv.org/abs/2602.09648v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09648v1)

<details><summary><b>Abstract</b></summary>

Domain Generalized Video Semantic Segmentation (DGVSS) is trained on a single labeled driving domain and is directly deployed on unseen domains without target labels and test-time adaptation while maintaining temporally consistent predictions over video streams. In practice, both domain shift and temporal-sampling shift break correspondence-based propagation and fixed-stride temporal aggregation, causing severe frame-to-frame flicker even in label-stable regions. We propose Time2General, a DGVSS framework built on Stability Queries. Time2General introduces a Spatio-Temporal Memory Decoder that aggregates multi-frame context into a clip-level spatio-temporal memory and decodes temporally consistent per-frame masks without explicit correspondence propagation. To further suppress flicker and improve robustness to varying sampling rates, the Masked Temporal Consistency Loss is proposed to regularize temporal prediction discrepancies across different strides, and randomize training strides to expose the model to diverse temporal gaps. Extensive experiments on multiple driving benchmarks show that Time2General achieves a substantial improvement in cross-domain accuracy and temporal stability over prior DGSS and VSS baselines while running at up to 18 FPS. Code will be released after the review process.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åŸŸå¹¿ä¹‰è§†é¢‘è¯­ä¹‰åˆ†å‰² (DGVSS) åœ¨å•ä¸ªæ ‡è®°çš„é©±åŠ¨åŸŸä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶ç›´æ¥éƒ¨ç½²åœ¨ä¸å¯è§çš„åŸŸä¸Šï¼Œæ— éœ€ç›®æ ‡æ ‡ç­¾å’Œæµ‹è¯•æ—¶é—´é€‚åº”ï¼ŒåŒæ—¶ä¿æŒè§†é¢‘æµçš„æ—¶é—´ä¸€è‡´é¢„æµ‹ã€‚åœ¨å®è·µä¸­ï¼ŒåŸŸç§»ä½å’Œæ—¶é—´é‡‡æ ·ç§»ä½éƒ½ä¼šç ´ååŸºäºå¯¹åº”çš„ä¼ æ’­å’Œå›ºå®šæ­¥é•¿æ—¶é—´èšåˆï¼Œå³ä½¿åœ¨æ ‡ç­¾ç¨³å®šåŒºåŸŸä¹Ÿä¼šå¯¼è‡´ä¸¥é‡çš„å¸§é—´é—ªçƒã€‚æˆ‘ä»¬æå‡ºäº† Time2Generalï¼Œä¸€ä¸ªåŸºäºç¨³å®šæ€§æŸ¥è¯¢æ„å»ºçš„ DGVSS æ¡†æ¶ã€‚ Time2General å¼•å…¥äº†æ—¶ç©ºå†…å­˜è§£ç å™¨ï¼Œå®ƒå°†å¤šå¸§ä¸Šä¸‹æ–‡èšåˆåˆ°å‰ªè¾‘çº§æ—¶ç©ºå†…å­˜ä¸­ï¼Œå¹¶åœ¨æ²¡æœ‰æ˜¾å¼å¯¹åº”ä¼ æ’­çš„æƒ…å†µä¸‹è§£ç æ—¶é—´ä¸€è‡´çš„æ¯å¸§æ©ç ã€‚ä¸ºäº†è¿›ä¸€æ­¥æŠ‘åˆ¶é—ªçƒå¹¶æé«˜å¯¹ä¸åŒé‡‡æ ·ç‡çš„é²æ£’æ€§ï¼Œæå‡ºäº†æ©è”½æ—¶é—´ä¸€è‡´æ€§æŸå¤±æ¥è§„èŒƒä¸åŒæ­¥å¹…ä¹‹é—´çš„æ—¶é—´é¢„æµ‹å·®å¼‚ï¼Œå¹¶éšæœºåŒ–è®­ç»ƒæ­¥å¹…ä»¥ä½¿æ¨¡å‹æš´éœ²äºä¸åŒçš„æ—¶é—´é—´éš™ã€‚å¯¹å¤šä¸ªé©¾é©¶åŸºå‡†çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸ä¹‹å‰çš„ DGSS å’Œ VSS åŸºçº¿ç›¸æ¯”ï¼ŒTime2General åœ¨è·¨åŸŸç²¾åº¦å’Œæ—¶é—´ç¨³å®šæ€§æ–¹é¢å–å¾—äº†å®è´¨æ€§æ”¹è¿›ï¼ŒåŒæ—¶è¿è¡Œé€Ÿåº¦é«˜è¾¾ 18 FPSã€‚ä»£ç å°†åœ¨å®¡æ ¸åå‘å¸ƒã€‚

</details>

---

## 145. Towards Training-free Multimodal Hate Localisation with Large Language Models

**ä¸­æ–‡æ ‡é¢˜**: åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å®ç°å…è®­ç»ƒå¤šæ¨¡å¼ä»‡æ¨æœ¬åœ°åŒ–

**Date**: 2026-02-10 | **arXiv**: [2602.09637v1](http://arxiv.org/abs/2602.09637v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09637v1)

<details><summary><b>Abstract</b></summary>

The proliferation of hateful content in online videos poses severe threats to individual well-being and societal harmony. However, existing solutions for video hate detection either rely heavily on large-scale human annotations or lack fine-grained temporal precision. In this work, we propose LELA, the first training-free Large Language Model (LLM) based framework for hate video localization. Distinct from state-of-the-art models that depend on supervised pipelines, LELA leverages LLMs and modality-specific captioning to detect and temporally localize hateful content in a training-free manner. Our method decomposes a video into five modalities, including image, speech, OCR, music, and video context, and uses a multi-stage prompting scheme to compute fine-grained hateful scores for each frame. We further introduce a composition matching mechanism to enhance cross-modal reasoning. Experiments on two challenging benchmarks, HateMM and MultiHateClip, demonstrate that LELA outperforms all existing training-free baselines by a large margin. We also provide extensive ablations and qualitative visualizations, establishing LELA as a strong foundation for scalable and interpretable hate video localization.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç½‘ç»œè§†é¢‘ä¸­ä»‡æ¨å†…å®¹çš„æ³›æ»¥å¯¹ä¸ªäººç¦ç¥‰å’Œç¤¾ä¼šå’Œè°æ„æˆä¸¥é‡å¨èƒã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†é¢‘ä»‡æ¨æ£€æµ‹è§£å†³æ–¹æ¡ˆè¦ä¹ˆä¸¥é‡ä¾èµ–å¤§è§„æ¨¡çš„äººç±»æ³¨é‡Šï¼Œè¦ä¹ˆç¼ºä¹ç»†ç²’åº¦çš„æ—¶é—´ç²¾åº¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† LELAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLM) çš„å…è®­ç»ƒä»‡æ¨è§†é¢‘æœ¬åœ°åŒ–æ¡†æ¶ã€‚ä¸ä¾èµ–ç›‘ç£ç®¡é“çš„æœ€å…ˆè¿›æ¨¡å‹ä¸åŒï¼ŒLELA åˆ©ç”¨ LLM å’Œç‰¹å®šæ¨¡æ€å­—å¹•ä»¥æ— éœ€åŸ¹è®­çš„æ–¹å¼æ£€æµ‹å’Œä¸´æ—¶å®šä½ä»‡æ¨å†…å®¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†è§†é¢‘åˆ†è§£ä¸ºäº”ç§æ¨¡å¼ï¼ŒåŒ…æ‹¬å›¾åƒã€è¯­éŸ³ã€OCRã€éŸ³ä¹å’Œè§†é¢‘ä¸Šä¸‹æ–‡ï¼Œå¹¶ä½¿ç”¨å¤šé˜¶æ®µæç¤ºæ–¹æ¡ˆæ¥è®¡ç®—æ¯å¸§çš„ç»†ç²’åº¦ä»‡æ¨åˆ†æ•°ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ç§ç»„åˆåŒ¹é…æœºåˆ¶æ¥å¢å¼ºè·¨æ¨¡æ€æ¨ç†ã€‚åœ¨ HateMM å’Œ MultiHateClip è¿™ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒLELA çš„æ€§èƒ½å¤§å¹…ä¼˜äºæ‰€æœ‰ç°æœ‰çš„å…è®­ç»ƒåŸºå‡†ã€‚æˆ‘ä»¬è¿˜æä¾›å¹¿æ³›çš„æ¶ˆèå’Œå®šæ€§å¯è§†åŒ–ï¼Œå°† LELA å»ºç«‹ä¸ºå¯æ‰©å±•å’Œå¯è§£é‡Šçš„ä»‡æ¨è§†é¢‘æœ¬åœ°åŒ–çš„åšå®åŸºç¡€ã€‚

</details>

---

## 146. AGMark: Attention-Guided Dynamic Watermarking for Large Vision-Language Models

**ä¸­æ–‡æ ‡é¢˜**: AGMarkï¼šå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„æ³¨æ„åŠ›å¼•å¯¼åŠ¨æ€æ°´å°

**Date**: 2026-02-10 | **arXiv**: [2602.09611v1](http://arxiv.org/abs/2602.09611v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09611v1)

<details><summary><b>Abstract</b></summary>

Watermarking has emerged as a pivotal solution for content traceability and intellectual property protection in Large Vision-Language Models (LVLMs). However, vision-agnostic watermarks may introduce visually irrelevant tokens and disrupt visual grounding by enforcing indiscriminate pseudo-random biases. Additionally, current vision-specific watermarks rely on a static, one-time estimation of vision critical weights and ignore the weight distribution density when determining the proportion of protected tokens. This design fails to account for dynamic changes in visual dependence during generation and may introduce low-quality tokens in the long tail. To address these challenges, we propose Attention-Guided Dynamic Watermarking (AGMark), a novel framework that embeds detectable signals while strictly preserving visual fidelity. At each decoding step, AGMark first dynamically identifies semantic-critical evidence based on attention weights for visual relevance, together with context-aware coherence cues, resulting in a more adaptive and well-calibrated evidence-weight distribution. It then determines the proportion of semantic-critical tokens by jointly considering uncertainty awareness (token entropy) and evidence calibration (weight density), thereby enabling adaptive vocabulary partitioning to avoid irrelevant tokens. Empirical results confirm that AGMark outperforms conventional methods, observably improving generation quality and yielding particularly strong gains in visual semantic fidelity in the later stages of generation. The framework maintains highly competitive detection accuracy (at least 99.36\% AUC) and robust attack resilience (at least 88.61\% AUC) without sacrificing inference efficiency, effectively establishing a new standard for reliability-preserving multi-modal watermarking.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æ°´å°å·²æˆä¸ºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ (LVLM) ä¸­å†…å®¹å¯è¿½æº¯æ€§å’ŒçŸ¥è¯†äº§æƒä¿æŠ¤çš„å…³é”®è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œä¸è§†è§‰æ— å…³çš„æ°´å°å¯èƒ½ä¼šå¼•å…¥è§†è§‰ä¸Šä¸ç›¸å…³çš„æ ‡è®°ï¼Œå¹¶é€šè¿‡å¼ºåˆ¶æ‰§è¡Œä¸åŠ åŒºåˆ«çš„ä¼ªéšæœºåå·®æ¥ç ´åè§†è§‰åŸºç¡€ã€‚æ­¤å¤–ï¼Œå½“å‰çš„è§†è§‰ç‰¹å®šæ°´å°ä¾èµ–äºè§†è§‰ä¸´ç•Œæƒé‡çš„é™æ€ä¸€æ¬¡æ€§ä¼°è®¡ï¼Œå¹¶ä¸”åœ¨ç¡®å®šå—ä¿æŠ¤ä»¤ç‰Œçš„æ¯”ä¾‹æ—¶å¿½ç•¥æƒé‡åˆ†å¸ƒå¯†åº¦ã€‚è¿™ç§è®¾è®¡æœªèƒ½è€ƒè™‘ç”Ÿæˆè¿‡ç¨‹ä¸­è§†è§‰ä¾èµ–æ€§çš„åŠ¨æ€å˜åŒ–ï¼Œå¹¶ä¸”å¯èƒ½ä¼šåœ¨é•¿å°¾ä¸­å¼•å…¥ä½è´¨é‡çš„ä»¤ç‰Œã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ³¨æ„åŠ›å¼•å¯¼åŠ¨æ€æ°´å°ï¼ˆAGMarkï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œå¯ä»¥åµŒå…¥å¯æ£€æµ‹ä¿¡å·ï¼ŒåŒæ—¶ä¸¥æ ¼ä¿æŒè§†è§‰ä¿çœŸåº¦ã€‚åœ¨æ¯ä¸ªè§£ç æ­¥éª¤ä¸­ï¼ŒAGMark é¦–å…ˆæ ¹æ®è§†è§‰ç›¸å…³æ€§çš„æ³¨æ„åŠ›æƒé‡ä»¥åŠä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¿è´¯æ€§çº¿ç´¢åŠ¨æ€è¯†åˆ«è¯­ä¹‰å…³é”®è¯æ®ï¼Œä»è€Œäº§ç”Ÿæ›´å…·é€‚åº”æ€§å’Œæ ¡å‡†è‰¯å¥½çš„è¯æ®æƒé‡åˆ†å¸ƒã€‚ç„¶åï¼Œå®ƒé€šè¿‡è”åˆè€ƒè™‘ä¸ç¡®å®šæ€§æ„è¯†ï¼ˆä»¤ç‰Œç†µï¼‰å’Œè¯æ®æ ¡å‡†ï¼ˆæƒé‡å¯†åº¦ï¼‰æ¥ç¡®å®šè¯­ä¹‰å…³é”®ä»¤ç‰Œçš„æ¯”ä¾‹ï¼Œä»è€Œå®ç°è‡ªé€‚åº”è¯æ±‡åˆ’åˆ†ä»¥é¿å…ä¸ç›¸å…³çš„ä»¤ç‰Œã€‚å®è¯ç»“æœè¯å®ï¼ŒAGMark ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œæ˜¾ç€æé«˜äº†ç”Ÿæˆè´¨é‡ï¼Œå¹¶åœ¨ç”ŸæˆåæœŸé˜¶æ®µçš„è§†è§‰è¯­ä¹‰ä¿çœŸåº¦æ–¹é¢å–å¾—äº†ç‰¹åˆ«å¼ºåŠ²çš„æˆæœã€‚è¯¥æ¡†æ¶åœ¨ä¸ç‰ºç‰²æ¨ç†æ•ˆç‡çš„æƒ…å†µä¸‹ï¼Œä¿æŒäº†æå…·ç«äº‰åŠ›çš„æ£€æµ‹ç²¾åº¦ï¼ˆè‡³å°‘99.36ï¼…AUCï¼‰å’Œå¼ºå¤§çš„æ”»å‡»å¼¹æ€§ï¼ˆè‡³å°‘88.61ï¼…AUCï¼‰ï¼Œæœ‰æ•ˆåœ°å»ºç«‹äº†ä¿ç•™å¯é æ€§çš„å¤šæ¨¡æ€æ°´å°çš„æ–°æ ‡å‡†ã€‚

</details>

---

## 147. Tele-Omni: a Unified Multimodal Framework for Video Generation and Editing

**ä¸­æ–‡æ ‡é¢˜**: Tele-Omniï¼šç”¨äºè§†é¢‘ç”Ÿæˆå’Œç¼–è¾‘çš„ç»Ÿä¸€å¤šæ¨¡å¼æ¡†æ¶

**Date**: 2026-02-10 | **arXiv**: [2602.09609v1](http://arxiv.org/abs/2602.09609v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09609v1)

<details><summary><b>Abstract</b></summary>

Recent advances in diffusion-based video generation have substantially improved visual fidelity and temporal coherence. However, most existing approaches remain task-specific and rely primarily on textual instructions, limiting their ability to handle multimodal inputs, contextual references, and diverse video generation and editing scenarios within a unified framework. Moreover, many video editing methods depend on carefully engineered pipelines tailored to individual operations, which hinders scalability and composability. In this paper, we propose Tele-Omni, a unified multimodal framework for video generation and editing that follows multimodal instructions, including text, images, and reference videos, within a single model. Tele-Omni leverages pretrained multimodal large language models to parse heterogeneous instructions and infer structured generation or editing intents, while diffusion-based generators perform high-quality video synthesis conditioned on these structured signals. To enable joint training across heterogeneous video tasks, we introduce a task-aware data processing pipeline that unifies multimodal inputs into a structured instruction format while preserving task-specific constraints. Tele-Omni supports a wide range of video-centric tasks, including text-to-video generation, image-to-video generation, first-last-frame video generation, in-context video generation, and in-context video editing. By decoupling instruction parsing from video synthesis and combining it with task-aware data design, Tele-Omni achieves flexible multimodal control while maintaining strong temporal coherence and visual consistency. Experimental results demonstrate that Tele-Omni achieves competitive performance across multiple tasks.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åŸºäºæ‰©æ•£çš„è§†é¢‘ç”Ÿæˆçš„æœ€æ–°è¿›å±•æå¤§åœ°æé«˜äº†è§†è§‰ä¿çœŸåº¦å’Œæ—¶é—´è¿è´¯æ€§ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä»ç„¶æ˜¯ç‰¹å®šäºä»»åŠ¡çš„ï¼Œå¹¶ä¸”ä¸»è¦ä¾èµ–äºæ–‡æœ¬æŒ‡ä»¤ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ç»Ÿä¸€æ¡†æ¶å†…å¤„ç†å¤šæ¨¡å¼è¾“å…¥ã€ä¸Šä¸‹æ–‡å‚è€ƒä»¥åŠä¸åŒè§†é¢‘ç”Ÿæˆå’Œç¼–è¾‘åœºæ™¯çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè®¸å¤šè§†é¢‘ç¼–è¾‘æ–¹æ³•ä¾èµ–äºé’ˆå¯¹å•ç‹¬æ“ä½œç²¾å¿ƒè®¾è®¡çš„ç®¡é“ï¼Œè¿™é˜»ç¢äº†å¯æ‰©å±•æ€§å’Œå¯ç»„åˆæ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† Tele-Omniï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè§†é¢‘ç”Ÿæˆå’Œç¼–è¾‘çš„ç»Ÿä¸€å¤šæ¨¡å¼æ¡†æ¶ï¼Œå®ƒéµå¾ªå•ä¸€æ¨¡å‹ä¸­çš„å¤šæ¨¡å¼æŒ‡ä»¤ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒå’Œå‚è€ƒè§†é¢‘ã€‚ Tele-Omni åˆ©ç”¨é¢„è®­ç»ƒçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ¥è§£æå¼‚æ„æŒ‡ä»¤å¹¶æ¨æ–­ç»“æ„åŒ–ç”Ÿæˆæˆ–ç¼–è¾‘æ„å›¾ï¼Œè€ŒåŸºäºæ‰©æ•£çš„ç”Ÿæˆå™¨åˆ™æ ¹æ®è¿™äº›ç»“æ„åŒ–ä¿¡å·æ‰§è¡Œé«˜è´¨é‡è§†é¢‘åˆæˆã€‚ä¸ºäº†å®ç°è·¨å¼‚æ„è§†é¢‘ä»»åŠ¡çš„è”åˆè®­ç»ƒï¼Œæˆ‘ä»¬å¼•å…¥äº†ä»»åŠ¡æ„ŸçŸ¥æ•°æ®å¤„ç†ç®¡é“ï¼Œå®ƒå°†å¤šæ¨¡æ€è¾“å…¥ç»Ÿä¸€ä¸ºç»“æ„åŒ–æŒ‡ä»¤æ ¼å¼ï¼ŒåŒæ—¶ä¿ç•™ç‰¹å®šäºä»»åŠ¡çš„çº¦æŸã€‚ Tele-Omni æ”¯æŒå„ç§ä»¥è§†é¢‘ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆã€å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆã€é¦–å°¾å¸§è§†é¢‘ç”Ÿæˆã€ä¸Šä¸‹æ–‡è§†é¢‘ç”Ÿæˆå’Œä¸Šä¸‹æ–‡è§†é¢‘ç¼–è¾‘ã€‚é€šè¿‡å°†æŒ‡ä»¤è§£æä¸è§†é¢‘åˆæˆè§£è€¦å¹¶å°†å…¶ä¸ä»»åŠ¡æ„ŸçŸ¥æ•°æ®è®¾è®¡ç›¸ç»“åˆï¼ŒTele-Omni å®ç°äº†çµæ´»çš„å¤šæ¨¡å¼æ§åˆ¶ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„æ—¶é—´è¿è´¯æ€§å’Œè§†è§‰ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTele-Omni åœ¨å¤šé¡¹ä»»åŠ¡ä¸­å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚

</details>

---

## 148. Hand2World: Autoregressive Egocentric Interaction Generation via Free-Space Hand Gestures

**ä¸­æ–‡æ ‡é¢˜**: Hand2Worldï¼šé€šè¿‡è‡ªç”±ç©ºé—´æ‰‹åŠ¿ç”Ÿæˆè‡ªå›å½’è‡ªæˆ‘ä¸­å¿ƒäº¤äº’

**Date**: 2026-02-10 | **arXiv**: [2602.09600v1](http://arxiv.org/abs/2602.09600v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09600v1)

<details><summary><b>Abstract</b></summary>

Egocentric interactive world models are essential for augmented reality and embodied AI, where visual generation must respond to user input with low latency, geometric consistency, and long-term stability. We study egocentric interaction generation from a single scene image under free-space hand gestures, aiming to synthesize photorealistic videos in which hands enter the scene, interact with objects, and induce plausible world dynamics under head motion. This setting introduces fundamental challenges, including distribution shift between free-space gestures and contact-heavy training data, ambiguity between hand motion and camera motion in monocular views, and the need for arbitrary-length video generation. We present Hand2World, a unified autoregressive framework that addresses these challenges through occlusion-invariant hand conditioning based on projected 3D hand meshes, allowing visibility and occlusion to be inferred from scene context rather than encoded in the control signal. To stabilize egocentric viewpoint changes, we inject explicit camera geometry via per-pixel PlÃ¼cker-ray embeddings, disentangling camera motion from hand motion and preventing background drift. We further develop a fully automated monocular annotation pipeline and distill a bidirectional diffusion model into a causal generator, enabling arbitrary-length synthesis. Experiments on three egocentric interaction benchmarks show substantial improvements in perceptual quality and 3D consistency while supporting camera control and long-horizon interactive generation.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„äº¤äº’å¼ä¸–ç•Œæ¨¡å‹å¯¹äºå¢å¼ºç°å®å’ŒåµŒå…¥å¼äººå·¥æ™ºèƒ½è‡³å…³é‡è¦ï¼Œå…¶ä¸­è§†è§‰ç”Ÿæˆå¿…é¡»ä»¥ä½å»¶è¿Ÿã€å‡ ä½•ä¸€è‡´æ€§å’Œé•¿æœŸç¨³å®šæ€§å“åº”ç”¨æˆ·è¾“å…¥ã€‚æˆ‘ä»¬ç ”ç©¶è‡ªç”±ç©ºé—´æ‰‹åŠ¿ä¸‹å•ä¸ªåœºæ™¯å›¾åƒçš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„äº¤äº’ç”Ÿæˆï¼Œæ—¨åœ¨åˆæˆé€¼çœŸçš„è§†é¢‘ï¼Œå…¶ä¸­æ‰‹è¿›å…¥åœºæ™¯ï¼Œä¸ç‰©ä½“äº¤äº’ï¼Œå¹¶åœ¨å¤´éƒ¨è¿åŠ¨ä¸‹è¯±å¯¼å¯ä¿¡çš„ä¸–ç•ŒåŠ¨æ€ã€‚è¿™ç§è®¾ç½®å¸¦æ¥äº†æ ¹æœ¬æ€§çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è‡ªç”±ç©ºé—´æ‰‹åŠ¿å’Œå¤§é‡æ¥è§¦è®­ç»ƒæ•°æ®ä¹‹é—´çš„åˆ†å¸ƒå˜åŒ–ã€å•ç›®è§†å›¾ä¸­æ‰‹éƒ¨è¿åŠ¨å’Œç›¸æœºè¿åŠ¨ä¹‹é—´çš„æ¨¡ç³Šæ€§ï¼Œä»¥åŠä»»æ„é•¿åº¦è§†é¢‘ç”Ÿæˆçš„éœ€è¦ã€‚æˆ‘ä»¬æå‡ºäº† Hand2Worldï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è‡ªå›å½’æ¡†æ¶ï¼Œå®ƒé€šè¿‡åŸºäºæŠ•å½± 3D æ‰‹éƒ¨ç½‘æ ¼çš„é®æŒ¡ä¸å˜æ‰‹è°ƒèŠ‚æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå…è®¸ä»åœºæ™¯ä¸Šä¸‹æ–‡ä¸­æ¨æ–­å¯è§æ€§å’Œé®æŒ¡ï¼Œè€Œä¸æ˜¯åœ¨æ§åˆ¶ä¿¡å·ä¸­è¿›è¡Œç¼–ç ã€‚ä¸ºäº†ç¨³å®šä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†ç‚¹å˜åŒ–ï¼Œæˆ‘ä»¬é€šè¿‡æ¯åƒç´  PlÃ¼cker å°„çº¿åµŒå…¥æ³¨å…¥æ˜¾å¼ç›¸æœºå‡ ä½•å½¢çŠ¶ï¼Œå°†ç›¸æœºè¿åŠ¨ä¸æ‰‹éƒ¨è¿åŠ¨åˆ†å¼€å¹¶é˜²æ­¢èƒŒæ™¯æ¼‚ç§»ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ä¸ªå…¨è‡ªåŠ¨çš„å•ç›®æ³¨é‡Šç®¡é“ï¼Œå¹¶å°†åŒå‘æ‰©æ•£æ¨¡å‹æç‚¼æˆå› æœç”Ÿæˆå™¨ï¼Œä»è€Œå®ç°ä»»æ„é•¿åº¦çš„åˆæˆã€‚å¯¹ä¸‰ä¸ªä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„äº¤äº’åŸºå‡†è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæ„ŸçŸ¥è´¨é‡å’Œ 3D ä¸€è‡´æ€§å¾—åˆ°äº†æ˜¾ç€æ”¹å–„ï¼ŒåŒæ—¶æ”¯æŒç›¸æœºæ§åˆ¶å’Œé•¿è§†è·äº¤äº’ç”Ÿæˆã€‚

</details>

---

## 149. Delving into Spectral Clustering with Vision-Language Representations

**ä¸­æ–‡æ ‡é¢˜**: ä½¿ç”¨è§†è§‰è¯­è¨€è¡¨ç¤ºæ·±å…¥ç ”ç©¶è°±èšç±»

**Date**: 2026-02-10 | **arXiv**: [2602.09586v1](http://arxiv.org/abs/2602.09586v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09586v1)

<details><summary><b>Abstract</b></summary>

Spectral clustering is known as a powerful technique in unsupervised data analysis. The vast majority of approaches to spectral clustering are driven by a single modality, leaving the rich information in multi-modal representations untapped. Inspired by the recent success of vision-language pre-training, this paper enriches the landscape of spectral clustering from a single-modal to a multi-modal regime. Particularly, we propose Neural Tangent Kernel Spectral Clustering that leverages cross-modal alignment in pre-trained vision-language models. By anchoring the neural tangent kernel with positive nouns, i.e., those semantically close to the images of interest, we arrive at formulating the affinity between images as a coupling of their visual proximity and semantic overlap. We show that this formulation amplifies within-cluster connections while suppressing spurious ones across clusters, hence encouraging block-diagonal structures. In addition, we present a regularized affinity diffusion mechanism that adaptively ensembles affinity matrices induced by different prompts. Extensive experiments on \textbf{16} benchmarks -- including classical, large-scale, fine-grained and domain-shifted datasets -- manifest that our method consistently outperforms the state-of-the-art by a large margin.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è°±èšç±»è¢«è®¤ä¸ºæ˜¯æ— ç›‘ç£æ•°æ®åˆ†æä¸­çš„å¼ºå¤§æŠ€æœ¯ã€‚ç»å¤§å¤šæ•°è°±èšç±»æ–¹æ³•éƒ½æ˜¯ç”±å•ä¸€æ¨¡æ€é©±åŠ¨çš„ï¼Œè€Œå¤šæ¨¡æ€è¡¨ç¤ºä¸­çš„ä¸°å¯Œä¿¡æ¯å°šæœªå¾—åˆ°åˆ©ç”¨ã€‚å—æœ€è¿‘è§†è§‰è¯­è¨€é¢„è®­ç»ƒæˆåŠŸçš„å¯å‘ï¼Œæœ¬æ–‡ä¸°å¯Œäº†è°±èšç±»ä»å•æ¨¡æ€åˆ°å¤šæ¨¡æ€çš„å‰æ™¯ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬æå‡ºäº†ç¥ç»åˆ‡çº¿æ ¸è°±èšç±»ï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„è·¨æ¨¡å¼å¯¹é½ã€‚é€šè¿‡ç”¨è‚¯å®šåè¯ï¼ˆå³è¯­ä¹‰ä¸Šæ¥è¿‘æ„Ÿå…´è¶£å›¾åƒçš„åè¯ï¼‰é”šå®šç¥ç»åˆ‡çº¿å†…æ ¸ï¼Œæˆ‘ä»¬å°†å›¾åƒä¹‹é—´çš„äº²å’ŒåŠ›è¡¨è¿°ä¸ºè§†è§‰æ¥è¿‘åº¦å’Œè¯­ä¹‰é‡å çš„è€¦åˆã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œè¿™ç§å…¬å¼æ”¾å¤§äº†ç°‡å†…è¿æ¥ï¼ŒåŒæ—¶æŠ‘åˆ¶äº†ç°‡é—´çš„è™šå‡è¿æ¥ï¼Œä»è€Œé¼“åŠ±äº†å—å¯¹è§’ç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ­£åˆ™åŒ–çš„äº²å’ŒåŠ›æ‰©æ•£æœºåˆ¶ï¼Œå¯ä»¥è‡ªé€‚åº”åœ°é›†æˆç”±ä¸åŒæç¤ºå¼•èµ·çš„äº²å’ŒåŠ›çŸ©é˜µã€‚å¯¹ \textbf{16} åŸºå‡†çš„å¹¿æ³›å®éªŒâ€”â€”åŒ…æ‹¬ç»å…¸ã€å¤§è§„æ¨¡ã€ç»†ç²’åº¦å’ŒåŸŸè½¬ç§»æ•°æ®é›†â€”â€”è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆå¤§å¹…ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚

</details>

---

## 150. Scalpel: Fine-Grained Alignment of Attention Activation Manifolds via Mixture Gaussian Bridges to Mitigate Multimodal Hallucination

**ä¸­æ–‡æ ‡é¢˜**: Scalpelï¼šé€šè¿‡æ··åˆé«˜æ–¯æ¡¥å¯¹æ³¨æ„åŠ›æ¿€æ´»æµå½¢è¿›è¡Œç»†ç²’åº¦å¯¹é½ï¼Œä»¥å‡è½»å¤šæ¨¡æ€å¹»è§‰

**Date**: 2026-02-10 | **arXiv**: [2602.09541v1](http://arxiv.org/abs/2602.09541v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09541v1)

<details><summary><b>Abstract</b></summary>

Rapid progress in large vision-language models (LVLMs) has achieved unprecedented performance in vision-language tasks. However, due to the strong prior of large language models (LLMs) and misaligned attention across modalities, LVLMs often generate outputs inconsistent with visual content - termed hallucination. To address this, we propose \textbf{Scalpel}, a method that reduces hallucination by refining attention activation distributions toward more credible regions. Scalpel predicts trusted attention directions for each head in Transformer layers during inference and adjusts activations accordingly. It employs a Gaussian mixture model to capture multi-peak distributions of attention in trust and hallucination manifolds, and uses entropic optimal transport (equivalent to SchrÃ¶dinger bridge problem) to map Gaussian components precisely. During mitigation, Scalpel dynamically adjusts intervention strength and direction based on component membership and mapping relationships between hallucination and trust activations. Extensive experiments across multiple datasets and benchmarks demonstrate that Scalpel effectively mitigates hallucinations, outperforming previous methods and achieving state-of-the-art performance. Moreover, Scalpel is model- and data-agnostic, requiring no additional computation, only a single decoding step.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰çš„å¿«é€Ÿè¿›å±•åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­å–å¾—äº†å‰æ‰€æœªæœ‰çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºå¤§è¯­è¨€æ¨¡å‹ (LLM) çš„å¼ºå¤§å…ˆéªŒå’Œè·¨æ¨¡æ€çš„æ³¨æ„åŠ›é”™ä½ï¼ŒLVLM ç»å¸¸ç”Ÿæˆä¸è§†è§‰å†…å®¹ä¸ä¸€è‡´çš„è¾“å‡º - ç§°ä¸ºå¹»è§‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº† \textbf{Scalpel}ï¼Œä¸€ç§é€šè¿‡ç»†åŒ–æ³¨æ„åŠ›æ¿€æ´»åˆ†å¸ƒåˆ°æ›´å¯ä¿¡åŒºåŸŸæ¥å‡å°‘å¹»è§‰çš„æ–¹æ³•ã€‚ Scalpel åœ¨æ¨ç†è¿‡ç¨‹ä¸­é¢„æµ‹ Transformer å±‚ä¸­æ¯ä¸ªå¤´çš„å¯ä¿¡æ³¨æ„åŠ›æ–¹å‘ï¼Œå¹¶ç›¸åº”åœ°è°ƒæ•´æ¿€æ´»ã€‚å®ƒé‡‡ç”¨é«˜æ–¯æ··åˆæ¨¡å‹æ¥æ•è·ä¿¡ä»»å’Œå¹»è§‰æµå½¢ä¸­æ³¨æ„åŠ›çš„å¤šå³°åˆ†å¸ƒï¼Œå¹¶ä½¿ç”¨ç†µæœ€ä¼˜ä¼ è¾“ï¼ˆç›¸å½“äºè–›å®šè°”æ¡¥é—®é¢˜ï¼‰æ¥ç²¾ç¡®æ˜ å°„é«˜æ–¯åˆ†é‡ã€‚åœ¨ç¼“è§£è¿‡ç¨‹ä¸­ï¼ŒScalpel æ ¹æ®ç»„ä»¶æˆå‘˜èµ„æ ¼ä»¥åŠå¹»è§‰å’Œä¿¡ä»»æ¿€æ´»ä¹‹é—´çš„æ˜ å°„å…³ç³»åŠ¨æ€è°ƒæ•´å¹²é¢„å¼ºåº¦å’Œæ–¹å‘ã€‚è·¨å¤šä¸ªæ•°æ®é›†å’ŒåŸºå‡†çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒScalpel å¯ä»¥æœ‰æ•ˆå‡è½»å¹»è§‰ï¼Œè¶…è¶Šä»¥å‰çš„æ–¹æ³•å¹¶å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒScalpel ä¸æ¨¡å‹å’Œæ•°æ®æ— å…³ï¼Œä¸éœ€è¦é¢å¤–çš„è®¡ç®—ï¼Œåªéœ€è¦ä¸€ä¸ªè§£ç æ­¥éª¤ã€‚

</details>

---

## 151. AUHead: Realistic Emotional Talking Head Generation via Action Units Control

**ä¸­æ–‡æ ‡é¢˜**: AUHeadï¼šé€šè¿‡åŠ¨ä½œå•å…ƒæ§åˆ¶ç”Ÿæˆé€¼çœŸçš„æƒ…æ„Ÿå¤´éƒ¨

**Date**: 2026-02-10 | **arXiv**: [2602.09534v1](http://arxiv.org/abs/2602.09534v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09534v1)

**Code**: https://github.com/laura990501/AUHead_ICLR

<details><summary><b>Abstract</b></summary>

Realistic talking-head video generation is critical for virtual avatars, film production, and interactive systems. Current methods struggle with nuanced emotional expressions due to the lack of fine-grained emotion control. To address this issue, we introduce a novel two-stage method (AUHead) to disentangle fine-grained emotion control, i.e. , Action Units (AUs), from audio and achieve controllable generation. In the first stage, we explore the AU generation abilities of large audio-language models (ALMs), by spatial-temporal AU tokenization and an "emotion-then-AU" chain-of-thought mechanism. It aims to disentangle AUs from raw speech, effectively capturing subtle emotional cues. In the second stage, we propose an AU-driven controllable diffusion model that synthesizes realistic talking-head videos conditioned on AU sequences. Specifically, we first map the AU sequences into the structured 2D facial representation to enhance spatial fidelity, and then model the AU-vision interaction within cross-attention modules. To achieve flexible AU-quality trade-off control, we introduce an AU disentanglement guidance strategy during inference, further refining the emotional expressiveness and identity consistency of the generated videos. Results on benchmark datasets demonstrate that our approach achieves competitive performance in emotional realism, accurate lip synchronization, and visual coherence, significantly surpassing existing techniques. Our implementation is available at https://github.com/laura990501/AUHead_ICLR

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

é€¼çœŸçš„å¤´éƒ¨è¯´è¯è§†é¢‘ç”Ÿæˆå¯¹äºè™šæ‹ŸåŒ–èº«ã€ç”µå½±åˆ¶ä½œå’Œäº¤äº’ç³»ç»Ÿè‡³å…³é‡è¦ã€‚ç”±äºç¼ºä¹ç»†ç²’åº¦çš„æƒ…ç»ªæ§åˆ¶ï¼Œå½“å‰çš„æ–¹æ³•éš¾ä»¥å¤„ç†å¾®å¦™çš„æƒ…ç»ªè¡¨è¾¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µæ–¹æ³•ï¼ˆAUHeadï¼‰æ¥ä»éŸ³é¢‘ä¸­åˆ†ç¦»å‡ºç»†ç²’åº¦çš„æƒ…æ„Ÿæ§åˆ¶ï¼Œå³åŠ¨ä½œå•å…ƒï¼ˆAUï¼‰ï¼Œå¹¶å®ç°å¯æ§ç”Ÿæˆã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡æ—¶ç©ºAUæ ‡è®°åŒ–å’Œâ€œæƒ…æ„Ÿç„¶åAUâ€çš„æ€æƒ³é“¾æœºåˆ¶æ¥æ¢ç´¢å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMï¼‰çš„AUç”Ÿæˆèƒ½åŠ›ã€‚å®ƒçš„ç›®çš„æ˜¯å°† AU ä¸åŸå§‹è¯­éŸ³åˆ†å¼€ï¼Œæœ‰æ•ˆæ•æ‰å¾®å¦™çš„æƒ…æ„Ÿçº¿ç´¢ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”± AU é©±åŠ¨çš„å¯æ§æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥åˆæˆä»¥ AU åºåˆ—ä¸ºæ¡ä»¶çš„é€¼çœŸçš„å¤´éƒ¨è¯´è¯è§†é¢‘ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå°† AU åºåˆ—æ˜ å°„åˆ°ç»“æ„åŒ– 2D é¢éƒ¨è¡¨ç¤ºä¸­ä»¥å¢å¼ºç©ºé—´ä¿çœŸåº¦ï¼Œç„¶ååœ¨äº¤å‰æ³¨æ„æ¨¡å—å†…å¯¹ AU è§†è§‰äº¤äº’è¿›è¡Œå»ºæ¨¡ã€‚ä¸ºäº†å®ç°çµæ´»çš„ AU è´¨é‡æƒè¡¡æ§åˆ¶ï¼Œæˆ‘ä»¬åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼•å…¥äº† AU è§£çº ç¼ æŒ‡å¯¼ç­–ç•¥ï¼Œè¿›ä¸€æ­¥ç»†åŒ–ç”Ÿæˆè§†é¢‘çš„æƒ…æ„Ÿè¡¨è¾¾å’Œèº«ä»½ä¸€è‡´æ€§ã€‚åŸºå‡†æ•°æ®é›†çš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æƒ…æ„ŸçœŸå®æ€§ã€å‡†ç¡®çš„å£å‹åŒæ­¥å’Œè§†è§‰è¿è´¯æ€§æ–¹é¢å®ç°äº†ç«äº‰æ€§èƒ½ï¼Œæ˜¾ç€è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚æˆ‘ä»¬çš„å®ç°å¯åœ¨ https://github.com/laura990501/AUHead_ICLR è·å–

</details>

---

## 152. DR.Experts: Differential Refinement of Distortion-Aware Experts for Blind Image Quality Assessment

**ä¸­æ–‡æ ‡é¢˜**: DR.Expertsï¼šç”¨äºç›²å›¾åƒè´¨é‡è¯„ä¼°çš„å¤±çœŸæ„ŸçŸ¥ä¸“å®¶çš„å·®å¼‚åŒ–ç»†åŒ–

**Date**: 2026-02-10 | **arXiv**: [2602.09531v1](http://arxiv.org/abs/2602.09531v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09531v1)

<details><summary><b>Abstract</b></summary>

Blind Image Quality Assessment, aiming to replicate human perception of visual quality without reference, plays a key role in vision tasks, yet existing models often fail to effectively capture subtle distortion cues, leading to a misalignment with human subjective judgments. We identify that the root cause of this limitation lies in the lack of reliable distortion priors, as methods typically learn shallow relationships between unified image features and quality scores, resulting in their insensitive nature to distortions and thus limiting their performance. To address this, we introduce DR.Experts, a novel prior-driven BIQA framework designed to explicitly incorporate distortion priors, enabling a reliable quality assessment. DR.Experts begins by leveraging a degradation-aware vision-language model to obtain distortion-specific priors, which are further refined and enhanced by the proposed Distortion-Saliency Differential Module through distinguishing them from semantic attentions, thereby ensuring the genuine representations of distortions. The refined priors, along with semantics and bridging representation, are then fused by a proposed mixture-of-experts style module named the Dynamic Distortion Weighting Module. This mechanism weights each distortion-specific feature as per its perceptual impact, ensuring that the final quality prediction aligns with human perception. Extensive experiments conducted on five challenging BIQA benchmarks demonstrate the superiority of DR.Experts over current methods and showcase its excellence in terms of generalization and data efficiency.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç›²å›¾åƒè´¨é‡è¯„ä¼°æ—¨åœ¨åœ¨æ²¡æœ‰å‚è€ƒçš„æƒ…å†µä¸‹å¤åˆ¶äººç±»å¯¹è§†è§‰è´¨é‡çš„æ„ŸçŸ¥ï¼Œåœ¨è§†è§‰ä»»åŠ¡ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ï¼Œä½†ç°æœ‰æ¨¡å‹å¾€å¾€æ— æ³•æœ‰æ•ˆæ•è·å¾®å¦™çš„å¤±çœŸçº¿ç´¢ï¼Œå¯¼è‡´ä¸äººç±»ä¸»è§‚åˆ¤æ–­çš„ä¸ä¸€è‡´ã€‚æˆ‘ä»¬å‘ç°è¿™ç§é™åˆ¶çš„æ ¹æœ¬åŸå› åœ¨äºç¼ºä¹å¯é çš„å¤±çœŸå…ˆéªŒï¼Œå› ä¸ºæ–¹æ³•é€šå¸¸å­¦ä¹ ç»Ÿä¸€å›¾åƒç‰¹å¾å’Œè´¨é‡åˆ†æ•°ä¹‹é—´çš„æµ…å±‚å…³ç³»ï¼Œå¯¼è‡´å®ƒä»¬å¯¹å¤±çœŸä¸æ•æ„Ÿï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬çš„æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº† DR.Expertsï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„å…ˆéªŒé©±åŠ¨çš„ BIQA æ¡†æ¶ï¼Œæ—¨åœ¨æ˜ç¡®åˆå¹¶å¤±çœŸå…ˆéªŒï¼Œä»è€Œå®ç°å¯é çš„è´¨é‡è¯„ä¼°ã€‚ DR.Experts é¦–å…ˆåˆ©ç”¨é€€åŒ–æ„ŸçŸ¥è§†è§‰è¯­è¨€æ¨¡å‹æ¥è·å–ç‰¹å®šäºå¤±çœŸçš„å…ˆéªŒï¼Œå¹¶é€šè¿‡æå‡ºçš„å¤±çœŸæ˜¾ç€æ€§å·®åˆ†æ¨¡å—å°†å…¶ä¸è¯­ä¹‰æ³¨æ„åŒºåˆ†å¼€æ¥è¿›ä¸€æ­¥ç»†åŒ–å’Œå¢å¼ºï¼Œä»è€Œç¡®ä¿å¤±çœŸçš„çœŸå®è¡¨ç¤ºã€‚ç„¶åï¼Œç»è¿‡æ”¹è¿›çš„å…ˆéªŒä»¥åŠè¯­ä¹‰å’Œæ¡¥æ¥è¡¨ç¤ºï¼Œç”±æå‡ºçš„åä¸ºåŠ¨æ€å¤±çœŸåŠ æƒæ¨¡å—çš„ä¸“å®¶æ··åˆé£æ ¼æ¨¡å—è¿›è¡Œèåˆã€‚è¯¥æœºåˆ¶æ ¹æ®æ¯ä¸ªå¤±çœŸç‰¹å®šç‰¹å¾çš„æ„ŸçŸ¥å½±å“å¯¹å…¶è¿›è¡ŒåŠ æƒï¼Œç¡®ä¿æœ€ç»ˆçš„è´¨é‡é¢„æµ‹ä¸äººç±»æ„ŸçŸ¥ä¸€è‡´ã€‚åœ¨äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ BIQA åŸºå‡†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¯æ˜äº† DR.Experts ç›¸å¯¹äºå½“å‰æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨æ³›åŒ–å’Œæ•°æ®æ•ˆç‡æ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚

</details>

---

## 153. SCA-Net: Spatial-Contextual Aggregation Network for Enhanced Small Building and Road Change Detection

**ä¸­æ–‡æ ‡é¢˜**: SCA-Netï¼šç”¨äºå¢å¼ºå°å‹å»ºç­‘å’Œé“è·¯å˜åŒ–æ£€æµ‹çš„ç©ºé—´ä¸Šä¸‹æ–‡èšåˆç½‘ç»œ

**Date**: 2026-02-10 | **arXiv**: [2602.09529v1](http://arxiv.org/abs/2602.09529v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09529v1)

<details><summary><b>Abstract</b></summary>

Automated change detection in remote sensing imagery is critical for urban management, environmental monitoring, and disaster assessment. While deep learning models have advanced this field, they often struggle with challenges like low sensitivity to small objects and high computational costs. This paper presents SCA-Net, an enhanced architecture built upon the Change-Agent framework for precise building and road change detection in bi-temporal images. Our model incorporates several key innovations: a novel Difference Pyramid Block for multi-scale change analysis, an Adaptive Multi-scale Processing module combining shape-aware and high-resolution enhancement blocks, and multi-level attention mechanisms (PPM and CSAGate) for joint contextual and detail processing. Furthermore, a dynamic composite loss function and a four-phase training strategy are introduced to stabilize training and accelerate convergence. Comprehensive evaluations on the LEVIR-CD and LEVIR-MCI datasets demonstrate SCA-Net's superior performance over Change-Agent and other state-of-the-art methods. Our approach achieves a significant 2.64% improvement in mean Intersection over Union (mIoU) on LEVIR-MCI and a remarkable 57.9% increase in IoU for small buildings, while reducing the training time by 61%. This work provides an efficient, accurate, and robust solution for practical change detection applications.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

é¥æ„Ÿå›¾åƒçš„è‡ªåŠ¨å˜åŒ–æ£€æµ‹å¯¹äºåŸå¸‚ç®¡ç†ã€ç¯å¢ƒç›‘æµ‹å’Œç¾å®³è¯„ä¼°è‡³å…³é‡è¦ã€‚è™½ç„¶æ·±åº¦å­¦ä¹ æ¨¡å‹æ¨åŠ¨äº†è¿™ä¸€é¢†åŸŸçš„å‘å±•ï¼Œä½†å®ƒä»¬ç»å¸¸é¢ä¸´è¯¸å¦‚å¯¹å°ç‰©ä½“çš„æ•æ„Ÿæ€§ä½å’Œè®¡ç®—æˆæœ¬é«˜ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº† SCA-Netï¼Œè¿™æ˜¯ä¸€ç§åŸºäº Change-Agent æ¡†æ¶æ„å»ºçš„å¢å¼ºæ¶æ„ï¼Œç”¨äºåŒæ—¶æ€å›¾åƒä¸­çš„ç²¾ç¡®å»ºç­‘å’Œé“è·¯å˜åŒ–æ£€æµ‹ã€‚æˆ‘ä»¬çš„æ¨¡å‹èåˆäº†å¤šé¡¹å…³é”®åˆ›æ–°ï¼šç”¨äºå¤šå°ºåº¦å˜åŒ–åˆ†æçš„æ–°é¢–å·®å¼‚é‡‘å­—å¡”æ¨¡å—ã€ç»“åˆå½¢çŠ¶æ„ŸçŸ¥å’Œé«˜åˆ†è¾¨ç‡å¢å¼ºæ¨¡å—çš„è‡ªé€‚åº”å¤šå°ºåº¦å¤„ç†æ¨¡å—ï¼Œä»¥åŠç”¨äºè”åˆä¸Šä¸‹æ–‡å’Œç»†èŠ‚å¤„ç†çš„å¤šçº§æ³¨æ„æœºåˆ¶ï¼ˆPPM å’Œ CSAGateï¼‰ã€‚æ­¤å¤–ï¼Œå¼•å…¥åŠ¨æ€å¤åˆæŸå¤±å‡½æ•°å’Œå››é˜¶æ®µè®­ç»ƒç­–ç•¥æ¥ç¨³å®šè®­ç»ƒå¹¶åŠ é€Ÿæ”¶æ•›ã€‚å¯¹ LEVIR-CD å’Œ LEVIR-MCI æ•°æ®é›†çš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒSCA-Net çš„æ€§èƒ½ä¼˜äº Change-Agent å’Œå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ LEVIR-MCI ä¸Šçš„å¹³å‡äº¤é›†æ¯” (mIoU) æ˜¾ç€æé«˜äº† 2.64%ï¼Œå°å‹å»ºç­‘ç‰©çš„ IoU æ˜¾ç€æé«˜äº† 57.9%ï¼ŒåŒæ—¶å‡å°‘äº† 61% çš„è®­ç»ƒæ—¶é—´ã€‚è¿™é¡¹å·¥ä½œä¸ºå®é™…å˜åŒ–æ£€æµ‹åº”ç”¨æä¾›äº†é«˜æ•ˆã€å‡†ç¡®ä¸”å¼ºå¤§çš„è§£å†³æ–¹æ¡ˆã€‚

</details>

---

## 154. SchrÃ¶Mind: Mitigating Hallucinations in Multimodal Large Language Models via Solving the SchrÃ¶dinger Bridge Problem

**ä¸­æ–‡æ ‡é¢˜**: SchrÃ¶Mindï¼šé€šè¿‡è§£å†³è–›å®šè°”æ¡¥é—®é¢˜å‡è½»å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰

**Date**: 2026-02-10 | **arXiv**: [2602.09528v1](http://arxiv.org/abs/2602.09528v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09528v1)

<details><summary><b>Abstract</b></summary>

Recent advancements in Multimodal Large Language Models (MLLMs) have achieved significant success across various domains. However, their use in high-stakes fields like healthcare remains limited due to persistent hallucinations, where generated text contradicts or ignores visual input. We contend that MLLMs can comprehend images but struggle to produce accurate token sequences. Minor perturbations can shift attention from truthful to untruthful states, and the autoregressive nature of text generation often prevents error correction. To address this, we propose SchrÃ¶Mind-a novel framework reducing hallucinations via solving the SchrÃ¶dinger bridge problem. It establishes a token-level mapping between hallucinatory and truthful activations with minimal transport cost through lightweight training, while preserving the model's original capabilities. Extensive experiments on the POPE and MME benchmarks demonstrate the superiority of SchrÃ¶dinger, which achieves state-of-the-art performance while introducing only minimal computational overhead.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLM) çš„æœ€æ–°è¿›å±•åœ¨å„ä¸ªé¢†åŸŸå–å¾—äº†å·¨å¤§çš„æˆåŠŸã€‚ç„¶è€Œï¼Œç”±äºæŒç»­çš„å¹»è§‰ï¼Œå®ƒä»¬åœ¨åŒ»ç–—ä¿å¥ç­‰é«˜é£é™©é¢†åŸŸçš„ä½¿ç”¨ä»ç„¶å—åˆ°é™åˆ¶ï¼Œå…¶ä¸­ç”Ÿæˆçš„æ–‡æœ¬ä¸è§†è§‰è¾“å…¥ç›¸çŸ›ç›¾æˆ–å¿½ç•¥ã€‚æˆ‘ä»¬è®¤ä¸º MLLM å¯ä»¥ç†è§£å›¾åƒï¼Œä½†éš¾ä»¥ç”Ÿæˆå‡†ç¡®çš„æ ‡è®°åºåˆ—ã€‚å¾®å°çš„æ‰°åŠ¨å¯èƒ½ä¼šå°†æ³¨æ„åŠ›ä»çœŸå®çŠ¶æ€è½¬ç§»åˆ°ä¸çœŸå®çŠ¶æ€ï¼Œè€Œæ–‡æœ¬ç”Ÿæˆçš„è‡ªå›å½’æ€§è´¨é€šå¸¸ä¼šé˜»æ­¢é”™è¯¯çº æ­£ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº† SchrÃ¶Mindâ€”â€”ä¸€ç§é€šè¿‡è§£å†³è–›å®šè°”æ¡¥é—®é¢˜æ¥å‡å°‘å¹»è§‰çš„æ–°é¢–æ¡†æ¶ã€‚å®ƒé€šè¿‡è½»é‡çº§è®­ç»ƒä»¥æœ€å°çš„ä¼ è¾“æˆæœ¬åœ¨å¹»è§‰å’ŒçœŸå®æ¿€æ´»ä¹‹é—´å»ºç«‹äº†ä»¤ç‰Œçº§æ˜ å°„ï¼ŒåŒæ—¶ä¿ç•™äº†æ¨¡å‹çš„åŸå§‹åŠŸèƒ½ã€‚ POPE å’Œ MME åŸºå‡†çš„å¤§é‡å®éªŒè¯æ˜äº†è–›å®šè°”çš„ä¼˜è¶Šæ€§ï¼Œå®ƒå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶åªå¼•å…¥äº†æœ€å°çš„è®¡ç®—å¼€é”€ã€‚

</details>

---

## 155. HLGFA: High-Low Resolution Guided Feature Alignment for Unsupervised Anomaly Detection

**ä¸­æ–‡æ ‡é¢˜**: HLGFAï¼šç”¨äºæ— ç›‘ç£å¼‚å¸¸æ£€æµ‹çš„é«˜ä½åˆ†è¾¨ç‡å¼•å¯¼ç‰¹å¾å¯¹é½

**Date**: 2026-02-10 | **arXiv**: [2602.09524v1](http://arxiv.org/abs/2602.09524v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09524v1)

<details><summary><b>Abstract</b></summary>

Unsupervised industrial anomaly detection (UAD) is essential for modern manufacturing inspection, where defect samples are scarce and reliable detection is required. In this paper, we propose HLGFA, a high-low resolution guided feature alignment framework that learns normality by modeling cross-resolution feature consistency between high-resolution and low-resolution representations of normal samples, instead of relying on pixel-level reconstruction. Dual-resolution inputs are processed by a shared frozen backbone to extract multi-level features, and high-resolution representations are decomposed into structure and detail priors to guide the refinement of low-resolution features through conditional modulation and gated residual correction. During inference, anomalies are naturally identified as regions where cross-resolution alignment breaks down. In addition, a noise-aware data augmentation strategy is introduced to suppress nuisance-induced responses commonly observed in industrial environments. Extensive experiments on standard benchmarks demonstrate the effectiveness of HLGFA, achieving 97.9% pixel-level AUROC and 97.5% image-level AUROC on the MVTec AD dataset, outperforming representative reconstruction-based and feature-based methods.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æ— ç›‘ç£å·¥ä¸šå¼‚å¸¸æ£€æµ‹ (UAD) å¯¹äºç¼ºé™·æ ·æœ¬ç¨€ç¼ºä¸”éœ€è¦å¯é æ£€æµ‹çš„ç°ä»£åˆ¶é€ æ£€æµ‹è‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† HLGFAï¼Œä¸€ç§é«˜ä½åˆ†è¾¨ç‡å¼•å¯¼ç‰¹å¾å¯¹é½æ¡†æ¶ï¼Œå®ƒé€šè¿‡å¯¹æ­£å¸¸æ ·æœ¬çš„é«˜åˆ†è¾¨ç‡å’Œä½åˆ†è¾¨ç‡è¡¨ç¤ºä¹‹é—´çš„è·¨åˆ†è¾¨ç‡ç‰¹å¾ä¸€è‡´æ€§è¿›è¡Œå»ºæ¨¡æ¥å­¦ä¹ æ­£æ€æ€§ï¼Œè€Œä¸æ˜¯ä¾èµ–äºåƒç´ çº§é‡å»ºã€‚åŒåˆ†è¾¨ç‡è¾“å…¥ç”±å…±äº«çš„å†»ç»“ä¸»å¹²å¤„ç†ä»¥æå–å¤šçº§ç‰¹å¾ï¼Œé«˜åˆ†è¾¨ç‡è¡¨ç¤ºè¢«åˆ†è§£ä¸ºç»“æ„å’Œç»†èŠ‚å…ˆéªŒï¼Œä»¥é€šè¿‡æ¡ä»¶è°ƒåˆ¶å’Œé—¨æ§æ®‹å·®æ ¡æ­£æŒ‡å¯¼ä½åˆ†è¾¨ç‡ç‰¹å¾çš„ç»†åŒ–ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå¼‚å¸¸è‡ªç„¶è¢«è¯†åˆ«ä¸ºè·¨åˆ†è¾¨ç‡å¯¹é½å¤±è´¥çš„åŒºåŸŸã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†å™ªå£°æ„ŸçŸ¥æ•°æ®å¢å¼ºç­–ç•¥æ¥æŠ‘åˆ¶å·¥ä¸šç¯å¢ƒä¸­å¸¸è§çš„æ»‹æ‰°å¼•èµ·çš„å“åº”ã€‚æ ‡å‡†åŸºå‡†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº† HLGFA çš„æœ‰æ•ˆæ€§ï¼Œåœ¨ MVTec AD æ•°æ®é›†ä¸Šå®ç°äº† 97.9% çš„åƒç´ çº§ AUROC å’Œ 97.5% çš„å›¾åƒçº§ AUROCï¼Œä¼˜äºä»£è¡¨æ€§çš„åŸºäºé‡å»ºå’ŒåŸºäºç‰¹å¾çš„æ–¹æ³•ã€‚

</details>

---

## 156. Singpath-VL Technical Report

**ä¸­æ–‡æ ‡é¢˜**: Singpath-VLæŠ€æœ¯æŠ¥å‘Š

**Date**: 2026-02-10 | **arXiv**: [2602.09523v1](http://arxiv.org/abs/2602.09523v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09523v1)

<details><summary><b>Abstract</b></summary>

We present Singpath-VL, a vision-language large model, to fill the vacancy of AI assistant in cervical cytology. Recent advances in multi-modal large language models (MLLMs) have significantly propelled the field of computational pathology. However, their application in cytopathology, particularly cervical cytology, remains underexplored, primarily due to the scarcity of large-scale, high-quality annotated datasets. To bridge this gap, we first develop a novel three-stage pipeline to synthesize a million-scale image-description dataset. The pipeline leverages multiple general-purpose MLLMs as weak annotators, refines their outputs through consensus fusion and expert knowledge injection, and produces high-fidelity descriptions of cell morphology. Using this dataset, we then fine-tune the Qwen3-VL-4B model via a multi-stage strategy to create a specialized cytopathology MLLM. The resulting model, named Singpath-VL, demonstrates superior performance in fine-grained morphological perception and cell-level diagnostic classification. To advance the field, we will open-source a portion of the synthetic dataset and benchmark.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æˆ‘ä»¬æ¨å‡ºè§†è§‰è¯­è¨€å¤§æ¨¡å‹Singpath-VLï¼Œå¡«è¡¥å®«é¢ˆç»†èƒå­¦äººå·¥æ™ºèƒ½åŠ©æ‰‹çš„ç©ºç¼ºã€‚å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æœ€æ–°è¿›å±•æå¤§åœ°æ¨åŠ¨äº†è®¡ç®—ç—…ç†å­¦é¢†åŸŸçš„å‘å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç»†èƒç—…ç†å­¦ï¼Œç‰¹åˆ«æ˜¯å®«é¢ˆç»†èƒå­¦ä¸­çš„åº”ç”¨ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºç¼ºä¹å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„æ³¨é‡Šæ•°æ®é›†ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬é¦–å…ˆå¼€å‘äº†ä¸€ç§æ–°é¢–çš„ä¸‰é˜¶æ®µç®¡é“æ¥åˆæˆç™¾ä¸‡çº§å›¾åƒæè¿°æ•°æ®é›†ã€‚è¯¥ç®¡é“åˆ©ç”¨å¤šä¸ªé€šç”¨ MLLM ä½œä¸ºå¼±æ³¨é‡Šå™¨ï¼Œé€šè¿‡å…±è¯†èåˆå’Œä¸“å®¶çŸ¥è¯†æ³¨å…¥å®Œå–„å…¶è¾“å‡ºï¼Œå¹¶ç”Ÿæˆç»†èƒå½¢æ€çš„é«˜ä¿çœŸæè¿°ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨è¯¥æ•°æ®é›†é€šè¿‡å¤šé˜¶æ®µç­–ç•¥å¾®è°ƒ Qwen3-VL-4B æ¨¡å‹ï¼Œä»¥åˆ›å»ºä¸“é—¨çš„ç»†èƒç—…ç†å­¦ MLLMã€‚ç”±æ­¤äº§ç”Ÿçš„æ¨¡å‹è¢«å‘½åä¸º Singpath-VLï¼Œåœ¨ç»†ç²’åº¦å½¢æ€æ„ŸçŸ¥å’Œç»†èƒçº§è¯Šæ–­åˆ†ç±»æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ä¸ºäº†æ¨è¿›è¯¥é¢†åŸŸçš„å‘å±•ï¼Œæˆ‘ä»¬å°†å¼€æºéƒ¨åˆ†åˆæˆæ•°æ®é›†å’ŒåŸºå‡†ã€‚

</details>

---

## 157. Attention to details, logits to truth: visual-aware attention and logits enhancement to mitigate hallucinations in LVLMs

**ä¸­æ–‡æ ‡é¢˜**: å…³æ³¨ç»†èŠ‚ï¼Œlogits çœŸå®ï¼šè§†è§‰æ„ŸçŸ¥æ³¨æ„åŠ›å’Œ logits å¢å¼ºä»¥å‡è½» LVLM ä¸­çš„å¹»è§‰

**Date**: 2026-02-10 | **arXiv**: [2602.09521v1](http://arxiv.org/abs/2602.09521v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09521v1)

<details><summary><b>Abstract</b></summary>

Existing Large Vision-Language Models (LVLMs) exhibit insufficient visual attention, leading to hallucinations. To alleviate this problem, some previous studies adjust and amplify visual attention. These methods present a limitation that boosting attention for all visual tokens inevitably increases attention to task irrelevant tokens. To tackle this challenge, we propose a training free attentional intervention algorithm to enhance the attention of task-relevant tokens based on the argument that task-relevant tokens generally demonstrate high visual-textual similarities. Specifically, the vision-text cross-attention submatrices, which represent visual-textual correlations, are extracted to construct the reweighting matrices to reallocate attention. Besides, to enhance the contribution of visual tokens, we inject visual attention values into the beam search decoding to identify solutions with higher visual attention. Extensive experiments demonstrate that this method significantly reduces hallucinations across mainstream LVLMs, while preserving the accuracy and coherence of generated content.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç°æœ‰çš„å¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰è¡¨ç°å‡ºè§†è§‰æ³¨æ„åŠ›ä¸è¶³ï¼Œå¯¼è‡´å¹»è§‰ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä¹‹å‰çš„ä¸€äº›ç ”ç©¶è°ƒæ•´å¹¶å¢å¼ºäº†è§†è§‰æ³¨æ„åŠ›ã€‚è¿™äº›æ–¹æ³•å­˜åœ¨ä¸€ä¸ªå±€é™æ€§ï¼Œå³æé«˜å¯¹æ‰€æœ‰è§†è§‰æ ‡è®°çš„æ³¨æ„åŠ›ä¸å¯é¿å…åœ°ä¼šå¢åŠ å¯¹ä¸ä»»åŠ¡æ— å…³çš„æ ‡è®°çš„æ³¨æ„åŠ›ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…è®­ç»ƒæ³¨æ„åŠ›å¹²é¢„ç®—æ³•ï¼Œä»¥åŸºäºä»»åŠ¡ç›¸å…³æ ‡è®°é€šå¸¸è¡¨ç°å‡ºé«˜åº¦è§†è§‰æ–‡æœ¬ç›¸ä¼¼æ€§çš„è®ºç‚¹æ¥å¢å¼ºä»»åŠ¡ç›¸å…³æ ‡è®°çš„æ³¨æ„åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæå–ä»£è¡¨è§†è§‰æ–‡æœ¬ç›¸å…³æ€§çš„è§†è§‰æ–‡æœ¬äº¤å‰æ³¨æ„åŠ›å­çŸ©é˜µæ¥æ„é€ é‡æ–°åŠ æƒçŸ©é˜µä»¥é‡æ–°åˆ†é…æ³¨æ„åŠ›ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¢å¼ºè§†è§‰æ ‡è®°çš„è´¡çŒ®ï¼Œæˆ‘ä»¬å°†è§†è§‰æ³¨æ„åŠ›å€¼æ³¨å…¥æ³¢æŸæœç´¢è§£ç ä¸­ï¼Œä»¥è¯†åˆ«å…·æœ‰æ›´é«˜è§†è§‰æ³¨æ„åŠ›çš„è§£å†³æ–¹æ¡ˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯æ˜¾ç€å‡å°‘ä¸»æµ LVLM ä¸­çš„å¹»è§‰ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆå†…å®¹çš„å‡†ç¡®æ€§å’Œè¿è´¯æ€§ã€‚

</details>

---

## 158. A Universal Action Space for General Behavior Analysis

**ä¸­æ–‡æ ‡é¢˜**: ç”¨äºä¸€èˆ¬è¡Œä¸ºåˆ†æçš„é€šç”¨è¡ŒåŠ¨ç©ºé—´

**Date**: 2026-02-10 | **arXiv**: [2602.09518v1](http://arxiv.org/abs/2602.09518v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09518v1)

**Code**: https://github.com/franktpmvu/Universal-Action-Space.

<details><summary><b>Abstract</b></summary>

Analyzing animal and human behavior has long been a challenging task in computer vision. Early approaches from the 1970s to the 1990s relied on hand-crafted edge detection, segmentation, and low-level features such as color, shape, and texture to locate objects and infer their identities-an inherently ill-posed problem. Behavior analysis in this era typically proceeded by tracking identified objects over time and modeling their trajectories using sparse feature points, which further limited robustness and generalization. A major shift occurred with the introduction of ImageNet by Deng and Li in 2010, which enabled large-scale visual recognition through deep neural networks and effectively served as a comprehensive visual dictionary. This development allowed object recognition to move beyond complex low-level processing toward learned high-level representations. In this work, we follow this paradigm to build a large-scale Universal Action Space (UAS) using existing labeled human-action datasets. We then use this UAS as the foundation for analyzing and categorizing mammalian and chimpanzee behavior datasets. The source code is released on GitHub at https://github.com/franktpmvu/Universal-Action-Space.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åˆ†æåŠ¨ç‰©å’Œäººç±»è¡Œä¸ºé•¿æœŸä»¥æ¥ä¸€ç›´æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ 20 ä¸–çºª 70 å¹´ä»£åˆ° 90 å¹´ä»£çš„æ—©æœŸæ–¹æ³•ä¾èµ–äºæ‰‹å·¥è¾¹ç¼˜æ£€æµ‹ã€åˆ†å‰²å’Œé¢œè‰²ã€å½¢çŠ¶å’Œçº¹ç†ç­‰ä½çº§ç‰¹å¾æ¥å®šä½å¯¹è±¡å¹¶æ¨æ–­å…¶èº«ä»½ï¼Œè¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªä¸é€‚å®šé—®é¢˜ã€‚è¿™ä¸ªæ—¶ä»£çš„è¡Œä¸ºåˆ†æé€šå¸¸æ˜¯é€šè¿‡éšç€æ—¶é—´çš„æ¨ç§»è·Ÿè¸ªå·²è¯†åˆ«çš„å¯¹è±¡å¹¶ä½¿ç”¨ç¨€ç–ç‰¹å¾ç‚¹å¯¹å…¶è½¨è¿¹è¿›è¡Œå»ºæ¨¡æ¥è¿›è¡Œçš„ï¼Œè¿™è¿›ä¸€æ­¥é™åˆ¶äº†é²æ£’æ€§å’Œæ³›åŒ–æ€§ã€‚ 2010 å¹´ï¼ŒDeng å’Œ Li å¼•å…¥ ImageNetï¼Œå‘ç”Ÿäº†é‡å¤§è½¬å˜ï¼Œå®ƒé€šè¿‡æ·±åº¦ç¥ç»ç½‘ç»œå®ç°äº†å¤§è§„æ¨¡è§†è§‰è¯†åˆ«ï¼Œå¹¶æœ‰æ•ˆåœ°å……å½“äº†ç»¼åˆè§†è§‰è¯å…¸ã€‚è¿™ä¸€å‘å±•ä½¿å¾—å¯¹è±¡è¯†åˆ«èƒ½å¤Ÿè¶…è¶Šå¤æ‚çš„ä½çº§å¤„ç†ï¼Œè½¬å‘å­¦ä¹ çš„é«˜çº§è¡¨ç¤ºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬éµå¾ªè¿™ä¸ªèŒƒä¾‹ï¼Œä½¿ç”¨ç°æœ‰çš„æ ‡è®°äººç±»è¡Œä¸ºæ•°æ®é›†æ„å»ºä¸€ä¸ªå¤§è§„æ¨¡çš„é€šç”¨è¡ŒåŠ¨ç©ºé—´ï¼ˆUASï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨è¯¥æ— äººæœºä½œä¸ºåˆ†æå’Œåˆ†ç±»å“ºä¹³åŠ¨ç‰©å’Œé»‘çŒ©çŒ©è¡Œä¸ºæ•°æ®é›†çš„åŸºç¡€ã€‚æºä»£ç å‘å¸ƒåœ¨ GitHub ä¸Šï¼šhttps://github.com/franktpmvu/Universal-Action-Spaceã€‚

</details>

---

## 159. Equilibrium contrastive learning for imbalanced image classification

**ä¸­æ–‡æ ‡é¢˜**: ä¸å¹³è¡¡å›¾åƒåˆ†ç±»çš„å¹³è¡¡å¯¹æ¯”å­¦ä¹ 

**Date**: 2026-02-10 | **arXiv**: [2602.09506v1](http://arxiv.org/abs/2602.09506v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09506v1)

<details><summary><b>Abstract</b></summary>

Contrastive learning (CL) is a predominant technique in image classification, but they showed limited performance with an imbalanced dataset. Recently, several supervised CL methods have been proposed to promote an ideal regular simplex geometric configuration in the representation space-characterized by intra-class feature collapse and uniform inter-class mean spacing, especially for imbalanced datasets. In particular, existing prototype-based methods include class prototypes, as additional samples to consider all classes. However, the existing CL methods suffer from two limitations. First, they do not consider the alignment between the class means/prototypes and classifiers, which could lead to poor generalization. Second, existing prototype-based methods treat prototypes as only one additional sample per class, making their influence depend on the number of class instances in a batch and causing unbalanced contributions across classes. To address these limitations, we propose Equilibrium Contrastive Learning (ECL), a supervised CL framework designed to promote geometric equilibrium, where class features, means, and classifiers are harmoniously balanced under data imbalance. The proposed ECL framework uses two main components. First, ECL promotes the representation geometric equilibrium (i.e., a regular simplex geometry characterized by collapsed class samples and uniformly distributed class means), while balancing the contributions of class-average features and class prototypes. Second, ECL establishes a classifier-class center geometric equilibrium by aligning classifier weights and class prototypes. We ran experiments with three long-tailed datasets, the CIFAR-10(0)-LT, ImageNet-LT, and the two imbalanced medical datasets, the ISIC 2019 and our constructed LCCT dataset. Results show that ECL outperforms existing SOTA supervised CL methods designed for imbalanced classification.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¯¹æ¯”å­¦ä¹ ï¼ˆCLï¼‰æ˜¯å›¾åƒåˆ†ç±»ä¸­çš„ä¸»è¦æŠ€æœ¯ï¼Œä½†å®ƒä»¬åœ¨ä¸å¹³è¡¡çš„æ•°æ®é›†ä¸Šè¡¨ç°å‡ºæœ‰é™çš„æ€§èƒ½ã€‚æœ€è¿‘ï¼Œäººä»¬æå‡ºäº†å‡ ç§æœ‰ç›‘ç£çš„ CL æ–¹æ³•æ¥ä¿ƒè¿›è¡¨ç¤ºç©ºé—´ä¸­ç†æƒ³çš„æ­£åˆ™å•çº¯å½¢å‡ ä½•é…ç½®ï¼Œå…¶ç‰¹å¾æ˜¯ç±»å†…ç‰¹å¾å´©æºƒå’Œå‡åŒ€çš„ç±»é—´å¹³å‡é—´è·ï¼Œç‰¹åˆ«æ˜¯å¯¹äºä¸å¹³è¡¡æ•°æ®é›†ã€‚ç‰¹åˆ«æ˜¯ï¼Œç°æœ‰çš„åŸºäºåŸå‹çš„æ–¹æ³•åŒ…æ‹¬ç±»åŸå‹ï¼Œä½œä¸ºè€ƒè™‘æ‰€æœ‰ç±»çš„é™„åŠ æ ·æœ¬ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ CL æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªå±€é™æ€§ã€‚é¦–å…ˆï¼Œä»–ä»¬æ²¡æœ‰è€ƒè™‘ç±»æ‰‹æ®µ/åŸå‹å’Œåˆ†ç±»å™¨ä¹‹é—´çš„å¯¹é½ï¼Œè¿™å¯èƒ½å¯¼è‡´æ³›åŒ–ä¸è‰¯ã€‚å…¶æ¬¡ï¼Œç°æœ‰çš„åŸºäºåŸå‹çš„æ–¹æ³•å°†åŸå‹è§†ä¸ºæ¯ä¸ªç±»çš„ä¸€ä¸ªé™„åŠ æ ·æœ¬ï¼Œä½¿å…¶å½±å“å–å†³äºä¸€æ‰¹ä¸­ç±»å®ä¾‹çš„æ•°é‡ï¼Œå¹¶å¯¼è‡´ç±»ä¹‹é—´çš„è´¡çŒ®ä¸å¹³è¡¡ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†å¹³è¡¡å¯¹æ¯”å­¦ä¹ ï¼ˆECLï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æœ‰ç›‘ç£çš„ CL æ¡†æ¶ï¼Œæ—¨åœ¨ä¿ƒè¿›å‡ ä½•å¹³è¡¡ï¼Œå…¶ä¸­ç±»ç‰¹å¾ã€å‡å€¼å’Œåˆ†ç±»å™¨åœ¨æ•°æ®ä¸å¹³è¡¡çš„æƒ…å†µä¸‹å’Œè°å¹³è¡¡ã€‚æ‰€æå‡ºçš„ ECL æ¡†æ¶ä½¿ç”¨ä¸¤ä¸ªä¸»è¦ç»„ä»¶ã€‚é¦–å…ˆï¼ŒECL ä¿ƒè¿›è¡¨ç¤ºå‡ ä½•å¹³è¡¡ï¼ˆå³ï¼Œä»¥æŠ˜å ç±»æ ·æœ¬å’Œå‡åŒ€åˆ†å¸ƒç±»å‡å€¼ä¸ºç‰¹å¾çš„æ­£åˆ™å•çº¯å½¢å‡ ä½•ï¼‰ï¼ŒåŒæ—¶å¹³è¡¡ç±»å¹³å‡ç‰¹å¾å’Œç±»åŸå‹çš„è´¡çŒ®ã€‚å…¶æ¬¡ï¼ŒECL é€šè¿‡å¯¹é½åˆ†ç±»å™¨æƒé‡å’Œç±»åŸå‹æ¥å»ºç«‹åˆ†ç±»å™¨ç±»ä¸­å¿ƒå‡ ä½•å¹³è¡¡ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸‰ä¸ªé•¿å°¾æ•°æ®é›†ï¼ˆCIFAR-10(0)-LTã€ImageNet-LTï¼‰å’Œä¸¤ä¸ªä¸å¹³è¡¡åŒ»å­¦æ•°æ®é›†ï¼ˆISIC 2019 å’Œæˆ‘ä»¬æ„å»ºçš„ LCCT æ•°æ®é›†ï¼‰è¿›è¡Œäº†å®éªŒã€‚ç»“æœè¡¨æ˜ï¼ŒECL ä¼˜äºç°æœ‰çš„ä¸“ä¸ºä¸å¹³è¡¡åˆ†ç±»è®¾è®¡çš„ SOTA ç›‘ç£ CL æ–¹æ³•ã€‚

</details>

---

## 160. Beyond Next-Token Alignment: Distilling Multimodal Large Language Models via Token Interactions

**ä¸­æ–‡æ ‡é¢˜**: è¶…è¶Šä¸‹ä¸€ä¸ªä»¤ç‰Œå¯¹é½ï¼šé€šè¿‡ä»¤ç‰Œäº¤äº’æç‚¼å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹

**Date**: 2026-02-10 | **arXiv**: [2602.09483v1](http://arxiv.org/abs/2602.09483v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09483v1)

**Code**: https://github.com/lchen1019/Align-TI.

<details><summary><b>Abstract</b></summary>

Multimodal Large Language Models (MLLMs) demonstrate impressive cross-modal capabilities, yet their substantial size poses significant deployment challenges. Knowledge distillation (KD) is a promising solution for compressing these models, but existing methods primarily rely on static next-token alignment, neglecting the dynamic token interactions, which embed essential capabilities for multimodal understanding and generation. To this end, we introduce Align-TI, a novel KD framework designed from the perspective of Token Interactions. Our approach is motivated by the insight that MLLMs rely on two primary interactions: vision-instruction token interactions to extract relevant visual information, and intra-response token interactions for coherent generation. Accordingly, Align-TI introduces two components: IVA enables the student model to imitate the teacher's instruction-relevant visual information extract capability by aligning on salient visual regions. TPA captures the teacher's dynamic generative logic by aligning the sequential token-to-token transition probabilities. Extensive experiments demonstrate Align-TI's superiority. Notably, our approach achieves $2.6\%$ relative improvement over Vanilla KD, and our distilled Align-TI-2B even outperforms LLaVA-1.5-7B (a much larger MLLM) by $7.0\%$, establishing a new state-of-the-art distillation framework for training parameter-efficient MLLMs. Code is available at https://github.com/lchen1019/Align-TI.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ (MLLM) å±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„è·¨æ¨¡æ€åŠŸèƒ½ï¼Œä½†å…¶åºå¤§çš„è§„æ¨¡å¸¦æ¥äº†é‡å¤§çš„éƒ¨ç½²æŒ‘æˆ˜ã€‚çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰æ˜¯å‹ç¼©è¿™äº›æ¨¡å‹çš„ä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºé™æ€ä¸‹ä¸€ä¸ªä»¤ç‰Œå¯¹é½ï¼Œå¿½ç•¥äº†åŠ¨æ€ä»¤ç‰Œäº¤äº’ï¼Œè€ŒåŠ¨æ€ä»¤ç‰Œäº¤äº’åµŒå…¥äº†å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„åŸºæœ¬åŠŸèƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥Align-TIï¼Œä¸€ä¸ªä»Tokenäº¤äº’è§’åº¦è®¾è®¡çš„æ–°é¢–çš„KDæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„åŠ¨æœºæ˜¯è®¤è¯†åˆ° MLLM ä¾èµ–äºä¸¤ç§ä¸»è¦äº¤äº’ï¼šç”¨äºæå–ç›¸å…³è§†è§‰ä¿¡æ¯çš„è§†è§‰æŒ‡ä»¤ä»¤ç‰Œäº¤äº’ï¼Œä»¥åŠç”¨äºè¿è´¯ç”Ÿæˆçš„å“åº”å†…ä»¤ç‰Œäº¤äº’ã€‚å› æ­¤ï¼ŒAlign-TI å¼•å…¥äº†ä¸¤ä¸ªç»„ä»¶ï¼šIVA ä½¿å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿé€šè¿‡å¯¹é½æ˜¾ç€è§†è§‰åŒºåŸŸæ¥æ¨¡ä»¿æ•™å¸ˆçš„ä¸æ•™å­¦ç›¸å…³çš„è§†è§‰ä¿¡æ¯æå–èƒ½åŠ›ã€‚ TPA é€šè¿‡è°ƒæ•´é¡ºåºæ ‡è®°åˆ°æ ‡è®°çš„è½¬æ¢æ¦‚ç‡æ¥æ•è·æ•™å¸ˆçš„åŠ¨æ€ç”Ÿæˆé€»è¾‘ã€‚å¤§é‡çš„å®éªŒè¯æ˜äº†Align-TI çš„ä¼˜è¶Šæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯” Vanilla KD å®ç°äº† 2.6\%$ çš„ç›¸å¯¹æ”¹è¿›ï¼Œå¹¶ä¸”æˆ‘ä»¬çš„è’¸é¦ Align-TI-2B ç”šè‡³æ¯” LLaVA-1.5-7Bï¼ˆæ›´å¤§çš„ MLLMï¼‰é«˜å‡º 7.0\%$ï¼Œä¸ºè®­ç»ƒå‚æ•°é«˜æ•ˆçš„ MLLM å»ºç«‹äº†ä¸€ä¸ªæ–°çš„æœ€å…ˆè¿›çš„è’¸é¦æ¡†æ¶ã€‚ä»£ç å¯åœ¨ https://github.com/lchen1019/Align-TI è·å–ã€‚

</details>

---

## 161. Weakly Supervised Contrastive Learning for Histopathology Patch Embeddings

**ä¸­æ–‡æ ‡é¢˜**: ç»„ç»‡ç—…ç†å­¦æ–‘å—åµŒå…¥çš„å¼±ç›‘ç£å¯¹æ¯”å­¦ä¹ 

**Date**: 2026-02-10 | **arXiv**: [2602.09477v1](http://arxiv.org/abs/2602.09477v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09477v1)

<details><summary><b>Abstract</b></summary>

Digital histopathology whole slide images (WSIs) provide gigapixel-scale high-resolution images that are highly useful for disease diagnosis. However, digital histopathology image analysis faces significant challenges due to the limited training labels, since manually annotating specific regions or small patches cropped from large WSIs requires substantial time and effort. Weakly supervised multiple instance learning (MIL) offers a practical and efficient solution by requiring only bag-level (slide-level) labels, while each bag typically contains multiple instances (patches). Most MIL methods directly use frozen image patch features generated by various image encoders as inputs and primarily focus on feature aggregation. However, feature representation learning for encoder pretraining in MIL settings has largely been neglected.   In our work, we propose a novel feature representation learning framework called weakly supervised contrastive learning (WeakSupCon) that incorporates bag-level label information during training. Our method does not rely on instance-level pseudo-labeling, yet it effectively separates patches with different labels in the feature space. Experimental results demonstrate that the image features generated by our WeakSupCon method lead to improved downstream MIL performance compared to self-supervised contrastive learning approaches in three datasets. Our related code is available at github.com/BzhangURU/Paper_WeakSupCon_for_MIL

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æ•°å­—ç»„ç»‡ç—…ç†å­¦å…¨åˆ‡ç‰‡å›¾åƒ (WSI) æä¾›åäº¿åƒç´ çº§é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œå¯¹äºç–¾ç—…è¯Šæ–­éå¸¸æœ‰ç”¨ã€‚ç„¶è€Œï¼Œç”±äºè®­ç»ƒæ ‡ç­¾æœ‰é™ï¼Œæ•°å­—ç»„ç»‡ç—…ç†å­¦å›¾åƒåˆ†æé¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºæ‰‹åŠ¨æ³¨é‡Šç‰¹å®šåŒºåŸŸæˆ–ä»å¤§å‹ WSI ä¸­è£å‰ªçš„å°å—éœ€è¦å¤§é‡æ—¶é—´å’Œç²¾åŠ›ã€‚å¼±ç›‘ç£å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æä¾›äº†ä¸€ç§å®ç”¨ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œåªéœ€è¦åŒ…çº§ï¼ˆå¹»ç¯ç‰‡çº§ï¼‰æ ‡ç­¾ï¼Œè€Œæ¯ä¸ªåŒ…é€šå¸¸åŒ…å«å¤šä¸ªå®ä¾‹ï¼ˆè¡¥ä¸ï¼‰ã€‚å¤§å¤šæ•° MIL æ–¹æ³•ç›´æ¥ä½¿ç”¨å„ç§å›¾åƒç¼–ç å™¨ç”Ÿæˆçš„å†»ç»“å›¾åƒå—ç‰¹å¾ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä¸”ä¸»è¦å…³æ³¨ç‰¹å¾èšåˆã€‚ç„¶è€Œï¼ŒMIL è®¾ç½®ä¸­ç¼–ç å™¨é¢„è®­ç»ƒçš„ç‰¹å¾è¡¨ç¤ºå­¦ä¹ åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šè¢«å¿½è§†äº†ã€‚   åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç‰¹å¾è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œç§°ä¸ºå¼±ç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼ˆWeakSupConï¼‰ï¼Œè¯¥æ¡†æ¶åœ¨è®­ç»ƒæœŸé—´ç»“åˆäº†è¢‹çº§æ ‡ç­¾ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä¾èµ–äºå®ä¾‹çº§ä¼ªæ ‡ç­¾ï¼Œä½†å®ƒæœ‰æ•ˆåœ°åˆ†ç¦»äº†ç‰¹å¾ç©ºé—´ä¸­å…·æœ‰ä¸åŒæ ‡ç­¾çš„è¡¥ä¸ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¸‰ä¸ªæ•°æ®é›†ä¸­çš„è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ WeakSupCon æ–¹æ³•ç”Ÿæˆçš„å›¾åƒç‰¹å¾å¯ä»¥æé«˜ä¸‹æ¸¸ MIL æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç›¸å…³ä»£ç å¯åœ¨ github.com/BzhangURU/Paper_WeakSupCon_for_MIL è·å–

</details>

---

## 162. FD-DB: Frequency-Decoupled Dual-Branch Network for Unpaired Synthetic-to-Real Domain Translation

**ä¸­æ–‡æ ‡é¢˜**: FD-DBï¼šç”¨äºä¸æˆå¯¹åˆæˆåˆ°çœŸå®åŸŸè½¬æ¢çš„é¢‘ç‡è§£è€¦åŒåˆ†æ”¯ç½‘ç»œ

**Date**: 2026-02-10 | **arXiv**: [2602.09476v2](http://arxiv.org/abs/2602.09476v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.09476v2)

<details><summary><b>Abstract</b></summary>

Synthetic data provide low-cost, accurately annotated samples for geometry-sensitive vision tasks, but appearance and imaging differences between synthetic and real domains cause severe domain shift and degrade downstream performance. Unpaired synthetic-to-real translation can reduce this gap without paired supervision, yet existing methods often face a trade-off between photorealism and structural stability: unconstrained generation may introduce deformation or spurious textures, while overly rigid constraints limit adaptation to real-domain statistics. We propose FD-DB, a frequency-decoupled dual-branch model that separates appearance transfer into low-frequency interpretable editing and high-frequency residual compensation. The interpretable branch predicts physically meaningful editing parameters (white balance, exposure, contrast, saturation, blur, and grain) to build a stable low-frequency appearance base with strong content preservation. The free branch complements fine details through residual generation, and a gated fusion mechanism combines the two branches under explicit frequency constraints to limit low-frequency drift. We further adopt a two-stage training schedule that first stabilizes the editing branch and then releases the residual branch to improve optimization stability. Experiments on the YCB-V dataset show that FD-DB improves real-domain appearance consistency and significantly boosts downstream semantic segmentation performance while preserving geometric and semantic structures.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åˆæˆæ•°æ®ä¸ºå‡ ä½•æ•æ„Ÿçš„è§†è§‰ä»»åŠ¡æä¾›äº†ä½æˆæœ¬ã€å‡†ç¡®æ³¨é‡Šçš„æ ·æœ¬ï¼Œä½†åˆæˆåŸŸå’ŒçœŸå®åŸŸä¹‹é—´çš„å¤–è§‚å’Œæˆåƒå·®å¼‚ä¼šå¯¼è‡´ä¸¥é‡çš„åŸŸåç§»å¹¶é™ä½ä¸‹æ¸¸æ€§èƒ½ã€‚ä¸æˆå¯¹çš„åˆæˆåˆ°çœŸå®çš„ç¿»è¯‘å¯ä»¥åœ¨æ²¡æœ‰æˆå¯¹ç›‘ç£çš„æƒ…å†µä¸‹ç¼©å°è¿™ç§å·®è·ï¼Œä½†ç°æœ‰çš„æ–¹æ³•é€šå¸¸é¢ä¸´ç…§ç‰‡çœŸå®æ€§å’Œç»“æ„ç¨³å®šæ€§ä¹‹é—´çš„æƒè¡¡ï¼šæ— çº¦æŸçš„ç”Ÿæˆå¯èƒ½ä¼šå¼•å…¥å˜å½¢æˆ–è™šå‡çº¹ç†ï¼Œè€Œè¿‡äºä¸¥æ ¼çš„çº¦æŸé™åˆ¶äº†å¯¹å®åŸŸç»Ÿè®¡æ•°æ®çš„é€‚åº”ã€‚æˆ‘ä»¬æå‡ºäº† FD-DBï¼Œä¸€ç§é¢‘ç‡è§£è€¦åŒåˆ†æ”¯æ¨¡å‹ï¼Œå°†å¤–è§‚ä¼ è¾“åˆ†ç¦»ä¸ºä½é¢‘å¯è§£é‡Šç¼–è¾‘å’Œé«˜é¢‘æ®‹å·®è¡¥å¿ã€‚å¯è§£é‡Šåˆ†æ”¯é¢„æµ‹ç‰©ç†ä¸Šæœ‰æ„ä¹‰çš„ç¼–è¾‘å‚æ•°ï¼ˆç™½å¹³è¡¡ã€æ›å…‰ã€å¯¹æ¯”åº¦ã€é¥±å’Œåº¦ã€æ¨¡ç³Šå’Œé¢—ç²’ï¼‰ï¼Œä»¥æ„å»ºå…·æœ‰å¼ºå¤§å†…å®¹ä¿ç•™çš„ç¨³å®šä½é¢‘å¤–è§‚åŸºç¡€ã€‚è‡ªç”±åˆ†æ”¯é€šè¿‡æ®‹å·®ç”Ÿæˆè¡¥å……ç²¾ç»†ç»†èŠ‚ï¼Œé—¨æ§èåˆæœºåˆ¶åœ¨æ˜ç¡®çš„é¢‘ç‡çº¦æŸä¸‹ç»„åˆä¸¤ä¸ªåˆ†æ”¯ä»¥é™åˆ¶ä½é¢‘æ¼‚ç§»ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒè®¡åˆ’ï¼Œé¦–å…ˆç¨³å®šç¼–è¾‘åˆ†æ”¯ï¼Œç„¶åé‡Šæ”¾æ®‹ä½™åˆ†æ”¯ä»¥æé«˜ä¼˜åŒ–ç¨³å®šæ€§ã€‚åœ¨ YCB-V æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFD-DB æé«˜äº†å®åŸŸå¤–è§‚ä¸€è‡´æ€§ï¼Œå¹¶æ˜¾ç€æé«˜äº†ä¸‹æ¸¸è¯­ä¹‰åˆ†å‰²æ€§èƒ½ï¼ŒåŒæ—¶ä¿ç•™äº†å‡ ä½•å’Œè¯­ä¹‰ç»“æ„ã€‚

</details>

---

## 163. A Scoping Review of Deep Learning for Urban Visual Pollution and Proposal of a Real-Time Monitoring Framework with a Visual Pollution Index

**ä¸­æ–‡æ ‡é¢˜**: åŸå¸‚è§†è§‰æ±¡æŸ“æ·±åº¦å­¦ä¹ çš„èŒƒå›´å®¡æŸ¥ä»¥åŠå…·æœ‰è§†è§‰æ±¡æŸ“æŒ‡æ•°çš„å®æ—¶ç›‘æµ‹æ¡†æ¶çš„æè®®

**Date**: 2026-02-10 | **arXiv**: [2602.09446v1](http://arxiv.org/abs/2602.09446v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09446v1)

<details><summary><b>Abstract</b></summary>

Urban Visual Pollution (UVP) has emerged as a critical concern, yet research on automatic detection and application remains fragmented. This scoping review maps the existing deep learning-based approaches for detecting, classifying, and designing a comprehensive application framework for visual pollution management. Following the PRISMA-ScR guidelines, seven academic databases (Scopus, Web of Science, IEEE Xplore, ACM DL, ScienceDirect, SpringerNatureLink, and Wiley) were systematically searched and reviewed, and 26 articles were found. Most research focuses on specific pollutant categories and employs variations of YOLO, Faster R-CNN, and EfficientDet architectures. Although several datasets exist, they are limited to specific areas and lack standardized taxonomies. Few studies integrate detection into real-time application systems, yet they tend to be geographically skewed. We proposed a framework for monitoring visual pollution that integrates a visual pollution index to assess the severity of visual pollution for a certain area. This review highlights the need for a unified UVP management system that incorporates pollutant taxonomy, a cross-city benchmark dataset, a generalized deep learning model, and an assessment index that supports sustainable urban aesthetics and enhances the well-being of urban dwellers.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åŸå¸‚è§†è§‰æ±¡æŸ“ï¼ˆUVPï¼‰å·²æˆä¸ºä¸€ä¸ªä¸¥é‡é—®é¢˜ï¼Œä½†è‡ªåŠ¨æ£€æµ‹å’Œåº”ç”¨çš„ç ”ç©¶ä»ç„¶åˆ†æ•£ã€‚æœ¬æ¬¡èŒƒå›´å®¡æŸ¥æç»˜äº†ç°æœ‰çš„åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ï¼Œç”¨äºæ£€æµ‹ã€åˆ†ç±»å’Œè®¾è®¡è§†è§‰æ±¡æŸ“ç®¡ç†çš„ç»¼åˆåº”ç”¨æ¡†æ¶ã€‚éµå¾ª PRISMA-ScR æŒ‡å—ï¼Œå¯¹ 7 ä¸ªå­¦æœ¯æ•°æ®åº“ï¼ˆScopusã€Web of Scienceã€IEEE Xploreã€ACM DLã€ScienceDirectã€SpringerNatureLink å’Œ Wileyï¼‰è¿›è¡Œäº†ç³»ç»Ÿæ£€ç´¢å’Œå®¡æŸ¥ï¼Œå…±æ‰¾åˆ° 26 ç¯‡æ–‡ç« ã€‚å¤§å¤šæ•°ç ”ç©¶éƒ½é›†ä¸­åœ¨ç‰¹å®šçš„æ±¡æŸ“ç‰©ç±»åˆ«ä¸Šï¼Œå¹¶é‡‡ç”¨ YOLOã€Faster R-CNN å’Œ EfficientDet æ¶æ„çš„å˜ä½“ã€‚å°½ç®¡å­˜åœ¨å¤šä¸ªæ•°æ®é›†ï¼Œä½†å®ƒä»¬ä»…é™äºç‰¹å®šé¢†åŸŸå¹¶ä¸”ç¼ºä¹æ ‡å‡†åŒ–åˆ†ç±»æ³•ã€‚å¾ˆå°‘æœ‰ç ”ç©¶å°†æ£€æµ‹é›†æˆåˆ°å®æ—¶åº”ç”¨ç³»ç»Ÿä¸­ï¼Œä½†å®ƒä»¬å¾€å¾€å­˜åœ¨åœ°åŸŸåå·®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè§†è§‰æ±¡æŸ“ç›‘æµ‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†è§†è§‰æ±¡æŸ“æŒ‡æ•°æ¥è¯„ä¼°ç‰¹å®šåŒºåŸŸè§†è§‰æ±¡æŸ“çš„ä¸¥é‡ç¨‹åº¦ã€‚æœ¬æ¬¡å®¡æŸ¥å¼ºè°ƒéœ€è¦ä¸€ä¸ªç»Ÿä¸€çš„UVPç®¡ç†ç³»ç»Ÿï¼Œè¯¥ç³»ç»ŸåŒ…å«æ±¡æŸ“ç‰©åˆ†ç±»ã€è·¨åŸå¸‚åŸºå‡†æ•°æ®é›†ã€å¹¿ä¹‰æ·±åº¦å­¦ä¹ æ¨¡å‹ä»¥åŠæ”¯æŒå¯æŒç»­åŸå¸‚ç¾å­¦å’Œæé«˜åŸå¸‚å±…æ°‘ç¦ç¥‰çš„è¯„ä¼°æŒ‡æ•°ã€‚

</details>

---

## 164. Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning

**ä¸­æ–‡æ ‡é¢˜**: Fine-T2Iï¼šç”¨äºé«˜è´¨é‡ T2I å¾®è°ƒçš„å¼€æ”¾ã€å¤§è§„æ¨¡ä¸”å¤šæ ·åŒ–çš„æ•°æ®é›†

**Date**: 2026-02-10 | **arXiv**: [2602.09439v1](http://arxiv.org/abs/2602.09439v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09439v1)

<details><summary><b>Abstract</b></summary>

High-quality and open datasets remain a major bottleneck for text-to-image (T2I) fine-tuning. Despite rapid progress in model architectures and training pipelines, most publicly available fine-tuning datasets suffer from low resolution, poor text-image alignment, or limited diversity, resulting in a clear performance gap between open research models and enterprise-grade models. In this work, we present Fine-T2I, a large-scale, high-quality, and fully open dataset for T2I fine-tuning. Fine-T2I spans 10 task combinations, 32 prompt categories, 11 visual styles, and 5 prompt templates, and combines synthetic images generated by strong modern models with carefully curated real images from professional photographers. All samples are rigorously filtered for text-image alignment, visual fidelity, and prompt quality, with over 95% of initial candidates removed. The final dataset contains over 6 million text-image pairs, around 2 TB on disk, approaching the scale of pretraining datasets while maintaining fine-tuning-level quality. Across a diverse set of pretrained diffusion and autoregressive models, fine-tuning on Fine-T2I consistently improves both generation quality and instruction adherence, as validated by human evaluation, visual comparison, and automatic metrics. We release Fine-T2I under an open license to help close the data gap in T2I fine-tuning in the open community.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

é«˜è´¨é‡å’Œå¼€æ”¾çš„æ•°æ®é›†ä»ç„¶æ˜¯æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰å¾®è°ƒçš„ä¸»è¦ç“¶é¢ˆã€‚å°½ç®¡æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæµç¨‹å–å¾—äº†å¿«é€Ÿè¿›å±•ï¼Œä½†å¤§å¤šæ•°å…¬å¼€å¯ç”¨çš„å¾®è°ƒæ•°æ®é›†éƒ½å­˜åœ¨åˆ†è¾¨ç‡ä½ã€æ–‡æœ¬å›¾åƒå¯¹é½å·®æˆ–å¤šæ ·æ€§æœ‰é™çš„é—®é¢˜ï¼Œå¯¼è‡´å¼€æ”¾ç ”ç©¶æ¨¡å‹å’Œä¼ä¸šçº§æ¨¡å‹ä¹‹é—´å­˜åœ¨æ˜æ˜¾çš„æ€§èƒ½å·®è·ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† Fine-T2Iï¼Œä¸€ä¸ªç”¨äº T2I å¾®è°ƒçš„å¤§è§„æ¨¡ã€é«˜è´¨é‡ã€å®Œå…¨å¼€æ”¾çš„æ•°æ®é›†ã€‚ Fine-T2Iè·¨è¶Š10ç§ä»»åŠ¡ç»„åˆã€32ç§æç¤ºç±»åˆ«ã€11ç§è§†è§‰é£æ ¼å’Œ5ç§æç¤ºæ¨¡æ¿ï¼Œå¹¶å°†å¼ºå¤§çš„ç°ä»£æ¨¡å‹ç”Ÿæˆçš„åˆæˆå›¾åƒä¸ä¸“ä¸šæ‘„å½±å¸ˆç²¾å¿ƒç­–åˆ’çš„çœŸå®å›¾åƒç›¸ç»“åˆã€‚æ‰€æœ‰æ ·æœ¬éƒ½ç»è¿‡ä¸¥æ ¼çš„æ–‡æœ¬å›¾åƒå¯¹é½ã€è§†è§‰ä¿çœŸåº¦å’Œæç¤ºè´¨é‡è¿‡æ»¤ï¼Œè¶…è¿‡ 95% çš„åˆå§‹å€™é€‰æ ·æœ¬è¢«åˆ é™¤ã€‚æœ€ç»ˆæ•°æ®é›†åŒ…å«è¶…è¿‡ 600 ä¸‡ä¸ªæ–‡æœ¬å›¾åƒå¯¹ï¼Œç£ç›˜å¤§å°çº¦ä¸º 2 TBï¼Œæ¥è¿‘é¢„è®­ç»ƒæ•°æ®é›†çš„è§„æ¨¡ï¼ŒåŒæ—¶ä¿æŒå¾®è°ƒçº§åˆ«çš„è´¨é‡ã€‚åœ¨ä¸€ç³»åˆ—ä¸åŒçš„é¢„è®­ç»ƒæ‰©æ•£å’Œè‡ªå›å½’æ¨¡å‹ä¸­ï¼ŒFine-T2I ä¸Šçš„å¾®è°ƒä¸æ–­æé«˜ç”Ÿæˆè´¨é‡å’ŒæŒ‡ä»¤ä¾ä»æ€§ï¼Œè¿™ä¸€ç‚¹å·²é€šè¿‡äººå·¥è¯„ä¼°ã€è§†è§‰æ¯”è¾ƒå’Œè‡ªåŠ¨æŒ‡æ ‡è¿›è¡ŒéªŒè¯ã€‚æˆ‘ä»¬åœ¨å¼€æ”¾è®¸å¯ä¸‹å‘å¸ƒ Fine-T2Iï¼Œä»¥å¸®åŠ©ç¼©å°å¼€æ”¾ç¤¾åŒºä¸­ T2I å¾®è°ƒçš„æ•°æ®å·®è·ã€‚

</details>

---

## 165. SceneReVis: A Self-Reflective Vision-Grounded Framework for 3D Indoor Scene Synthesis via Multi-turn RL

**ä¸­æ–‡æ ‡é¢˜**: SceneReVisï¼šé€šè¿‡å¤šè½¬ RL è¿›è¡Œ 3D å®¤å†…åœºæ™¯åˆæˆçš„è‡ªåå°„è§†è§‰æ¡†æ¶

**Date**: 2026-02-10 | **arXiv**: [2602.09432v1](http://arxiv.org/abs/2602.09432v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09432v1)

<details><summary><b>Abstract</b></summary>

Current one-pass 3D scene synthesis methods often suffer from spatial hallucinations, such as collisions, due to a lack of deliberative reasoning. To bridge this gap, we introduce SceneReVis, a vision-grounded self-reflection framework that employs an iterative ``diagnose-and-act'' loop to explicitly intercept and resolve spatial conflicts using multi-modal feedback. To support this step-wise paradigm, we construct SceneChain-12k, a large-scale dataset of causal construction trajectories derived through a novel reverse engineering pipeline. We further propose a two-stage training recipe that transitions from Supervised Fine-Tuning to Agentic Reinforcement Learning, evolving the model into an active spatial planner. Extensive experiments demonstrate that SceneReVis achieves state-of-the-art performance in high-fidelity generation and goal-oriented optimization, with robust generalization to long-tail domains.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç”±äºç¼ºä¹æ·±æ€ç†Ÿè™‘çš„æ¨ç†ï¼Œå½“å‰çš„ä¸€æ¬¡æ€§ 3D åœºæ™¯åˆæˆæ–¹æ³•ç»å¸¸ä¼šå‡ºç°ç©ºé—´å¹»è§‰ï¼Œä¾‹å¦‚ç¢°æ’ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº† SceneReVisï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè§†è§‰çš„è‡ªæˆ‘åæ€æ¡†æ¶ï¼Œå®ƒé‡‡ç”¨è¿­ä»£çš„â€œè¯Šæ–­å’Œè¡ŒåŠ¨â€å¾ªç¯ï¼Œä½¿ç”¨å¤šæ¨¡æ€åé¦ˆæ¥æ˜ç¡®æ‹¦æˆªå’Œè§£å†³ç©ºé—´å†²çªã€‚ä¸ºäº†æ”¯æŒè¿™ç§é€æ­¥èŒƒä¾‹ï¼Œæˆ‘ä»¬æ„å»ºäº† SceneChain-12kï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡æ–°é¢–çš„é€†å‘å·¥ç¨‹ç®¡é“å¯¼å‡ºçš„å› æœæ„å»ºè½¨è¿¹çš„å¤§å‹æ•°æ®é›†ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è®­ç»ƒæ–¹æ³•ï¼Œä»ç›‘ç£å¾®è°ƒè¿‡æ¸¡åˆ°ä»£ç†å¼ºåŒ–å­¦ä¹ ï¼Œå°†æ¨¡å‹æ¼”å˜æˆä¸»åŠ¨ç©ºé—´è§„åˆ’å™¨ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSceneReVis åœ¨é«˜ä¿çœŸç”Ÿæˆå’Œé¢å‘ç›®æ ‡çš„ä¼˜åŒ–æ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å¯¹é•¿å°¾åŸŸå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚

</details>

---

## 166. Understanding and Enhancing Encoder-based Adversarial Transferability against Large Vision-Language Models

**ä¸­æ–‡æ ‡é¢˜**: ç†è§£å’Œå¢å¼ºé’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„åŸºäºç¼–ç å™¨çš„å¯¹æŠ—æ€§å¯è¿ç§»æ€§

**Date**: 2026-02-10 | **arXiv**: [2602.09431v1](http://arxiv.org/abs/2602.09431v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09431v1)

<details><summary><b>Abstract</b></summary>

Large vision-language models (LVLMs) have achieved impressive success across multimodal tasks, but their reliance on visual inputs exposes them to significant adversarial threats. Existing encoder-based attacks perturb the input image by optimizing solely on the vision encoder, rather than the entire LVLM, offering a computationally efficient alternative to end-to-end optimization. However, their transferability across different LVLM architectures in realistic black-box scenarios remains poorly understood. To address this gap, we present the first systematic study towards encoder-based adversarial transferability in LVLMs. Our contributions are threefold. First, through large-scale benchmarking over eight diverse LVLMs, we reveal that existing attacks exhibit severely limited transferability. Second, we perform in-depth analysis, disclosing two root causes that hinder the transferability: (1) inconsistent visual grounding across models, where different models focus their attention on distinct regions; (2) redundant semantic alignment within models, where a single object is dispersed across multiple overlapping token representations. Third, we propose Semantic-Guided Multimodal Attack (SGMA), a novel framework to enhance the transferability. Inspired by the discovered causes in our analysis, SGMA directs perturbations toward semantically critical regions and disrupts cross-modal grounding at both global and local levels. Extensive experiments across different victim models and tasks show that SGMA achieves higher transferability than existing attacks. These results expose critical security risks in LVLM deployment and underscore the urgent need for robust multimodal defenses.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰åœ¨å¤šæ¨¡å¼ä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æˆåŠŸï¼Œä½†å®ƒä»¬å¯¹è§†è§‰è¾“å…¥çš„ä¾èµ–ä½¿å®ƒä»¬é¢ä¸´é‡å¤§çš„å¯¹æŠ—æ€§å¨èƒã€‚ç°æœ‰çš„åŸºäºç¼–ç å™¨çš„æ”»å‡»é€šè¿‡ä»…ä¼˜åŒ–è§†è§‰ç¼–ç å™¨è€Œä¸æ˜¯æ•´ä¸ª LVLM æ¥æ‰°ä¹±è¾“å…¥å›¾åƒï¼Œä»è€Œä¸ºç«¯åˆ°ç«¯ä¼˜åŒ–æä¾›äº†è®¡ç®—é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç°å®é»‘ç›’åœºæ™¯ä¸­è·¨ä¸åŒ LVLM æ¶æ„çš„å¯è½¬ç§»æ€§ä»ç„¶çŸ¥ä¹‹ç”šå°‘ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ä¸ªé’ˆå¯¹ LVLM ä¸­åŸºäºç¼–ç å™¨çš„å¯¹æŠ—æ€§å¯è½¬ç§»æ€§çš„ç³»ç»Ÿç ”ç©¶ã€‚æˆ‘ä»¬çš„è´¡çŒ®æ˜¯ä¸‰é‡çš„ã€‚é¦–å…ˆï¼Œé€šè¿‡å¯¹å…«ä¸ªä¸åŒçš„ LVLM è¿›è¡Œå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å‘ç°ç°æœ‰æ”»å‡»çš„å¯è½¬ç§»æ€§å—åˆ°ä¸¥é‡é™åˆ¶ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œæ­ç¤ºäº†é˜»ç¢å¯è¿ç§»æ€§çš„ä¸¤ä¸ªæ ¹æœ¬åŸå› ï¼šï¼ˆ1ï¼‰æ¨¡å‹é—´è§†è§‰åŸºç¡€ä¸ä¸€è‡´ï¼Œä¸åŒæ¨¡å‹å°†æ³¨æ„åŠ›é›†ä¸­åœ¨ä¸åŒçš„åŒºåŸŸï¼› (2) æ¨¡å‹å†…çš„å†—ä½™è¯­ä¹‰å¯¹é½ï¼Œå…¶ä¸­å•ä¸ªå¯¹è±¡åˆ†æ•£åœ¨å¤šä¸ªé‡å çš„æ ‡è®°è¡¨ç¤ºä¸­ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬æå‡ºäº†è¯­ä¹‰å¼•å¯¼å¤šæ¨¡æ€æ”»å‡»ï¼ˆSGMAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¢å¼ºå¯è½¬ç§»æ€§çš„æ–°é¢–æ¡†æ¶ã€‚å—åˆ°æˆ‘ä»¬åˆ†æä¸­å‘ç°çš„åŸå› çš„å¯å‘ï¼ŒSGMA å°†æ‰°åŠ¨å¯¼å‘è¯­ä¹‰å…³é”®åŒºåŸŸï¼Œå¹¶ç ´åå…¨å±€å’Œå±€éƒ¨å±‚é¢çš„è·¨æ¨¡å¼åŸºç¡€ã€‚è·¨ä¸åŒå—å®³è€…æ¨¡å‹å’Œä»»åŠ¡çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSGMA æ¯”ç°æœ‰æ”»å‡»å®ç°äº†æ›´é«˜çš„å¯è½¬ç§»æ€§ã€‚è¿™äº›ç»“æœæš´éœ²äº† LVLM éƒ¨ç½²ä¸­çš„å…³é”®å®‰å…¨é£é™©ï¼Œå¹¶å¼ºè°ƒäº†å¯¹å¼ºå¤§çš„å¤šæ¨¡å¼é˜²å¾¡çš„è¿«åˆ‡éœ€è¦ã€‚

</details>

---

## 167. Bridging the Modality Gap in Roadside LiDAR: A Training-Free Vision-Language Model Framework for Vehicle Classification

**ä¸­æ–‡æ ‡é¢˜**: å¼¥åˆè·¯è¾¹æ¿€å…‰é›·è¾¾çš„æ¨¡æ€å·®è·ï¼šç”¨äºè½¦è¾†åˆ†ç±»çš„å…è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹æ¡†æ¶

**Date**: 2026-02-10 | **arXiv**: [2602.09425v1](http://arxiv.org/abs/2602.09425v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09425v1)

<details><summary><b>Abstract</b></summary>

Fine-grained truck classification is critical for intelligent transportation systems (ITS), yet current LiDAR-based methods face scalability challenges due to their reliance on supervised deep learning and labor-intensive manual annotation. Vision-Language Models (VLMs) offer promising few-shot generalization, but their application to roadside LiDAR is limited by a modality gap between sparse 3D point clouds and dense 2D imagery. We propose a framework that bridges this gap by adapting off-the-shelf VLMs for fine-grained truck classification without parameter fine-tuning. Our new depth-aware image generation pipeline applies noise removal, spatial and temporal registration, orientation rectification, morphological operations, and anisotropic smoothing to transform sparse, occluded LiDAR scans into depth-encoded 2D visual proxies. Validated on a real-world dataset of 20 vehicle classes, our approach achieves competitive classification accuracy with as few as 16-30 examples per class, offering a scalable alternative to data-intensive supervised baselines. We further observe a "Semantic Anchor" effect: text-based guidance regularizes performance in ultra-low-shot regimes $k < 4$, but degrades accuracy in more-shot settings due to semantic mismatch. Furthermore, we demonstrate the efficacy of this framework as a Cold Start strategy, using VLM-generated labels to bootstrap lightweight supervised models. Notably, the few-shot VLM-based model achieves over correct classification rate of 75 percent for specific drayage categories (20ft, 40ft, and 53ft containers) entirely without the costly training or fine-tuning, significantly reducing the intensive demands of initial manual labeling, thus achieving a method of practical use in ITS applications.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç»†ç²’åº¦çš„å¡è½¦åˆ†ç±»å¯¹äºæ™ºèƒ½äº¤é€šç³»ç»Ÿ (ITS) è‡³å…³é‡è¦ï¼Œä½†å½“å‰åŸºäº LiDAR çš„æ–¹æ³•ç”±äºä¾èµ–ç›‘ç£æ·±åº¦å­¦ä¹ å’ŒåŠ³åŠ¨å¯†é›†å‹æ‰‹åŠ¨æ³¨é‡Šè€Œé¢ä¸´å¯æ‰©å±•æ€§æŒ‘æˆ˜ã€‚è§†è§‰è¯­è¨€æ¨¡å‹ (VLM) æä¾›äº†æœ‰å¸Œæœ›çš„å°‘æ ·æœ¬æ³›åŒ–ï¼Œä½†å®ƒä»¬åœ¨è·¯è¾¹ LiDAR ä¸­çš„åº”ç”¨å—åˆ°ç¨€ç– 3D ç‚¹äº‘å’Œå¯†é›† 2D å›¾åƒä¹‹é—´æ¨¡æ€å·®è·çš„é™åˆ¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œé€šè¿‡é‡‡ç”¨ç°æˆçš„ VLM æ¥è¿›è¡Œç»†ç²’åº¦çš„å¡è½¦åˆ†ç±»ï¼Œè€Œæ— éœ€è¿›è¡Œå‚æ•°å¾®è°ƒï¼Œä»è€Œå¼¥è¡¥äº†è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬æ–°çš„æ·±åº¦æ„ŸçŸ¥å›¾åƒç”Ÿæˆç®¡é“åº”ç”¨å™ªå£°å»é™¤ã€ç©ºé—´å’Œæ—¶é—´é…å‡†ã€æ–¹å‘æ ¡æ­£ã€å½¢æ€æ“ä½œå’Œå„å‘å¼‚æ€§å¹³æ»‘æ¥å°†ç¨€ç–ã€é®æŒ¡çš„ LiDAR æ‰«æè½¬æ¢ä¸ºæ·±åº¦ç¼–ç çš„ 2D è§†è§‰ä»£ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨åŒ…å« 20 ä¸ªè½¦è¾†ç±»åˆ«çš„çœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œæ¯ç±»åªéœ€ 16-30 ä¸ªç¤ºä¾‹å³å¯å®ç°æœ‰ç«äº‰åŠ›çš„åˆ†ç±»å‡†ç¡®æ€§ï¼Œä¸ºæ•°æ®å¯†é›†å‹ç›‘ç£åŸºçº¿æä¾›äº†å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è§‚å¯Ÿåˆ°â€œè¯­ä¹‰é”šâ€æ•ˆåº”ï¼šåŸºäºæ–‡æœ¬çš„æŒ‡å¯¼è§„èŒƒäº†â€‹â€‹è¶…ä½é•œå¤´çŠ¶æ€ $k < 4$ ä¸­çš„æ€§èƒ½ï¼Œä½†ç”±äºè¯­ä¹‰ä¸åŒ¹é…è€Œé™ä½äº†æ›´å¤šé•œå¤´è®¾ç½®ä¸­çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†è¯¥æ¡†æ¶ä½œä¸ºå†·å¯åŠ¨ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œä½¿ç”¨ VLM ç”Ÿæˆçš„æ ‡ç­¾æ¥å¼•å¯¼è½»é‡çº§ç›‘ç£æ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒåŸºäºfew-shot VLMçš„æ¨¡å‹å¯¹ç‰¹å®šæ‹–è¿ç±»åˆ«ï¼ˆ20è‹±å°ºã€40è‹±å°ºå’Œ53è‹±å°ºé›†è£…ç®±ï¼‰çš„æ­£ç¡®åˆ†ç±»ç‡è¾¾åˆ°äº†75%ä»¥ä¸Šï¼Œå®Œå…¨ä¸éœ€è¦æ˜‚è´µçš„åŸ¹è®­æˆ–å¾®è°ƒï¼Œæ˜¾ç€å‡å°‘äº†åˆå§‹æ‰‹åŠ¨æ ‡ç­¾çš„å¯†é›†éœ€æ±‚ï¼Œä»è€Œå®ç°äº†åœ¨ITSåº”ç”¨ä¸­å®é™…ä½¿ç”¨çš„æ–¹æ³•ã€‚

</details>

---

## 168. AD$^2$: Analysis and Detection of Adversarial Threats in Visual Perception for End-to-End Autonomous Driving Systems

**ä¸­æ–‡æ ‡é¢˜**: AD$^2$ï¼šç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿè§†è§‰æ„ŸçŸ¥ä¸­çš„å¯¹æŠ—æ€§å¨èƒåˆ†æå’Œæ£€æµ‹

**Date**: 2026-02-10 | **arXiv**: [2602.10160v1](http://arxiv.org/abs/2602.10160v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10160v1)

<details><summary><b>Abstract</b></summary>

End-to-end autonomous driving systems have achieved significant progress, yet their adversarial robustness remains largely underexplored. In this work, we conduct a closed-loop evaluation of state-of-the-art autonomous driving agents under black-box adversarial threat models in CARLA. Specifically, we consider three representative attack vectors on the visual perception pipeline: (i) a physics-based blur attack induced by acoustic waves, (ii) an electromagnetic interference attack that distorts captured images, and (iii) a digital attack that adds ghost objects as carefully crafted bounded perturbations on images. Our experiments on two advanced agents, Transfuser and Interfuser, reveal severe vulnerabilities to such attacks, with driving scores dropping by up to 99% in the worst case, raising valid safety concerns. To help mitigate such threats, we further propose a lightweight Attack Detection model for Autonomous Driving systems (AD$^2$) based on attention mechanisms that capture spatial-temporal consistency. Comprehensive experiments across multi-camera inputs on CARLA show that our detector achieves superior detection capability and computational efficiency compared to existing approaches.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å…¶å¯¹æŠ—é²æ£’æ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åœ¨ CARLA çš„é»‘ç›’å¯¹æŠ—å¨èƒæ¨¡å‹ä¸‹å¯¹æœ€å…ˆè¿›çš„è‡ªåŠ¨é©¾é©¶ä»£ç†è¿›è¡Œäº†é—­ç¯è¯„ä¼°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è€ƒè™‘è§†è§‰æ„ŸçŸ¥ç®¡é“ä¸Šçš„ä¸‰ä¸ªä»£è¡¨æ€§æ”»å‡»å‘é‡ï¼šï¼ˆiï¼‰ç”±å£°æ³¢å¼•èµ·çš„åŸºäºç‰©ç†çš„æ¨¡ç³Šæ”»å‡»ï¼Œï¼ˆiiï¼‰æ‰­æ›²æ•è·å›¾åƒçš„ç”µç£å¹²æ‰°æ”»å‡»ï¼Œä»¥åŠï¼ˆiiiï¼‰å°†å¹½çµå¯¹è±¡æ·»åŠ ä¸ºå›¾åƒä¸Šç²¾å¿ƒè®¾è®¡çš„æœ‰ç•Œæ‰°åŠ¨çš„æ•°å­—æ”»å‡»ã€‚æˆ‘ä»¬å¯¹ä¸¤ç§å…ˆè¿›ä»£ç† Transfuser å’Œ Interfuser è¿›è¡Œçš„å®éªŒæ­ç¤ºäº†æ­¤ç±»æ”»å‡»çš„ä¸¥é‡æ¼æ´ï¼Œåœ¨æœ€åçš„æƒ…å†µä¸‹é©¾é©¶åˆ†æ•°ä¸‹é™é«˜è¾¾ 99%ï¼Œå¼•å‘äº†åˆç†çš„å®‰å…¨æ‹…å¿§ã€‚ä¸ºäº†å¸®åŠ©å‡è½»æ­¤ç±»å¨èƒï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§åŸºäºæ•è·æ—¶ç©ºä¸€è‡´æ€§çš„æ³¨æ„åŠ›æœºåˆ¶çš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„è½»é‡çº§æ”»å‡»æ£€æµ‹æ¨¡å‹ï¼ˆAD$^2$ï¼‰ã€‚ CARLA ä¸Šå¤šæ‘„åƒæœºè¾“å…¥çš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ£€æµ‹å™¨å®ç°äº†å“è¶Šçš„æ£€æµ‹èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡ã€‚

</details>

---

## 169. LARV: Data-Free Layer-wise Adaptive Rescaling Veneer for Model Merging

**ä¸­æ–‡æ ‡é¢˜**: LARVï¼šç”¨äºæ¨¡å‹åˆå¹¶çš„æ— æ•°æ®é€å±‚è‡ªé€‚åº”ç¼©æ”¾èƒ¶åˆæ¿

**Date**: 2026-02-10 | **arXiv**: [2602.09413v1](http://arxiv.org/abs/2602.09413v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09413v1)

<details><summary><b>Abstract</b></summary>

Model merging aims to combine multiple fine-tuned models into a single multi-task model without access to training data. Existing task-vector merging methods such as TIES, TSV-M, and Iso-C/CTS differ in their aggregation rules but treat all layers nearly uniformly. This assumption overlooks the strong layer-wise heterogeneity in large vision transformers, where shallow layers are sensitive to interference while deeper layers encode stable task-specific features. We introduce LARV, a training-free, data-free, merger-agnostic Layer-wise Adaptive Rescaling Veneer that plugs into any task-vector merger and assigns a per-layer scale to each task vector before aggregation, and show it consistently boosts diverse merging rules. LARV adaptively suppresses shallow-layer interference and amplifies deeper-layer alignment using a simple deterministic schedule, requiring no retraining or modification to existing mergers. To our knowledge, this is the first work to perform layer-aware scaling for task-vector merging. LARV computes simple data-free layer proxies and turns them into scales through a lightweight rule; we study several instantiations within one framework (e.g., tiered two/three-level scaling with fixed values, or continuous mappings) and show that tiered choices offer the best robustness, while continuous mappings remain an ablation. LARV is orthogonal to the base merger and adds negligible cost. On FusionBench with Vision Transformers, LARV consistently improves all task-vector baselines across 8/14/20-task settings; for example, Iso-C + LARV reaches 85.9% on ViT-B/32, 89.2% on ViT-B/16, and 92.6% on ViT-L/14. Layerwise analysis and corruption tests further indicate that LARV suppresses shallow-layer interference while modestly amplifying deeper, task-stable features, turning model merging into a robust, layer-aware procedure rather than a uniform one.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æ¨¡å‹åˆå¹¶æ—¨åœ¨å°†å¤šä¸ªå¾®è°ƒæ¨¡å‹ç»„åˆæˆä¸€ä¸ªå¤šä»»åŠ¡æ¨¡å‹ï¼Œè€Œæ— éœ€è®¿é—®è®­ç»ƒæ•°æ®ã€‚ç°æœ‰çš„ä»»åŠ¡å‘é‡åˆå¹¶æ–¹æ³•ï¼ˆä¾‹å¦‚ TIESã€TSV-M å’Œ Iso-C/CTSï¼‰çš„èšåˆè§„åˆ™æœ‰æ‰€ä¸åŒï¼Œä½†å¯¹æ‰€æœ‰å±‚çš„å¤„ç†å‡ ä¹ä¸€è‡´ã€‚è¿™ç§å‡è®¾å¿½ç•¥äº†å¤§å‹è§†è§‰å˜æ¢å™¨ä¸­å¼ºçƒˆçš„åˆ†å±‚å¼‚è´¨æ€§ï¼Œå…¶ä¸­æµ…å±‚å¯¹å¹²æ‰°æ•æ„Ÿï¼Œè€Œæ›´æ·±çš„å±‚ç¼–ç ç¨³å®šçš„ç‰¹å®šäºä»»åŠ¡çš„ç‰¹å¾ã€‚æˆ‘ä»¬å¼•å…¥äº† LARVï¼Œè¿™æ˜¯ä¸€ç§å…è®­ç»ƒã€å…æ•°æ®ã€ä¸åˆå¹¶æ— å…³çš„é€å±‚è‡ªé€‚åº”é‡ç¼©æ”¾å•æ¿ï¼Œå¯æ’å…¥ä»»ä½•ä»»åŠ¡å‘é‡åˆå¹¶ï¼Œå¹¶åœ¨èšåˆä¹‹å‰ä¸ºæ¯ä¸ªä»»åŠ¡å‘é‡åˆ†é…æ¯å±‚å°ºåº¦ï¼Œå¹¶è¡¨æ˜å®ƒå§‹ç»ˆå¦‚ä¸€åœ°å¢å¼ºä¸åŒçš„åˆå¹¶è§„åˆ™ã€‚ LARV ä½¿ç”¨ç®€å•çš„ç¡®å®šæ€§è°ƒåº¦è‡ªé€‚åº”åœ°æŠ‘åˆ¶æµ…å±‚å¹²æ‰°å¹¶æ”¾å¤§æ›´æ·±å±‚å¯¹é½ï¼Œæ— éœ€å¯¹ç°æœ‰åˆå¹¶è¿›è¡Œé‡æ–°è®­ç»ƒæˆ–ä¿®æ”¹ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹ä»»åŠ¡å‘é‡åˆå¹¶æ‰§è¡Œåˆ†å±‚æ„ŸçŸ¥ç¼©æ”¾çš„å·¥ä½œã€‚ LARVè®¡ç®—ç®€å•çš„æ— æ•°æ®å±‚ä»£ç†ï¼Œå¹¶é€šè¿‡è½»é‡çº§è§„åˆ™å°†å…¶è½¬åŒ–ä¸ºå°ºåº¦ï¼›æˆ‘ä»¬ç ”ç©¶äº†ä¸€ä¸ªæ¡†æ¶å†…çš„å‡ ä¸ªå®ä¾‹ï¼ˆä¾‹å¦‚ï¼Œå…·æœ‰å›ºå®šå€¼çš„åˆ†å±‚ä¸¤çº§/ä¸‰çº§ç¼©æ”¾æˆ–è¿ç»­æ˜ å°„ï¼‰ï¼Œå¹¶è¡¨æ˜åˆ†å±‚é€‰æ‹©æä¾›äº†æœ€ä½³çš„é²æ£’æ€§ï¼Œè€Œè¿ç»­æ˜ å°„ä»ç„¶æ˜¯ä¸€ç§æ¶ˆèã€‚ LARV ä¸åŸºç¡€åˆå¹¶æ­£äº¤ï¼Œå¢åŠ çš„æˆæœ¬å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚åœ¨å¸¦æœ‰ Vision Transformers çš„ FusionBench ä¸Šï¼ŒLARV æŒç»­æ”¹è¿›äº† 8/14/20 ä»»åŠ¡è®¾ç½®ä¸­çš„æ‰€æœ‰ä»»åŠ¡å‘é‡åŸºçº¿ï¼›ä¾‹å¦‚ï¼ŒIso-C + LARV åœ¨ ViT-B/32 ä¸Šè¾¾åˆ° 85.9%ï¼Œåœ¨ ViT-B/16 ä¸Šè¾¾åˆ° 89.2%ï¼Œåœ¨ ViT-L/14 ä¸Šè¾¾åˆ° 92.6%ã€‚åˆ†å±‚åˆ†æå’ŒæŸåæµ‹è¯•è¿›ä¸€æ­¥è¡¨æ˜ï¼ŒLARV æŠ‘åˆ¶æµ…å±‚å¹²æ‰°ï¼ŒåŒæ—¶é€‚åº¦æ”¾å¤§æ›´æ·±å±‚æ¬¡çš„ã€ä»»åŠ¡ç¨³å®šçš„ç‰¹å¾ï¼Œå°†æ¨¡å‹åˆå¹¶è½¬å˜ä¸ºä¸€ç§ç¨³å¥çš„ã€åˆ†å±‚æ„ŸçŸ¥çš„è¿‡ç¨‹ï¼Œè€Œä¸æ˜¯ç»Ÿä¸€çš„è¿‡ç¨‹ã€‚

</details>

---

## 170. K-Sort Eval: Efficient Preference Evaluation for Visual Generation via Corrected VLM-as-a-Judge

**ä¸­æ–‡æ ‡é¢˜**: K-Sort Evalï¼šé€šè¿‡ä¿®æ­£çš„ VLM-as-a-Judge å¯¹è§†è§‰ç”Ÿæˆè¿›è¡Œæœ‰æ•ˆçš„åå¥½è¯„ä¼°

**Date**: 2026-02-10 | **arXiv**: [2602.09411v1](http://arxiv.org/abs/2602.09411v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09411v1)

<details><summary><b>Abstract</b></summary>

The rapid development of visual generative models raises the need for more scalable and human-aligned evaluation methods. While the crowdsourced Arena platforms offer human preference assessments by collecting human votes, they are costly and time-consuming, inherently limiting their scalability. Leveraging vision-language model (VLMs) as substitutes for manual judgments presents a promising solution. However, the inherent hallucinations and biases of VLMs hinder alignment with human preferences, thus compromising evaluation reliability. Additionally, the static evaluation approach lead to low efficiency. In this paper, we propose K-Sort Eval, a reliable and efficient VLM-based evaluation framework that integrates posterior correction and dynamic matching. Specifically, we curate a high-quality dataset from thousands of human votes in K-Sort Arena, with each instance containing the outputs and rankings of K models. When evaluating a new model, it undergoes (K+1)-wise free-for-all comparisons with existing models, and the VLM provide the rankings. To enhance alignment and reliability, we propose a posterior correction method, which adaptively corrects the posterior probability in Bayesian updating based on the consistency between the VLM prediction and human supervision. Moreover, we propose a dynamic matching strategy, which balances uncertainty and diversity to maximize the expected benefit of each comparison, thus ensuring more efficient evaluation. Extensive experiments show that K-Sort Eval delivers evaluation results consistent with K-Sort Arena, typically requiring fewer than 90 model runs, demonstrating both its efficiency and reliability.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è§†è§‰ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•æå‡ºäº†å¯¹æ›´å…·å¯æ‰©å±•æ€§å’Œäººæ€§åŒ–è¯„ä¼°æ–¹æ³•çš„éœ€æ±‚ã€‚è™½ç„¶ä¼—åŒ… Arena å¹³å°é€šè¿‡æ”¶é›†äººç±»æŠ•ç¥¨æ¥æä¾›äººç±»åå¥½è¯„ä¼°ï¼Œä½†å®ƒä»¬æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ï¼Œæœ¬è´¨ä¸Šé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ›¿ä»£äººå·¥åˆ¤æ–­æ˜¯ä¸€ç§å¾ˆæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼ŒVLM å›ºæœ‰çš„å¹»è§‰å’Œåå·®é˜»ç¢äº†ä¸äººç±»åå¥½çš„ä¸€è‡´æ€§ï¼Œä»è€ŒæŸå®³äº†è¯„ä¼°çš„å¯é æ€§ã€‚æ­¤å¤–ï¼Œé™æ€è¯„ä¼°æ–¹æ³•å¯¼è‡´æ•ˆç‡ä½ä¸‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† K-Sort Evalï¼Œè¿™æ˜¯ä¸€ç§å¯é ä¸”é«˜æ•ˆçš„åŸºäº VLM çš„è¯„ä¼°æ¡†æ¶ï¼Œé›†æˆäº†åéªŒæ ¡æ­£å’ŒåŠ¨æ€åŒ¹é…ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä» K-Sort Arena ä¸­æ•°åƒä¸ªäººç±»æŠ•ç¥¨ä¸­ç­–åˆ’äº†ä¸€ä¸ªé«˜è´¨é‡çš„æ•°æ®é›†ï¼Œæ¯ä¸ªå®ä¾‹éƒ½åŒ…å« K ä¸ªæ¨¡å‹çš„è¾“å‡ºå’Œæ’åã€‚åœ¨è¯„ä¼°æ–°æ¨¡å‹æ—¶ï¼Œå®ƒä¼šä¸ç°æœ‰æ¨¡å‹è¿›è¡Œ (K+1) æ–¹å¼çš„è‡ªç”±æ¯”è¾ƒï¼Œç„¶å VLM æä¾›æ’åã€‚ä¸ºäº†å¢å¼ºå¯¹é½å’Œå¯é æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åéªŒæ ¡æ­£æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ ¹æ® VLM é¢„æµ‹å’Œäººç±»ç›‘ç£ä¹‹é—´çš„ä¸€è‡´æ€§è‡ªé€‚åº”åœ°æ ¡æ­£è´å¶æ–¯æ›´æ–°ä¸­çš„åéªŒæ¦‚ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŠ¨æ€åŒ¹é…ç­–ç•¥ï¼Œå¹³è¡¡ä¸ç¡®å®šæ€§å’Œå¤šæ ·æ€§ï¼Œä»¥æœ€å¤§åŒ–æ¯æ¬¡æ¯”è¾ƒçš„é¢„æœŸæ”¶ç›Šï¼Œä»è€Œç¡®ä¿æ›´æœ‰æ•ˆçš„è¯„ä¼°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒK-Sort Eval æä¾›çš„è¯„ä¼°ç»“æœä¸ K-Sort Arena ä¸€è‡´ï¼Œé€šå¸¸éœ€è¦å°‘äº 90 æ¬¡æ¨¡å‹è¿è¡Œï¼Œè¯æ˜äº†å…¶æ•ˆç‡å’Œå¯é æ€§ã€‚

</details>

---

## 171. Single-Slice-to-3D Reconstruction in Medical Imaging and Natural Objects: A Comparative Benchmark with SAM 3D

**ä¸­æ–‡æ ‡é¢˜**: åŒ»å­¦æˆåƒå’Œè‡ªç„¶ç‰©ä½“ä¸­çš„å•åˆ‡ç‰‡åˆ° 3D é‡å»ºï¼šä¸ SAM 3D çš„æ¯”è¾ƒåŸºå‡†

**Date**: 2026-02-10 | **arXiv**: [2602.09407v1](http://arxiv.org/abs/2602.09407v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09407v1)

<details><summary><b>Abstract</b></summary>

A 3D understanding of anatomy is central to diagnosis and treatment planning, yet volumetric imaging remains costly with long wait times. Image-to-3D foundations models can solve this issue by reconstructing 3D data from 2D modalites. Current foundation models are trained on natural image distributions to reconstruct naturalistic objects from a single image by leveraging geometric priors across pixels. However, it is unclear whether these learned geometric priors transfer to medical data. In this study, we present a controlled zero-shot benchmark of single slice medical image-to-3D reconstruction across five state-of-the-art image-to-3D models: SAM3D, Hunyuan3D-2.1, Direct3D, Hi3DGen, and TripoSG. These are evaluated across six medical datasets spanning anatomical and pathological structures and two natrual datasets, using voxel based metrics and point cloud distance metrics. Across medical datasets, voxel based overlap remains moderate for all models, consistent with a depth reconstruction failure mode when inferring volume from a single slice. In contrast, global distance metrics show more separation between methods: SAM3D achieves the strongest overall topological similarity to ground truth medical 3D data, while alternative models are more prone to over-simplication of reconstruction. Our results quantify the limits of single-slice medical reconstruction and highlight depth ambiguity caused by the planar nature of 2D medical data, motivating multi-view image-to-3D reconstruction to enable reliable medical 3D inference.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¯¹è§£å‰–ç»“æ„çš„ 3D äº†è§£å¯¹äºè¯Šæ–­å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ï¼Œä½†ä½“ç§¯æˆåƒçš„æˆæœ¬ä»ç„¶å¾ˆé«˜ï¼Œä¸”ç­‰å¾…æ—¶é—´è¾ƒé•¿ã€‚å›¾åƒåˆ° 3D åŸºç¡€æ¨¡å‹å¯ä»¥é€šè¿‡ä» 2D æ¨¡æ€é‡å»º 3D æ•°æ®æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å½“å‰çš„åŸºç¡€æ¨¡å‹æ˜¯æ ¹æ®è‡ªç„¶å›¾åƒåˆ†å¸ƒè¿›è¡Œè®­ç»ƒçš„ï¼Œé€šè¿‡åˆ©ç”¨è·¨åƒç´ çš„å‡ ä½•å…ˆéªŒä»å•ä¸ªå›¾åƒé‡å»ºè‡ªç„¶å¯¹è±¡ã€‚ç„¶è€Œï¼Œç›®å‰å°šä¸æ¸…æ¥šè¿™äº›å­¦ä¹ åˆ°çš„å‡ ä½•å…ˆéªŒæ˜¯å¦ä¼šè½¬ç§»åˆ°åŒ»å­¦æ•°æ®ä¸­ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è·¨äº”ä¸ªæœ€å…ˆè¿›çš„å›¾åƒåˆ° 3D æ¨¡å‹çš„å•åˆ‡ç‰‡åŒ»å­¦å›¾åƒåˆ° 3D é‡å»ºçš„å—æ§é›¶æ ·æœ¬åŸºå‡†ï¼šSAM3Dã€Hunyuan3D-2.1ã€Direct3Dã€Hi3DGen å’Œ TripoSGã€‚ä½¿ç”¨åŸºäºä½“ç´ çš„æŒ‡æ ‡å’Œç‚¹äº‘è·ç¦»æŒ‡æ ‡ï¼Œå¯¹æ¶µç›–è§£å‰–å’Œç—…ç†ç»“æ„çš„å…­ä¸ªåŒ»å­¦æ•°æ®é›†ä»¥åŠä¸¤ä¸ªè‡ªç„¶æ•°æ®é›†è¿›è¡Œè¯„ä¼°ã€‚åœ¨æ•´ä¸ªåŒ»å­¦æ•°æ®é›†ä¸­ï¼ŒåŸºäºä½“ç´ çš„é‡å å¯¹äºæ‰€æœ‰æ¨¡å‹éƒ½ä¿æŒé€‚åº¦ï¼Œè¿™ä¸ä»å•ä¸ªåˆ‡ç‰‡æ¨æ–­ä½“ç§¯æ—¶çš„æ·±åº¦é‡å»ºå¤±è´¥æ¨¡å¼ä¸€è‡´ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå…¨å±€è·ç¦»åº¦é‡æ˜¾ç¤ºæ–¹æ³•ä¹‹é—´æœ‰æ›´å¤šçš„åˆ†ç¦»ï¼šSAM3D å®ç°äº†ä¸åœ°é¢å®å†µåŒ»å­¦ 3D æ•°æ®æœ€å¼ºçš„æ•´ä½“æ‹“æ‰‘ç›¸ä¼¼æ€§ï¼Œè€Œæ›¿ä»£æ¨¡å‹æ›´å®¹æ˜“è¿‡åº¦ç®€åŒ–é‡å»ºã€‚æˆ‘ä»¬çš„ç»“æœé‡åŒ–äº†å•åˆ‡ç‰‡åŒ»å­¦é‡å»ºçš„å±€é™æ€§ï¼Œå¹¶çªå‡ºäº†ç”± 2D åŒ»å­¦æ•°æ®çš„å¹³é¢æ€§è´¨å¼•èµ·çš„æ·±åº¦æ¨¡ç³Šæ€§ï¼Œæ¿€å‘äº†å¤šè§†å›¾å›¾åƒåˆ° 3D é‡å»ºï¼Œä»¥å®ç°å¯é çš„åŒ»å­¦ 3D æ¨ç†ã€‚

</details>

---

## 172. Beyond Closed-Pool Video Retrieval: A Benchmark and Agent Framework for Real-World Video Search and Moment Localization

**ä¸­æ–‡æ ‡é¢˜**: è¶…è¶Šé—­æ± è§†é¢‘æ£€ç´¢ï¼šç°å®ä¸–ç•Œè§†é¢‘æœç´¢å’Œæ—¶åˆ»å®šä½çš„åŸºå‡†å’Œä»£ç†æ¡†æ¶

**Date**: 2026-02-10 | **arXiv**: [2602.10159v1](http://arxiv.org/abs/2602.10159v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10159v1)

<details><summary><b>Abstract</b></summary>

Traditional video retrieval benchmarks focus on matching precise descriptions to closed video pools, failing to reflect real-world searches characterized by fuzzy, multi-dimensional memories on the open web. We present \textbf{RVMS-Bench}, a comprehensive system for evaluating real-world video memory search. It consists of \textbf{1,440 samples} spanning \textbf{20 diverse categories} and \textbf{four duration groups}, sourced from \textbf{real-world open-web videos}. RVMS-Bench utilizes a hierarchical description framework encompassing \textbf{Global Impression, Key Moment, Temporal Context, and Auditory Memory} to mimic realistic multi-dimensional search cues, with all samples strictly verified via a human-in-the-loop protocol. We further propose \textbf{RACLO}, an agentic framework that employs abductive reasoning to simulate the human ``Recall-Search-Verify'' cognitive process, effectively addressing the challenge of searching for videos via fuzzy memories in the real world. Experiments reveal that existing MLLMs still demonstrate insufficient capabilities in real-world Video Retrieval and Moment Localization based on fuzzy memories. We believe this work will facilitate the advancement of video retrieval robustness in real-world unstructured scenarios.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ä¼ ç»Ÿçš„è§†é¢‘æ£€ç´¢åŸºå‡†ä¾§é‡äºå°†ç²¾ç¡®æè¿°ä¸å°é—­è§†é¢‘æ± ç›¸åŒ¹é…ï¼Œæ— æ³•åæ˜ å¼€æ”¾ç½‘ç»œä¸Šä»¥æ¨¡ç³Šã€å¤šç»´è®°å¿†ä¸ºç‰¹å¾çš„ç°å®ä¸–ç•Œæœç´¢ã€‚æˆ‘ä»¬æå‡ºäº† \textbf{RVMS-Bench}ï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°ç°å®ä¸–ç•Œè§†é¢‘å†…å­˜æœç´¢çš„ç»¼åˆç³»ç»Ÿã€‚å®ƒç”± \textbf{1,440 ä¸ªæ ·æœ¬}ç»„æˆï¼Œæ¶µç›– \textbf{20 ä¸ªä¸åŒç±»åˆ«}å’Œ \textbf{å››ä¸ªæŒç»­æ—¶é—´ç»„}ï¼Œæºè‡ª \textbf{çœŸå®ä¸–ç•Œçš„å¼€æ”¾ç½‘ç»œè§†é¢‘}ã€‚ RVMS-Bench åˆ©ç”¨åŒ…å« \textbf{å…¨å±€å°è±¡ã€å…³é”®æ—¶åˆ»ã€æ—¶é—´ä¸Šä¸‹æ–‡å’Œå¬è§‰è®°å¿†}çš„åˆ†å±‚æè¿°æ¡†æ¶æ¥æ¨¡æ‹Ÿç°å®çš„å¤šç»´æœç´¢çº¿ç´¢ï¼Œæ‰€æœ‰æ ·æœ¬éƒ½é€šè¿‡äººæœºäº¤äº’åè®®è¿›è¡Œä¸¥æ ¼éªŒè¯ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡º\textbf{RACLO}ï¼Œä¸€ç§ä»£ç†æ¡†æ¶ï¼Œé‡‡ç”¨æº¯å› æ¨ç†æ¥æ¨¡æ‹Ÿäººç±»çš„â€œå›å¿†-æœç´¢-éªŒè¯â€è®¤çŸ¥è¿‡ç¨‹ï¼Œæœ‰æ•ˆè§£å†³é€šè¿‡ç°å®ä¸–ç•Œä¸­çš„æ¨¡ç³Šè®°å¿†æœç´¢è§†é¢‘çš„æŒ‘æˆ˜ã€‚å®éªŒè¡¨æ˜ï¼Œç°æœ‰çš„ MLLM åœ¨åŸºäºæ¨¡ç³Šè®°å¿†çš„ç°å®è§†é¢‘æ£€ç´¢å’Œæ—¶åˆ»å®šä½æ–¹é¢ä»ç„¶è¡¨ç°å‡ºä¸è¶³çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™é¡¹å·¥ä½œå°†æœ‰åŠ©äºæé«˜ç°å®ä¸–ç•Œéç»“æ„åŒ–åœºæ™¯ä¸­è§†é¢‘æ£€ç´¢çš„é²æ£’æ€§ã€‚

</details>

---

## 173. Fully Differentiable Bidirectional Dual-Task Synergistic Learning for Semi-Supervised 3D Medical Image Segmentation

**ä¸­æ–‡æ ‡é¢˜**: ç”¨äºåŠç›‘ç£ 3D åŒ»å­¦å›¾åƒåˆ†å‰²çš„å®Œå…¨å¯å¾®åŒå‘åŒä»»åŠ¡ååŒå­¦ä¹ 

**Date**: 2026-02-10 | **arXiv**: [2602.09378v1](http://arxiv.org/abs/2602.09378v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09378v1)

<details><summary><b>Abstract</b></summary>

Semi-supervised learning relaxes the need of large pixel-wise labeled datasets for image segmentation by leveraging unlabeled data. The scarcity of high-quality labeled data remains a major challenge in medical image analysis due to the high annotation costs and the need for specialized clinical expertise. Semi-supervised learning has demonstrated significant potential in addressing this bottleneck, with pseudo-labeling and consistency regularization emerging as two predominant paradigms. Dual-task collaborative learning, an emerging consistency-aware paradigm, seeks to derive supplementary supervision by establishing prediction consistency between related tasks. However, current methodologies are limited to unidirectional interaction mechanisms (typically regression-to-segmentation), as segmentation results can only be transformed into regression outputs in an offline manner, thereby failing to fully exploit the potential benefits of online bidirectional cross-task collaboration. Thus, we propose a fully Differentiable Bidirectional Synergistic Learning (DBiSL) framework, which seamlessly integrates and enhances four critical SSL components: supervised learning, consistency regularization, pseudo-supervised learning, and uncertainty estimation. Experiments on two benchmark datasets demonstrate our method's state-of-the-art performance. Beyond technical contributions, this work provides new insights into unified SSL framework design and establishes a new architectural foundation for dual-task-driven SSL, while offering a generic multitask learning framework applicable to broader computer vision applications. The code will be released on github upon acceptance.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åŠç›‘ç£å­¦ä¹ é€šè¿‡åˆ©ç”¨æœªæ ‡è®°çš„æ•°æ®ï¼Œç¼“è§£äº†å›¾åƒåˆ†å‰²å¯¹å¤§å‹åƒç´ çº§æ ‡è®°æ•°æ®é›†çš„éœ€æ±‚ã€‚ç”±äºæ³¨é‡Šæˆæœ¬é«˜ä¸”éœ€è¦ä¸“é—¨çš„ä¸´åºŠä¸“ä¸šçŸ¥è¯†ï¼Œé«˜è´¨é‡æ ‡è®°æ•°æ®çš„ç¨€ç¼ºä»ç„¶æ˜¯åŒ»å­¦å›¾åƒåˆ†æçš„ä¸»è¦æŒ‘æˆ˜ã€‚åŠç›‘ç£å­¦ä¹ åœ¨è§£å†³è¿™ä¸€ç“¶é¢ˆæ–¹é¢è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ï¼Œä¼ªæ ‡ç­¾å’Œä¸€è‡´æ€§æ­£åˆ™åŒ–æˆä¸ºä¸¤ä¸ªä¸»è¦èŒƒå¼ã€‚åŒä»»åŠ¡åä½œå­¦ä¹ æ˜¯ä¸€ç§æ–°å…´çš„ä¸€è‡´æ€§æ„è¯†èŒƒå¼ï¼Œæ—¨åœ¨é€šè¿‡å»ºç«‹ç›¸å…³ä»»åŠ¡ä¹‹é—´çš„é¢„æµ‹ä¸€è‡´æ€§æ¥è·å¾—è¡¥å……ç›‘ç£ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä»…é™äºå•å‘äº¤äº’æœºåˆ¶ï¼ˆé€šå¸¸æ˜¯å›å½’åˆ°åˆ†å‰²ï¼‰ï¼Œå› ä¸ºåˆ†å‰²ç»“æœåªèƒ½ä»¥ç¦»çº¿æ–¹å¼è½¬æ¢ä¸ºå›å½’è¾“å‡ºï¼Œä»è€Œæ— æ³•å……åˆ†åˆ©ç”¨åœ¨çº¿åŒå‘è·¨ä»»åŠ¡åä½œçš„æ½œåœ¨ä¼˜åŠ¿ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå®Œå…¨å¯å¾®çš„åŒå‘ååŒå­¦ä¹ ï¼ˆDBiSLï¼‰æ¡†æ¶ï¼Œå®ƒæ— ç¼é›†æˆå’Œå¢å¼ºäº†å››ä¸ªå…³é”®çš„ SSL ç»„ä»¶ï¼šç›‘ç£å­¦ä¹ ã€ä¸€è‡´æ€§æ­£åˆ™åŒ–ã€ä¼ªç›‘ç£å­¦ä¹ å’Œä¸ç¡®å®šæ€§ä¼°è®¡ã€‚å¯¹ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†çš„å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚é™¤äº†æŠ€æœ¯è´¡çŒ®ä¹‹å¤–ï¼Œè¿™é¡¹å·¥ä½œè¿˜æä¾›äº†å¯¹ç»Ÿä¸€ SSL æ¡†æ¶è®¾è®¡çš„æ–°è§è§£ï¼Œå¹¶ä¸ºåŒä»»åŠ¡é©±åŠ¨çš„ SSL å»ºç«‹äº†æ–°çš„æ¶æ„åŸºç¡€ï¼ŒåŒæ—¶æä¾›äº†é€‚ç”¨äºæ›´å¹¿æ³›çš„è®¡ç®—æœºè§†è§‰åº”ç”¨çš„é€šç”¨å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ã€‚ä»£ç ä¸€ç»æ¥å—å°†åœ¨githubä¸Šå‘å¸ƒã€‚

</details>

---

## 174. Deep Modeling and Interpretation for Bladder Cancer Classification

**ä¸­æ–‡æ ‡é¢˜**: è†€èƒ±ç™Œåˆ†ç±»çš„æ·±åº¦å»ºæ¨¡å’Œè§£é‡Š

**Date**: 2026-02-10 | **arXiv**: [2602.09324v1](http://arxiv.org/abs/2602.09324v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09324v1)

<details><summary><b>Abstract</b></summary>

Deep models based on vision transformer (ViT) and convolutional neural network (CNN) have demonstrated remarkable performance on natural datasets. However, these models may not be similar in medical imaging, where abnormal regions cover only a small portion of the image. This challenge motivates this study to investigate the latest deep models for bladder cancer classification tasks. We propose the following to evaluate these deep models: 1) standard classification using 13 models (four CNNs and eight transormer-based models), 2) calibration analysis to examine if these models are well calibrated for bladder cancer classification, and 3) we use GradCAM++ to evaluate the interpretability of these models for clinical diagnosis. We simulate $\sim 300$ experiments on a publicly multicenter bladder cancer dataset, and the experimental results demonstrate that the ConvNext series indicate limited generalization ability to classify bladder cancer images (e.g., $\sim 60\%$ accuracy). In addition, ViTs show better calibration effects compared to ConvNext and swin transformer series. We also involve test time augmentation to improve the models interpretability. Finally, no model provides a one-size-fits-all solution for a feasible interpretable model. ConvNext series are suitable for in-distribution samples, while ViT and its variants are suitable for interpreting out-of-distribution samples.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åŸºäºè§†è§‰å˜æ¢å™¨ï¼ˆViTï¼‰å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„æ·±åº¦æ¨¡å‹åœ¨è‡ªç„¶æ•°æ®é›†ä¸Šè¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨åŒ»å­¦æˆåƒä¸­å¯èƒ½å¹¶ä¸ç›¸ä¼¼ï¼Œå…¶ä¸­å¼‚å¸¸åŒºåŸŸä»…è¦†ç›–å›¾åƒçš„ä¸€å°éƒ¨åˆ†ã€‚è¿™ä¸€æŒ‘æˆ˜ä¿ƒä½¿æœ¬ç ”ç©¶ç ”ç©¶è†€èƒ±ç™Œåˆ†ç±»ä»»åŠ¡çš„æœ€æ–°æ·±åº¦æ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºä»¥ä¸‹æ–¹æ³•æ¥è¯„ä¼°è¿™äº›æ·±åº¦æ¨¡å‹ï¼š1ï¼‰ä½¿ç”¨ 13 ä¸ªæ¨¡å‹ï¼ˆå››ä¸ª CNN å’Œå…«ä¸ªåŸºäº Transormer çš„æ¨¡å‹ï¼‰è¿›è¡Œæ ‡å‡†åˆ†ç±»ï¼Œ2ï¼‰æ ¡å‡†åˆ†æä»¥æ£€æŸ¥è¿™äº›æ¨¡å‹æ˜¯å¦é’ˆå¯¹è†€èƒ±ç™Œåˆ†ç±»è¿›è¡Œäº†è‰¯å¥½æ ¡å‡†ï¼Œ3ï¼‰æˆ‘ä»¬ä½¿ç”¨ GradCAM++ æ¥è¯„ä¼°è¿™äº›æ¨¡å‹å¯¹ä¸´åºŠè¯Šæ–­çš„å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬åœ¨å…¬å¼€çš„å¤šä¸­å¿ƒè†€èƒ±ç™Œæ•°æ®é›†ä¸Šæ¨¡æ‹Ÿ $\sim 300$ å®éªŒï¼Œå®éªŒç»“æœè¡¨æ˜ ConvNext ç³»åˆ—è¡¨æ˜è†€èƒ±ç™Œå›¾åƒåˆ†ç±»çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼ˆä¾‹å¦‚ $\sim 60\%$ å‡†ç¡®åº¦ï¼‰ã€‚æ­¤å¤–ï¼Œä¸ConvNextå’Œswinå˜å‹å™¨ç³»åˆ—ç›¸æ¯”ï¼ŒViTsè¡¨ç°å‡ºæ›´å¥½çš„æ ¡å‡†æ•ˆæœã€‚æˆ‘ä»¬è¿˜å¢åŠ äº†æµ‹è¯•æ—¶é—´ä»¥æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚æœ€åï¼Œæ²¡æœ‰ä»»ä½•æ¨¡å‹èƒ½å¤Ÿä¸ºå¯è¡Œçš„å¯è§£é‡Šæ¨¡å‹æä¾›ä¸€åˆ€åˆ‡çš„è§£å†³æ–¹æ¡ˆã€‚ ConvNextç³»åˆ—é€‚ç”¨äºåˆ†å¸ƒå†…æ ·æœ¬ï¼Œè€ŒViTåŠå…¶å˜ä½“é€‚ç”¨äºè§£é‡Šåˆ†å¸ƒå¤–æ ·æœ¬ã€‚

</details>

---

## 175. GAFR-Net: A Graph Attention and Fuzzy-Rule Network for Interpretable Breast Cancer Image Classification

**ä¸­æ–‡æ ‡é¢˜**: GAFR-Netï¼šç”¨äºå¯è§£é‡Šä¹³è…ºç™Œå›¾åƒåˆ†ç±»çš„å›¾æ³¨æ„åŠ›å’Œæ¨¡ç³Šè§„åˆ™ç½‘ç»œ

**Date**: 2026-02-10 | **arXiv**: [2602.09318v1](http://arxiv.org/abs/2602.09318v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09318v1)

<details><summary><b>Abstract</b></summary>

Accurate classification of breast cancer histopathology images is pivotal for early oncological diagnosis and therapeutic intervention.However, conventional deep learning architectures often encounter performance degradation under limited annotations and suffer from a "blackbox" nature, hindering their clinical integration. To mitigate these limitations, we propose GAFRNet, a robust and interpretable Graph Attention and FuzzyRule Network specifically engineered for histopathology image classification with scarce supervision. GAFRNet constructs a similarity-driven graph representation to model intersample relationships and employs a multihead graph attention mechanism to capture complex relational features across heterogeneous tissue structures.Concurrently, a differentiable fuzzy-rule module encodes intrinsic topological descriptorsincluding node degree, clustering coefficient, and label consistencyinto explicit, human-understandable diagnostic logic. This design establishes transparent "IF-THEN" mappings that mimic the heuristic deduction process of medical experts, providing clear reasoning behind each prediction without relying on post-hoc attribution methods. Extensive evaluations on three benchmark datasets (BreakHis, Mini-DDSM, and ICIAR2018) demonstrate that GAFR-Net consistently outperforms various state-of-the-art methods across multiple magnifications and classification tasks. These results validate the superior generalization and practical utility of GAFR-Net as a reliable decision-support tool for weakly supervised medical image analysis.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ä¹³è…ºç™Œç»„ç»‡ç—…ç†å­¦å›¾åƒçš„å‡†ç¡®åˆ†ç±»å¯¹äºæ—©æœŸè‚¿ç˜¤è¯Šæ–­å’Œæ²»ç–—å¹²é¢„è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æ¶æ„ç»å¸¸åœ¨æœ‰é™çš„æ³¨é‡Šä¸‹é‡åˆ°æ€§èƒ½ä¸‹é™ï¼Œå¹¶ä¸”å…·æœ‰â€œé»‘ç®±â€æ€§è´¨ï¼Œé˜»ç¢äº†å…¶ä¸´åºŠæ•´åˆã€‚ä¸ºäº†å‡è½»è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº† GAFRNetï¼Œè¿™æ˜¯ä¸€ç§å¼ºå¤§ä¸”å¯è§£é‡Šçš„å›¾æ³¨æ„åŠ›å’Œæ¨¡ç³Šè§„åˆ™ç½‘ç»œï¼Œä¸“é—¨ç”¨äºç¼ºä¹ç›‘ç£çš„ç»„ç»‡ç—…ç†å­¦å›¾åƒåˆ†ç±»ã€‚ GAFRNet æ„å»ºç›¸ä¼¼æ€§é©±åŠ¨çš„å›¾è¡¨ç¤ºæ¥å»ºæ¨¡æ ·æœ¬é—´å…³ç³»ï¼Œå¹¶é‡‡ç”¨å¤šå¤´å›¾æ³¨æ„æœºåˆ¶æ¥æ•è·å¼‚è´¨ç»„ç»‡ç»“æ„ä¸­çš„å¤æ‚å…³ç³»ç‰¹å¾ã€‚åŒæ—¶ï¼Œå¯å¾®åˆ†æ¨¡ç³Šè§„åˆ™æ¨¡å—å°†å†…åœ¨æ‹“æ‰‘æè¿°ç¬¦ï¼ˆåŒ…æ‹¬èŠ‚ç‚¹åº¦ã€èšç±»ç³»æ•°å’Œæ ‡ç­¾ä¸€è‡´æ€§ï¼‰ç¼–ç ä¸ºæ˜ç¡®çš„ã€äººç±»å¯ç†è§£çš„è¯Šæ–­é€»è¾‘ã€‚è¯¥è®¾è®¡å»ºç«‹äº†é€æ˜çš„â€œIF-THENâ€æ˜ å°„ï¼Œæ¨¡ä»¿åŒ»å­¦ä¸“å®¶çš„å¯å‘å¼æ¼”ç»è¿‡ç¨‹ï¼Œä¸ºæ¯ä¸ªé¢„æµ‹èƒŒåæä¾›æ¸…æ™°çš„æ¨ç†ï¼Œè€Œä¸ä¾èµ–äºäº‹åå½’å› æ–¹æ³•ã€‚å¯¹ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ï¼ˆBreakHisã€Mini-DDSM å’Œ ICIAR2018ï¼‰çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒGAFR-Net åœ¨å¤šä¸ªæ”¾å¤§å€ç‡å’Œåˆ†ç±»ä»»åŠ¡ä¸­å§‹ç»ˆä¼˜äºå„ç§æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¿™äº›ç»“æœéªŒè¯äº† GAFR-Net ä½œä¸ºå¼±ç›‘ç£åŒ»å­¦å›¾åƒåˆ†æçš„å¯é å†³ç­–æ”¯æŒå·¥å…·çš„å“è¶Šæ³›åŒ–æ€§å’Œå®ç”¨æ€§ã€‚

</details>

---

## 176. X-Mark: Saliency-Guided Robust Dataset Ownership Verification for Medical Imaging

**ä¸­æ–‡æ ‡é¢˜**: X-Markï¼šæ˜¾ç€æ€§å¼•å¯¼çš„ç¨³å¥æ•°æ®é›†æ‰€æœ‰æƒéªŒè¯ç”¨äºåŒ»å­¦æˆåƒ

**Date**: 2026-02-10 | **arXiv**: [2602.09284v1](http://arxiv.org/abs/2602.09284v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09284v1)

<details><summary><b>Abstract</b></summary>

High-quality medical imaging datasets are essential for training deep learning models, but their unauthorized use raises serious copyright and ethical concerns. Medical imaging presents a unique challenge for existing dataset ownership verification methods designed for natural images, as static watermark patterns generated in fixed-scale images scale poorly dynamic and high-resolution scans with limited visual diversity and subtle anatomical structures, while preserving diagnostic quality. In this paper, we propose X-Mark, a sample-specific clean-label watermarking method for chest x-ray copyright protection. Specifically, X-Mark uses a conditional U-Net to generate unique perturbations within salient regions of each sample. We design a multi-component training objective to ensure watermark efficacy, robustness against dynamic scaling processes while preserving diagnostic quality and visual-distinguishability. We incorporate Laplacian regularization into our training objective to penalize high-frequency perturbations and achieve watermark scale-invariance. Ownership verification is performed in a black-box setting to detect characteristic behaviors in suspicious models. Extensive experiments on CheXpert verify the effectiveness of X-Mark, achieving WSR of 100% and reducing probability of false positives in Ind-M scenario by 12%, while demonstrating resistance to potential adaptive attacks.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

é«˜è´¨é‡çš„åŒ»å­¦æˆåƒæ•°æ®é›†å¯¹äºè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹è‡³å…³é‡è¦ï¼Œä½†æœªç»æˆæƒçš„ä½¿ç”¨ä¼šå¼•å‘ä¸¥é‡çš„ç‰ˆæƒå’Œé“å¾·é—®é¢˜ã€‚åŒ»å­¦æˆåƒå¯¹ä¸ºè‡ªç„¶å›¾åƒè®¾è®¡çš„ç°æœ‰æ•°æ®é›†æ‰€æœ‰æƒéªŒè¯æ–¹æ³•æå‡ºäº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå›ºå®šæ¯”ä¾‹å›¾åƒä¸­ç”Ÿæˆçš„é™æ€æ°´å°å›¾æ¡ˆåœ¨ä¿æŒè¯Šæ–­è´¨é‡çš„åŒæ—¶ï¼Œæ— æ³•åŠ¨æ€ç¼©æ”¾åŠ¨æ€å’Œé«˜åˆ†è¾¨ç‡æ‰«æï¼Œä¸”è§†è§‰å¤šæ ·æ€§æœ‰é™ï¼Œè§£å‰–ç»“æ„å¾®å¦™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† X-Markï¼Œä¸€ç§ç”¨äºèƒ¸éƒ¨ X å°„çº¿ç‰ˆæƒä¿æŠ¤çš„æ ·æœ¬ç‰¹å®šæ¸…æ´æ ‡ç­¾æ°´å°æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒX-Mark ä½¿ç”¨æ¡ä»¶ U-Net åœ¨æ¯ä¸ªæ ·æœ¬çš„æ˜¾ç€åŒºåŸŸå†…ç”Ÿæˆç‹¬ç‰¹çš„æ‰°åŠ¨ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šç»„ä»¶è®­ç»ƒç›®æ ‡ï¼Œä»¥ç¡®ä¿æ°´å°åŠŸæ•ˆã€åŠ¨æ€ç¼©æ”¾è¿‡ç¨‹çš„é²æ£’æ€§ï¼ŒåŒæ—¶ä¿æŒè¯Šæ–­è´¨é‡å’Œè§†è§‰å¯åŒºåˆ†æ€§ã€‚æˆ‘ä»¬å°†æ‹‰æ™®æ‹‰æ–¯æ­£åˆ™åŒ–çº³å…¥æˆ‘ä»¬çš„è®­ç»ƒç›®æ ‡ä¸­ï¼Œä»¥æƒ©ç½šé«˜é¢‘æ‰°åŠ¨å¹¶å®ç°æ°´å°å°ºåº¦ä¸å˜æ€§ã€‚æ‰€æœ‰æƒéªŒè¯åœ¨é»‘ç›’è®¾ç½®ä¸­æ‰§è¡Œï¼Œä»¥æ£€æµ‹å¯ç–‘æ¨¡å‹ä¸­çš„ç‰¹å¾è¡Œä¸ºã€‚ CheXpert ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº† X-Mark çš„æœ‰æ•ˆæ€§ï¼Œå®ç°äº† 100% çš„ WSRï¼Œå°† Ind-M åœºæ™¯ä¸­çš„è¯¯æŠ¥æ¦‚ç‡é™ä½äº† 12%ï¼ŒåŒæ—¶è¯æ˜äº†å¯¹æ½œåœ¨è‡ªé€‚åº”æ”»å‡»çš„æŠµæŠ—åŠ›ã€‚

</details>

---

## 177. The Complexity of Bayesian Network Learning: Revisiting the Superstructure

**ä¸­æ–‡æ ‡é¢˜**: è´å¶æ–¯ç½‘ç»œå­¦ä¹ çš„å¤æ‚æ€§ï¼šé‡æ–°å®¡è§†ä¸Šå±‚å»ºç­‘

**Date**: 2026-02-10 | **arXiv**: [2602.10253v1](http://arxiv.org/abs/2602.10253v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10253v1)

<details><summary><b>Abstract</b></summary>

We investigate the parameterized complexity of Bayesian Network Structure Learning (BNSL), a classical problem that has received significant attention in empirical but also purely theoretical studies. We follow up on previous works that have analyzed the complexity of BNSL w.r.t. the so-called superstructure of the input. While known results imply that BNSL is unlikely to be fixed-parameter tractable even when parameterized by the size of a vertex cover in the superstructure, here we show that a different kind of parameterization - notably by the size of a feedback edge set - yields fixed-parameter tractability. We proceed by showing that this result can be strengthened to a localized version of the feedback edge set, and provide corresponding lower bounds that complement previous results to provide a complexity classification of BNSL w.r.t. virtually all well-studied graph parameters.   We then analyze how the complexity of BNSL depends on the representation of the input. In particular, while the bulk of past theoretical work on the topic assumed the use of the so-called non-zero representation, here we prove that if an additive representation can be used instead then BNSL becomes fixed-parameter tractable even under significantly milder restrictions to the superstructure, notably when parameterized by the treewidth alone. Last but not least, we show how our results can be extended to the closely related problem of Polytree Learning.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æˆ‘ä»¬ç ”ç©¶äº†è´å¶æ–¯ç½‘ç»œç»“æ„å­¦ä¹ ï¼ˆBNSLï¼‰çš„å‚æ•°åŒ–å¤æ‚æ€§ï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨å®è¯ç ”ç©¶å’Œçº¯ç†è®ºç ”ç©¶ä¸­éƒ½å—åˆ°å¹¿æ³›å…³æ³¨çš„ç»å…¸é—®é¢˜ã€‚æˆ‘ä»¬è·Ÿè¿›ä¹‹å‰åˆ†æ BNSL w.r.t. å¤æ‚æ€§çš„å·¥ä½œã€‚æ‰€è°“è¾“å…¥çš„ä¸Šå±‚å»ºç­‘ã€‚è™½ç„¶å·²çŸ¥ç»“æœè¡¨æ˜ BNSL ä¸å¤ªå¯èƒ½æ˜¯å›ºå®šå‚æ•°æ˜“å¤„ç†çš„ï¼Œå³ä½¿é€šè¿‡ä¸Šéƒ¨ç»“æ„ä¸­é¡¶ç‚¹è¦†ç›–çš„å¤§å°è¿›è¡Œå‚æ•°åŒ–ï¼Œä½†è¿™é‡Œæˆ‘ä»¬è¡¨æ˜ï¼Œå¦ä¸€ç§å‚æ•°åŒ–ï¼ˆç‰¹åˆ«æ˜¯é€šè¿‡åé¦ˆè¾¹é›†çš„å¤§å°ï¼‰äº§ç”Ÿå›ºå®šå‚æ•°æ˜“å¤„ç†æ€§ã€‚æˆ‘ä»¬ç»§ç»­è¡¨æ˜ï¼Œè¿™ä¸ªç»“æœå¯ä»¥å¢å¼ºä¸ºåé¦ˆè¾¹ç¼˜é›†çš„æœ¬åœ°åŒ–ç‰ˆæœ¬ï¼Œå¹¶æä¾›ç›¸åº”çš„ä¸‹ç•Œæ¥è¡¥å……ä¹‹å‰çš„ç»“æœï¼Œä»¥æä¾› BNSL w.r.t. çš„å¤æ‚æ€§åˆ†ç±»ã€‚å‡ ä¹æ‰€æœ‰ç»è¿‡å……åˆ†ç ”ç©¶çš„å›¾å½¢å‚æ•°ã€‚   ç„¶åæˆ‘ä»¬åˆ†æ BNSL çš„å¤æ‚æ€§å¦‚ä½•å–å†³äºè¾“å…¥çš„è¡¨ç¤ºã€‚ç‰¹åˆ«æ˜¯ï¼Œè™½ç„¶è¿‡å»å…³äºè¯¥ä¸»é¢˜çš„å¤§éƒ¨åˆ†ç†è®ºå·¥ä½œéƒ½å‡è®¾ä½¿ç”¨æ‰€è°“çš„éé›¶è¡¨ç¤ºï¼Œä½†åœ¨è¿™é‡Œæˆ‘ä»¬è¯æ˜ï¼Œå¦‚æœå¯ä»¥ä½¿ç”¨åŠ æ³•è¡¨ç¤ºï¼Œé‚£ä¹ˆå³ä½¿åœ¨å¯¹ä¸Šå±‚ç»“æ„çš„é™åˆ¶æ˜æ˜¾æ›´æ¸©å’Œçš„æƒ…å†µä¸‹ï¼ŒBNSL ä¹Ÿä¼šå˜å¾—æ˜“äºå¤„ç†çš„å›ºå®šå‚æ•°ï¼Œç‰¹åˆ«æ˜¯å½“ä»…é€šè¿‡æ ‘å®½è¿›è¡Œå‚æ•°åŒ–æ—¶ã€‚æœ€åä½†å¹¶éæœ€ä¸é‡è¦çš„ä¸€ç‚¹æ˜¯ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å°†æˆ‘ä»¬çš„ç»“æœæ‰©å±•åˆ°å¯†åˆ‡ç›¸å…³çš„ Polytree å­¦ä¹ é—®é¢˜ã€‚

</details>

---

## 178. Transforming Policy-Car Swerving for Mitigating Stop-and-Go Traffic Waves: A Practice-Oriented Jam-Absorption Driving Strategy

**ä¸­æ–‡æ ‡é¢˜**: è½¬å˜æ”¿ç­–æ±½è½¦è½¬å‘ä»¥ç¼“è§£èµ°èµ°åœåœçš„äº¤é€šæ³¢ï¼šä»¥å®è·µä¸ºå¯¼å‘çš„å¸æ”¶å µå¡é©¾é©¶ç­–ç•¥

**Date**: 2026-02-10 | **arXiv**: [2602.10234v1](http://arxiv.org/abs/2602.10234v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10234v1)

**Code**: https://github.com/gotrafficgo.

<details><summary><b>Abstract</b></summary>

Stop-and-go waves, as a major form of freeway traffic congestion, cause severe and long-lasting adverse effects, including reduced traffic efficiency, increased driving risks, and higher vehicle emissions. Amongst the highway traffic management strategies, jam-absorption driving (JAD), in which a dedicated vehicle performs "slow-in" and "fast-out" maneuvers before being captured by a stop-and-go wave, has been proposed as a potential method for preventing the propagation of such waves. However, most existing JAD strategies remain impractical mainly due to the lack of discussion regarding implementation vehicles and operational conditions. Inspired by real-world observations of police-car swerving behavior, this paper first introduces a Single-Vehicle Two-Detector Jam-Absorption Driving (SVDD-JAD) problem, and then proposes a practical JAD strategy that transforms such behavior into a maneuver capable of suppressing the propagation of an isolated stop-and-go wave. Five key parameters that significantly affect the proposed strategy, namely, JAD speed, inflow traffic speed, wave width, wave speed, and in-wave speed, are identified and systematically analyzed. Using a SUMO-based simulation as an illustrative example, we further demonstrate how these parameters can be measured in practice with two stationary roadside traffic detectors. The results show that the proposed JAD strategy successfully suppresses the propagation of a stop-and-go wave, without triggering a secondary wave. This paper is expected to take a significant step toward making JAD practical, advancing it from a theoretical concept to a feasible and implementable strategy. To promote reproducibility in the transportation domain, we have also open-sourced all the code on our GitHub repository https://github.com/gotrafficgo.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

èµ°èµ°åœåœçš„æ³¢æµªä½œä¸ºé«˜é€Ÿå…¬è·¯äº¤é€šæ‹¥å µçš„ä¸»è¦å½¢å¼ï¼Œä¼šé€ æˆä¸¥é‡è€ŒæŒä¹…çš„ä¸åˆ©å½±å“ï¼ŒåŒ…æ‹¬äº¤é€šæ•ˆç‡é™ä½ã€é©¾é©¶é£é™©å¢åŠ ã€è½¦è¾†æ’æ”¾å¢åŠ ç­‰ã€‚åœ¨é«˜é€Ÿå…¬è·¯äº¤é€šç®¡ç†ç­–ç•¥ä¸­ï¼Œæ‹¥å µå¸æ”¶é©¾é©¶ï¼ˆJADï¼‰å·²è¢«æè®®ä½œä¸ºé˜²æ­¢æ­¤ç±»æ³¢ä¼ æ’­çš„æ½œåœ¨æ–¹æ³•ï¼Œå…¶ä¸­ä¸“ç”¨è½¦è¾†åœ¨è¢«èµ°èµ°åœåœçš„æ³¢æ•è·ä¹‹å‰æ‰§è¡Œâ€œæ…¢è¿›â€å’Œâ€œå¿«å‡ºâ€æœºåŠ¨ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„è”åˆADç­–ç•¥ä»ç„¶ä¸åˆ‡å®é™…ï¼Œä¸»è¦æ˜¯ç”±äºç¼ºä¹å¯¹å®æ–½å·¥å…·å’Œæ“ä½œæ¡ä»¶çš„è®¨è®ºã€‚å—ç°å®ä¸–ç•Œä¸­è­¦è½¦è½¬å‘è¡Œä¸ºè§‚å¯Ÿçš„å¯å‘ï¼Œæœ¬æ–‡é¦–å…ˆä»‹ç»äº†å•è½¦åŒæ¢æµ‹å™¨å¹²æ‰°å¸æ”¶é©¾é©¶ï¼ˆSVDD-JADï¼‰é—®é¢˜ï¼Œç„¶åæå‡ºäº†ä¸€ç§å®ç”¨çš„ JAD ç­–ç•¥ï¼Œå°†è¿™ç§è¡Œä¸ºè½¬åŒ–ä¸ºèƒ½å¤ŸæŠ‘åˆ¶å­¤ç«‹èµ°èµ°åœåœæ³¢ä¼ æ’­çš„ç­–ç•¥ã€‚ç¡®å®šå¹¶ç³»ç»Ÿåˆ†æäº†å¯¹æ‰€æå‡ºçš„ç­–ç•¥æœ‰æ˜¾ç€å½±å“çš„äº”ä¸ªå…³é”®å‚æ•°ï¼Œå³ JAD é€Ÿåº¦ã€æµå…¥äº¤é€šé€Ÿåº¦ã€æ³¢å®½ã€æ³¢é€Ÿå’Œæ³¢å†…é€Ÿåº¦ã€‚ä½¿ç”¨åŸºäº SUMO çš„æ¨¡æ‹Ÿä½œä¸ºè¯´æ˜æ€§ç¤ºä¾‹ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨ä¸¤ä¸ªå›ºå®šè·¯è¾¹äº¤é€šæ£€æµ‹å™¨åœ¨å®è·µä¸­æµ‹é‡è¿™äº›å‚æ•°ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ JAD ç­–ç•¥æˆåŠŸåœ°æŠ‘åˆ¶äº†èµ°èµ°åœåœæ³¢çš„ä¼ æ’­ï¼Œè€Œæ²¡æœ‰è§¦å‘äºŒæ¬¡æ³¢ã€‚æœ¬æ–‡æœ‰æœ›åœ¨ JAD å®ç”¨åŒ–æ–¹é¢è¿ˆå‡ºé‡è¦ä¸€æ­¥ï¼Œå°†å…¶ä»ç†è®ºæ¦‚å¿µæå‡ä¸ºå¯è¡Œä¸”å¯å®æ–½çš„ç­–ç•¥ã€‚ä¸ºäº†ä¿ƒè¿›äº¤é€šé¢†åŸŸçš„å¯é‡å¤æ€§ï¼Œæˆ‘ä»¬è¿˜åœ¨ GitHub å­˜å‚¨åº“ https://github.com/gotrafficgo ä¸Šå¼€æºäº†æ‰€æœ‰ä»£ç ã€‚

</details>

---

## 179. ImprovEvolve: Ask AlphaEvolve to Improve the Input Solution and Then Improvise

**ä¸­æ–‡æ ‡é¢˜**: ImprovEvolveï¼šè¦æ±‚ AlphaEvolve æ”¹è¿›è¾“å…¥è§£å†³æ–¹æ¡ˆï¼Œç„¶åå³å…´åˆ›ä½œ

**Date**: 2026-02-10 | **arXiv**: [2602.10233v1](http://arxiv.org/abs/2602.10233v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10233v1)

<details><summary><b>Abstract</b></summary>

Recent advances in LLM-guided evolutionary computation, particularly AlphaEvolve, have demonstrated remarkable success in discovering novel mathematical constructions and solving challenging optimization problems. In this article, we present ImprovEvolve, a simple yet effective technique for enhancing LLM-based evolutionary approaches such as AlphaEvolve. Given an optimization problem, the standard approach is to evolve program code that, when executed, produces a solution close to the optimum. We propose an alternative program parameterization that maintains the ability to construct optimal solutions while reducing the cognitive load on the LLM. Specifically, we evolve a program (implementing, e.g., a Python class with a prescribed interface) that provides the following functionality: (1) propose a valid initial solution, (2) improve any given solution in terms of fitness, and (3) perturb a solution with a specified intensity. The optimum can then be approached by iteratively applying improve() and perturb() with a scheduled intensity. We evaluate ImprovEvolve on challenging problems from the AlphaEvolve paper: hexagon packing in a hexagon and the second autocorrelation inequality. For hexagon packing, the evolved program achieves new state-of-the-art results for 11, 12, 15, and 16 hexagons; a lightly human-edited variant further improves results for 14, 17, and 23 hexagons. For the second autocorrelation inequality, the human-edited program achieves a new state-of-the-art lower bound of 0.96258, improving upon AlphaEvolve's 0.96102.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

LLM å¼•å¯¼çš„è¿›åŒ–è®¡ç®—ï¼ˆç‰¹åˆ«æ˜¯ AlphaEvolveï¼‰çš„æœ€æ–°è¿›å±•åœ¨å‘ç°æ–°é¢–çš„æ•°å­¦ç»“æ„å’Œè§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„ä¼˜åŒ–é—®é¢˜æ–¹é¢å–å¾—äº†æ˜¾ç€çš„æˆåŠŸã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† ImprovEvolveï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æŠ€æœ¯ï¼Œç”¨äºå¢å¼ºåŸºäº LLM çš„è¿›åŒ–æ–¹æ³•ï¼ˆä¾‹å¦‚ AlphaEvolveï¼‰ã€‚ç»™å®šä¼˜åŒ–é—®é¢˜ï¼Œæ ‡å‡†æ–¹æ³•æ˜¯æ”¹è¿›ç¨‹åºä»£ç ï¼Œåœ¨æ‰§è¡Œæ—¶äº§ç”Ÿæ¥è¿‘æœ€ä¼˜çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ›¿ä»£ç¨‹åºå‚æ•°åŒ–ï¼Œå®ƒä¿æŒæ„å»ºæœ€ä½³è§£å†³æ–¹æ¡ˆçš„èƒ½åŠ›ï¼ŒåŒæ—¶å‡å°‘æ³•å­¦ç¡•å£«çš„è®¤çŸ¥è´Ÿæ‹…ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼€å‘ä¸€ä¸ªç¨‹åºï¼ˆä¾‹å¦‚ï¼Œå®ç°å…·æœ‰è§„å®šæ¥å£çš„ Python ç±»ï¼‰ï¼Œè¯¥ç¨‹åºæä¾›ä»¥ä¸‹åŠŸèƒ½ï¼šï¼ˆ1ï¼‰æå‡ºæœ‰æ•ˆçš„åˆå§‹è§£å†³æ–¹æ¡ˆï¼Œï¼ˆ2ï¼‰åœ¨é€‚åº”åº¦æ–¹é¢æ”¹è¿›ä»»ä½•ç»™å®šçš„è§£å†³æ–¹æ¡ˆï¼Œä»¥åŠï¼ˆ3ï¼‰ä»¥æŒ‡å®šçš„å¼ºåº¦æ‰°åŠ¨è§£å†³æ–¹æ¡ˆã€‚ç„¶åå¯ä»¥é€šè¿‡ä»¥é¢„å®šå¼ºåº¦è¿­ä»£åº”ç”¨ Improve() å’Œ perturb() æ¥æ¥è¿‘æœ€ä½³å€¼ã€‚æˆ‘ä»¬é’ˆå¯¹ AlphaEvolve è®ºæ–‡ä¸­çš„æŒ‘æˆ˜æ€§é—®é¢˜è¯„ä¼°äº† ImprovEvolveï¼šå…­è¾¹å½¢ä¸­çš„å…­è¾¹å½¢å †ç§¯å’Œç¬¬äºŒè‡ªç›¸å…³ä¸ç­‰å¼ã€‚å¯¹äºå…­è¾¹å½¢å †ç§¯ï¼Œæ”¹è¿›çš„ç¨‹åºå¯¹äº 11ã€12ã€15 å’Œ 16 å…­è¾¹å½¢å®ç°äº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼›ç»è¿‡è½»å¾®äººå·¥ç¼–è¾‘çš„å˜ä½“è¿›ä¸€æ­¥æ”¹å–„äº† 14ã€17 å’Œ 23 å…­è¾¹å½¢çš„ç»“æœã€‚å¯¹äºç¬¬äºŒä¸ªè‡ªç›¸å…³ä¸ç­‰å¼ï¼Œäººå·¥ç¼–è¾‘çš„ç¨‹åºå®ç°äº†æ–°çš„æœ€å…ˆè¿›ä¸‹é™ 0.96258ï¼Œæ¯” AlphaEvolve çš„ 0.96102 æœ‰æ‰€æ”¹è¿›ã€‚

</details>

---

## 180. Towards Autonomous Mathematics Research

**ä¸­æ–‡æ ‡é¢˜**: èµ°å‘è‡ªä¸»æ•°å­¦ç ”ç©¶

**Date**: 2026-02-10 | **arXiv**: [2602.10177v1](http://arxiv.org/abs/2602.10177v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10177v1)

<details><summary><b>Abstract</b></summary>

Recent advances in foundational models have yielded reasoning systems capable of achieving a gold-medal standard at the International Mathematical Olympiad. The transition from competition-level problem-solving to professional research, however, requires navigating vast literature and constructing long-horizon proofs. In this work, we introduce Aletheia, a math research agent that iteratively generates, verifies, and revises solutions end-to-end in natural language. Specifically, Aletheia is powered by an advanced version of Gemini Deep Think for challenging reasoning problems, a novel inference-time scaling law that extends beyond Olympiad-level problems, and intensive tool use to navigate the complexities of mathematical research. We demonstrate the capability of Aletheia from Olympiad problems to PhD-level exercises and most notably, through several distinct milestones in AI-assisted mathematics research: (a) a research paper (Feng26) generated by AI without any human intervention in calculating certain structure constants in arithmetic geometry called eigenweights; (b) a research paper (LeeSeo26) demonstrating human-AI collaboration in proving bounds on systems of interacting particles called independent sets; and (c) an extensive semi-autonomous evaluation (Feng et al., 2026a) of 700 open problems on Bloom's Erdos Conjectures database, including autonomous solutions to four open questions. In order to help the public better understand the developments pertaining to AI and mathematics, we suggest codifying standard levels quantifying autonomy and novelty of AI-assisted results. We conclude with reflections on human-AI collaboration in mathematics.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åŸºç¡€æ¨¡å‹çš„æœ€æ–°è¿›å±•å·²ç»äº§ç”Ÿäº†èƒ½å¤Ÿåœ¨å›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹ç«èµ›ä¸­è¾¾åˆ°é‡‘ç‰Œæ ‡å‡†çš„æ¨ç†ç³»ç»Ÿã€‚ç„¶è€Œï¼Œä»ç«èµ›çº§é—®é¢˜è§£å†³åˆ°ä¸“ä¸šç ”ç©¶çš„è½¬å˜éœ€è¦æŸ¥é˜…å¤§é‡æ–‡çŒ®å¹¶æ„å»ºé•¿æœŸè¯æ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† Aletheiaï¼Œè¿™æ˜¯ä¸€ç§æ•°å­¦ç ”ç©¶ä»£ç†ï¼Œå¯ä»¥ç”¨è‡ªç„¶è¯­è¨€ç«¯åˆ°ç«¯åœ°è¿­ä»£ç”Ÿæˆã€éªŒè¯å’Œä¿®æ”¹è§£å†³æ–¹æ¡ˆã€‚å…·ä½“æ¥è¯´ï¼ŒAletheia ç”±ç”¨äºæŒ‘æˆ˜æ€§æ¨ç†é—®é¢˜çš„ Gemini Deep Think é«˜çº§ç‰ˆæœ¬ã€è¶…è¶Šå¥¥æ—åŒ¹å…‹çº§åˆ«é—®é¢˜çš„æ–°é¢–æ¨ç†æ—¶é—´ç¼©æ”¾æ³•åˆ™ä»¥åŠç”¨äºé©¾é©­æ•°å­¦ç ”ç©¶å¤æ‚æ€§çš„å¯†é›†å·¥å…·ä½¿ç”¨æä¾›æ”¯æŒã€‚æˆ‘ä»¬å±•ç¤ºäº† Aletheia ä»å¥¥æ—åŒ¹å…‹é—®é¢˜åˆ°åšå£«çº§åˆ«ç»ƒä¹ çš„èƒ½åŠ›ï¼Œæœ€å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé€šè¿‡äººå·¥æ™ºèƒ½è¾…åŠ©æ•°å­¦ç ”ç©¶ä¸­çš„å‡ ä¸ªä¸åŒé‡Œç¨‹ç¢‘ï¼šï¼ˆaï¼‰ç”±äººå·¥æ™ºèƒ½ç”Ÿæˆçš„ç ”ç©¶è®ºæ–‡ï¼ˆFeng26ï¼‰ï¼Œåœ¨è®¡ç®—ç®—æœ¯å‡ ä½•ä¸­ç§°ä¸ºç‰¹å¾æƒé‡çš„æŸäº›ç»“æ„å¸¸æ•°æ—¶æ— éœ€ä»»ä½•äººä¸ºå¹²é¢„ï¼› (b) ä¸€ç¯‡ç ”ç©¶è®ºæ–‡ï¼ˆLeeSeo26ï¼‰å±•ç¤ºäº†äººç±»ä¸äººå·¥æ™ºèƒ½çš„åä½œï¼Œè¯æ˜äº†ç§°ä¸ºç‹¬ç«‹é›†çš„ç›¸äº’ä½œç”¨ç²’å­ç³»ç»Ÿçš„ç•Œé™ï¼› (c) å¯¹ Bloom çš„é„‚å°”å¤šæ–¯çŒœæƒ³æ•°æ®åº“ä¸­çš„ 700 ä¸ªå¼€æ”¾é—®é¢˜è¿›è¡Œå¹¿æ³›çš„åŠè‡ªä¸»è¯„ä¼°ï¼ˆFeng ç­‰äººï¼Œ2026aï¼‰ï¼ŒåŒ…æ‹¬å››ä¸ªå¼€æ”¾é—®é¢˜çš„è‡ªä¸»è§£å†³æ–¹æ¡ˆã€‚ä¸ºäº†å¸®åŠ©å…¬ä¼—æ›´å¥½åœ°äº†è§£äººå·¥æ™ºèƒ½å’Œæ•°å­¦çš„å‘å±•ï¼Œæˆ‘ä»¬å»ºè®®åˆ¶å®šæ ‡å‡†æ°´å¹³ï¼Œé‡åŒ–äººå·¥æ™ºèƒ½è¾…åŠ©ç»“æœçš„è‡ªä¸»æ€§å’Œæ–°é¢–æ€§ã€‚æœ€åæˆ‘ä»¬å¯¹äººç±»ä¸äººå·¥æ™ºèƒ½åœ¨æ•°å­¦é¢†åŸŸçš„åˆä½œè¿›è¡Œäº†åæ€ã€‚

</details>

---

## 181. Anagent For Enhancing Scientific Table & Figure Analysis

**ä¸­æ–‡æ ‡é¢˜**: å¢å¼ºç§‘å­¦è¡¨æ ¼å’Œå›¾å½¢åˆ†æçš„è¯•å‰‚

**Date**: 2026-02-10 | **arXiv**: [2602.10081v1](http://arxiv.org/abs/2602.10081v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10081v1)

<details><summary><b>Abstract</b></summary>

In scientific research, analysis requires accurately interpreting complex multimodal knowledge, integrating evidence from different sources, and drawing inferences grounded in domain-specific knowledge. However, current artificial intelligence (AI) systems struggle to consistently demonstrate such capabilities. The complexity and variability of scientific tables and figures, combined with heterogeneous structures and long-context requirements, pose fundamental obstacles to scientific table \& figure analysis. To quantify these challenges, we introduce AnaBench, a large-scale benchmark featuring $63,178$ instances from nine scientific domains, systematically categorized along seven complexity dimensions. To tackle these challenges, we propose Anagent, a multi-agent framework for enhanced scientific table \& figure analysis through four specialized agents: Planner decomposes tasks into actionable subtasks, Expert retrieves task-specific information through targeted tool execution, Solver synthesizes information to generate coherent analysis, and Critic performs iterative refinement through five-dimensional quality assessment. We further develop modular training strategies that leverage supervised finetuning and specialized reinforcement learning to optimize individual capabilities while maintaining effective collaboration. Comprehensive evaluation across 170 subdomains demonstrates that Anagent achieves substantial improvements, up to $\uparrow 13.43\%$ in training-free settings and $\uparrow 42.12\%$ with finetuning, while revealing that task-oriented reasoning and context-aware problem-solving are essential for high-quality scientific table \& figure analysis. Our project page: https://xhguo7.github.io/Anagent/.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åœ¨ç§‘å­¦ç ”ç©¶ä¸­ï¼Œåˆ†æéœ€è¦å‡†ç¡®è§£é‡Šå¤æ‚çš„å¤šæ¨¡æ€çŸ¥è¯†ï¼Œæ•´åˆä¸åŒæ¥æºçš„è¯æ®ï¼Œå¹¶æ ¹æ®ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†å¾—å‡ºæ¨è®ºã€‚ç„¶è€Œï¼Œå½“å‰çš„äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ç³»ç»Ÿå¾ˆéš¾å§‹ç»ˆå¦‚ä¸€åœ°å±•ç¤ºè¿™ç§èƒ½åŠ›ã€‚ç§‘å­¦è¡¨æ ¼å’Œå›¾å½¢çš„å¤æ‚æ€§å’Œå¯å˜æ€§ï¼ŒåŠ ä¸Šå¼‚æ„ç»“æ„å’Œé•¿ä¸Šä¸‹æ–‡è¦æ±‚ï¼Œå¯¹ç§‘å­¦è¡¨æ ¼å’Œå›¾å½¢åˆ†ææ„æˆäº†æ ¹æœ¬éšœç¢ã€‚ä¸ºäº†é‡åŒ–è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº† AnaBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§å‹åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«æ¥è‡ªä¹ä¸ªç§‘å­¦é¢†åŸŸçš„ä»·å€¼ 63,178 ç¾å…ƒçš„å®ä¾‹ï¼Œå¹¶æŒ‰ç…§ä¸ƒä¸ªå¤æ‚æ€§ç»´åº¦è¿›è¡Œç³»ç»Ÿåˆ†ç±»ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº† Anagentï¼Œä¸€ä¸ªé€šè¿‡å››ä¸ªä¸“é—¨ä»£ç†æ¥å¢å¼ºç§‘å­¦è¡¨æ ¼å’Œå›¾å½¢åˆ†æçš„å¤šä»£ç†æ¡†æ¶ï¼šPlanner å°†ä»»åŠ¡åˆ†è§£ä¸ºå¯æ“ä½œçš„å­ä»»åŠ¡ï¼ŒExpert é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„å·¥å…·æ‰§è¡Œæ£€ç´¢ç‰¹å®šäºä»»åŠ¡çš„ä¿¡æ¯ï¼ŒSolver ç»¼åˆä¿¡æ¯ä»¥ç”Ÿæˆè¿è´¯çš„åˆ†æï¼ŒCritic é€šè¿‡äº”ç»´è´¨é‡è¯„ä¼°è¿›è¡Œè¿­ä»£ç»†åŒ–ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘æ¨¡å—åŒ–åŸ¹è®­ç­–ç•¥ï¼Œåˆ©ç”¨ç›‘ç£å¾®è°ƒå’Œä¸“ä¸šå¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–ä¸ªäººèƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒæœ‰æ•ˆçš„åä½œã€‚è·¨ 170 ä¸ªå­åŸŸçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ Anagent å–å¾—äº†æ˜¾ç€çš„æ”¹è¿›ï¼Œåœ¨å…è®­ç»ƒè®¾ç½®ä¸­é«˜è¾¾ $\uparrow 13.43\%$ï¼Œé€šè¿‡å¾®è°ƒé«˜è¾¾ $\uparrow 42.12\%$ï¼ŒåŒæ—¶æ­ç¤ºäº†é¢å‘ä»»åŠ¡çš„æ¨ç†å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„é—®é¢˜è§£å†³å¯¹äºé«˜è´¨é‡çš„ç§‘å­¦è¡¨æ ¼å’Œå›¾å½¢åˆ†æè‡³å…³é‡è¦ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼šhttps://xhguo7.github.io/Anagent/ã€‚

</details>

---

## 182. RoboSubtaskNet: Temporal Sub-task Segmentation for Human-to-Robot Skill Transfer in Real-World Environments

**ä¸­æ–‡æ ‡é¢˜**: RoboSubtaskNetï¼šç°å®ç¯å¢ƒä¸­äººæœºæŠ€èƒ½è½¬ç§»çš„æ—¶é—´å­ä»»åŠ¡åˆ†å‰²

**Date**: 2026-02-10 | **arXiv**: [2602.10015v2](http://arxiv.org/abs/2602.10015v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.10015v2)

<details><summary><b>Abstract</b></summary>

Temporally locating and classifying fine-grained sub-task segments in long, untrimmed videos is crucial to safe human-robot collaboration. Unlike generic activity recognition, collaborative manipulation requires sub-task labels that are directly robot-executable. We present RoboSubtaskNet, a multi-stage human-to-robot sub-task segmentation framework that couples attention-enhanced I3D features (RGB plus optical flow) with a modified MS-TCN employing a Fibonacci dilation schedule to capture better short-horizon transitions such as reach-pick-place. The network is trained with a composite objective comprising cross-entropy and temporal regularizers (truncated MSE and a transition-aware term) to reduce over-segmentation and to encourage valid sub-task progressions. To close the gap between vision benchmarks and control, we introduce RoboSubtask, a dataset of healthcare and industrial demonstrations annotated at the sub-task level and designed for deterministic mapping to manipulator primitives. Empirically, RoboSubtaskNet outperforms MS-TCN and MS-TCN++ on GTEA and our RoboSubtask benchmark (boundary-sensitive and sequence metrics), while remaining competitive on the long-horizon Breakfast benchmark. Specifically, RoboSubtaskNet attains F1 @ 50 = 79.5%, Edit = 88.6%, Acc = 78.9% on GTEA; F1 @ 50 = 30.4%, Edit = 52.0%, Acc = 53.5% on Breakfast; and F1 @ 50 = 94.2%, Edit = 95.6%, Acc = 92.2% on RoboSubtask. We further validate the full perception-to-execution pipeline on a 7-DoF Kinova Gen3 manipulator, achieving reliable end-to-end behavior in physical trials (overall task success approx 91.25%). These results demonstrate a practical path from sub-task level video understanding to deployed robotic manipulation in real-world settings.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åœ¨æœªç»ä¿®å‰ªçš„é•¿è§†é¢‘ä¸­ä¸´æ—¶å®šä½å’Œåˆ†ç±»ç»†ç²’åº¦çš„å­ä»»åŠ¡ç‰‡æ®µå¯¹äºå®‰å…¨çš„äººæœºåä½œè‡³å…³é‡è¦ã€‚ä¸é€šç”¨æ´»åŠ¨è¯†åˆ«ä¸åŒï¼Œåä½œæ“ä½œéœ€è¦æœºå™¨äººå¯ä»¥ç›´æ¥æ‰§è¡Œçš„å­ä»»åŠ¡æ ‡ç­¾ã€‚æˆ‘ä»¬æå‡ºäº† RoboSubtaskNetï¼Œä¸€ä¸ªå¤šé˜¶æ®µçš„äººæœºå­ä»»åŠ¡åˆ†å‰²æ¡†æ¶ï¼Œå®ƒå°†æ³¨æ„åŠ›å¢å¼ºçš„ I3D ç‰¹å¾ï¼ˆRGB åŠ å…‰æµï¼‰ä¸æ”¹è¿›çš„ MS-TCN ç»“åˆèµ·æ¥ï¼Œé‡‡ç”¨æ–æ³¢é‚£å¥‘æ‰©å¼ è®¡åˆ’æ¥æ•è·æ›´å¥½çš„çŸ­è§†é‡è¿‡æ¸¡ï¼Œä¾‹å¦‚åˆ°è¾¾-æ‹¾å–-æ”¾ç½®ã€‚è¯¥ç½‘ç»œé‡‡ç”¨ç”±äº¤å‰ç†µå’Œæ—¶é—´æ­£åˆ™åŒ–å™¨ï¼ˆæˆªæ–­çš„ MSE å’Œè½¬æ¢æ„ŸçŸ¥é¡¹ï¼‰ç»„æˆçš„å¤åˆç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œä»¥å‡å°‘è¿‡åº¦åˆ†å‰²å¹¶é¼“åŠ±æœ‰æ•ˆçš„å­ä»»åŠ¡è¿›å±•ã€‚ä¸ºäº†ç¼©å°è§†è§‰åŸºå‡†å’Œæ§åˆ¶ä¹‹é—´çš„å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº† RoboSubtaskï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨å­ä»»åŠ¡çº§åˆ«æ³¨é‡Šçš„åŒ»ç–—ä¿å¥å’Œå·¥ä¸šæ¼”ç¤ºæ•°æ®é›†ï¼Œæ—¨åœ¨ç”¨äºç¡®å®šæ€§æ˜ å°„åˆ°æ“çºµå™¨åŸºå…ƒã€‚æ ¹æ®ç»éªŒï¼ŒRoboSubtaskNet åœ¨ GTEA å’Œæˆ‘ä»¬çš„ RoboSubtask åŸºå‡†ï¼ˆè¾¹ç•Œæ•æ„Ÿå’Œåºåˆ—æŒ‡æ ‡ï¼‰ä¸Šä¼˜äº MS-TCN å’Œ MS-TCN++ï¼ŒåŒæ—¶åœ¨é•¿æœŸæ—©é¤åŸºå‡†ä¸Šä¿æŒç«äº‰åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒRoboSubtaskNet åœ¨ GTEA ä¸Šè¾¾åˆ° F1 @ 50 = 79.5%ï¼ŒEdit = 88.6%ï¼ŒAcc = 78.9%ï¼›æ—©é¤æ—¶ F1 @ 50 = 30.4%ï¼Œç¼–è¾‘ = 52.0%ï¼ŒAcc = 53.5%ï¼› RoboSubtask ä¸Šçš„ F1 @ 50 = 94.2%ï¼Œç¼–è¾‘ = 95.6%ï¼ŒAcc = 92.2%ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥éªŒè¯äº† 7-DoF Kinova Gen3 æœºæ¢°è‡‚ä¸Šçš„å®Œæ•´æ„ŸçŸ¥åˆ°æ‰§è¡Œæµç¨‹ï¼Œåœ¨ç‰©ç†è¯•éªŒä¸­å®ç°äº†å¯é çš„ç«¯åˆ°ç«¯è¡Œä¸ºï¼ˆæ€»ä½“ä»»åŠ¡æˆåŠŸç‡çº¦ä¸º 91.25%ï¼‰ã€‚è¿™äº›ç»“æœå±•ç¤ºäº†ä»å­ä»»åŠ¡çº§è§†é¢‘ç†è§£åˆ°åœ¨ç°å®ç¯å¢ƒä¸­éƒ¨ç½²æœºå™¨äººæ“ä½œçš„å®ç”¨è·¯å¾„ã€‚

</details>

---

## 183. Discovering High Level Patterns from Simulation Traces

**ä¸­æ–‡æ ‡é¢˜**: ä»ä»¿çœŸè·Ÿè¸ªä¸­å‘ç°é«˜çº§æ¨¡å¼

**Date**: 2026-02-10 | **arXiv**: [2602.10009v1](http://arxiv.org/abs/2602.10009v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10009v1)

<details><summary><b>Abstract</b></summary>

Artificial intelligence (AI) agents embedded in environments with physics-based interaction face many challenges including reasoning, planning, summarization, and question answering. This problem is exacerbated when a human user wishes to either guide or interact with the agent in natural language. Although the use of Language Models (LMs) is the default choice, as an AI tool, they struggle with tasks involving physics. The LM's capability for physical reasoning is learned from observational data, rather than being grounded in simulation. A common approach is to include simulation traces as context, but this suffers from poor scalability as simulation traces contain larger volumes of fine-grained numerical and semantic data. In this paper, we propose a natural language guided method to discover coarse-grained patterns (e.g., 'rigid-body collision', 'stable support', etc.) from detailed simulation logs. Specifically, we synthesize programs that operate on simulation logs and map them to a series of high level activated patterns. We show, through two physics benchmarks, that this annotated representation of the simulation log is more amenable to natural language reasoning about physical systems. We demonstrate how this method enables LMs to generate effective reward programs from goals specified in natural language, which may be used within the context of planning or supervised learning.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åµŒå…¥åŸºäºç‰©ç†äº¤äº’çš„ç¯å¢ƒä¸­çš„äººå·¥æ™ºèƒ½ (AI) ä»£ç†é¢ä¸´ç€è®¸å¤šæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ¨ç†ã€è§„åˆ’ã€æ€»ç»“å’Œé—®ç­”ã€‚å½“äººç±»ç”¨æˆ·å¸Œæœ›ç”¨è‡ªç„¶è¯­è¨€æŒ‡å¯¼ä»£ç†æˆ–ä¸ä»£ç†äº¤äº’æ—¶ï¼Œè¿™ä¸ªé—®é¢˜ä¼šæ›´åŠ ä¸¥é‡ã€‚å°½ç®¡è¯­è¨€æ¨¡å‹ (LM) çš„ä½¿ç”¨æ˜¯é»˜è®¤é€‰æ‹©ï¼Œä½†ä½œä¸ºäººå·¥æ™ºèƒ½å·¥å…·ï¼Œå®ƒä»¬åœ¨æ¶‰åŠç‰©ç†çš„ä»»åŠ¡ä¸Šé‡åˆ°äº†å›°éš¾ã€‚ LM çš„ç‰©ç†æ¨ç†èƒ½åŠ›æ˜¯ä»è§‚æµ‹æ•°æ®ä¸­å­¦ä¹ çš„ï¼Œè€Œä¸æ˜¯åŸºäºæ¨¡æ‹Ÿã€‚ä¸€ç§å¸¸è§çš„æ–¹æ³•æ˜¯å°†æ¨¡æ‹Ÿè·Ÿè¸ªä½œä¸ºä¸Šä¸‹æ–‡åŒ…å«åœ¨å†…ï¼Œä½†è¿™ä¼šå¯¼è‡´å¯æ‰©å±•æ€§è¾ƒå·®ï¼Œå› ä¸ºæ¨¡æ‹Ÿè·Ÿè¸ªåŒ…å«å¤§é‡ç»†ç²’åº¦çš„æ•°å€¼å’Œè¯­ä¹‰æ•°æ®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªç„¶è¯­è¨€å¼•å¯¼çš„æ–¹æ³•ï¼Œä»è¯¦ç»†çš„æ¨¡æ‹Ÿæ—¥å¿—ä¸­å‘ç°ç²—ç²’åº¦æ¨¡å¼ï¼ˆä¾‹å¦‚â€œåˆšä½“ç¢°æ’â€ã€â€œç¨³å®šæ”¯æ’‘â€ç­‰ï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç»¼åˆäº†å¯¹æ¨¡æ‹Ÿæ—¥å¿—è¿›è¡Œæ“ä½œçš„ç¨‹åºï¼Œå¹¶å°†å®ƒä»¬æ˜ å°„åˆ°ä¸€ç³»åˆ—é«˜çº§æ¿€æ´»æ¨¡å¼ã€‚æˆ‘ä»¬é€šè¿‡ä¸¤ä¸ªç‰©ç†åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œè¿™ç§å¸¦æ³¨é‡Šçš„æ¨¡æ‹Ÿæ—¥å¿—è¡¨ç¤ºæ›´é€‚åˆå…³äºç‰©ç†ç³»ç»Ÿçš„è‡ªç„¶è¯­è¨€æ¨ç†ã€‚æˆ‘ä»¬æ¼”ç¤ºäº†è¿™ç§æ–¹æ³•å¦‚ä½•ä½¿ LM æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡å®šçš„ç›®æ ‡ç”Ÿæˆæœ‰æ•ˆçš„å¥–åŠ±è®¡åˆ’ï¼Œè¿™äº›è®¡åˆ’å¯ä»¥åœ¨è§„åˆ’æˆ–ç›‘ç£å­¦ä¹ çš„èƒŒæ™¯ä¸‹ä½¿ç”¨ã€‚

</details>

---

## 184. ESTAR: Early-Stopping Token-Aware Reasoning For Efficient Inference

**ä¸­æ–‡æ ‡é¢˜**: ESTARï¼šæå‰åœæ­¢ä»¤ç‰Œæ„ŸçŸ¥æ¨ç†ä»¥å®ç°é«˜æ•ˆæ¨ç†

**Date**: 2026-02-10 | **arXiv**: [2602.10004v1](http://arxiv.org/abs/2602.10004v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10004v1)

<details><summary><b>Abstract</b></summary>

Large reasoning models (LRMs) achieve state-of-the-art performance by generating long chains-of-thought, but often waste computation on redundant reasoning after the correct answer has already been reached. We introduce Early-Stopping for Token-Aware Reasoning (ESTAR), which detects and reduces such reasoning redundancy to improve efficiency without sacrificing accuracy. Our method combines (i) a trajectory-based classifier that identifies when reasoning can be safely stopped, (ii) supervised fine-tuning to teach LRMs to propose self-generated <stop> signals, and (iii) <stop>-aware reinforcement learning that truncates rollouts at self-generated stop points with compute-aware rewards. Experiments on four reasoning datasets show that ESTAR reduces reasoning length by about 3.7x (from 4,799 to 1,290) while preserving accuracy (74.9% vs. 74.2%), with strong cross-domain generalization. These results highlight early stopping as a simple yet powerful mechanism for improving reasoning efficiency in LRMs.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰é€šè¿‡ç”Ÿæˆé•¿çš„æ€ç»´é“¾æ¥å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½†åœ¨å¾—å‡ºæ­£ç¡®ç­”æ¡ˆåå¾€å¾€ä¼šåœ¨å†—ä½™æ¨ç†ä¸Šæµªè´¹è®¡ç®—ã€‚æˆ‘ä»¬å¼•å…¥äº†æ—©æœŸåœæ­¢ä»¤ç‰Œæ„ŸçŸ¥æ¨ç†ï¼ˆESTARï¼‰ï¼Œå®ƒå¯ä»¥æ£€æµ‹å¹¶å‡å°‘è¿™ç§æ¨ç†å†—ä½™ï¼Œä»¥åœ¨ä¸ç‰ºç‰²å‡†ç¡®æ€§çš„æƒ…å†µä¸‹æé«˜æ•ˆç‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ï¼ˆiï¼‰åŸºäºè½¨è¿¹çš„åˆ†ç±»å™¨ï¼Œç”¨äºè¯†åˆ«ä½•æ—¶å¯ä»¥å®‰å…¨åœæ­¢æ¨ç†ï¼Œï¼ˆiiï¼‰ç›‘ç£å¾®è°ƒï¼Œä»¥æ•™å¯¼ LRM æå‡ºè‡ªç”Ÿæˆçš„ <stop> ä¿¡å·ï¼Œä»¥åŠï¼ˆiiiï¼‰ <stop> æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡è®¡ç®—æ„ŸçŸ¥å¥–åŠ±åœ¨è‡ªç”Ÿæˆçš„åœæ­¢ç‚¹å¤„æˆªæ–­æ¨å‡ºã€‚åœ¨å››ä¸ªæ¨ç†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒESTAR å°†æ¨ç†é•¿åº¦å‡å°‘äº†çº¦ 3.7 å€ï¼ˆä» 4,799 åˆ° 1,290ï¼‰ï¼ŒåŒæ—¶ä¿æŒäº†å‡†ç¡®æ€§ï¼ˆ74.9% vs. 74.2%ï¼‰ï¼Œå…·æœ‰å¾ˆå¼ºçš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›ç»“æœå¼ºè°ƒæ—©æœŸåœæ­¢æ˜¯ä¸€ç§ç®€å•è€Œå¼ºå¤§çš„æœºåˆ¶ï¼Œå¯ä»¥æé«˜ LRM çš„æ¨ç†æ•ˆç‡ã€‚

</details>

---

## 185. A Unified Assessment of the Poverty of the Stimulus Argument for Neural Language Models

**ä¸­æ–‡æ ‡é¢˜**: ç¥ç»è¯­è¨€æ¨¡å‹åˆºæ¿€è®ºè¯è´«å›°çš„ç»Ÿä¸€è¯„ä¼°

**Date**: 2026-02-10 | **arXiv**: [2602.09992v1](http://arxiv.org/abs/2602.09992v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09992v1)

<details><summary><b>Abstract</b></summary>

How can children acquire native-level syntax from limited input? According to the Poverty of the Stimulus Hypothesis (PoSH), the linguistic input children receive is insufficient to explain certain generalizations that are robustly learned; innate linguistic constraints, many have argued, are thus necessary to explain language learning. Neural language models, which lack such language-specific constraints in their design, offer a computational test of this longstanding (but controversial) claim. We introduce \poshbench, a training-and-evaluation suite targeting question formation, islands to movement, and other English phenomena at the center of the PoSH arguments. Training Transformer models on 10--50M words of developmentally plausible text, we find indications of generalization on all phenomena even without direct positive evidence -- yet neural models remain less data-efficient and their generalizations are weaker than those of children. We further enhance our models with three recently proposed cognitively motivated inductive biases. We find these biases improve general syntactic competence but not \poshbench performance. Our findings challenge the claim that innate syntax is the only possible route to generalization, while suggesting that human-like data efficiency requires inductive biases beyond those tested here.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å­©å­å¦‚ä½•ä»æœ‰é™çš„è¾“å…¥ä¸­è·å¾—æ¯è¯­æ°´å¹³çš„è¯­æ³•ï¼Ÿæ ¹æ®åˆºæ¿€è´«å›°å‡è¯´ï¼ˆPoSHï¼‰ï¼Œå„¿ç«¥æ¥å—çš„è¯­è¨€è¾“å…¥ä¸è¶³ä»¥è§£é‡ŠæŸäº›ç»è¿‡ç¨³å¥å­¦ä¹ çš„æ¦‚æ‹¬ï¼›è®¸å¤šäººè®¤ä¸ºï¼Œå…ˆå¤©çš„è¯­è¨€é™åˆ¶å¯¹äºè§£é‡Šè¯­è¨€å­¦ä¹ æ˜¯å¿…è¦çš„ã€‚ç¥ç»è¯­è¨€æ¨¡å‹åœ¨è®¾è®¡ä¸­ç¼ºä¹è¿™ç§ç‰¹å®šäºè¯­è¨€çš„çº¦æŸï¼Œä¸ºè¿™ä¸€é•¿æœŸå­˜åœ¨ï¼ˆä½†æœ‰äº‰è®®ï¼‰çš„ä¸»å¼ æä¾›äº†è®¡ç®—æµ‹è¯•ã€‚æˆ‘ä»¬å¼•å…¥äº† \poshbenchï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é—®é¢˜å½¢æˆã€å²›å±¿è¿åŠ¨ä»¥åŠ PoSH äº‰è®ºä¸­å¿ƒçš„å…¶ä»–è‹±è¯­ç°è±¡çš„è®­ç»ƒå’Œè¯„ä¼°å¥—ä»¶ã€‚ç”¨ 10--50M å­—çš„å‘å±•åˆç†æ–‡æœ¬è®­ç»ƒ Transformer æ¨¡å‹ï¼Œå³ä½¿æ²¡æœ‰ç›´æ¥çš„ç§¯æè¯æ®ï¼Œæˆ‘ä»¬ä¹Ÿå‘ç°äº†æ‰€æœ‰ç°è±¡çš„æ³›åŒ–è¿¹è±¡â€”â€”ä½†ç¥ç»æ¨¡å‹çš„æ•°æ®æ•ˆç‡ä»ç„¶è¾ƒä½ï¼Œè€Œä¸”å®ƒä»¬çš„æ³›åŒ–èƒ½åŠ›æ¯”å„¿ç«¥çš„è¦å¼±ã€‚æˆ‘ä»¬é€šè¿‡æœ€è¿‘æå‡ºçš„ä¸‰ç§è®¤çŸ¥åŠ¨æœºå½’çº³åå·®è¿›ä¸€æ­¥å¢å¼ºäº†æˆ‘ä»¬çš„æ¨¡å‹ã€‚æˆ‘ä»¬å‘ç°è¿™äº›åå·®æé«˜äº†ä¸€èˆ¬å¥æ³•èƒ½åŠ›ï¼Œä½†æ²¡æœ‰æé«˜ \poshbench æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæŒ‘æˆ˜äº†å…ˆå¤©è¯­æ³•æ˜¯æ³›åŒ–å”¯ä¸€å¯èƒ½é€”å¾„çš„è¯´æ³•ï¼ŒåŒæ—¶è¡¨æ˜ç±»äººæ•°æ®æ•ˆç‡éœ€è¦è¶…å‡ºæ­¤å¤„æµ‹è¯•çš„å½’çº³åå·®ã€‚

</details>

---

## 186. Empirical Stability Analysis of Kolmogorov-Arnold Networks in Hard-Constrained Recurrent Physics-Informed Discovery

**ä¸­æ–‡æ ‡é¢˜**: ç¡¬çº¦æŸå¾ªç¯ç‰©ç†å‘ç°ä¸­æŸ¯å°”è«å“¥æ´›å¤«-é˜¿è¯ºå¾·ç½‘ç»œçš„ç»éªŒç¨³å®šæ€§åˆ†æ

**Date**: 2026-02-10 | **arXiv**: [2602.09988v1](http://arxiv.org/abs/2602.09988v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09988v1)

<details><summary><b>Abstract</b></summary>

We investigate the integration of Kolmogorov-Arnold Networks (KANs) into hard-constrained recurrent physics-informed architectures (HRPINN) to evaluate the fidelity of learned residual manifolds in oscillatory systems. Motivated by the Kolmogorov-Arnold representation theorem and preliminary gray-box results, we hypothesized that KANs would enable efficient recovery of unknown terms compared to MLPs. Through initial sensitivity analysis on configuration sensitivity, parameter scale, and training paradigm, we found that while small KANs are competitive on univariate polynomial residuals (Duffing), they exhibit severe hyperparameter fragility, instability in deeper configurations, and consistent failure on multiplicative terms (Van der Pol), generally outperformed by standard MLPs. These empirical challenges highlight limitations of the additive inductive bias in the original KAN formulation for state coupling and provide preliminary empirical evidence of inductive bias limitations for future hybrid modeling.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æˆ‘ä»¬ç ”ç©¶äº†å°†æŸ¯å°”è«å“¥æ´›å¤«-é˜¿è¯ºå¾·ç½‘ç»œ (KAN) é›†æˆåˆ°ç¡¬çº¦æŸå¾ªç¯ç‰©ç†ä¿¡æ¯æ¶æ„ (HRPINN) ä¸­ï¼Œä»¥è¯„ä¼°æŒ¯è¡ç³»ç»Ÿä¸­å­¦ä¹ çš„æ®‹å·®æµå½¢çš„ä¿çœŸåº¦ã€‚å— Kolmogorov-Arnold è¡¨ç¤ºå®šç†å’Œåˆæ­¥ç°ç›’ç»“æœçš„å¯å‘ï¼Œæˆ‘ä»¬å‡è®¾ä¸ MLP ç›¸æ¯”ï¼ŒKAN èƒ½å¤Ÿæœ‰æ•ˆæ¢å¤æœªçŸ¥é¡¹ã€‚é€šè¿‡å¯¹é…ç½®æ•æ„Ÿæ€§ã€å‚æ•°è§„æ¨¡å’Œè®­ç»ƒèŒƒå¼çš„åˆå§‹æ•æ„Ÿæ€§åˆ†æï¼Œæˆ‘ä»¬å‘ç°ï¼Œè™½ç„¶å°å‹ KAN åœ¨å•å˜é‡å¤šé¡¹å¼æ®‹å·®ï¼ˆDuffingï¼‰æ–¹é¢å…·æœ‰ç«äº‰åŠ›ï¼Œä½†å®ƒä»¬è¡¨ç°å‡ºä¸¥é‡çš„è¶…å‚æ•°è„†å¼±æ€§ã€æ›´æ·±å±‚æ¬¡é…ç½®çš„ä¸ç¨³å®šæ€§ä»¥åŠä¹˜æ³•é¡¹ä¸Šçš„ä¸€è‡´å¤±è´¥ï¼ˆVan der Polï¼‰ï¼Œé€šå¸¸ä¼˜äºæ ‡å‡† MLPã€‚è¿™äº›ç»éªŒæŒ‘æˆ˜å‡¸æ˜¾äº†åŸå§‹ KAN çŠ¶æ€è€¦åˆå…¬å¼ä¸­çš„åŠ æ€§å½’çº³åå·®çš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥æ··åˆå»ºæ¨¡çš„å½’çº³åå·®å±€é™æ€§æä¾›äº†åˆæ­¥çš„ç»éªŒè¯æ®ã€‚

</details>

---

## 187. Infusion: Shaping Model Behavior by Editing Training Data via Influence Functions

**ä¸­æ–‡æ ‡é¢˜**: Infusionï¼šé€šè¿‡å½±å“å‡½æ•°ç¼–è¾‘è®­ç»ƒæ•°æ®æ¥å¡‘é€ æ¨¡å‹è¡Œä¸º

**Date**: 2026-02-10 | **arXiv**: [2602.09987v2](http://arxiv.org/abs/2602.09987v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.09987v2)

**Code**: https://github.com/jrosseruk/infusion.

<details><summary><b>Abstract</b></summary>

Influence functions are commonly used to attribute model behavior to training documents. We explore the reverse: crafting training data that induces model behavior. Our framework, Infusion, uses scalable influence-function approximations to compute small perturbations to training documents that induce targeted changes in model behavior through parameter shifts. We evaluate Infusion on data poisoning tasks across vision and language domains. On CIFAR-10, we show that making subtle edits via Infusion to just 0.2% (100/45,000) of the training documents can be competitive with the baseline of inserting a small number of explicit behavior examples. We also find that Infusion transfers across architectures (ResNet $\leftrightarrow$ CNN), suggesting a single poisoned corpus can affect multiple independently trained models. In preliminary language experiments, we characterize when our approach increases the probability of target behaviors and when it fails, finding it most effective at amplifying behaviors the model has already learned. Taken together, these results show that small, subtle edits to training data can systematically shape model behavior, underscoring the importance of training data interpretability for adversaries and defenders alike. We provide the code here: https://github.com/jrosseruk/infusion.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å½±å“å‡½æ•°é€šå¸¸ç”¨äºå°†æ¨¡å‹è¡Œä¸ºå½’å› äºè®­ç»ƒæ–‡æ¡£ã€‚æˆ‘ä»¬æ¢ç´¢ç›¸åçš„æ–¹å‘ï¼šåˆ¶ä½œè¯±å¯¼æ¨¡å‹è¡Œä¸ºçš„è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬çš„æ¡†æ¶ Infusion ä½¿ç”¨å¯æ‰©å±•çš„å½±å“å‡½æ•°è¿‘ä¼¼æ¥è®¡ç®—å¯¹è®­ç»ƒæ–‡æ¡£çš„å°æ‰°åŠ¨ï¼Œè¿™äº›æ‰°åŠ¨é€šè¿‡å‚æ•°å˜åŒ–å¼•èµ·æ¨¡å‹è¡Œä¸ºçš„æœ‰é’ˆå¯¹æ€§çš„å˜åŒ–ã€‚æˆ‘ä»¬è¯„ä¼° Infusion å¯¹è·¨è§†è§‰å’Œè¯­è¨€é¢†åŸŸçš„æ•°æ®ä¸­æ¯’ä»»åŠ¡çš„å½±å“ã€‚åœ¨ CIFAR-10 ä¸Šï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œé€šè¿‡ Infusion å¯¹ 0.2% (100/45,000) çš„è®­ç»ƒæ–‡æ¡£è¿›è¡Œç»†å¾®ç¼–è¾‘å¯ä»¥ä¸æ’å…¥å°‘é‡æ˜¾å¼è¡Œä¸ºç¤ºä¾‹çš„åŸºçº¿ç›¸åª²ç¾ã€‚æˆ‘ä»¬è¿˜å‘ç° Infusion è·¨æ¶æ„ä¼ è¾“ï¼ˆResNet $\leftrightarrow$ CNNï¼‰ï¼Œè¿™è¡¨æ˜å•ä¸ªä¸­æ¯’è¯­æ–™åº“å¯ä»¥å½±å“å¤šä¸ªç‹¬ç«‹è®­ç»ƒçš„æ¨¡å‹ã€‚åœ¨åˆæ­¥çš„è¯­è¨€å®éªŒä¸­ï¼Œæˆ‘ä»¬æè¿°äº†æˆ‘ä»¬çš„æ–¹æ³•ä½•æ—¶å¢åŠ ç›®æ ‡è¡Œä¸ºçš„æ¦‚ç‡ä»¥åŠä½•æ—¶å¤±è´¥ï¼Œå‘ç°å®ƒåœ¨æ”¾å¤§æ¨¡å‹å·²ç»å­¦åˆ°çš„è¡Œä¸ºæ–¹é¢æœ€æœ‰æ•ˆã€‚æ€»è€Œè¨€ä¹‹ï¼Œè¿™äº›ç»“æœè¡¨æ˜ï¼Œå¯¹è®­ç»ƒæ•°æ®è¿›è¡Œå°çš„ã€å¾®å¦™çš„ç¼–è¾‘å¯ä»¥ç³»ç»Ÿåœ°å¡‘é€ æ¨¡å‹è¡Œä¸ºï¼Œå¼ºè°ƒäº†è®­ç»ƒæ•°æ®å¯è§£é‡Šæ€§å¯¹äºå¯¹æ‰‹å’Œé˜²å¾¡è€…çš„é‡è¦æ€§ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œæä¾›ä»£ç ï¼šhttps://github.com/jrosseruk/infusionã€‚

</details>

---

## 188. Supervised Metric Regularization Through Alternating Optimization for Multi-Regime Physics-Informed Neural Networks

**ä¸­æ–‡æ ‡é¢˜**: é€šè¿‡å¤šæœºåˆ¶ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œçš„äº¤æ›¿ä¼˜åŒ–æ¥ç›‘ç£åº¦é‡æ­£åˆ™åŒ–

**Date**: 2026-02-10 | **arXiv**: [2602.09980v1](http://arxiv.org/abs/2602.09980v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09980v1)

<details><summary><b>Abstract</b></summary>

Standard Physics-Informed Neural Networks (PINNs) often face challenges when modeling parameterized dynamical systems with sharp regime transitions, such as bifurcations. In these scenarios, the continuous mapping from parameters to solutions can result in spectral bias or "mode collapse", where the network averages distinct physical behaviors. We propose a Topology-Aware PINN (TAPINN) that aims to mitigate this challenge by structuring the latent space via Supervised Metric Regularization. Unlike standard parametric PINNs that map physical parameters directly to solutions, our method conditions the solver on a latent state optimized to reflect the metric-based separation between regimes, showing ~49% lower physics residual (0.082 vs. 0.160). We train this architecture using a phase-based Alternating Optimization (AO) schedule to manage gradient conflicts between the metric and physics objectives. Preliminary experiments on the Duffing Oscillator demonstrate that while standard baselines suffer from spectral bias and high-capacity Hypernetworks overfit (memorizing data while violating physics), our approach achieves stable convergence with 2.18x lower gradient variance than a multi-output Sobolev Error baseline, and 5x fewer parameters than a hypernetwork-based alternative.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åœ¨å¯¹å…·æœ‰æ€¥å‰§çŠ¶æ€è½¬å˜ï¼ˆä¾‹å¦‚åˆ†å²”ï¼‰çš„å‚æ•°åŒ–åŠ¨åŠ›ç³»ç»Ÿè¿›è¡Œå»ºæ¨¡æ—¶ï¼Œæ ‡å‡†ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ (PINN) ç»å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œä»å‚æ•°åˆ°è§£å†³æ–¹æ¡ˆçš„è¿ç»­æ˜ å°„å¯èƒ½ä¼šå¯¼è‡´é¢‘è°±åå·®æˆ–â€œæ¨¡å¼å´©æºƒâ€ï¼Œå…¶ä¸­ç½‘ç»œå¯¹ä¸åŒçš„ç‰©ç†è¡Œä¸ºè¿›è¡Œå¹³å‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ‹“æ‰‘æ„ŸçŸ¥ PINN (TAPINN)ï¼Œæ—¨åœ¨é€šè¿‡ç›‘ç£åº¦é‡æ­£åˆ™åŒ–æ„å»ºæ½œåœ¨ç©ºé—´æ¥ç¼“è§£è¿™ä¸€æŒ‘æˆ˜ã€‚ä¸å°†ç‰©ç†å‚æ•°ç›´æ¥æ˜ å°„åˆ°è§£çš„æ ‡å‡†å‚æ•° PINN ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†æ±‚è§£å™¨ç½®äºä¼˜åŒ–çš„æ½œåœ¨çŠ¶æ€ä¸Šï¼Œä»¥åæ˜ çŠ¶æ€ä¹‹é—´åŸºäºåº¦é‡çš„åˆ†ç¦»ï¼Œæ˜¾ç¤ºç‰©ç†æ®‹å·®é™ä½çº¦ 49%ï¼ˆ0.082 ä¸ 0.160ï¼‰ã€‚æˆ‘ä»¬ä½¿ç”¨åŸºäºé˜¶æ®µçš„äº¤æ›¿ä¼˜åŒ–ï¼ˆAOï¼‰è®¡åˆ’æ¥è®­ç»ƒè¯¥æ¶æ„ï¼Œä»¥ç®¡ç†åº¦é‡ç›®æ ‡å’Œç‰©ç†ç›®æ ‡ä¹‹é—´çš„æ¢¯åº¦å†²çªã€‚ Duffing æŒ¯è¡å™¨çš„åˆæ­¥å®éªŒè¡¨æ˜ï¼Œè™½ç„¶æ ‡å‡†åŸºçº¿å­˜åœ¨è°±åå·®å’Œé«˜å®¹é‡è¶…ç½‘ç»œè¿‡åº¦æ‹Ÿåˆï¼ˆåœ¨è¿åç‰©ç†çš„æƒ…å†µä¸‹è®°å¿†æ•°æ®ï¼‰ï¼Œä½†æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†ç¨³å®šæ”¶æ•›ï¼Œæ¢¯åº¦æ–¹å·®æ¯”å¤šè¾“å‡º Sobolev è¯¯å·®åŸºçº¿ä½ 2.18 å€ï¼Œå‚æ•°æ¯”åŸºäºè¶…ç½‘ç»œçš„æ›¿ä»£æ–¹æ¡ˆå°‘ 5 å€ã€‚

</details>

---

## 189. Closing Reasoning Gaps in Clinical Agents with Differential Reasoning Learning

**ä¸­æ–‡æ ‡é¢˜**: é€šè¿‡å·®å¼‚æ¨ç†å­¦ä¹ ç¼©å°ä¸´åºŠä»£ç†çš„æ¨ç†å·®è·

**Date**: 2026-02-10 | **arXiv**: [2602.09945v1](http://arxiv.org/abs/2602.09945v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09945v1)

<details><summary><b>Abstract</b></summary>

Clinical decision support requires not only correct answers but also clinically valid reasoning. We propose Differential Reasoning Learning (DRL), a framework that improves clinical agents by learning from reasoning discrepancies. From reference reasoning rationales (e.g., physician-authored clinical rationale, clinical guidelines, or outputs from more capable models) and the agent's free-form chain-of-thought (CoT), DRL extracts reasoning graphs as directed acyclic graphs (DAGs) and performs a clinically weighted graph edit distance (GED)-based discrepancy analysis. An LLM-as-a-judge aligns semantically equivalent nodes and diagnoses discrepancies between graphs. These graph-level discrepancy diagnostics are converted into natural-language instructions and stored in a Differential Reasoning Knowledge Base (DR-KB). At inference, we retrieve top-$k$ instructions via Retrieval-Augmented Generation (RAG) to augment the agent prompt and patch likely logic gaps. Evaluation on open medical question answering (QA) benchmarks and a Return Visit Admissions (RVA) prediction task from internal clinical data demonstrates gains over baselines, improving both final-answer accuracy and reasoning fidelity. Ablation studies confirm gains from infusing reference reasoning rationales and the top-$k$ retrieval strategy. Clinicians' review of the output provides further assurance of the approach. Together, results suggest that DRL supports more reliable clinical decision-making in complex reasoning scenarios and offers a practical mechanism for deployment under limited token budgets.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ä¸´åºŠå†³ç­–æ”¯æŒä¸ä»…éœ€è¦æ­£ç¡®çš„ç­”æ¡ˆï¼Œè¿˜éœ€è¦ä¸´åºŠæœ‰æ•ˆçš„æ¨ç†ã€‚æˆ‘ä»¬æå‡ºäº†å·®å¼‚æ¨ç†å­¦ä¹ ï¼ˆDRLï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡ä»æ¨ç†å·®å¼‚ä¸­å­¦ä¹ æ¥æ”¹è¿›ä¸´åºŠä»£ç†çš„æ¡†æ¶ã€‚æ ¹æ®å‚è€ƒæ¨ç†åŸç†ï¼ˆä¾‹å¦‚ï¼ŒåŒ»ç”Ÿæ’°å†™çš„ä¸´åºŠåŸç†ã€ä¸´åºŠæŒ‡å—æˆ–åŠŸèƒ½æ›´å¼ºå¤§çš„æ¨¡å‹çš„è¾“å‡ºï¼‰å’Œä»£ç†çš„è‡ªç”±å½¢å¼æ€æƒ³é“¾ (CoT)ï¼ŒDRL å°†æ¨ç†å›¾æå–ä¸ºæœ‰å‘æ— ç¯å›¾ (DAG)ï¼Œå¹¶æ‰§è¡ŒåŸºäºä¸´åºŠåŠ æƒå›¾ç¼–è¾‘è·ç¦» (GED) çš„å·®å¼‚åˆ†æã€‚æ³•å­¦ç¡•å£«ä½œä¸ºæ³•å®˜å¯¹é½è¯­ä¹‰ä¸Šç­‰æ•ˆçš„èŠ‚ç‚¹å¹¶è¯Šæ–­å›¾è¡¨ä¹‹é—´çš„å·®å¼‚ã€‚è¿™äº›å›¾å½¢çº§å·®å¼‚è¯Šæ–­è¢«è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æŒ‡ä»¤å¹¶å­˜å‚¨åœ¨å·®å¼‚æ¨ç†çŸ¥è¯†åº“ (DR-KB) ä¸­ã€‚åœ¨æ¨ç†æ—¶ï¼Œæˆ‘ä»¬é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ£€ç´¢ top-$k$ æŒ‡ä»¤ï¼Œä»¥å¢å¼ºä»£ç†æç¤ºå¹¶ä¿®è¡¥å¯èƒ½çš„é€»è¾‘é—´éš™ã€‚æ ¹æ®å†…éƒ¨ä¸´åºŠæ•°æ®å¯¹å¼€æ”¾å¼åŒ»å­¦é—®ç­” (QA) åŸºå‡†å’Œå›è¯Šå…¥é™¢ (RVA) é¢„æµ‹ä»»åŠ¡è¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼Œä¸åŸºçº¿ç›¸æ¯”æœ‰æ‰€æé«˜ï¼Œæœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®æ€§å’Œæ¨ç†ä¿çœŸåº¦å‡å¾—åˆ°æé«˜ã€‚æ¶ˆèç ”ç©¶è¯å®äº†æ³¨å…¥å‚è€ƒæ¨ç†åŸç†å’Œ top-$k$ æ£€ç´¢ç­–ç•¥çš„æ”¶ç›Šã€‚ä¸´åºŠåŒ»ç”Ÿå¯¹è¾“å‡ºçš„å®¡æŸ¥ä¸ºè¯¥æ–¹æ³•æä¾›äº†è¿›ä¸€æ­¥çš„ä¿è¯ã€‚æ€»ä¹‹ï¼Œç»“æœè¡¨æ˜ DRL æ”¯æŒå¤æ‚æ¨ç†åœºæ™¯ä¸­æ›´å¯é çš„ä¸´åºŠå†³ç­–ï¼Œå¹¶æä¾›äº†åœ¨æœ‰é™ä»£å¸é¢„ç®—ä¸‹éƒ¨ç½²çš„å®ç”¨æœºåˆ¶ã€‚

</details>

---

## 190. Instruct2Act: From Human Instruction to Actions Sequencing and Execution via Robot Action Network for Robotic Manipulation

**ä¸­æ–‡æ ‡é¢˜**: Instruct2Actï¼šä»äººç±»æŒ‡ä»¤åˆ°é€šè¿‡æœºå™¨äººæ“ä½œç½‘ç»œè¿›è¡ŒåŠ¨ä½œæ’åºå’Œæ‰§è¡Œ

**Date**: 2026-02-10 | **arXiv**: [2602.09940v1](http://arxiv.org/abs/2602.09940v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09940v1)

<details><summary><b>Abstract</b></summary>

Robots often struggle to follow free-form human instructions in real-world settings due to computational and sensing limitations. We address this gap with a lightweight, fully on-device pipeline that converts natural-language commands into reliable manipulation. Our approach has two stages: (i) the instruction to actions module (Instruct2Act), a compact BiLSTM with a multi-head-attention autoencoder that parses an instruction into an ordered sequence of atomic actions (e.g., reach, grasp, move, place); and (ii) the robot action network (RAN), which uses the dynamic adaptive trajectory radial network (DATRN) together with a vision-based environment analyzer (YOLOv8) to generate precise control trajectories for each sub-action. The entire system runs on a modest system with no cloud services. On our custom proprietary dataset, Instruct2Act attains 91.5% sub-actions prediction accuracy while retaining a small footprint. Real-robot evaluations across four tasks (pick-place, pick-pour, wipe, and pick-give) yield an overall 90% success; sub-action inference completes in < 3.8s, with end-to-end executions in 30-60s depending on task complexity. These results demonstrate that fine-grained instruction-to-action parsing, coupled with DATRN-based trajectory generation and vision-guided grounding, provides a practical path to deterministic, real-time manipulation in resource-constrained, single-camera settings.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç”±äºè®¡ç®—å’Œä¼ æ„Ÿçš„é™åˆ¶ï¼Œæœºå™¨äººå¸¸å¸¸éš¾ä»¥åœ¨ç°å®ä¸–ç•Œä¸­éµå¾ªè‡ªç”±å½¢å¼çš„äººç±»æŒ‡ä»¤ã€‚æˆ‘ä»¬é€šè¿‡è½»é‡çº§ã€å®Œå…¨åœ¨è®¾å¤‡ä¸Šçš„ç®¡é“æ¥è§£å†³è¿™ä¸€å·®è·ï¼Œè¯¥ç®¡é“å°†è‡ªç„¶è¯­è¨€å‘½ä»¤è½¬æ¢ä¸ºå¯é çš„æ“ä½œã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ‰ä¸¤ä¸ªé˜¶æ®µï¼šï¼ˆiï¼‰åŠ¨ä½œæŒ‡ä»¤æ¨¡å—ï¼ˆInstruct2Actï¼‰ï¼Œä¸€ä¸ªç´§å‡‘çš„ BiLSTMï¼Œå…·æœ‰å¤šå¤´æ³¨æ„åŠ›è‡ªåŠ¨ç¼–ç å™¨ï¼Œå¯å°†æŒ‡ä»¤è§£æä¸ºæœ‰åºçš„åŸå­åŠ¨ä½œåºåˆ—ï¼ˆä¾‹å¦‚ï¼Œåˆ°è¾¾ã€æŠ“å–ã€ç§»åŠ¨ã€æ”¾ç½®ï¼‰ï¼› (ii) æœºå™¨äººåŠ¨ä½œç½‘ç»œ (RAN)ï¼Œå®ƒä½¿ç”¨åŠ¨æ€è‡ªé€‚åº”è½¨è¿¹å¾„å‘ç½‘ç»œ (DATRN) å’ŒåŸºäºè§†è§‰çš„ç¯å¢ƒåˆ†æå™¨ (YOLOv8) æ¥ä¸ºæ¯ä¸ªå­åŠ¨ä½œç”Ÿæˆç²¾ç¡®çš„æ§åˆ¶è½¨è¿¹ã€‚æ•´ä¸ªç³»ç»Ÿè¿è¡Œåœ¨ä¸€ä¸ªæ²¡æœ‰äº‘æœåŠ¡çš„æ™®é€šç³»ç»Ÿä¸Šã€‚åœ¨æˆ‘ä»¬çš„è‡ªå®šä¹‰ä¸“æœ‰æ•°æ®é›†ä¸Šï¼ŒInstruct2Act å®ç°äº† 91.5% çš„å­åŠ¨ä½œé¢„æµ‹å‡†ç¡®ç‡ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒå°çš„å ç”¨ç©ºé—´ã€‚å¯¹å››é¡¹ä»»åŠ¡ï¼ˆæ‹¾å–æ”¾ç½®ã€æ‹¾å–å€¾å€’ã€æ“¦æ‹­å’Œæ‹¾å–-ç»™äºˆï¼‰è¿›è¡Œçš„çœŸå®æœºå™¨äººè¯„ä¼°æ€»ä½“æˆåŠŸç‡ä¸º 90%ï¼›å­åŠ¨ä½œæ¨ç†åœ¨ 3.8 ç§’å†…å®Œæˆï¼Œæ ¹æ®ä»»åŠ¡å¤æ‚æ€§ï¼Œç«¯åˆ°ç«¯æ‰§è¡Œåœ¨ 30-60 ç§’å†…å®Œæˆã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œç»†ç²’åº¦çš„æŒ‡ä»¤åˆ°åŠ¨ä½œè§£æä¸åŸºäº DATRN çš„è½¨è¿¹ç”Ÿæˆå’Œè§†è§‰å¼•å¯¼æ¥åœ°ç›¸ç»“åˆï¼Œä¸ºèµ„æºå—é™çš„å•æ‘„åƒå¤´è®¾ç½®ä¸­çš„ç¡®å®šæ€§å®æ—¶æ“ä½œæä¾›äº†ä¸€æ¡å®ç”¨è·¯å¾„ã€‚

</details>

---

## 191. Cosmo3DFlow: Wavelet Flow Matching for Spatial-to-Spectral Compression in Reconstructing the Early Universe

**ä¸­æ–‡æ ‡é¢˜**: Cosmo3DFlowï¼šé‡å»ºæ—©æœŸå®‡å®™ä¸­ç”¨äºç©ºé—´åˆ°å…‰è°±å‹ç¼©çš„å°æ³¢æµåŒ¹é…

**Date**: 2026-02-10 | **arXiv**: [2602.10172v1](http://arxiv.org/abs/2602.10172v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10172v1)

<details><summary><b>Abstract</b></summary>

Reconstructing the early Universe from the evolved present-day Universe is a challenging and computationally demanding problem in modern astrophysics. We devise a novel generative framework, Cosmo3DFlow, designed to address dimensionality and sparsity, the critical bottlenecks inherent in current state-of-the-art methods for cosmological inference. By integrating 3D Discrete Wavelet Transform (DWT) with flow matching, we effectively represent high-dimensional cosmological structures. The Wavelet Transform addresses the ``void problem'' by translating spatial emptiness into spectral sparsity. It decouples high-frequency details from low-frequency structures through spatial compression, and wavelet-space velocity fields facilitate stable ordinary differential equation (ODE) solvers with large step sizes. Using large-scale cosmological $N$-body simulations, at $128^3$ resolution, we achieve up to $50\times$ faster sampling than diffusion models, combining a $10\times$ reduction in integration steps with lower per-step computational cost from wavelet compression. Our results enable initial conditions to be sampled in seconds, compared to minutes for previous methods.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ä»è¿›åŒ–çš„å½“ä»Šå®‡å®™é‡å»ºæ—©æœŸå®‡å®™æ˜¯ç°ä»£å¤©ä½“ç‰©ç†å­¦ä¸­çš„ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§å’Œè®¡ç®—è¦æ±‚çš„é—®é¢˜ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„ç”Ÿæˆæ¡†æ¶ Cosmo3DFlowï¼Œæ—¨åœ¨è§£å†³ç»´åº¦å’Œç¨€ç–æ€§ï¼Œè¿™æ˜¯å½“å‰æœ€å…ˆè¿›çš„å®‡å®™å­¦æ¨ç†æ–¹æ³•å›ºæœ‰çš„å…³é”®ç“¶é¢ˆã€‚é€šè¿‡å°† 3D ç¦»æ•£å°æ³¢å˜æ¢ (DWT) ä¸æµåŒ¹é…ç›¸ç»“åˆï¼Œæˆ‘ä»¬å¯ä»¥æœ‰æ•ˆåœ°è¡¨ç¤ºé«˜ç»´å®‡å®™ç»“æ„ã€‚å°æ³¢å˜æ¢é€šè¿‡å°†ç©ºé—´ç©ºè™šè½¬åŒ–ä¸ºå…‰è°±ç¨€ç–æ¥è§£å†³â€œç©ºè™šé—®é¢˜â€ã€‚å®ƒé€šè¿‡ç©ºé—´å‹ç¼©å°†é«˜é¢‘ç»†èŠ‚ä¸ä½é¢‘ç»“æ„è§£è€¦ï¼Œå°æ³¢ç©ºé—´é€Ÿåº¦åœºæœ‰åŠ©äºç¨³å®šçš„å¤§æ­¥é•¿å¸¸å¾®åˆ†æ–¹ç¨‹ (ODE) æ±‚è§£å™¨ã€‚ä½¿ç”¨å¤§è§„æ¨¡å®‡å®™å­¦ $N$ ä½“æ¨¡æ‹Ÿï¼Œåœ¨ $128^3$ åˆ†è¾¨ç‡ä¸‹ï¼Œæˆ‘ä»¬å®ç°äº†æ¯”æ‰©æ•£æ¨¡å‹å¿« $50\time$ çš„é‡‡æ ·é€Ÿåº¦ï¼ŒåŒæ—¶å‡å°‘äº† $10\times$ çš„ç§¯åˆ†æ­¥éª¤ä»¥åŠé™ä½äº†å°æ³¢å‹ç¼©çš„æ¯æ­¥è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬çš„ç»“æœä½¿å¾—åˆå§‹æ¡ä»¶èƒ½å¤Ÿåœ¨å‡ ç§’é’Ÿå†…è¿›è¡Œé‡‡æ ·ï¼Œè€Œä»¥å‰çš„æ–¹æ³•éœ€è¦å‡ åˆ†é’Ÿã€‚

</details>

---

## 192. TaCo: A Benchmark for Lossless and Lossy Codecs of Heterogeneous Tactile Data

**ä¸­æ–‡æ ‡é¢˜**: TaCoï¼šå¼‚æ„è§¦è§‰æ•°æ®æ— æŸå’Œæœ‰æŸç¼–è§£ç å™¨çš„åŸºå‡†

**Date**: 2026-02-10 | **arXiv**: [2602.09893v1](http://arxiv.org/abs/2602.09893v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09893v1)

<details><summary><b>Abstract</b></summary>

Tactile sensing is crucial for embodied intelligence, providing fine-grained perception and control in complex environments. However, efficient tactile data compression, which is essential for real-time robotic applications under strict bandwidth constraints, remains underexplored. The inherent heterogeneity and spatiotemporal complexity of tactile data further complicate this challenge. To bridge this gap, we introduce TaCo, the first comprehensive benchmark for Tactile data Codecs. TaCo evaluates 30 compression methods, including off-the-shelf compression algorithms and neural codecs, across five diverse datasets from various sensor types. We systematically assess both lossless and lossy compression schemes on four key tasks: lossless storage, human visualization, material and object classification, and dexterous robotic grasping. Notably, we pioneer the development of data-driven codecs explicitly trained on tactile data, TaCo-LL (lossless) and TaCo-L (lossy). Results have validated the superior performance of our TaCo-LL and TaCo-L. This benchmark provides a foundational framework for understanding the critical trade-offs between compression efficiency and task performance, paving the way for future advances in tactile perception.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è§¦è§‰ä¼ æ„Ÿå¯¹äºä½“ç°æ™ºèƒ½è‡³å…³é‡è¦ï¼Œå¯ä»¥åœ¨å¤æ‚ç¯å¢ƒä¸­æä¾›ç»†ç²’åº¦çš„æ„ŸçŸ¥å’Œæ§åˆ¶ã€‚ç„¶è€Œï¼Œæœ‰æ•ˆçš„è§¦è§‰æ•°æ®å‹ç¼©å¯¹äºä¸¥æ ¼å¸¦å®½é™åˆ¶ä¸‹çš„å®æ—¶æœºå™¨äººåº”ç”¨ç¨‹åºè‡³å…³é‡è¦ï¼Œä½†ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚è§¦è§‰æ•°æ®å›ºæœ‰çš„å¼‚è´¨æ€§å’Œæ—¶ç©ºå¤æ‚æ€§ä½¿è¿™ä¸€æŒ‘æˆ˜è¿›ä¸€æ­¥å¤æ‚åŒ–ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº† TaCoï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹è§¦è§‰æ•°æ®ç¼–è§£ç å™¨çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚ TaCo åœ¨æ¥è‡ªä¸åŒä¼ æ„Ÿå™¨ç±»å‹çš„äº”ä¸ªä¸åŒæ•°æ®é›†ä¸Šè¯„ä¼°äº† 30 ç§å‹ç¼©æ–¹æ³•ï¼ŒåŒ…æ‹¬ç°æˆçš„å‹ç¼©ç®—æ³•å’Œç¥ç»ç¼–è§£ç å™¨ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†æ— æŸå’Œæœ‰æŸå‹ç¼©æ–¹æ¡ˆçš„å››ä¸ªå…³é”®ä»»åŠ¡ï¼šæ— æŸå­˜å‚¨ã€äººç±»å¯è§†åŒ–ã€ææ–™å’Œç‰©ä½“åˆ†ç±»ä»¥åŠçµå·§çš„æœºå™¨äººæŠ“å–ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬ç‡å…ˆå¼€å‘äº†é’ˆå¯¹è§¦è§‰æ•°æ®ã€TaCo-LLï¼ˆæ— æŸï¼‰å’Œ TaCo-Lï¼ˆæœ‰æŸï¼‰è¿›è¡Œæ˜ç¡®è®­ç»ƒçš„æ•°æ®é©±åŠ¨ç¼–è§£ç å™¨ã€‚ç»“æœéªŒè¯äº†æˆ‘ä»¬çš„ TaCo-LL å’Œ TaCo-L çš„å“è¶Šæ€§èƒ½ã€‚è¯¥åŸºå‡†æµ‹è¯•ä¸ºç†è§£å‹ç¼©æ•ˆç‡å’Œä»»åŠ¡æ€§èƒ½ä¹‹é—´çš„å…³é”®æƒè¡¡æä¾›äº†ä¸€ä¸ªåŸºç¡€æ¡†æ¶ï¼Œä¸ºè§¦è§‰æ„ŸçŸ¥çš„æœªæ¥è¿›æ­¥é“ºå¹³äº†é“è·¯ã€‚

</details>

---

## 193. Efficient Unsupervised Environment Design through Hierarchical Policy Representation Learning

**ä¸­æ–‡æ ‡é¢˜**: é€šè¿‡åˆ†å±‚ç­–ç•¥è¡¨ç¤ºå­¦ä¹ è¿›è¡Œé«˜æ•ˆçš„æ— ç›‘ç£ç¯å¢ƒè®¾è®¡

**Date**: 2026-02-10 | **arXiv**: [2602.09813v1](http://arxiv.org/abs/2602.09813v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09813v1)

<details><summary><b>Abstract</b></summary>

Unsupervised Environment Design (UED) has emerged as a promising approach to developing general-purpose agents through automated curriculum generation. Popular UED methods focus on Open-Endedness, where teacher algorithms rely on stochastic processes for infinite generation of useful environments. This assumption becomes impractical in resource-constrained scenarios where teacher-student interaction opportunities are limited. To address this challenge, we introduce a hierarchical Markov Decision Process (MDP) framework for environment design. Our framework features a teacher agent that leverages student policy representations derived from discovered evaluation environments, enabling it to generate training environments based on the student's capabilities. To improve efficiency, we incorporate a generative model that augments the teacher's training dataset with synthetic data, reducing the need for teacher-student interactions. In experiments across several domains, we show that our method outperforms baseline approaches while requiring fewer teacher-student interactions in a single episode. The results suggest the applicability of our approach in settings where training opportunities are limited.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æ— ç›‘ç£ç¯å¢ƒè®¾è®¡ï¼ˆUEDï¼‰å·²æˆä¸ºé€šè¿‡è‡ªåŠ¨è¯¾ç¨‹ç”Ÿæˆå¼€å‘é€šç”¨ä»£ç†çš„ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ã€‚æµè¡Œçš„ UED æ–¹æ³•ä¾§é‡äºå¼€æ”¾æ€§ï¼Œå…¶ä¸­æ•™å¸ˆç®—æ³•ä¾èµ–éšæœºè¿‡ç¨‹æ¥æ— é™ç”Ÿæˆæœ‰ç”¨çš„ç¯å¢ƒã€‚åœ¨èµ„æºæœ‰é™ã€å¸ˆç”Ÿäº’åŠ¨æœºä¼šæœ‰é™çš„æƒ…å†µä¸‹ï¼Œè¿™ç§å‡è®¾å˜å¾—ä¸åˆ‡å®é™…ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç”¨äºç¯å¢ƒè®¾è®¡çš„åˆ†å±‚é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶å…·æœ‰ä¸€ä¸ªæ•™å¸ˆä»£ç†ï¼Œå®ƒåˆ©ç”¨ä»å‘ç°çš„è¯„ä¼°ç¯å¢ƒä¸­æ´¾ç”Ÿçš„å­¦ç”Ÿç­–ç•¥è¡¨ç¤ºï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®å­¦ç”Ÿçš„èƒ½åŠ›ç”ŸæˆåŸ¹è®­ç¯å¢ƒã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ç”Ÿæˆæ¨¡å‹ï¼Œç”¨åˆæˆæ•°æ®å¢å¼ºæ•™å¸ˆçš„è®­ç»ƒæ•°æ®é›†ï¼Œå‡å°‘å¸ˆç”Ÿäº’åŠ¨çš„éœ€è¦ã€‚åœ¨è·¨å¤šä¸ªé¢†åŸŸçš„å®éªŒä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºåŸºçº¿æ–¹æ³•ï¼ŒåŒæ—¶åœ¨å•é›†ä¸­éœ€è¦æ›´å°‘çš„å¸ˆç”Ÿäº’åŠ¨ã€‚ç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸ¹è®­æœºä¼šæœ‰é™çš„ç¯å¢ƒä¸­çš„é€‚ç”¨æ€§ã€‚

</details>

---

## 194. A Controlled Study of Double DQN and Dueling DQN Under Cross-Environment Transfer

**ä¸­æ–‡æ ‡é¢˜**: è·¨ç¯å¢ƒä¼ è¾“ä¸‹åŒDQNå’Œå†³æ–—DQNçš„å¯¹ç…§ç ”ç©¶

**Date**: 2026-02-10 | **arXiv**: [2602.09810v2](http://arxiv.org/abs/2602.09810v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.09810v2)

<details><summary><b>Abstract</b></summary>

Transfer learning in deep reinforcement learning is often motivated by improved stability and reduced training cost, but it can also fail under substantial domain shift. This paper presents a controlled empirical study examining how architectural differences between Double Deep Q-Networks (DDQN) and Dueling DQN influence transfer behavior across environments. Using CartPole as a source task and LunarLander as a structurally distinct target task, we evaluate a fixed layer-wise representation transfer protocol under identical hyperparameters and training conditions, with baseline agents trained from scratch used to contextualize transfer effects. Empirical results show that DDQN consistently avoids negative transfer under the examined setup and maintains learning dynamics comparable to baseline performance in the target environment. In contrast, Dueling DQN consistently exhibits negative transfer under identical conditions, characterized by degraded rewards and unstable optimization behavior. Statistical analysis across multiple random seeds confirms a significant performance gap under transfer. These findings suggest that architectural inductive bias is strongly associated with robustness to cross-environment transfer in value-based deep reinforcement learning under the examined transfer protocol.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­çš„è¿ç§»å­¦ä¹ é€šå¸¸æ˜¯å‡ºäºæé«˜ç¨³å®šæ€§å’Œé™ä½è®­ç»ƒæˆæœ¬çš„åŠ¨æœºï¼Œä½†å®ƒä¹Ÿå¯èƒ½åœ¨å¤§é‡é¢†åŸŸè½¬ç§»çš„æƒ…å†µä¸‹å¤±è´¥ã€‚æœ¬æ–‡æå‡ºäº†ä¸€é¡¹å—æ§å®è¯ç ”ç©¶ï¼Œæ¢è®¨åŒæ·±åº¦ Q ç½‘ç»œ (DDQN) å’Œå†³æ–— DQN ä¹‹é—´çš„æ¶æ„å·®å¼‚å¦‚ä½•å½±å“è·¨ç¯å¢ƒçš„ä¼ è¾“è¡Œä¸ºã€‚ä½¿ç”¨ CartPole ä½œä¸ºæºä»»åŠ¡ï¼ŒLunarLander ä½œä¸ºç»“æ„ä¸åŒçš„ç›®æ ‡ä»»åŠ¡ï¼Œæˆ‘ä»¬åœ¨ç›¸åŒçš„è¶…å‚æ•°å’Œè®­ç»ƒæ¡ä»¶ä¸‹è¯„ä¼°å›ºå®šçš„é€å±‚è¡¨ç¤ºä¼ è¾“åè®®ï¼Œå¹¶ä½¿ç”¨ä»å¤´å¼€å§‹è®­ç»ƒçš„åŸºçº¿ä»£ç†æ¥å°†ä¼ è¾“æ•ˆæœæƒ…å¢ƒåŒ–ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼ŒDDQN åœ¨æ‰€æ£€æŸ¥çš„è®¾ç½®ä¸‹å§‹ç»ˆé¿å…è´Ÿè¿ç§»ï¼Œå¹¶ä¿æŒä¸ç›®æ ‡ç¯å¢ƒä¸­çš„åŸºçº¿æ€§èƒ½ç›¸å½“çš„å­¦ä¹ åŠ¨æ€ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒDueling DQN åœ¨ç›¸åŒæ¡ä»¶ä¸‹å§‹ç»ˆè¡¨ç°å‡ºè´Ÿè¿ç§»ï¼Œå…¶ç‰¹ç‚¹æ˜¯å¥–åŠ±é™ä½å’Œä¼˜åŒ–è¡Œä¸ºä¸ç¨³å®šã€‚å¤šä¸ªéšæœºç§å­çš„ç»Ÿè®¡åˆ†æè¯å®äº†ä¼ è¾“ä¸‹çš„æ˜¾ç€æ€§èƒ½å·®è·ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œåœ¨æ‰€æ£€æŸ¥çš„ä¼ è¾“åè®®ä¸‹ï¼Œæ¶æ„å½’çº³åå·®ä¸åŸºäºä»·å€¼çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­çš„è·¨ç¯å¢ƒä¼ è¾“çš„é²æ£’æ€§å¯†åˆ‡ç›¸å…³ã€‚

</details>

---

## 195. Would a Large Language Model Pay Extra for a View? Inferring Willingness to Pay from Subjective Choices

**ä¸­æ–‡æ ‡é¢˜**: å¤§å‹è¯­è¨€æ¨¡å‹ä¼šä¸ºè§†å›¾æ”¯ä»˜é¢å¤–è´¹ç”¨å—ï¼Ÿä»ä¸»è§‚é€‰æ‹©æ¨æ–­æ”¯ä»˜æ„æ„¿

**Date**: 2026-02-10 | **arXiv**: [2602.09802v1](http://arxiv.org/abs/2602.09802v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09802v1)

<details><summary><b>Abstract</b></summary>

As Large Language Models (LLMs) are increasingly deployed in applications such as travel assistance and purchasing support, they are often required to make subjective choices on behalf of users in settings where no objectively correct answer exists. We study LLM decision-making in a travel-assistant context by presenting models with choice dilemmas and analyzing their responses using multinomial logit models to derive implied willingness to pay (WTP) estimates. These WTP values are subsequently compared to human benchmark values from the economics literature. In addition to a baseline setting, we examine how model behavior changes under more realistic conditions, including the provision of information about users' past choices and persona-based prompting. Our results show that while meaningful WTP values can be derived for larger LLMs, they also display systematic deviations at the attribute level. Additionally, they tend to overestimate human WTP overall, particularly when expensive options or business-oriented personas are introduced. Conditioning models on prior preferences for cheaper options yields valuations that are closer to human benchmarks. Overall, our findings highlight both the potential and the limitations of using LLMs for subjective decision support and underscore the importance of careful model selection, prompt design, and user representation when deploying such systems in practice.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) è¶Šæ¥è¶Šå¤šåœ°éƒ¨ç½²åœ¨æ—…è¡Œæ´åŠ©å’Œè´­ä¹°æ”¯æŒç­‰åº”ç”¨ä¸­ï¼Œå®ƒä»¬é€šå¸¸éœ€è¦åœ¨ä¸å­˜åœ¨å®¢è§‚æ­£ç¡®ç­”æ¡ˆçš„ç¯å¢ƒä¸­ä»£è¡¨ç”¨æˆ·åšå‡ºä¸»è§‚é€‰æ‹©ã€‚æˆ‘ä»¬é€šè¿‡æå‡ºå…·æœ‰é€‰æ‹©å›°å¢ƒçš„æ¨¡å‹å¹¶ä½¿ç”¨å¤šé¡¹ Logit æ¨¡å‹åˆ†æå…¶å“åº”æ¥å¾—å‡ºéšå«çš„æ”¯ä»˜æ„æ„¿ (WTP) ä¼°è®¡ï¼Œä»è€Œç ”ç©¶æ—…è¡Œè¾…åŠ©èƒŒæ™¯ä¸‹çš„æ³•å­¦ç¡•å£«å†³ç­–ã€‚éšåå°†è¿™äº› WTP å€¼ä¸ç»æµå­¦æ–‡çŒ®ä¸­çš„äººç±»åŸºå‡†å€¼è¿›è¡Œæ¯”è¾ƒã€‚é™¤äº†åŸºçº¿è®¾ç½®ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶æ¨¡å‹è¡Œä¸ºåœ¨æ›´ç°å®çš„æ¡ä»¶ä¸‹å¦‚ä½•å˜åŒ–ï¼ŒåŒ…æ‹¬æä¾›æœ‰å…³ç”¨æˆ·è¿‡å»é€‰æ‹©å’ŒåŸºäºè§’è‰²çš„æç¤ºçš„ä¿¡æ¯ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè™½ç„¶å¯¹äºè¾ƒå¤§çš„æ³•å­¦ç¡•å£«å¯ä»¥å¾—å‡ºæœ‰æ„ä¹‰çš„æ”¯ä»˜æ„æ„¿å€¼ï¼Œä½†å®ƒä»¬ä¹Ÿæ˜¾ç¤ºå‡ºå±æ€§çº§åˆ«çš„ç³»ç»Ÿåå·®ã€‚æ­¤å¤–ï¼Œä»–ä»¬å¾€å¾€ä¼šé«˜ä¼°äººç±»çš„æ•´ä½“æ”¯ä»˜æ„æ„¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼•å…¥æ˜‚è´µçš„é€‰é¡¹æˆ–é¢å‘ä¸šåŠ¡çš„è§’è‰²æ—¶ã€‚æ ¹æ®å…ˆå‰å¯¹æ›´ä¾¿å®œæœŸæƒçš„åå¥½å»ºç«‹çš„æ¡ä»¶æ¨¡å‹ä¼šäº§ç”Ÿæ›´æ¥è¿‘äººç±»åŸºå‡†çš„ä¼°å€¼ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†ä½¿ç”¨æ³•å­¦ç¡•å£«è¿›è¡Œä¸»è§‚å†³ç­–æ”¯æŒçš„æ½œåŠ›å’Œå±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†åœ¨å®è·µä¸­éƒ¨ç½²æ­¤ç±»ç³»ç»Ÿæ—¶ä»”ç»†é€‰æ‹©æ¨¡å‹ã€æç¤ºè®¾è®¡å’Œç”¨æˆ·è¡¨ç¤ºçš„é‡è¦æ€§ã€‚

</details>

---

## 196. GHS-TDA: A Synergistic Reasoning Framework Integrating Global Hypothesis Space with Topological Data Analysis

**ä¸­æ–‡æ ‡é¢˜**: GHS-TDAï¼šå…¨å±€å‡è®¾ç©ºé—´ä¸æ‹“æ‰‘æ•°æ®åˆ†æç›¸ç»“åˆçš„ååŒæ¨ç†æ¡†æ¶

**Date**: 2026-02-10 | **arXiv**: [2602.09794v1](http://arxiv.org/abs/2602.09794v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09794v1)

<details><summary><b>Abstract</b></summary>

Chain-of-Thought (CoT) has been shown to significantly improve the reasoning accuracy of large language models (LLMs) on complex tasks. However, due to the autoregressive, step-by-step generation paradigm, existing CoT methods suffer from two fundamental limitations. First, the reasoning process is highly sensitive to early decisions: once an initial error is introduced, it tends to propagate and amplify through subsequent steps, while the lack of a global coordination and revision mechanism makes such errors difficult to correct, ultimately leading to distorted reasoning chains. Second, current CoT approaches lack structured analysis techniques for filtering redundant reasoning and extracting key reasoning features, resulting in unstable reasoning processes and limited interpretability. To address these issues, we propose GHS-TDA. GHS-TDA first constructs a semantically enriched global hypothesis graph to aggregate, align, and coordinate multiple candidate reasoning paths, thereby providing alternative global correction routes when local reasoning fails. It then applies topological data analysis based on persistent homology to capture stable multi-scale structures, remove redundancy and inconsistencies, and extract a more reliable reasoning skeleton. By jointly leveraging reasoning diversity and topological stability, GHS-TDA achieves self-adaptive convergence, produces high-confidence and interpretable reasoning paths, and consistently outperforms strong baselines in terms of both accuracy and robustness across multiple reasoning benchmarks.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æ€æƒ³é“¾ (CoT) å·²è¢«è¯æ˜å¯ä»¥æ˜¾ç€æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„æ¨ç†å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç”±äºè‡ªå›å½’ã€é€æ­¥ç”ŸæˆèŒƒå¼ï¼Œç°æœ‰çš„ CoT æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªåŸºæœ¬é™åˆ¶ã€‚é¦–å…ˆï¼Œæ¨ç†è¿‡ç¨‹å¯¹æ—©æœŸå†³ç­–é«˜åº¦æ•æ„Ÿï¼šä¸€æ—¦å¼•å…¥åˆå§‹é”™è¯¯ï¼Œå®ƒå¾€å¾€ä¼šé€šè¿‡åç»­æ­¥éª¤ä¼ æ’­å’Œæ”¾å¤§ï¼Œè€Œç¼ºä¹å…¨å±€åè°ƒå’Œä¿®æ­£æœºåˆ¶ä½¿å¾—æ­¤ç±»é”™è¯¯éš¾ä»¥çº æ­£ï¼Œæœ€ç»ˆå¯¼è‡´æ¨ç†é“¾æ‰­æ›²ã€‚å…¶æ¬¡ï¼Œå½“å‰çš„ CoT æ–¹æ³•ç¼ºä¹è¿‡æ»¤å†—ä½™æ¨ç†å’Œæå–å…³é”®æ¨ç†ç‰¹å¾çš„ç»“æ„åŒ–åˆ†ææŠ€æœ¯ï¼Œå¯¼è‡´æ¨ç†è¿‡ç¨‹ä¸ç¨³å®šå’Œå¯è§£é‡Šæ€§æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº† GHS-TDAã€‚ GHS-TDAé¦–å…ˆæ„å»ºè¯­ä¹‰ä¸°å¯Œçš„å…¨å±€å‡è®¾å›¾æ¥èšåˆã€å¯¹é½å’Œåè°ƒå¤šä¸ªå€™é€‰æ¨ç†è·¯å¾„ï¼Œä»è€Œåœ¨å±€éƒ¨æ¨ç†å¤±è´¥æ—¶æä¾›æ›¿ä»£çš„å…¨å±€æ ¡æ­£è·¯å¾„ã€‚ç„¶åï¼Œå®ƒåº”ç”¨åŸºäºæŒä¹…åŒæºæ€§çš„æ‹“æ‰‘æ•°æ®åˆ†ææ¥æ•è·ç¨³å®šçš„å¤šå°ºåº¦ç»“æ„ï¼Œæ¶ˆé™¤å†—ä½™å’Œä¸ä¸€è‡´ï¼Œå¹¶æå–æ›´å¯é çš„æ¨ç†éª¨æ¶ã€‚é€šè¿‡å…±åŒåˆ©ç”¨æ¨ç†å¤šæ ·æ€§å’Œæ‹“æ‰‘ç¨³å®šæ€§ï¼ŒGHS-TDA å®ç°äº†è‡ªé€‚åº”æ”¶æ•›ï¼Œäº§ç”Ÿé«˜ç½®ä¿¡åº¦å’Œå¯è§£é‡Šçš„æ¨ç†è·¯å¾„ï¼Œå¹¶åœ¨å¤šä¸ªæ¨ç†åŸºå‡†çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§æ–¹é¢å§‹ç»ˆä¼˜äºå¼ºå¤§çš„åŸºçº¿ã€‚

</details>

---

## 197. Grounding LTL Tasks in Sub-Symbolic RL Environments for Zero-Shot Generalization

**ä¸­æ–‡æ ‡é¢˜**: å°† LTL ä»»åŠ¡ç½®äºå­ç¬¦å· RL ç¯å¢ƒä¸­ä»¥å®ç°é›¶æ ·æœ¬æ³›åŒ–

**Date**: 2026-02-10 | **arXiv**: [2602.09761v1](http://arxiv.org/abs/2602.09761v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09761v1)

<details><summary><b>Abstract</b></summary>

In this work we address the problem of training a Reinforcement Learning agent to follow multiple temporally-extended instructions expressed in Linear Temporal Logic in sub-symbolic environments. Previous multi-task work has mostly relied on knowledge of the mapping between raw observations and symbols appearing in the formulae. We drop this unrealistic assumption by jointly training a multi-task policy and a symbol grounder with the same experience. The symbol grounder is trained only from raw observations and sparse rewards via Neural Reward Machines in a semi-supervised fashion. Experiments on vision-based environments show that our method achieves performance comparable to using the true symbol grounding and significantly outperforms state-of-the-art methods for sub-symbolic environments.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†è®­ç»ƒå¼ºåŒ–å­¦ä¹ ä»£ç†ä»¥éµå¾ªå­ç¬¦å·ç¯å¢ƒä¸­ä»¥çº¿æ€§æ—¶åºé€»è¾‘è¡¨ç¤ºçš„å¤šä¸ªæ—¶é—´æ‰©å±•æŒ‡ä»¤çš„é—®é¢˜ã€‚ä»¥å‰çš„å¤šä»»åŠ¡å·¥ä½œä¸»è¦ä¾èµ–äºåŸå§‹è§‚å¯Ÿç»“æœå’Œå…¬å¼ä¸­å‡ºç°çš„ç¬¦å·ä¹‹é—´çš„æ˜ å°„çŸ¥è¯†ã€‚æˆ‘ä»¬é€šè¿‡è”åˆè®­ç»ƒå¤šä»»åŠ¡ç­–ç•¥å’Œå…·æœ‰ç›¸åŒç»éªŒçš„ç¬¦å·æ¥åœ°å™¨æ¥æ”¾å¼ƒè¿™ç§ä¸åˆ‡å®é™…çš„å‡è®¾ã€‚ç¬¦å·æ¥åœ°å™¨ä»…é€šè¿‡ç¥ç»å¥–åŠ±æœºå™¨ä»¥åŠç›‘ç£çš„æ–¹å¼æ ¹æ®åŸå§‹è§‚å¯Ÿå’Œç¨€ç–å¥–åŠ±è¿›è¡Œè®­ç»ƒã€‚åŸºäºè§†è§‰çš„ç¯å¢ƒçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†ä¸ä½¿ç”¨çœŸå®ç¬¦å·æ¥åœ°ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶ä¸”æ˜¾ç€ä¼˜äºå­ç¬¦å·ç¯å¢ƒçš„æœ€å…ˆè¿›æ–¹æ³•ã€‚

</details>

---

## 198. ClinAlign: Scaling Healthcare Alignment from Clinician Preference

**ä¸­æ–‡æ ‡é¢˜**: ClinAlignï¼šæ ¹æ®ä¸´åºŠåŒ»ç”Ÿåå¥½è°ƒæ•´åŒ»ç–—ä¿å¥ä¸€è‡´æ€§

**Date**: 2026-02-10 | **arXiv**: [2602.09653v2](http://arxiv.org/abs/2602.09653v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.09653v2)

<details><summary><b>Abstract</b></summary>

Although large language models (LLMs) demonstrate expert-level medical knowledge, aligning their open-ended outputs with fine-grained clinician preferences remains challenging. Existing methods often rely on coarse objectives or unreliable automated judges that are weakly grounded in professional guidelines. We propose a two-stage framework to address this gap. First, we introduce HealthRubrics, a dataset of 7,034 physician-verified preference examples in which clinicians refine LLM-drafted rubrics to meet rigorous medical standards. Second, we distill these rubrics into HealthPrinciples: 119 broadly reusable, clinically grounded principles organized by clinical dimensions, enabling scalable supervision beyond manual annotation. We use HealthPrinciples for (1) offline alignment by synthesizing rubrics for unlabeled queries and (2) an inference-time tool for guided self-revision. A 30B-A3B model trained with our framework achieves 33.4% on HealthBench-Hard, outperforming much larger models including Deepseek-R1 and o3, establishing a resource-efficient baseline for clinical alignment.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç¤ºäº†ä¸“å®¶çº§çš„åŒ»å­¦çŸ¥è¯†ï¼Œä½†å°†å…¶å¼€æ”¾å¼è¾“å‡ºä¸ç»†ç²’åº¦çš„ä¸´åºŠåŒ»ç”Ÿåå¥½ä¿æŒä¸€è‡´ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºç²—ç•¥çš„ç›®æ ‡æˆ–ä¸å¯é çš„è‡ªåŠ¨åˆ¤æ–­ï¼Œè¿™äº›åˆ¤æ–­ç¼ºä¹ä¸“ä¸šæŒ‡å¯¼ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶æ¥è§£å†³è¿™ä¸€å·®è·ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»‹ç» HealthRubricsï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« 7,034 ä¸ªç»åŒ»ç”ŸéªŒè¯çš„åå¥½ç¤ºä¾‹çš„æ•°æ®é›†ï¼Œä¸´åºŠåŒ»ç”Ÿåœ¨å…¶ä¸­å®Œå–„äº†æ³•å­¦ç¡•å£«èµ·è‰çš„è§„åˆ™ï¼Œä»¥æ»¡è¶³ä¸¥æ ¼çš„åŒ»ç–—æ ‡å‡†ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†è¿™äº›å‡†åˆ™æç‚¼ä¸º HealthPrinciplesï¼š119 ä¸ªæŒ‰ä¸´åºŠç»´åº¦ç»„ç»‡çš„å¯å¹¿æ³›é‡å¤ä½¿ç”¨ã€åŸºäºä¸´åºŠçš„åŸåˆ™ï¼Œä»è€Œå®ç°è¶…è¶Šæ‰‹åŠ¨æ³¨é‡Šçš„å¯æ‰©å±•ç›‘ç£ã€‚æˆ‘ä»¬ä½¿ç”¨ HealthPrinciples æ¥å®ç° (1) é€šè¿‡åˆæˆæœªæ ‡è®°æŸ¥è¯¢çš„é‡è§„è¿›è¡Œç¦»çº¿å¯¹é½ï¼Œä»¥åŠ (2) ç”¨äºæŒ‡å¯¼è‡ªæˆ‘ä¿®è®¢çš„æ¨ç†æ—¶é—´å·¥å…·ã€‚ä½¿ç”¨æˆ‘ä»¬çš„æ¡†æ¶è®­ç»ƒçš„ 30B-A3B æ¨¡å‹åœ¨ HealthBench-Hard ä¸Šè¾¾åˆ°äº† 33.4%ï¼Œä¼˜äºåŒ…æ‹¬ Deepseek-R1 å’Œ o3 åœ¨å†…çš„æ›´å¤§æ¨¡å‹ï¼Œä¸ºä¸´åºŠå¯¹é½å»ºç«‹äº†èµ„æºé«˜æ•ˆçš„åŸºçº¿ã€‚

</details>

---

## 199. Rethinking Security of Diffusion-based Generative Steganography

**ä¸­æ–‡æ ‡é¢˜**: é‡æ–°æ€è€ƒåŸºäºæ‰©æ•£çš„ç”Ÿæˆéšå†™æœ¯çš„å®‰å…¨æ€§

**Date**: 2026-02-10 | **arXiv**: [2602.10219v1](http://arxiv.org/abs/2602.10219v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10219v1)

<details><summary><b>Abstract</b></summary>

Generative image steganography is a technique that conceals secret messages within generated images, without relying on pre-existing cover images. Recently, a number of diffusion model-based generative image steganography (DM-GIS) methods have been introduced, which effectively combat traditional steganalysis techniques. In this paper, we identify the key factors that influence DM-GIS security and revisit the security of existing methods. Specifically, we first provide an overview of the general pipelines of current DM-GIS methods, finding that the noise space of diffusion models serves as the primary embedding domain. Further, we analyze the relationship between DM-GIS security and noise distribution of diffusion models, theoretically demonstrating that any steganographic operation that disrupts the noise distribution compromise DM-GIS security. Building on this insight, we propose a Noise Space-based Diffusion Steganalyzer (NS-DSer)-a simple yet effective steganalysis framework allowing for detecting DM-GIS generated images in the diffusion model noise space. We reevaluate the security of existing DM-GIS methods using NS-DSer across increasingly challenging detection scenarios. Experimental results validate our theoretical analysis of DM-GIS security and show the effectiveness of NS-DSer across diverse detection scenarios.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç”Ÿæˆå›¾åƒéšå†™æœ¯æ˜¯ä¸€ç§åœ¨ç”Ÿæˆçš„å›¾åƒä¸­éšè—ç§˜å¯†æ¶ˆæ¯çš„æŠ€æœ¯ï¼Œè€Œä¸ä¾èµ–äºé¢„å…ˆå­˜åœ¨çš„å°é¢å›¾åƒã€‚æœ€è¿‘ï¼Œä¸€äº›åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå›¾åƒéšå†™æœ¯ï¼ˆDM-GISï¼‰æ–¹æ³•è¢«å¼•å…¥ï¼Œæœ‰æ•ˆå¯¹æŠ—ä¼ ç»Ÿçš„éšå†™åˆ†ææŠ€æœ¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç¡®å®šäº†å½±å“ DM-GIS å®‰å…¨æ€§çš„å…³é”®å› ç´ ï¼Œå¹¶é‡æ–°å®¡è§†äº†ç°æœ‰æ–¹æ³•çš„å®‰å…¨æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæ¦‚è¿°äº†å½“å‰ DM-GIS æ–¹æ³•çš„ä¸€èˆ¬æµç¨‹ï¼Œå‘ç°æ‰©æ•£æ¨¡å‹çš„å™ªå£°ç©ºé—´ä½œä¸ºä¸»è¦åµŒå…¥åŸŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ†æäº† DM-GIS å®‰å…¨æ€§ä¸æ‰©æ•£æ¨¡å‹å™ªå£°åˆ†å¸ƒä¹‹é—´çš„å…³ç³»ï¼Œä»ç†è®ºä¸Šè¯æ˜äº†ä»»ä½•ç ´åå™ªå£°åˆ†å¸ƒçš„éšå†™æ“ä½œéƒ½ä¼šæŸå®³ DM-GIS å®‰å…¨æ€§ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå™ªå£°ç©ºé—´çš„æ‰©æ•£éšå†™åˆ†æå™¨ï¼ˆNS-DSerï¼‰â€”â€”ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„éšå†™åˆ†ææ¡†æ¶ï¼Œå…è®¸åœ¨æ‰©æ•£æ¨¡å‹å™ªå£°ç©ºé—´ä¸­æ£€æµ‹ DM-GIS ç”Ÿæˆçš„å›¾åƒã€‚æˆ‘ä»¬åœ¨æ—¥ç›Šå…·æœ‰æŒ‘æˆ˜æ€§çš„æ£€æµ‹åœºæ™¯ä¸­ä½¿ç”¨ NS-DSer é‡æ–°è¯„ä¼°ç°æœ‰ DM-GIS æ–¹æ³•çš„å®‰å…¨æ€§ã€‚å®éªŒç»“æœéªŒè¯äº†æˆ‘ä»¬å¯¹ DM-GIS å®‰å…¨æ€§çš„ç†è®ºåˆ†æï¼Œå¹¶æ˜¾ç¤ºäº† NS-DSer åœ¨ä¸åŒæ£€æµ‹åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚

</details>

---

## 200. Solving Geodesic Equations with Composite Bernstein Polynomials for Trajectory Planning

**ä¸­æ–‡æ ‡é¢˜**: ç”¨å¤åˆä¼¯æ©æ–¯å¦å¤šé¡¹å¼æ±‚è§£æµ‹åœ°çº¿æ–¹ç¨‹ä»¥è¿›è¡Œè½¨è¿¹è§„åˆ’

**Date**: 2026-02-10 | **arXiv**: [2602.10365v1](http://arxiv.org/abs/2602.10365v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10365v1)

<details><summary><b>Abstract</b></summary>

This work presents a trajectory planning method based on composite Bernstein polynomials for autonomous systems navigating complex environments. The method is implemented in a symbolic optimization framework that enables continuous paths and precise control over trajectory shape. Trajectories are planned over a cost surface that encodes obstacles as continuous fields rather than discrete boundaries. Regions near obstacles are assigned higher costs, naturally encouraging the trajectory to maintain a safe distance while still allowing efficient routing through constrained spaces. The use of composite Bernstein polynomials preserves continuity while enabling fine control over local curvature to satisfy geodesic constraints. The symbolic representation supports exact derivatives, improving optimization efficiency. The method applies to both two- and three-dimensional environments and is suitable for ground, aerial, underwater, and space systems. In spacecraft trajectory planning, for example, it enables the generation of continuous, dynamically feasible trajectories with high numerical efficiency, making it well suited for orbital maneuvers, rendezvous and proximity operations, cluttered gravitational environments, and planetary exploration missions with limited onboard computational resources. Demonstrations show that the approach efficiently generates smooth, collision-free paths in scenarios with multiple obstacles, maintaining clearance without extensive sampling or post-processing. The optimization incorporates three constraint types: (1) a Gaussian surface inequality enforcing minimum obstacle clearance; (2) geodesic equations guiding the path along locally efficient directions on the cost surface; and (3) boundary constraints enforcing fixed start and end conditions. The method can serve as a standalone planner or as an initializer for more complex motion planning problems.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ç§åŸºäºå¤åˆä¼¯æ©æ–¯å¦å¤šé¡¹å¼çš„è½¨è¿¹è§„åˆ’æ–¹æ³•ï¼Œç”¨äºå¯¼èˆªå¤æ‚ç¯å¢ƒçš„è‡ªä¸»ç³»ç»Ÿã€‚è¯¥æ–¹æ³•åœ¨ç¬¦å·ä¼˜åŒ–æ¡†æ¶ä¸­å®ç°ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå®ç°è¿ç»­è·¯å¾„å’Œå¯¹è½¨è¿¹å½¢çŠ¶çš„ç²¾ç¡®æ§åˆ¶ã€‚è½¨è¿¹æ˜¯åœ¨æˆæœ¬é¢ä¸Šè§„åˆ’çš„ï¼Œè¯¥æˆæœ¬é¢å°†éšœç¢ç‰©ç¼–ç ä¸ºè¿ç»­åœºè€Œä¸æ˜¯ç¦»æ•£è¾¹ç•Œã€‚é è¿‘éšœç¢ç‰©çš„åŒºåŸŸè¢«åˆ†é…æ›´é«˜çš„æˆæœ¬ï¼Œè‡ªç„¶ä¼šé¼“åŠ±è½¨è¿¹ä¿æŒå®‰å…¨è·ç¦»ï¼ŒåŒæ—¶ä»ç„¶å…è®¸é€šè¿‡å—é™ç©ºé—´è¿›è¡Œæœ‰æ•ˆè·¯ç”±ã€‚å¤åˆä¼¯æ©æ–¯å¦å¤šé¡¹å¼çš„ä½¿ç”¨ä¿ç•™äº†è¿ç»­æ€§ï¼ŒåŒæ—¶èƒ½å¤Ÿå¯¹å±€éƒ¨æ›²ç‡è¿›è¡Œç²¾ç»†æ§åˆ¶ä»¥æ»¡è¶³æµ‹åœ°çº¿çº¦æŸã€‚ç¬¦å·è¡¨ç¤ºæ”¯æŒç²¾ç¡®å¯¼æ•°ï¼Œæé«˜ä¼˜åŒ–æ•ˆç‡ã€‚è¯¥æ–¹æ³•é€‚ç”¨äºäºŒç»´å’Œä¸‰ç»´ç¯å¢ƒï¼Œé€‚ç”¨äºåœ°é¢ã€ç©ºä¸­ã€æ°´ä¸‹å’Œç©ºé—´ç³»ç»Ÿã€‚ä¾‹å¦‚ï¼Œåœ¨èˆªå¤©å™¨è½¨è¿¹è§„åˆ’ä¸­ï¼Œå®ƒèƒ½å¤Ÿä»¥é«˜æ•°å€¼æ•ˆç‡ç”Ÿæˆè¿ç»­ã€åŠ¨æ€å¯è¡Œçš„è½¨è¿¹ï¼Œä½¿å…¶éå¸¸é€‚åˆè½¨é“æœºåŠ¨ã€äº¤ä¼šå’Œé‚»è¿‘æ“ä½œã€æ‚ä¹±çš„å¼•åŠ›ç¯å¢ƒä»¥åŠæœºè½½è®¡ç®—èµ„æºæœ‰é™çš„è¡Œæ˜Ÿæ¢ç´¢ä»»åŠ¡ã€‚æ¼”ç¤ºè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœ‰å¤šä¸ªéšœç¢ç‰©çš„æƒ…å†µä¸‹æœ‰æ•ˆç”Ÿæˆå¹³æ»‘ã€æ— ç¢°æ’çš„è·¯å¾„ï¼Œæ— éœ€å¤§é‡é‡‡æ ·æˆ–åå¤„ç†å³å¯ä¿æŒé—´éš™ã€‚è¯¥ä¼˜åŒ–åŒ…å«ä¸‰ç§çº¦æŸç±»å‹ï¼šï¼ˆ1ï¼‰å¼ºåˆ¶æ‰§è¡Œæœ€å°éšœç¢ç‰©é—´éš™çš„é«˜æ–¯è¡¨é¢ä¸ç­‰å¼ï¼› (2) æµ‹åœ°æ–¹ç¨‹å¼•å¯¼è·¯å¾„æ²¿ç€æˆæœ¬é¢ä¸Šçš„å±€éƒ¨æœ‰æ•ˆæ–¹å‘ï¼› (3) å¼ºåˆ¶æ‰§è¡Œå›ºå®šå¼€å§‹å’Œç»“æŸæ¡ä»¶çš„è¾¹ç•Œçº¦æŸã€‚è¯¥æ–¹æ³•å¯ä»¥ç”¨ä½œç‹¬ç«‹çš„è§„åˆ’å™¨æˆ–ä½œä¸ºæ›´å¤æ‚çš„è¿åŠ¨è§„åˆ’é—®é¢˜çš„åˆå§‹åŒ–å™¨ã€‚

</details>

---

## 201. Adaptive Time Step Flow Matching for Autonomous Driving Motion Planning

**ä¸­æ–‡æ ‡é¢˜**: è‡ªåŠ¨é©¾é©¶è¿åŠ¨è§„åˆ’çš„è‡ªé€‚åº”æ—¶é—´æ­¥æµåŒ¹é…

**Date**: 2026-02-10 | **arXiv**: [2602.10285v1](http://arxiv.org/abs/2602.10285v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10285v1)

<details><summary><b>Abstract</b></summary>

Autonomous driving requires reasoning about interactions with surrounding traffic. A prevailing approach is large-scale imitation learning on expert driving datasets, aimed at generalizing across diverse real-world scenarios. For online trajectory generation, such methods must operate at real-time rates. Diffusion models require hundreds of denoising steps at inference, resulting in high latency. Consistency models mitigate this issue but rely on carefully tuned noise schedules to capture the multimodal action distributions common in autonomous driving. Adapting the schedule, typically requires expensive retraining. To address these limitations, we propose a framework based on conditional flow matching that jointly predicts future motions of surrounding agents and plans the ego trajectory in real time. We train a lightweight variance estimator that selects the number of inference steps online, removing the need for retraining to balance runtime and imitation learning performance. To further enhance ride quality, we introduce a trajectory post-processing step cast as a convex quadratic program, with negligible computational overhead. Trained on the Waymo Open Motion Dataset, the framework performs maneuvers such as lane changes, cruise control, and navigating unprotected left turns without requiring scenario-specific tuning. Our method maintains a 20 Hz update rate on an NVIDIA RTX 3070 GPU, making it suitable for online deployment. Compared to transformer, diffusion, and consistency model baselines, we achieve improved trajectory smoothness and better adherence to dynamic constraints. Experiment videos and code implementations can be found at https://flow-matching-self-driving.github.io/.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è‡ªåŠ¨é©¾é©¶éœ€è¦æ¨ç†ä¸å‘¨å›´äº¤é€šçš„äº¤äº’ã€‚ä¸€ç§æµè¡Œçš„æ–¹æ³•æ˜¯å¯¹ä¸“å®¶é©¾é©¶æ•°æ®é›†è¿›è¡Œå¤§è§„æ¨¡æ¨¡ä»¿å­¦ä¹ ï¼Œæ—¨åœ¨æ³›åŒ–ä¸åŒçš„ç°å®åœºæ™¯ã€‚å¯¹äºåœ¨çº¿è½¨è¿¹ç”Ÿæˆï¼Œæ­¤ç±»æ–¹æ³•å¿…é¡»ä»¥å®æ—¶é€Ÿç‡è¿è¡Œã€‚æ‰©æ•£æ¨¡å‹åœ¨æ¨ç†æ—¶éœ€è¦æ•°ç™¾ä¸ªå»å™ªæ­¥éª¤ï¼Œå¯¼è‡´é«˜å»¶è¿Ÿã€‚ä¸€è‡´æ€§æ¨¡å‹ç¼“è§£äº†è¿™ä¸ªé—®é¢˜ï¼Œä½†ä¾èµ–äºä»”ç»†è°ƒæ•´çš„å™ªå£°è®¡åˆ’æ¥æ•è·è‡ªåŠ¨é©¾é©¶ä¸­å¸¸è§çš„å¤šæ¨¡å¼åŠ¨ä½œåˆ†å¸ƒã€‚è°ƒæ•´æ—¶é—´è¡¨é€šå¸¸éœ€è¦æ˜‚è´µçš„å†åŸ¹è®­ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºæ¡ä»¶æµåŒ¹é…çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…±åŒé¢„æµ‹å‘¨å›´æ™ºèƒ½ä½“çš„æœªæ¥è¿åŠ¨å¹¶å®æ—¶è§„åˆ’è‡ªæˆ‘è½¨è¿¹ã€‚æˆ‘ä»¬è®­ç»ƒä¸€ä¸ªè½»é‡çº§æ–¹å·®ä¼°è®¡å™¨ï¼Œå®ƒåœ¨çº¿é€‰æ‹©æ¨ç†æ­¥éª¤çš„æ•°é‡ï¼Œä»è€Œæ— éœ€é‡æ–°è®­ç»ƒæ¥å¹³è¡¡è¿è¡Œæ—¶å’Œæ¨¡ä»¿å­¦ä¹ æ€§èƒ½ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜ä¹˜åè´¨é‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†è½¨è¿¹åå¤„ç†æ­¥éª¤ï¼Œå°†å…¶è½¬æ¢ä¸ºå‡¸äºŒæ¬¡ç¨‹åºï¼Œè®¡ç®—å¼€é”€å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚è¯¥æ¡†æ¶åœ¨ Waymo å¼€æ”¾è¿åŠ¨æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥æ‰§è¡Œå˜é“ã€å·¡èˆªæ§åˆ¶å’Œå¯¼èˆªæ— ä¿æŠ¤å·¦è½¬ç­‰æ“ä½œï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šåœºæ™¯è¿›è¡Œè°ƒæ•´ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ NVIDIA RTX 3070 GPU ä¸Šä¿æŒ 20 Hz çš„æ›´æ–°ç‡ï¼Œä½¿å…¶é€‚åˆåœ¨çº¿éƒ¨ç½²ã€‚ä¸å˜å‹å™¨ã€æ‰©æ•£å’Œä¸€è‡´æ€§æ¨¡å‹åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬å®ç°äº†æ”¹è¿›çš„è½¨è¿¹å¹³æ»‘åº¦å’Œæ›´å¥½åœ°éµå®ˆåŠ¨æ€çº¦æŸã€‚å®éªŒè§†é¢‘å’Œä»£ç å®ç°å¯ä»¥åœ¨ https://flow-matching-self-driven.github.io/ æ‰¾åˆ°ã€‚

</details>

---

## 202. Decoupled MPPI-Based Multi-Arm Motion Planning

**ä¸­æ–‡æ ‡é¢˜**: åŸºäº MPPI çš„è§£è€¦å¤šè‡‚è¿åŠ¨è§„åˆ’

**Date**: 2026-02-10 | **arXiv**: [2602.10114v1](http://arxiv.org/abs/2602.10114v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10114v1)

<details><summary><b>Abstract</b></summary>

Recent advances in sampling-based motion planning algorithms for high DOF arms leverage GPUs to provide SOTA performance. These algorithms can be used to control multiple arms jointly, but this approach scales poorly. To address this, we extend STORM, a sampling-based model-predictive-control (MPC) motion planning algorithm, to handle multiple robots in a distributed fashion. First, we modify STORM to handle dynamic obstacles. Then, we let each arm compute its own motion plan prefix, which it shares with the other arms, which treat it as a dynamic obstacle. Finally, we add a dynamic priority scheme. The new algorithm, MR-STORM, demonstrates clear empirical advantages over SOTA algorithms when operating with both static and dynamic obstacles.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

é«˜è‡ªç”±åº¦è‡‚åŸºäºé‡‡æ ·çš„è¿åŠ¨è§„åˆ’ç®—æ³•çš„æœ€æ–°è¿›å±•åˆ©ç”¨ GPU æä¾› SOTA æ€§èƒ½ã€‚è¿™äº›ç®—æ³•å¯ç”¨äºè”åˆæ§åˆ¶å¤šä¸ªæ‰‹è‡‚ï¼Œä½†è¿™ç§æ–¹æ³•çš„æ‰©å±•æ€§å¾ˆå·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ‰©å±•äº† STORMï¼Œä¸€ç§åŸºäºé‡‡æ ·çš„æ¨¡å‹é¢„æµ‹æ§åˆ¶ (MPC) è¿åŠ¨è§„åˆ’ç®—æ³•ï¼Œä»¥åˆ†å¸ƒå¼æ–¹å¼å¤„ç†å¤šä¸ªæœºå™¨äººã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä¿®æ”¹ STORM ä»¥å¤„ç†åŠ¨æ€éšœç¢ã€‚ç„¶åï¼Œæˆ‘ä»¬è®©æ¯ä¸ªæ‰‹è‡‚è®¡ç®—è‡ªå·±çš„è¿åŠ¨è®¡åˆ’å‰ç¼€ï¼Œå¹¶ä¸å…¶ä»–æ‰‹è‡‚å…±äº«è¯¥å‰ç¼€ï¼Œå…¶ä»–æ‰‹è‡‚å°†å…¶è§†ä¸ºåŠ¨æ€éšœç¢ç‰©ã€‚æœ€åï¼Œæˆ‘ä»¬æ·»åŠ ä¸€ä¸ªåŠ¨æ€ä¼˜å…ˆçº§æ–¹æ¡ˆã€‚æ–°ç®—æ³• MR-STORM åœ¨å¤„ç†é™æ€å’ŒåŠ¨æ€éšœç¢ç‰©æ—¶è¡¨ç°å‡ºæ˜æ˜¾ä¼˜äº SOTA ç®—æ³•çš„ç»éªŒä¼˜åŠ¿ã€‚

</details>

---

## 203. ST4VLA: Spatially Guided Training for Vision-Language-Action Models

**ä¸­æ–‡æ ‡é¢˜**: ST4VLAï¼šè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„ç©ºé—´å¼•å¯¼è®­ç»ƒ

**Date**: 2026-02-10 | **arXiv**: [2602.10109v1](http://arxiv.org/abs/2602.10109v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10109v1)

<details><summary><b>Abstract</b></summary>

Large vision-language models (VLMs) excel at multimodal understanding but fall short when extended to embodied tasks, where instructions must be transformed into low-level motor actions. We introduce ST4VLA, a dual-system Vision-Language-Action framework that leverages Spatial Guided Training to align action learning with spatial priors in VLMs. ST4VLA includes two stages: (i) spatial grounding pre-training, which equips the VLM with transferable priors via scalable point, box, and trajectory prediction from both web-scale and robot-specific data, and (ii) spatially guided action post-training, which encourages the model to produce richer spatial priors to guide action generation via spatial prompting. This design preserves spatial grounding during policy learning and promotes consistent optimization across spatial and action objectives. Empirically, ST4VLA achieves substantial improvements over vanilla VLA, with performance increasing from 66.1 -> 84.6 on Google Robot and from 54.7 -> 73.2 on WidowX Robot, establishing new state-of-the-art results on SimplerEnv. It also demonstrates stronger generalization to unseen objects and paraphrased instructions, as well as robustness to long-horizon perturbations in real-world settings. These results highlight scalable spatially guided training as a promising direction for robust, generalizable robot learning. Source code, data and models are released at https://internrobotics.github.io/internvla-m1.github.io/

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ“…é•¿å¤šæ¨¡æ€ç†è§£ï¼Œä½†åœ¨æ‰©å±•åˆ°å…·ä½“ä»»åŠ¡æ—¶å´è¡¨ç°ä¸ä½³ï¼Œåœ¨å…·ä½“ä»»åŠ¡ä¸­æŒ‡ä»¤å¿…é¡»è½¬æ¢ä¸ºä½çº§è¿åŠ¨åŠ¨ä½œã€‚æˆ‘ä»¬å¼•å…¥äº† ST4VLAï¼Œè¿™æ˜¯ä¸€ç§åŒç³»ç»Ÿè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ç©ºé—´å¼•å¯¼è®­ç»ƒå°†åŠ¨ä½œå­¦ä¹ ä¸ VLM ä¸­çš„ç©ºé—´å…ˆéªŒä¿æŒä¸€è‡´ã€‚ ST4VLA åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼š(i) ç©ºé—´åŸºç¡€é¢„è®­ç»ƒï¼Œé€šè¿‡æ¥è‡ªç½‘ç»œè§„æ¨¡å’Œæœºå™¨äººç‰¹å®šæ•°æ®çš„å¯æ‰©å±•ç‚¹ã€æ¡†å’Œè½¨è¿¹é¢„æµ‹ï¼Œä¸º VLM é…å¤‡å¯è½¬ç§»çš„å…ˆéªŒï¼›(ii) ç©ºé—´å¼•å¯¼åŠ¨ä½œåè®­ç»ƒï¼Œé¼“åŠ±æ¨¡å‹äº§ç”Ÿæ›´ä¸°å¯Œçš„ç©ºé—´å…ˆéªŒï¼Œä»¥é€šè¿‡ç©ºé—´æç¤ºæŒ‡å¯¼åŠ¨ä½œç”Ÿæˆã€‚è¿™ç§è®¾è®¡åœ¨æ”¿ç­–å­¦ä¹ æœŸé—´ä¿ç•™äº†ç©ºé—´åŸºç¡€ï¼Œå¹¶ä¿ƒè¿›ç©ºé—´å’Œè¡ŒåŠ¨ç›®æ ‡çš„ä¸€è‡´ä¼˜åŒ–ã€‚æ ¹æ®ç»éªŒï¼ŒST4VLA æ¯”æ™®é€š VLA å–å¾—äº†å®è´¨æ€§æ”¹è¿›ï¼ŒGoogle Robot ä¸Šçš„æ€§èƒ½ä» 66.1 -> 84.6 å¢åŠ ï¼ŒWidowX Robot ä¸Šçš„æ€§èƒ½ä» 54.7 -> 73.2 å¢åŠ ï¼Œåœ¨ SimplerEnv ä¸Šå»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›ç»“æœã€‚å®ƒè¿˜å±•ç¤ºäº†å¯¹çœ‹ä¸è§çš„ç‰©ä½“å’Œé‡Šä¹‰æŒ‡ä»¤çš„æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œä»¥åŠå¯¹ç°å®ä¸–ç•Œç¯å¢ƒä¸­çš„é•¿è§†é‡æ‰°åŠ¨çš„é²æ£’æ€§ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†å¯æ‰©å±•çš„ç©ºé—´å¼•å¯¼è®­ç»ƒæ˜¯ç¨³å¥ã€å¯æ¨å¹¿çš„æœºå™¨äººå­¦ä¹ çš„ä¸€ä¸ªæœ‰å‰é€”çš„æ–¹å‘ã€‚æºä»£ç ã€æ•°æ®å’Œæ¨¡å‹å‘å¸ƒäº https://internrobotics.github.io/internvla-m1.github.io/

</details>

---

## 204. EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration

**ä¸­æ–‡æ ‡é¢˜**: EgoHumanoidï¼šé€šè¿‡æ— æœºå™¨äººçš„è‡ªæˆ‘ä¸­å¿ƒæ¼”ç¤ºè§£é”é‡å¤–å±€éƒ¨æ“ä½œ

**Date**: 2026-02-10 | **arXiv**: [2602.10106v1](http://arxiv.org/abs/2602.10106v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10106v1)

<details><summary><b>Abstract</b></summary>

Human demonstrations offer rich environmental diversity and scale naturally, making them an appealing alternative to robot teleoperation. While this paradigm has advanced robot-arm manipulation, its potential for the more challenging, data-hungry problem of humanoid loco-manipulation remains largely unexplored. We present EgoHumanoid, the first framework to co-train a vision-language-action policy using abundant egocentric human demonstrations together with a limited amount of robot data, enabling humanoids to perform loco-manipulation across diverse real-world environments. To bridge the embodiment gap between humans and robots, including discrepancies in physical morphology and viewpoint, we introduce a systematic alignment pipeline spanning from hardware design to data processing. A portable system for scalable human data collection is developed, and we establish practical collection protocols to improve transferability. At the core of our human-to-humanoid alignment pipeline lies two key components. The view alignment reduces visual domain discrepancies caused by camera height and perspective variation. The action alignment maps human motions into a unified, kinematically feasible action space for humanoid control. Extensive real-world experiments demonstrate that incorporating robot-free egocentric data significantly outperforms robot-only baselines by 51\%, particularly in unseen environments. Our analysis further reveals which behaviors transfer effectively and the potential for scaling human data.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

äººç±»æ¼”ç¤ºè‡ªç„¶åœ°æä¾›äº†ä¸°å¯Œçš„ç¯å¢ƒå¤šæ ·æ€§å’Œè§„æ¨¡ï¼Œä½¿å…¶æˆä¸ºæœºå™¨äººè¿œç¨‹æ“ä½œçš„æœ‰å¸å¼•åŠ›çš„æ›¿ä»£æ–¹æ¡ˆã€‚è™½ç„¶è¿™ç§èŒƒä¾‹å…·æœ‰å…ˆè¿›çš„æœºå™¨äººæ‰‹è‡‚æ“çºµï¼Œä½†å…¶è§£å†³æ›´å…·æŒ‘æˆ˜æ€§ã€éœ€è¦æ•°æ®çš„äººå½¢æœºå™¨äººæ“çºµé—®é¢˜çš„æ½œåŠ›åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä»æœªå¾—åˆ°æ¢ç´¢ã€‚æˆ‘ä»¬æå‡ºäº† EgoHumanoidï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä½¿ç”¨ä¸°å¯Œçš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„äººç±»æ¼”ç¤ºå’Œæœ‰é™çš„æœºå™¨äººæ•°æ®æ¥å…±åŒè®­ç»ƒè§†è§‰-è¯­è¨€-åŠ¨ä½œç­–ç•¥çš„æ¡†æ¶ï¼Œä½¿ç±»äººæœºå™¨äººèƒ½å¤Ÿåœ¨ä¸åŒçš„ç°å®ä¸–ç•Œç¯å¢ƒä¸­æ‰§è¡Œå±€éƒ¨æ“ä½œã€‚ä¸ºäº†å¼¥åˆäººç±»å’Œæœºå™¨äººä¹‹é—´çš„ä½“ç°å·®è·ï¼ŒåŒ…æ‹¬ç‰©ç†å½¢æ€å’Œè§‚ç‚¹çš„å·®å¼‚ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä»ç¡¬ä»¶è®¾è®¡åˆ°æ•°æ®å¤„ç†çš„ç³»ç»Ÿå¯¹å‡†ç®¡é“ã€‚å¼€å‘äº†ä¸€ç§ç”¨äºå¯æ‰©å±•äººç±»æ•°æ®æ”¶é›†çš„ä¾¿æºå¼ç³»ç»Ÿï¼Œå¹¶ä¸”æˆ‘ä»¬å»ºç«‹äº†å®ç”¨çš„æ”¶é›†åè®®ä»¥æé«˜å¯è½¬ç§»æ€§ã€‚æˆ‘ä»¬çš„äººä¸äººä¹‹é—´çš„å¯¹é½æµç¨‹çš„æ ¸å¿ƒæ˜¯ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚è§†å›¾å¯¹é½å‡å°‘äº†ç”±ç›¸æœºé«˜åº¦å’Œé€è§†å˜åŒ–å¼•èµ·çš„è§†åŸŸå·®å¼‚ã€‚åŠ¨ä½œå¯¹é½å°†äººä½“è¿åŠ¨æ˜ å°„åˆ°ä¸€ä¸ªç»Ÿä¸€çš„ã€è¿åŠ¨å­¦ä¸Šå¯è¡Œçš„åŠ¨ä½œç©ºé—´ä¸­ï¼Œç”¨äºäººå½¢æ§åˆ¶ã€‚å¹¿æ³›çš„ç°å®ä¸–ç•Œå®éªŒè¡¨æ˜ï¼Œåˆå¹¶æ— æœºå™¨äººçš„è‡ªæˆ‘ä¸­å¿ƒæ•°æ®æ˜¾ç€ä¼˜äºä»…æœºå™¨äººçš„åŸºçº¿ 51%ï¼Œç‰¹åˆ«æ˜¯åœ¨çœ‹ä¸è§çš„ç¯å¢ƒä¸­ã€‚æˆ‘ä»¬çš„åˆ†æè¿›ä¸€æ­¥æ­ç¤ºäº†å“ªäº›è¡Œä¸ºå¯ä»¥æœ‰æ•ˆè½¬ç§»ä»¥åŠæ‰©å±•äººç±»æ•°æ®çš„æ½œåŠ›ã€‚

</details>

---

## 205. DexImit: Learning Bimanual Dexterous Manipulation from Monocular Human Videos

**ä¸­æ–‡æ ‡é¢˜**: DexImitï¼šä»å•çœ¼äººç±»è§†é¢‘ä¸­å­¦ä¹ åŒæ‰‹çµå·§æ“ä½œ

**Date**: 2026-02-10 | **arXiv**: [2602.10105v1](http://arxiv.org/abs/2602.10105v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10105v1)

<details><summary><b>Abstract</b></summary>

Data scarcity fundamentally limits the generalization of bimanual dexterous manipulation, as real-world data collection for dexterous hands is expensive and labor-intensive. Human manipulation videos, as a direct carrier of manipulation knowledge, offer significant potential for scaling up robot learning. However, the substantial embodiment gap between human hands and robotic dexterous hands makes direct pretraining from human videos extremely challenging. To bridge this gap and unleash the potential of large-scale human manipulation video data, we propose DexImit, an automated framework that converts monocular human manipulation videos into physically plausible robot data, without any additional information. DexImit employs a four-stage generation pipeline: (1) reconstructing hand-object interactions from arbitrary viewpoints with near-metric scale; (2) performing subtask decomposition and bimanual scheduling; (3) synthesizing robot trajectories consistent with the demonstrated interactions; (4) comprehensive data augmentation for zero-shot real-world deployment. Building on these designs, DexImit can generate large-scale robot data based on human videos, either from the Internet or video generation models. DexImit is capable of handling diverse manipulation tasks, including tool use (e.g., cutting an apple), long-horizon tasks (e.g., making a beverage), and fine-grained manipulations (e.g., stacking cups).

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æ•°æ®ç¨€ç¼ºä»æ ¹æœ¬ä¸Šé™åˆ¶äº†åŒæ‰‹çµå·§æ“ä½œçš„æ™®åŠï¼Œå› ä¸ºçµå·§æ‰‹çš„ç°å®ä¸–ç•Œæ•°æ®æ”¶é›†æˆæœ¬é«˜æ˜‚ä¸”åŠ³åŠ¨å¯†é›†å‹ã€‚äººç±»æ“ä½œè§†é¢‘ä½œä¸ºæ“ä½œçŸ¥è¯†çš„ç›´æ¥è½½ä½“ï¼Œä¸ºæ‰©å¤§æœºå™¨äººå­¦ä¹ æä¾›äº†å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œäººæ‰‹å’Œæœºå™¨äººçµå·§æ‰‹ä¹‹é—´çš„å·¨å¤§ä½“ç°å·®è·ä½¿å¾—ä»äººç±»è§†é¢‘ç›´æ¥è¿›è¡Œé¢„è®­ç»ƒæå…·æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·å¹¶é‡Šæ”¾å¤§è§„æ¨¡äººç±»æ“çºµè§†é¢‘æ•°æ®çš„æ½œåŠ›ï¼Œæˆ‘ä»¬æå‡ºäº† DexImitï¼Œè¿™æ˜¯ä¸€ç§è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œå¯ä»¥å°†å•çœ¼äººç±»æ“çºµè§†é¢‘è½¬æ¢ä¸ºç‰©ç†ä¸Šåˆç†çš„æœºå™¨äººæ•°æ®ï¼Œè€Œæ— éœ€ä»»ä½•é™„åŠ ä¿¡æ¯ã€‚ DexImit é‡‡ç”¨å››é˜¶æ®µç”Ÿæˆæµç¨‹ï¼šï¼ˆ1ï¼‰ä»ä»»æ„è§’åº¦ä»¥æ¥è¿‘å…¬åˆ¶çš„å°ºåº¦é‡å»ºæ‰‹éƒ¨ä¸ç‰©ä½“çš„äº¤äº’ï¼› (2)è¿›è¡Œå­ä»»åŠ¡åˆ†è§£å’ŒåŒæ‰‹è°ƒåº¦ï¼› (3) åˆæˆä¸æ‰€æ¼”ç¤ºçš„äº¤äº’ä¸€è‡´çš„æœºå™¨äººè½¨è¿¹ï¼› (4) å…¨é¢çš„æ•°æ®å¢å¼ºï¼Œç”¨äºé›¶æ ·æœ¬çš„ç°å®ä¸–ç•Œéƒ¨ç½²ã€‚åŸºäºè¿™äº›è®¾è®¡ï¼ŒDexImit å¯ä»¥æ ¹æ®æ¥è‡ªäº’è”ç½‘æˆ–è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„äººç±»è§†é¢‘ç”Ÿæˆå¤§è§„æ¨¡æœºå™¨äººæ•°æ®ã€‚ DexImit èƒ½å¤Ÿå¤„ç†å„ç§æ“ä½œä»»åŠ¡ï¼ŒåŒ…æ‹¬å·¥å…·ä½¿ç”¨ï¼ˆä¾‹å¦‚åˆ‡è‹¹æœï¼‰ã€é•¿æœŸä»»åŠ¡ï¼ˆä¾‹å¦‚åˆ¶ä½œé¥®æ–™ï¼‰å’Œç»†ç²’åº¦æ“ä½œï¼ˆä¾‹å¦‚å †å æ¯å­ï¼‰ã€‚

</details>

---

## 206. Robo3R: Enhancing Robotic Manipulation with Accurate Feed-Forward 3D Reconstruction

**ä¸­æ–‡æ ‡é¢˜**: Robo3Rï¼šé€šè¿‡ç²¾ç¡®çš„å‰é¦ˆ 3D é‡å»ºå¢å¼ºæœºå™¨äººæ“ä½œ

**Date**: 2026-02-10 | **arXiv**: [2602.10101v1](http://arxiv.org/abs/2602.10101v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10101v1)

<details><summary><b>Abstract</b></summary>

3D spatial perception is fundamental to generalizable robotic manipulation, yet obtaining reliable, high-quality 3D geometry remains challenging. Depth sensors suffer from noise and material sensitivity, while existing reconstruction models lack the precision and metric consistency required for physical interaction. We introduce Robo3R, a feed-forward, manipulation-ready 3D reconstruction model that predicts accurate, metric-scale scene geometry directly from RGB images and robot states in real time. Robo3R jointly infers scale-invariant local geometry and relative camera poses, which are unified into the scene representation in the canonical robot frame via a learned global similarity transformation. To meet the precision demands of manipulation, Robo3R employs a masked point head for sharp, fine-grained point clouds, and a keypoint-based Perspective-n-Point (PnP) formulation to refine camera extrinsics and global alignment. Trained on Robo3R-4M, a curated large-scale synthetic dataset with four million high-fidelity annotated frames, Robo3R consistently outperforms state-of-the-art reconstruction methods and depth sensors. Across downstream tasks including imitation learning, sim-to-real transfer, grasp synthesis, and collision-free motion planning, we observe consistent gains in performance, suggesting the promise of this alternative 3D sensing module for robotic manipulation.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

3D ç©ºé—´æ„ŸçŸ¥æ˜¯é€šç”¨æœºå™¨äººæ“ä½œçš„åŸºç¡€ï¼Œä½†è·å¾—å¯é ã€é«˜è´¨é‡çš„ 3D å‡ ä½•å½¢çŠ¶ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æ·±åº¦ä¼ æ„Ÿå™¨å—åˆ°å™ªå£°å’Œææ–™æ•æ„Ÿæ€§çš„å½±å“ï¼Œè€Œç°æœ‰çš„é‡å»ºæ¨¡å‹ç¼ºä¹ç‰©ç†äº¤äº’æ‰€éœ€çš„ç²¾åº¦å’Œåº¦é‡ä¸€è‡´æ€§ã€‚æˆ‘ä»¬æ¨å‡ºäº† Robo3Rï¼Œè¿™æ˜¯ä¸€ç§å‰é¦ˆã€å¯æ“ä½œçš„ 3D é‡å»ºæ¨¡å‹ï¼Œå¯ç›´æ¥æ ¹æ® RGB å›¾åƒå’Œæœºå™¨äººçŠ¶æ€å®æ—¶é¢„æµ‹å‡†ç¡®çš„å…¬åˆ¶å°ºåº¦åœºæ™¯å‡ ä½•å½¢çŠ¶ã€‚ Robo3R è”åˆæ¨æ–­å°ºåº¦ä¸å˜çš„å±€éƒ¨å‡ ä½•å½¢çŠ¶å’Œç›¸å¯¹ç›¸æœºå§¿æ€ï¼Œé€šè¿‡å­¦ä¹ çš„å…¨å±€ç›¸ä¼¼æ€§å˜æ¢å°†å…¶ç»Ÿä¸€åˆ°è§„èŒƒæœºå™¨äººæ¡†æ¶ä¸­çš„åœºæ™¯è¡¨ç¤ºä¸­ã€‚ä¸ºäº†æ»¡è¶³æ“çºµçš„ç²¾åº¦è¦æ±‚ï¼ŒRobo3R é‡‡ç”¨è’™ç‰ˆç‚¹å¤´æ¥å®ç°æ¸…æ™°ã€ç»†ç²’åº¦çš„ç‚¹äº‘ï¼Œå¹¶é‡‡ç”¨åŸºäºå…³é”®ç‚¹çš„é€è§† n ç‚¹ (PnP) å…¬å¼æ¥ç»†åŒ–ç›¸æœºå¤–å‚å’Œå…¨å±€å¯¹é½ã€‚ Robo3R-4M æ˜¯ä¸€ä¸ªç²¾å¿ƒç­–åˆ’çš„å¤§å‹åˆæˆæ•°æ®é›†ï¼Œå…·æœ‰ 400 ä¸‡ä¸ªé«˜ä¿çœŸæ³¨é‡Šå¸§ï¼Œç»è¿‡è®­ç»ƒåï¼ŒRobo3R çš„æ€§èƒ½å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„é‡å»ºæ–¹æ³•å’Œæ·±åº¦ä¼ æ„Ÿå™¨ã€‚åœ¨æ¨¡ä»¿å­¦ä¹ ã€æ¨¡æ‹Ÿåˆ°çœŸå®è¿ç§»ã€æŠ“å–åˆæˆå’Œæ— ç¢°æ’è¿åŠ¨è§„åˆ’ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ€§èƒ½çš„æŒç»­æå‡ï¼Œè¡¨æ˜è¿™ç§æ›¿ä»£ 3D ä¼ æ„Ÿæ¨¡å—åœ¨æœºå™¨äººæ“ä½œæ–¹é¢çš„å‰æ™¯ã€‚

</details>

---

## 207. UniVTAC: A Unified Simulation Platform for Visuo-Tactile Manipulation Data Generation, Learning, and Benchmarking

**ä¸­æ–‡æ ‡é¢˜**: UniVTACï¼šç”¨äºè§†è§‰è§¦è§‰æ“ä½œæ•°æ®ç”Ÿæˆã€å­¦ä¹ å’ŒåŸºå‡†æµ‹è¯•çš„ç»Ÿä¸€ä»¿çœŸå¹³å°

**Date**: 2026-02-10 | **arXiv**: [2602.10093v1](http://arxiv.org/abs/2602.10093v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10093v1)

<details><summary><b>Abstract</b></summary>

Robotic manipulation has seen rapid progress with vision-language-action (VLA) policies. However, visuo-tactile perception is critical for contact-rich manipulation, as tasks such as insertion are difficult to complete robustly using vision alone. At the same time, acquiring large-scale and reliable tactile data in the physical world remains costly and challenging, and the lack of a unified evaluation platform further limits policy learning and systematic analysis. To address these challenges, we propose UniVTAC, a simulation-based visuo-tactile data synthesis platform that supports three commonly used visuo-tactile sensors and enables scalable and controllable generation of informative contact interactions. Based on this platform, we introduce the UniVTAC Encoder, a visuo-tactile encoder trained on large-scale simulation-synthesized data with designed supervisory signals, providing tactile-centric visuo-tactile representations for downstream manipulation tasks. In addition, we present the UniVTAC Benchmark, which consists of eight representative visuo-tactile manipulation tasks for evaluating tactile-driven policies. Experimental results show that integrating the UniVTAC Encoder improves average success rates by 17.1% on the UniVTAC Benchmark, while real-world robotic experiments further demonstrate a 25% improvement in task success. Our webpage is available at https://univtac.github.io/.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

é€šè¿‡è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ”¿ç­–ï¼Œæœºå™¨äººæ“ä½œå–å¾—äº†å¿«é€Ÿè¿›å±•ã€‚ç„¶è€Œï¼Œè§†è§‰è§¦è§‰æ„ŸçŸ¥å¯¹äºå¯Œå«æ¥è§¦çš„æ“ä½œè‡³å…³é‡è¦ï¼Œå› ä¸ºä»…ä½¿ç”¨è§†è§‰å¾ˆéš¾ç¨³å¥åœ°å®Œæˆè¯¸å¦‚æ’å…¥ä¹‹ç±»çš„ä»»åŠ¡ã€‚ä¸æ­¤åŒæ—¶ï¼Œåœ¨ç‰©ç†ä¸–ç•Œä¸­è·å–å¤§è§„æ¨¡ä¸”å¯é çš„è§¦è§‰æ•°æ®ä»ç„¶æˆæœ¬é«˜æ˜‚ä¸”å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè€Œä¸”ç¼ºä¹ç»Ÿä¸€çš„è¯„ä¼°å¹³å°è¿›ä¸€æ­¥é™åˆ¶äº†æ”¿ç­–å­¦ä¹ å’Œç³»ç»Ÿåˆ†æã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº† UniVTACï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ¨¡æ‹Ÿçš„è§†è§‰è§¦è§‰æ•°æ®åˆæˆå¹³å°ï¼Œæ”¯æŒä¸‰ç§å¸¸ç”¨çš„è§†è§‰è§¦è§‰ä¼ æ„Ÿå™¨ï¼Œå¹¶èƒ½å¤Ÿç”Ÿæˆå¯æ‰©å±•ä¸”å¯æ§çš„ä¿¡æ¯æ¥è§¦äº¤äº’ã€‚åŸºäºè¯¥å¹³å°ï¼Œæˆ‘ä»¬æ¨å‡ºäº† UniVTAC ç¼–ç å™¨ï¼Œè¿™æ˜¯ä¸€ç§è§†è§‰è§¦è§‰ç¼–ç å™¨ï¼Œä½¿ç”¨è®¾è®¡çš„ç›‘æ§ä¿¡å·åœ¨å¤§è§„æ¨¡æ¨¡æ‹Ÿåˆæˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä¸ºä¸‹æ¸¸æ“ä½œä»»åŠ¡æä¾›ä»¥è§¦è§‰ä¸ºä¸­å¿ƒçš„è§†è§‰è§¦è§‰è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº† UniVTAC åŸºå‡†ï¼Œå®ƒç”±å…«ä¸ªä»£è¡¨æ€§çš„è§†è§‰è§¦è§‰æ“ä½œä»»åŠ¡ç»„æˆï¼Œç”¨äºè¯„ä¼°è§¦è§‰é©±åŠ¨çš„ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé›†æˆ UniVTAC ç¼–ç å™¨åœ¨ UniVTAC åŸºå‡†ä¸Šå°†å¹³å‡æˆåŠŸç‡æé«˜äº† 17.1%ï¼Œè€Œç°å®ä¸–ç•Œçš„æœºå™¨äººå®éªŒè¿›ä¸€æ­¥è¯æ˜ä»»åŠ¡æˆåŠŸç‡æé«˜äº† 25%ã€‚æˆ‘ä»¬çš„ç½‘é¡µä½äº https://univtac.github.io/ã€‚

</details>

---

## 208. RoboInter: A Holistic Intermediate Representation Suite Towards Robotic Manipulation

**ä¸­æ–‡æ ‡é¢˜**: RoboInterï¼šé¢å‘æœºå™¨äººæ“ä½œçš„æ•´ä½“ä¸­é—´è¡¨ç¤ºå¥—ä»¶

**Date**: 2026-02-10 | **arXiv**: [2602.09973v1](http://arxiv.org/abs/2602.09973v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09973v1)

<details><summary><b>Abstract</b></summary>

Advances in large vision-language models (VLMs) have stimulated growing interest in vision-language-action (VLA) systems for robot manipulation. However, existing manipulation datasets remain costly to curate, highly embodiment-specific, and insufficient in coverage and diversity, thereby hindering the generalization of VLA models. Recent approaches attempt to mitigate these limitations via a plan-then-execute paradigm, where high-level plans (e.g., subtasks, trace) are first generated and subsequently translated into low-level actions, but they critically rely on extra intermediate supervision, which is largely absent from existing datasets. To bridge this gap, we introduce the RoboInter Manipulation Suite, a unified resource including data, benchmarks, and models of intermediate representations for manipulation. It comprises RoboInter-Tool, a lightweight GUI that enables semi-automatic annotation of diverse representations, and RoboInter-Data, a large-scale dataset containing over 230k episodes across 571 diverse scenes, which provides dense per-frame annotations over more than 10 categories of intermediate representations, substantially exceeding prior work in scale and annotation quality. Building upon this foundation, RoboInter-VQA introduces 9 spatial and 20 temporal embodied VQA categories to systematically benchmark and enhance the embodied reasoning capabilities of VLMs. Meanwhile, RoboInter-VLA offers an integrated plan-then-execute framework, supporting modular and end-to-end VLA variants that bridge high-level planning with low-level execution via intermediate supervision. In total, RoboInter establishes a practical foundation for advancing robust and generalizable robotic learning via fine-grained and diverse intermediate representations.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ (VLM) çš„è¿›æ­¥æ¿€å‘äº†äººä»¬å¯¹ç”¨äºæœºå™¨äººæ“ä½œçš„è§†è§‰è¯­è¨€åŠ¨ä½œ (VLA) ç³»ç»Ÿæ—¥ç›Šå¢é•¿çš„å…´è¶£ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ“ä½œæ•°æ®é›†çš„ç®¡ç†æˆæœ¬ä»ç„¶å¾ˆé«˜ï¼Œé«˜åº¦ç‰¹å®šäºå®æ–½ä¾‹ï¼Œå¹¶ä¸”è¦†ç›–èŒƒå›´å’Œå¤šæ ·æ€§ä¸è¶³ï¼Œä»è€Œé˜»ç¢äº† VLA æ¨¡å‹çš„æ³›åŒ–ã€‚æœ€è¿‘çš„æ–¹æ³•è¯•å›¾é€šè¿‡å…ˆè®¡åˆ’åæ‰§è¡Œçš„èŒƒä¾‹æ¥å‡è½»è¿™äº›é™åˆ¶ï¼Œå…¶ä¸­é¦–å…ˆç”Ÿæˆé«˜çº§è®¡åˆ’ï¼ˆä¾‹å¦‚å­ä»»åŠ¡ã€è·Ÿè¸ªï¼‰ï¼Œç„¶åå°†å…¶è½¬æ¢ä¸ºä½çº§æ“ä½œï¼Œä½†å®ƒä»¬ä¸¥é‡ä¾èµ–äºé¢å¤–çš„ä¸­é—´ç›‘ç£ï¼Œè€Œç°æœ‰æ•°æ®é›†ä¸­åŸºæœ¬ä¸Šä¸å­˜åœ¨è¿™ç§ç›‘ç£ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº† RoboInter Manipulation Suiteï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„èµ„æºï¼ŒåŒ…æ‹¬æ•°æ®ã€åŸºå‡†æµ‹è¯•å’Œæ“ä½œä¸­é—´è¡¨ç¤ºæ¨¡å‹ã€‚å®ƒç”± RoboInter-Tool å’Œ RoboInter-Data ç»„æˆï¼ŒRoboInter-Tool æ˜¯ä¸€ä¸ªè½»é‡çº§ GUIï¼Œå¯å¯¹ä¸åŒè¡¨ç¤ºè¿›è¡ŒåŠè‡ªåŠ¨æ³¨é‡Šï¼›RoboInter-Data æ˜¯ä¸€ä¸ªå¤§å‹æ•°æ®é›†ï¼ŒåŒ…å« 571 ä¸ªä¸åŒåœºæ™¯çš„ 23 ä¸‡å¤šä¸ªç‰‡æ®µï¼Œå¯åœ¨ 10 å¤šä¸ªç±»åˆ«çš„ä¸­é—´è¡¨ç¤ºä¸Šæä¾›å¯†é›†çš„æ¯å¸§æ³¨é‡Šï¼Œåœ¨è§„æ¨¡å’Œæ³¨é‡Šè´¨é‡æ–¹é¢å¤§å¤§è¶…è¿‡äº†ä¹‹å‰çš„å·¥ä½œã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼ŒRoboInter-VQA å¼•å…¥äº† 9 ä¸ªç©ºé—´å’Œ 20 ä¸ªæ—¶é—´çš„ä½“ç° VQA ç±»åˆ«ï¼Œä»¥ç³»ç»Ÿåœ°åŸºå‡†æµ‹è¯•å’Œå¢å¼º VLM çš„ä½“ç°æ¨ç†èƒ½åŠ›ã€‚åŒæ—¶ï¼ŒRoboInter-VLA æä¾›äº†ä¸€ä¸ªé›†æˆçš„è®¡åˆ’ç„¶åæ‰§è¡Œæ¡†æ¶ï¼Œæ”¯æŒæ¨¡å—åŒ–å’Œç«¯åˆ°ç«¯çš„ VLA å˜ä½“ï¼Œé€šè¿‡ä¸­é—´ç›‘ç£å°†é«˜å±‚è§„åˆ’ä¸ä½å±‚æ‰§è¡Œè”ç³»èµ·æ¥ã€‚æ€»çš„æ¥è¯´ï¼ŒRoboInter ä¸ºé€šè¿‡ç»†ç²’åº¦å’Œå¤šæ ·åŒ–çš„ä¸­é—´è¡¨ç¤ºæ¨è¿›ç¨³å¥å’Œé€šç”¨çš„æœºå™¨äººå­¦ä¹ å¥ å®šäº†å®è·µåŸºç¡€ã€‚

</details>

---

## 209. Hydra-Nav: Object Navigation via Adaptive Dual-Process Reasoning

**ä¸­æ–‡æ ‡é¢˜**: Hydra-Navï¼šé€šè¿‡è‡ªé€‚åº”åŒè¿›ç¨‹æ¨ç†è¿›è¡Œå¯¹è±¡å¯¼èˆª

**Date**: 2026-02-10 | **arXiv**: [2602.09972v1](http://arxiv.org/abs/2602.09972v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09972v1)

<details><summary><b>Abstract</b></summary>

While large vision-language models (VLMs) show promise for object goal navigation, current methods still struggle with low success rates and inefficient localization of unseen objects--failures primarily attributed to weak temporal-spatial reasoning. Meanwhile, recent attempts to inject reasoning into VLM-based agents improve success rates but incur substantial computational overhead. To address both the ineffectiveness and inefficiency of existing approaches, we introduce Hydra-Nav, a unified VLM architecture that adaptively switches between a deliberative slow system for analyzing exploration history and formulating high-level plans, and a reactive fast system for efficient execution. We train Hydra-Nav through a three-stage curriculum: (i) spatial-action alignment to strengthen trajectory planning, (ii) memory-reasoning integration to enhance temporal-spatial reasoning over long-horizon exploration, and (iii) iterative rejection fine-tuning to enable selective reasoning at critical decision points. Extensive experiments demonstrate that Hydra-Nav achieves state-of-the-art performance on the HM3D, MP3D, and OVON benchmarks, outperforming the second-best methods by 11.1%, 17.4%, and 21.2%, respectively. Furthermore, we introduce SOT (Success weighted by Operation Time), a new metric to measure search efficiency across VLMs with varying reasoning intensity. Results show that adaptive reasoning significantly enhances search efficiency over fixed-frequency baselines.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è™½ç„¶å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ˜¾ç¤ºå‡ºç‰©ä½“ç›®æ ‡å¯¼èˆªçš„å¸Œæœ›ï¼Œä½†å½“å‰çš„æ–¹æ³•ä»ç„¶é¢ä¸´æˆåŠŸç‡ä½å’Œçœ‹ä¸è§çš„ç‰©ä½“å®šä½æ•ˆç‡ä½çš„é—®é¢˜â€”â€”å¤±è´¥ä¸»è¦å½’å› äºæ—¶ç©ºæ¨ç†èƒ½åŠ›å¼±ã€‚ä¸æ­¤åŒæ—¶ï¼Œæœ€è¿‘å°è¯•å°†æ¨ç†æ³¨å…¥åŸºäº VLM çš„ä»£ç†ä¸­ï¼Œæé«˜äº†æˆåŠŸç‡ï¼Œä½†ä¼šäº§ç”Ÿå¤§é‡çš„è®¡ç®—å¼€é”€ã€‚ä¸ºäº†è§£å†³ç°æœ‰æ–¹æ³•çš„ä½æ•ˆå’Œä½æ•ˆé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº† Hydra-Navï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„ VLM æ¶æ„ï¼Œå¯ä»¥åœ¨ç”¨äºåˆ†æå‹˜æ¢å†å²å’Œåˆ¶å®šé«˜çº§è®¡åˆ’çš„æ·±æ€ç†Ÿè™‘çš„æ…¢ç³»ç»Ÿå’Œç”¨äºé«˜æ•ˆæ‰§è¡Œçš„ååº”å¿«é€Ÿç³»ç»Ÿä¹‹é—´è‡ªé€‚åº”åœ°åˆ‡æ¢ã€‚æˆ‘ä»¬é€šè¿‡ä¸‰é˜¶æ®µè¯¾ç¨‹æ¥è®­ç»ƒ Hydra-Navï¼šï¼ˆiï¼‰ç©ºé—´åŠ¨ä½œå¯¹é½ä»¥åŠ å¼ºè½¨è¿¹è§„åˆ’ï¼Œï¼ˆiiï¼‰è®°å¿†æ¨ç†æ•´åˆä»¥å¢å¼ºé•¿è§†é‡æ¢ç´¢ä¸­çš„æ—¶ç©ºæ¨ç†ï¼Œä»¥åŠï¼ˆiiiï¼‰è¿­ä»£æ‹’ç»å¾®è°ƒä»¥åœ¨å…³é”®å†³ç­–ç‚¹å®ç°é€‰æ‹©æ€§æ¨ç†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHydra-Nav åœ¨ HM3Dã€MP3D å’Œ OVON åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåˆ†åˆ«æ¯”ç¬¬äºŒå¥½çš„æ–¹æ³•é«˜å‡º 11.1%ã€17.4% å’Œ 21.2%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº† SOTï¼ˆæŒ‰æ“ä½œæ—¶é—´åŠ æƒçš„æˆåŠŸï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°æŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡å…·æœ‰ä¸åŒæ¨ç†å¼ºåº¦çš„ VLM çš„æœç´¢æ•ˆç‡ã€‚ç»“æœè¡¨æ˜ï¼Œè‡ªé€‚åº”æ¨ç†æ¯”å›ºå®šé¢‘ç‡åŸºçº¿æ˜¾ç€æé«˜äº†æœç´¢æ•ˆç‡ã€‚

</details>

---

## 210. TriPilot-FF: Coordinated Whole-Body Teleoperation with Force Feedback

**ä¸­æ–‡æ ‡é¢˜**: TriPilot-FFï¼šå…·æœ‰åŠ›åé¦ˆçš„åè°ƒå…¨èº«è¿œç¨‹æ“ä½œ

**Date**: 2026-02-10 | **arXiv**: [2602.09888v1](http://arxiv.org/abs/2602.09888v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09888v1)

<details><summary><b>Abstract</b></summary>

Mobile manipulators broaden the operational envelope for robot manipulation. However, the whole-body teleoperation of such robots remains a problem: operators must coordinate a wheeled base and two arms while reasoning about obstacles and contact. Existing interfaces are predominantly hand-centric (e.g., VR controllers and joysticks), leaving foot-operated channels underexplored for continuous base control. We present TriPilot-FF, an open-source whole-body teleoperation system for a custom bimanual mobile manipulator that introduces a foot-operated pedal with lidar-driven pedal haptics, coupled with upper-body bimanual leader-follower teleoperation. Using only a low-cost base-mounted lidar, TriPilot-FF renders a resistive pedal cue from proximity-to-obstacle signals in the commanded direction, shaping operator commands toward collision-averse behaviour without an explicit collision-avoidance controller. The system also supports arm-side force reflection for contact awareness and provides real-time force and visual guidance of bimanual manipulability to prompt mobile base repositioning, thereby improving reach. We demonstrate the capability of TriPilot-FF to effectively ``co-pilot'' the human operator over long time-horizons and tasks requiring precise mobile base movement and coordination. Finally, we incorporate teleoperation feedback signals into an Action Chunking with Transformers (ACT) policy and demonstrate improved performance when the additional information is available. We release the pedal device design, full software stack, and conduct extensive real-world evaluations on a bimanual wheeled platform. The project page of TriPilot-FF is http://bit.ly/46H3ZJT.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç§»åŠ¨æœºæ¢°æ‰‹æ‹“å®½äº†æœºå™¨äººæ“çºµçš„æ“ä½œèŒƒå›´ã€‚ç„¶è€Œï¼Œæ­¤ç±»æœºå™¨äººçš„å…¨èº«è¿œç¨‹æ“ä½œä»ç„¶æ˜¯ä¸€ä¸ªé—®é¢˜ï¼šæ“ä½œå‘˜å¿…é¡»åè°ƒè½®å¼åº•åº§å’Œä¸¤ä¸ªæ‰‹è‡‚ï¼ŒåŒæ—¶æ¨ç†éšœç¢ç‰©å’Œæ¥è§¦ã€‚ç°æœ‰çš„ç•Œé¢ä¸»è¦ä»¥æ‰‹åŠ¨ä¸ºä¸­å¿ƒï¼ˆä¾‹å¦‚ VR æ§åˆ¶å™¨å’Œæ“çºµæ†ï¼‰ï¼Œè€Œå¯¹äºè¿ç»­åŸºç¡€æ§åˆ¶è€Œè¨€ï¼Œè„šè¸æ“ä½œé€šé“å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬æ¨å‡ºäº† TriPilot-FFï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå®šåˆ¶åŒæ‰‹ç§»åŠ¨æœºæ¢°æ‰‹çš„å¼€æºå…¨èº«è¿œç¨‹æ“ä½œç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå¼•å…¥äº†å¸¦æœ‰æ¿€å…‰é›·è¾¾é©±åŠ¨è¸æ¿è§¦è§‰çš„è„šè¸è¸æ¿ï¼Œå¹¶ç»“åˆäº†ä¸Šèº«åŒæ‰‹é¢†å¯¼è€…-è·Ÿéšè€…è¿œç¨‹æ“ä½œã€‚ TriPilot-FF ä»…ä½¿ç”¨ä½æˆæœ¬åº•åº§å®‰è£…çš„æ¿€å…‰é›·è¾¾ï¼Œæ ¹æ®æŒ‡ä»¤æ–¹å‘ä¸Šæ¥è¿‘éšœç¢ç‰©çš„ä¿¡å·å‘ˆç°ç”µé˜»è¸æ¿æç¤ºï¼Œä»è€Œåœ¨æ²¡æœ‰æ˜ç¡®çš„é˜²æ’æ§åˆ¶å™¨çš„æƒ…å†µä¸‹å°†æ“ä½œå‘˜å‘½ä»¤å¡‘é€ ä¸ºé˜²ç¢°æ’è¡Œä¸ºã€‚è¯¥ç³»ç»Ÿè¿˜æ”¯æŒæ‰‹è‡‚ä¾§åŠ›åå°„ä»¥å®ç°æ¥è§¦æ„ŸçŸ¥ï¼Œå¹¶æä¾›åŒæ‰‹å¯æ“ä½œæ€§çš„å®æ—¶åŠ›å’Œè§†è§‰å¼•å¯¼ï¼Œä»¥æç¤ºç§»åŠ¨åº•åº§é‡æ–°å®šä½ï¼Œä»è€Œæé«˜è¦†ç›–èŒƒå›´ã€‚æˆ‘ä»¬å±•ç¤ºäº† TriPilot-FF èƒ½å¤Ÿåœ¨é•¿æ—¶é—´èŒƒå›´å†…å’Œéœ€è¦ç²¾ç¡®çš„ç§»åŠ¨åŸºåœ°ç§»åŠ¨å’Œåè°ƒçš„ä»»åŠ¡ä¸­æœ‰æ•ˆåœ°â€œå‰¯é©¾é©¶â€äººç±»æ“ä½œå‘˜ã€‚æœ€åï¼Œæˆ‘ä»¬å°†è¿œç¨‹æ“ä½œåé¦ˆä¿¡å·åˆå¹¶åˆ° Transformers çš„ Action Chunking (ACT) ç­–ç•¥ä¸­ï¼Œå¹¶åœ¨é™„åŠ ä¿¡æ¯å¯ç”¨æ—¶å±•ç¤ºäº†æ”¹è¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬å‘å¸ƒäº†è¸æ¿è®¾å¤‡è®¾è®¡ã€å®Œæ•´çš„è½¯ä»¶å †æ ˆï¼Œå¹¶åœ¨åŒæ‰‹è½®å¼å¹³å°ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®é™…è¯„ä¼°ã€‚ TriPilot-FFçš„é¡¹ç›®é¡µé¢ä¸ºhttp://bit.ly/46H3ZJTã€‚

</details>

---

## 211. BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation

**ä¸­æ–‡æ ‡é¢˜**: BagelVLAï¼šé€šè¿‡äº¤é”™çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œç”Ÿæˆå¢å¼ºé•¿è§†é‡æ“ä½œ

**Date**: 2026-02-10 | **arXiv**: [2602.09849v2](http://arxiv.org/abs/2602.09849v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.09849v2)

<details><summary><b>Abstract</b></summary>

Equipping embodied agents with the ability to reason about tasks, foresee physical outcomes, and generate precise actions is essential for general-purpose manipulation. While recent Vision-Language-Action (VLA) models have leveraged pre-trained foundation models, they typically focus on either linguistic planning or visual forecasting in isolation. These methods rarely integrate both capabilities simultaneously to guide action generation, leading to suboptimal performance in complex, long-horizon manipulation tasks. To bridge this gap, we propose BagelVLA, a unified model that integrates linguistic planning, visual forecasting, and action generation within a single framework. Initialized from a pretrained unified understanding and generative model, BagelVLA is trained to interleave textual reasoning and visual prediction directly into the action execution loop. To efficiently couple these modalities, we introduce Residual Flow Guidance (RFG), which initializes from current observation and leverages single-step denoising to extract predictive visual features, guiding action generation with minimal latency. Extensive experiments demonstrate that BagelVLA outperforms existing baselines by a significant margin on multiple simulated and real-world benchmarks, particularly in tasks requiring multi-stage reasoning.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ä¸ºå®ä½“æ™ºèƒ½ä½“é…å¤‡æ¨ç†ä»»åŠ¡ã€é¢„è§ç‰©ç†ç»“æœå’Œç”Ÿæˆç²¾ç¡®åŠ¨ä½œçš„èƒ½åŠ›å¯¹äºé€šç”¨æ“çºµè‡³å…³é‡è¦ã€‚è™½ç„¶æœ€è¿‘çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åˆ©ç”¨äº†é¢„å…ˆè®­ç»ƒçš„åŸºç¡€æ¨¡å‹ï¼Œä½†å®ƒä»¬é€šå¸¸å•ç‹¬å…³æ³¨è¯­è¨€è§„åˆ’æˆ–è§†è§‰é¢„æµ‹ã€‚è¿™äº›æ–¹æ³•å¾ˆå°‘åŒæ—¶é›†æˆè¿™ä¸¤ç§åŠŸèƒ½æ¥æŒ‡å¯¼åŠ¨ä½œç”Ÿæˆï¼Œä»è€Œå¯¼è‡´åœ¨å¤æ‚çš„é•¿è§†é‡æ“ä½œä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº† BagelVLAï¼Œè¿™æ˜¯ä¸€ç§å°†è¯­è¨€è§„åˆ’ã€è§†è§‰é¢„æµ‹å’ŒåŠ¨ä½œç”Ÿæˆé›†æˆåœ¨å•ä¸€æ¡†æ¶å†…çš„ç»Ÿä¸€æ¨¡å‹ã€‚ BagelVLA ä»é¢„è®­ç»ƒçš„ç»Ÿä¸€ç†è§£å’Œç”Ÿæˆæ¨¡å‹åˆå§‹åŒ–ï¼Œç»è¿‡è®­ç»ƒå¯å°†æ–‡æœ¬æ¨ç†å’Œè§†è§‰é¢„æµ‹ç›´æ¥æ’å…¥åˆ°åŠ¨ä½œæ‰§è¡Œå¾ªç¯ä¸­ã€‚ä¸ºäº†æœ‰æ•ˆåœ°è€¦åˆè¿™äº›æ¨¡å¼ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ®‹å·®æµå¼•å¯¼ï¼ˆRFGï¼‰ï¼Œå®ƒä»å½“å‰è§‚å¯Ÿä¸­åˆå§‹åŒ–ï¼Œå¹¶åˆ©ç”¨å•æ­¥å»å™ªæ¥æå–é¢„æµ‹è§†è§‰ç‰¹å¾ï¼Œä»¥æœ€å°çš„å»¶è¿ŸæŒ‡å¯¼åŠ¨ä½œç”Ÿæˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒBagelVLA åœ¨å¤šä¸ªæ¨¡æ‹Ÿå’Œç°å®ä¸–ç•ŒåŸºå‡†ä¸Šæ˜æ˜¾ä¼˜äºç°æœ‰åŸºå‡†ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤šé˜¶æ®µæ¨ç†çš„ä»»åŠ¡ä¸­ã€‚

</details>

---

## 212. Diverse Skill Discovery for Quadruped Robots via Unsupervised Learning

**ä¸­æ–‡æ ‡é¢˜**: é€šè¿‡æ— ç›‘ç£å­¦ä¹ å‘ç°å››è¶³æœºå™¨äººçš„å¤šæ ·åŒ–æŠ€èƒ½

**Date**: 2026-02-10 | **arXiv**: [2602.09767v1](http://arxiv.org/abs/2602.09767v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09767v1)

<details><summary><b>Abstract</b></summary>

Reinforcement learning necessitates meticulous reward shaping by specialists to elicit target behaviors, while imitation learning relies on costly task-specific data. In contrast, unsupervised skill discovery can potentially reduce these burdens by learning a diverse repertoire of useful skills driven by intrinsic motivation. However, existing methods exhibit two key limitations: they typically rely on a single policy to master a versatile repertoire of behaviors without modeling the shared structure or distinctions among them, which results in low learning efficiency; moreover, they are susceptible to reward hacking, where the reward signal increases and converges rapidly while the learned skills display insufficient actual diversity. In this work, we introduce an Orthogonal Mixture-of-Experts (OMoE) architecture that prevents diverse behaviors from collapsing into overlapping representations, enabling a single policy to master a wide spectrum of locomotion skills. In addition, we design a multi-discriminator framework in which different discriminators operate on distinct observation spaces, effectively mitigating reward hacking. We evaluated our method on the 12-DOF Unitree A1 quadruped robot, demonstrating a diverse set of locomotion skills. Our experiments demonstrate that the proposed framework boosts training efficiency and yields an 18.3\% expansion in state-space coverage compared to the baseline.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¼ºåŒ–å­¦ä¹ éœ€è¦ä¸“å®¶ç²¾å¿ƒè®¾è®¡å¥–åŠ±æ¥å¼•å‘ç›®æ ‡è¡Œä¸ºï¼Œè€Œæ¨¡ä»¿å­¦ä¹ åˆ™ä¾èµ–äºæ˜‚è´µçš„ç‰¹å®šä»»åŠ¡æ•°æ®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ— ç›‘ç£çš„æŠ€èƒ½å‘ç°å¯ä»¥é€šè¿‡å­¦ä¹ ç”±å†…åœ¨åŠ¨æœºé©±åŠ¨çš„å„ç§æœ‰ç”¨æŠ€èƒ½æ¥æ½œåœ¨åœ°å‡è½»è¿™äº›è´Ÿæ‹…ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªå…³é”®å±€é™æ€§ï¼šå®ƒä»¬é€šå¸¸ä¾èµ–äºå•ä¸€ç­–ç•¥æ¥æŒæ¡å¤šç§è¡Œä¸ºï¼Œè€Œä¸å¯¹å®ƒä»¬ä¹‹é—´çš„å…±äº«ç»“æ„æˆ–åŒºåˆ«è¿›è¡Œå»ºæ¨¡ï¼Œè¿™å¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ï¼›æ­¤å¤–ï¼Œå®ƒä»¬å¾ˆå®¹æ˜“å—åˆ°å¥–åŠ±é»‘å®¢æ”»å‡»ï¼Œå³å¥–åŠ±ä¿¡å·è¿…é€Ÿå¢åŠ å’Œæ”¶æ•›ï¼Œè€Œæ‰€å­¦æŠ€èƒ½çš„å®é™…å¤šæ ·æ€§å´ä¸è¶³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ­£äº¤ä¸“å®¶æ··åˆï¼ˆOMoEï¼‰æ¶æ„ï¼Œè¯¥æ¶æ„å¯ä»¥é˜²æ­¢ä¸åŒçš„è¡Œä¸ºé™·å…¥é‡å çš„è¡¨ç¤ºï¼Œä»è€Œä½¿å•ä¸ªç­–ç•¥èƒ½å¤ŸæŒæ¡å¹¿æ³›çš„è¿åŠ¨æŠ€èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šé‰´åˆ«å™¨æ¡†æ¶ï¼Œå…¶ä¸­ä¸åŒçš„é‰´åˆ«å™¨åœ¨ä¸åŒçš„è§‚å¯Ÿç©ºé—´ä¸Šè¿è¡Œï¼Œæœ‰æ•ˆåœ°å‡è½»äº†å¥–åŠ±é»‘å®¢è¡Œä¸ºã€‚æˆ‘ä»¬åœ¨ 12 è‡ªç”±åº¦ Unitree A1 å››è¶³æœºå™¨äººä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå±•ç¤ºäº†å¤šç§è¿åŠ¨æŠ€èƒ½ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¡†æ¶æé«˜äº†è®­ç»ƒæ•ˆç‡ï¼Œå¹¶ä¸”ä¸åŸºçº¿ç›¸æ¯”ï¼ŒçŠ¶æ€ç©ºé—´è¦†ç›–èŒƒå›´æ‰©å¤§äº† 18.3%ã€‚

</details>

---

## 213. NavDreamer: Video Models as Zero-Shot 3D Navigators

**ä¸­æ–‡æ ‡é¢˜**: NavDreamerï¼šä½œä¸ºé›¶é•œå¤´ 3D å¯¼èˆªå™¨çš„è§†é¢‘æ¨¡å‹

**Date**: 2026-02-10 | **arXiv**: [2602.09765v1](http://arxiv.org/abs/2602.09765v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09765v1)

<details><summary><b>Abstract</b></summary>

Previous Vision-Language-Action models face critical limitations in navigation: scarce, diverse data from labor-intensive collection and static representations that fail to capture temporal dynamics and physical laws. We propose NavDreamer, a video-based framework for 3D navigation that leverages generative video models as a universal interface between language instructions and navigation trajectories. Our main hypothesis is that video's ability to encode spatiotemporal information and physical dynamics, combined with internet-scale availability, enables strong zero-shot generalization in navigation. To mitigate the stochasticity of generative predictions, we introduce a sampling-based optimization method that utilizes a VLM for trajectory scoring and selection. An inverse dynamics model is employed to decode executable waypoints from generated video plans for navigation. To systematically evaluate this paradigm in several video model backbones, we introduce a comprehensive benchmark covering object navigation, precise navigation, spatial grounding, language control, and scene reasoning. Extensive experiments demonstrate robust generalization across novel objects and unseen environments, with ablation studies revealing that navigation's high-level decision-making nature makes it particularly suited for video-based planning.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ä»¥å‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åœ¨å¯¼èˆªæ–¹é¢é¢ä¸´ç€ä¸¥é‡çš„å±€é™æ€§ï¼šæ¥è‡ªåŠ³åŠ¨å¯†é›†å‹æ”¶é›†çš„ç¨€ç¼ºä¸”å¤šæ ·åŒ–çš„æ•°æ®ä»¥åŠæ— æ³•æ•æ‰æ—¶é—´åŠ¨æ€å’Œç‰©ç†å®šå¾‹çš„é™æ€è¡¨ç¤ºã€‚æˆ‘ä»¬æå‡ºäº† NavDreamerï¼Œä¸€ç§åŸºäºè§†é¢‘çš„ 3D å¯¼èˆªæ¡†æ¶ï¼Œåˆ©ç”¨ç”Ÿæˆè§†é¢‘æ¨¡å‹ä½œä¸ºè¯­è¨€æŒ‡ä»¤å’Œå¯¼èˆªè½¨è¿¹ä¹‹é—´çš„é€šç”¨æ¥å£ã€‚æˆ‘ä»¬çš„ä¸»è¦å‡è®¾æ˜¯ï¼Œè§†é¢‘ç¼–ç æ—¶ç©ºä¿¡æ¯å’Œç‰©ç†åŠ¨åŠ›å­¦çš„èƒ½åŠ›ï¼Œä¸äº’è”ç½‘è§„æ¨¡çš„å¯ç”¨æ€§ç›¸ç»“åˆï¼Œå¯ä»¥åœ¨å¯¼èˆªä¸­å®ç°å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–ã€‚ä¸ºäº†å‡è½»ç”Ÿæˆé¢„æµ‹çš„éšæœºæ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºé‡‡æ ·çš„ä¼˜åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ VLM è¿›è¡Œè½¨è¿¹è¯„åˆ†å’Œé€‰æ‹©ã€‚é‡‡ç”¨é€†åŠ¨æ€æ¨¡å‹ä»ç”Ÿæˆçš„å¯¼èˆªè§†é¢‘è®¡åˆ’ä¸­è§£ç å¯æ‰§è¡Œèˆªè·¯ç‚¹ã€‚ä¸ºäº†ç³»ç»Ÿåœ°è¯„ä¼°å‡ ä¸ªè§†é¢‘æ¨¡å‹ä¸»å¹²ä¸­çš„è¿™ç§èŒƒå¼ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ¶µç›–å¯¹è±¡å¯¼èˆªã€ç²¾ç¡®å¯¼èˆªã€ç©ºé—´åŸºç¡€ã€è¯­è¨€æ§åˆ¶å’Œåœºæ™¯æ¨ç†çš„ç»¼åˆåŸºå‡†ã€‚å¤§é‡çš„å®éªŒè¯æ˜äº†å¯¹æ–°ç‰©ä½“å’Œçœ‹ä¸è§çš„ç¯å¢ƒçš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ï¼Œæ¶ˆèç ”ç©¶è¡¨æ˜å¯¼èˆªçš„é«˜çº§å†³ç­–æ€§è´¨ä½¿å…¶ç‰¹åˆ«é€‚åˆåŸºäºè§†é¢‘çš„è§„åˆ’ã€‚

</details>

---

## 214. Rethinking Visual-Language-Action Model Scaling: Alignment, Mixture, and Regularization

**ä¸­æ–‡æ ‡é¢˜**: é‡æ–°æ€è€ƒè§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹ç¼©æ”¾ï¼šå¯¹é½ã€æ··åˆå’Œæ­£åˆ™åŒ–

**Date**: 2026-02-10 | **arXiv**: [2602.09722v1](http://arxiv.org/abs/2602.09722v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09722v1)

<details><summary><b>Abstract</b></summary>

While Vision-Language-Action (VLA) models show strong promise for generalist robot control, it remains unclear whether -- and under what conditions -- the standard "scale data" recipe translates to robotics, where training data is inherently heterogeneous across embodiments, sensors, and action spaces. We present a systematic, controlled study of VLA scaling that revisits core training choices for pretraining across diverse robots. Using a representative VLA framework that combines a vision-language backbone with flow-matching, we ablate key design decisions under matched conditions and evaluate in extensive simulation and real-robot experiments. To improve the reliability of real-world results, we introduce a Grouped Blind Ensemble protocol that blinds operators to model identity and separates policy execution from outcome judgment, reducing experimenter bias. Our analysis targets three dimensions of VLA scaling. (1) Physical alignment: we show that a unified end-effector (EEF)-relative action representation is critical for robust cross-embodiment transfer. (2) Embodiment mixture: we find that naively pooling heterogeneous robot datasets often induces negative transfer rather than gains, underscoring the fragility of indiscriminate data scaling. (3) Training regularization: we observe that intuitive strategies, such as sensory dropout and multi-stage fine-tuning, do not consistently improve performance at scale. Together, this study challenge some common assumptions about embodied scaling and provide practical guidance for training large-scale VLA policies from diverse robotic data. Project website: https://research.beingbeyond.com/rethink_vla

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è™½ç„¶è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨é€šç”¨æœºå™¨äººæ§åˆ¶æ–¹é¢æ˜¾ç¤ºå‡ºå¼ºå¤§çš„å‰æ™¯ï¼Œä½†ä»ä¸æ¸…æ¥šæ ‡å‡†çš„â€œè§„æ¨¡æ•°æ®â€é…æ–¹æ˜¯å¦ä»¥åŠåœ¨ä»€ä¹ˆæ¡ä»¶ä¸‹å¯ä»¥è½¬åŒ–ä¸ºæœºå™¨äººæŠ€æœ¯ï¼Œå…¶ä¸­è®­ç»ƒæ•°æ®åœ¨å®æ–½ä¾‹ã€ä¼ æ„Ÿå™¨å’ŒåŠ¨ä½œç©ºé—´ä¹‹é—´æœ¬è´¨ä¸Šæ˜¯å¼‚æ„çš„ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€é¡¹å…³äº VLA ç¼©æ”¾çš„ç³»ç»Ÿæ€§ã€å—æ§ç ”ç©¶ï¼Œé‡æ–°å®¡è§†äº†ä¸åŒæœºå™¨äººé¢„è®­ç»ƒçš„æ ¸å¿ƒè®­ç»ƒé€‰æ‹©ã€‚ä½¿ç”¨å°†è§†è§‰è¯­è¨€ä¸»å¹²ä¸æµç¨‹åŒ¹é…ç›¸ç»“åˆçš„ä»£è¡¨æ€§ VLA æ¡†æ¶ï¼Œæˆ‘ä»¬åœ¨åŒ¹é…æ¡ä»¶ä¸‹æ¶ˆé™¤äº†å…³é”®è®¾è®¡å†³ç­–ï¼Œå¹¶åœ¨å¹¿æ³›çš„æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººå®éªŒä¸­è¿›è¡Œè¯„ä¼°ã€‚ä¸ºäº†æé«˜ç°å®ä¸–ç•Œç»“æœçš„å¯é æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†åˆ†ç»„ç›²é›†æˆåè®®ï¼Œè¯¥åè®®ä½¿æ“ä½œå‘˜æ— æ³•å¯¹èº«ä»½è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶å°†ç­–ç•¥æ‰§è¡Œä¸ç»“æœåˆ¤æ–­åˆ†å¼€ï¼Œä»è€Œå‡å°‘å®éªŒè€…çš„åè§ã€‚æˆ‘ä»¬çš„åˆ†æé’ˆå¯¹ VLA ç¼©æ”¾çš„ä¸‰ä¸ªç»´åº¦ã€‚ ï¼ˆ1ï¼‰ç‰©ç†å¯¹é½ï¼šæˆ‘ä»¬è¡¨æ˜ï¼Œç»Ÿä¸€çš„æœ«ç«¯æ‰§è¡Œå™¨ï¼ˆEEFï¼‰ç›¸å…³åŠ¨ä½œè¡¨ç¤ºå¯¹äºç¨³å¥çš„è·¨å®ä½“è¿ç§»è‡³å…³é‡è¦ã€‚ (2) å®æ–½ä¾‹æ··åˆï¼šæˆ‘ä»¬å‘ç°ï¼Œå¤©çœŸåœ°æ±‡é›†å¼‚æ„æœºå™¨äººæ•°æ®é›†é€šå¸¸ä¼šå¼•èµ·è´Ÿè¿ç§»è€Œä¸æ˜¯å¢ç›Šï¼Œè¿™å‡¸æ˜¾äº†ä¸åŠ åŒºåˆ«çš„æ•°æ®æ‰©å±•çš„è„†å¼±æ€§ã€‚ (3) è®­ç»ƒæ­£åˆ™åŒ–ï¼šæˆ‘ä»¬è§‚å¯Ÿåˆ°ç›´è§‰ç­–ç•¥ï¼Œä¾‹å¦‚æ„Ÿè§‰ä¸¢å¼ƒå’Œå¤šé˜¶æ®µå¾®è°ƒï¼Œå¹¶ä¸èƒ½æŒç»­å¤§è§„æ¨¡åœ°æé«˜æ€§èƒ½ã€‚æ€»ä¹‹ï¼Œè¿™é¡¹ç ”ç©¶æŒ‘æˆ˜äº†æœ‰å…³ä½“ç°æ‰©å±•çš„ä¸€äº›å¸¸è§å‡è®¾ï¼Œå¹¶ä¸ºä»ä¸åŒçš„æœºå™¨äººæ•°æ®ä¸­è®­ç»ƒå¤§è§„æ¨¡ VLA ç­–ç•¥æä¾›äº†å®ç”¨æŒ‡å¯¼ã€‚é¡¹ç›®ç½‘ç«™ï¼šhttps://research.beingbeyond.com/rethink_vla

</details>

---

## 215. Fast Motion Planning for Non-Holonomic Mobile Robots via a Rectangular Corridor Representation of Structured Environments

**ä¸­æ–‡æ ‡é¢˜**: é€šè¿‡ç»“æ„åŒ–ç¯å¢ƒçš„çŸ©å½¢èµ°å»Šè¡¨ç¤ºçš„éå®Œæ•´ç§»åŠ¨æœºå™¨äººçš„å¿«é€Ÿè¿åŠ¨è§„åˆ’

**Date**: 2026-02-10 | **arXiv**: [2602.09714v1](http://arxiv.org/abs/2602.09714v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09714v1)

<details><summary><b>Abstract</b></summary>

We present a complete framework for fast motion planning of non-holonomic autonomous mobile robots in highly complex but structured environments. Conventional grid-based planners struggle with scalability, while many kinematically-feasible planners impose a significant computational burden due to their search space complexity. To overcome these limitations, our approach introduces a deterministic free-space decomposition that creates a compact graph of overlapping rectangular corridors. This method enables a significant reduction in the search space, without sacrificing path resolution. The framework then performs online motion planning by finding a sequence of rectangles and generating a near-time-optimal, kinematically-feasible trajectory using an analytical planner. The result is a highly efficient solution for large-scale navigation. We validate our framework through extensive simulations and on a physical robot. The implementation is publicly available as open-source software.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå®Œæ•´çš„æ¡†æ¶ï¼Œç”¨äºåœ¨é«˜åº¦å¤æ‚ä½†ç»“æ„åŒ–çš„ç¯å¢ƒä¸­éå®Œæ•´è‡ªä¸»ç§»åŠ¨æœºå™¨äººçš„å¿«é€Ÿè¿åŠ¨è§„åˆ’ã€‚ä¼ ç»Ÿçš„åŸºäºç½‘æ ¼çš„è§„åˆ’å™¨åœ¨å¯æ‰©å±•æ€§æ–¹é¢é‡åˆ°äº†å›°éš¾ï¼Œè€Œè®¸å¤šè¿åŠ¨å­¦ä¸Šå¯è¡Œçš„è§„åˆ’å™¨ç”±äºå…¶æœç´¢ç©ºé—´çš„å¤æ‚æ€§è€Œå¸¦æ¥äº†å·¨å¤§çš„è®¡ç®—è´Ÿæ‹…ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ç¡®å®šæ€§è‡ªç”±ç©ºé—´åˆ†è§£ï¼Œåˆ›å»ºäº†é‡å çŸ©å½¢èµ°å»Šçš„ç´§å‡‘å›¾ã€‚è¯¥æ–¹æ³•å¯ä»¥æ˜¾ç€å‡å°‘æœç´¢ç©ºé—´ï¼Œè€Œä¸ç‰ºç‰²è·¯å¾„åˆ†è¾¨ç‡ã€‚ç„¶åï¼Œè¯¥æ¡†æ¶é€šè¿‡æŸ¥æ‰¾ä¸€ç³»åˆ—çŸ©å½¢å¹¶ä½¿ç”¨åˆ†æè§„åˆ’å™¨ç”Ÿæˆè¿‘ä¹æ—¶é—´æœ€ä¼˜çš„ã€è¿åŠ¨å­¦ä¸Šå¯è¡Œçš„è½¨è¿¹æ¥æ‰§è¡Œåœ¨çº¿è¿åŠ¨è§„åˆ’ã€‚å…¶ç»“æœæ˜¯å¤§è§„æ¨¡å¯¼èˆªçš„é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬é€šè¿‡å¹¿æ³›çš„æ¨¡æ‹Ÿå’Œåœ¨ç‰©ç†æœºå™¨äººä¸ŠéªŒè¯æˆ‘ä»¬çš„æ¡†æ¶ã€‚è¯¥å®ç°ä½œä¸ºå¼€æºè½¯ä»¶å…¬å¼€å¯ç”¨ã€‚

</details>

---

## 216. RANT: Ant-Inspired Multi-Robot Rainforest Exploration Using Particle Filter Localisation and Virtual Pheromone Coordination

**ä¸­æ–‡æ ‡é¢˜**: RANTï¼šä½¿ç”¨ç²’å­è¿‡æ»¤å™¨å®šä½å’Œè™šæ‹Ÿä¿¡æ¯ç´ åè°ƒçš„å—èš‚èšå¯å‘çš„å¤šæœºå™¨äººé›¨æ—æ¢ç´¢

**Date**: 2026-02-10 | **arXiv**: [2602.09661v1](http://arxiv.org/abs/2602.09661v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09661v1)

<details><summary><b>Abstract</b></summary>

This paper presents RANT, an ant-inspired multi-robot exploration framework for noisy, uncertain environments. A team of differential-drive robots navigates a 10 x 10 m terrain, collects noisy probe measurements of a hidden richness field, and builds local probabilistic maps while the supervisor maintains a global evaluation. RANT combines particle-filter localisation, a behaviour-based controller with gradient-driven hotspot exploitation, and a lightweight no-revisit coordination mechanism based on virtual pheromone blocking. We experimentally analyse how team size, localisation fidelity, and coordination influence coverage, hotspot recall, and redundancy. Results show that particle filtering is essential for reliable hotspot engagement, coordination substantially reduces overlap, and increasing team size improves coverage but yields diminishing returns due to interference.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æœ¬æ–‡æå‡ºäº† RANTï¼Œä¸€ç§å—èš‚èšå¯å‘çš„å¤šæœºå™¨äººæ¢ç´¢æ¡†æ¶ï¼Œé€‚ç”¨äºå˜ˆæ‚ã€ä¸ç¡®å®šçš„ç¯å¢ƒã€‚ä¸€ç»„å·®åŠ¨é©±åŠ¨æœºå™¨äººåœ¨ 10 x 10 m çš„åœ°å½¢ä¸­å¯¼èˆªï¼Œæ”¶é›†éšè—ä¸°å¯Œåº¦åœºçš„å™ªå£°æ¢é’ˆæµ‹é‡å€¼ï¼Œå¹¶æ„å»ºå±€éƒ¨æ¦‚ç‡å›¾ï¼ŒåŒæ—¶ç›‘ç£å‘˜ç»´æŠ¤å…¨å±€è¯„ä¼°ã€‚ RANT ç»“åˆäº†ç²’å­è¿‡æ»¤å™¨å®šä½ã€åŸºäºè¡Œä¸ºçš„æ§åˆ¶å™¨å’Œæ¢¯åº¦é©±åŠ¨çš„çƒ­ç‚¹å¼€å‘ï¼Œä»¥åŠåŸºäºè™šæ‹Ÿä¿¡æ¯ç´ é˜»å¡çš„è½»é‡çº§å…é‡è®¿åè°ƒæœºåˆ¶ã€‚æˆ‘ä»¬é€šè¿‡å®éªŒåˆ†æå›¢é˜Ÿè§„æ¨¡ã€æœ¬åœ°åŒ–ä¿çœŸåº¦å’Œåè°ƒå¦‚ä½•å½±å“è¦†ç›–èŒƒå›´ã€çƒ­ç‚¹å¬å›å’Œå†—ä½™ã€‚ç»“æœè¡¨æ˜ï¼Œç²’å­è¿‡æ»¤å¯¹äºå¯é çš„çƒ­ç‚¹å‚ä¸è‡³å…³é‡è¦ï¼Œåè°ƒå¯æ˜¾ç€å‡å°‘é‡å ï¼Œå¢åŠ å›¢é˜Ÿè§„æ¨¡å¯æé«˜è¦†ç›–èŒƒå›´ï¼Œä½†ä¼šå› å¹²æ‰°è€Œäº§ç”Ÿæ”¶ç›Šé€’å‡ã€‚

</details>

---

## 217. AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild

**ä¸­æ–‡æ ‡é¢˜**: AutoFlyï¼šæ— äººæœºé‡å¤–è‡ªä¸»å¯¼èˆªçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹

**Date**: 2026-02-10 | **arXiv**: [2602.09657v1](http://arxiv.org/abs/2602.09657v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09657v1)

<details><summary><b>Abstract</b></summary>

Vision-language navigation (VLN) requires intelligent agents to navigate environments by interpreting linguistic instructions alongside visual observations, serving as a cornerstone task in Embodied AI. Current VLN research for unmanned aerial vehicles (UAVs) relies on detailed, pre-specified instructions to guide the UAV along predetermined routes. However, real-world outdoor exploration typically occurs in unknown environments where detailed navigation instructions are unavailable. Instead, only coarse-grained positional or directional guidance can be provided, requiring UAVs to autonomously navigate through continuous planning and obstacle avoidance. To bridge this gap, we propose AutoFly, an end-to-end Vision-Language-Action (VLA) model for autonomous UAV navigation. AutoFly incorporates a pseudo-depth encoder that derives depth-aware features from RGB inputs to enhance spatial reasoning, coupled with a progressive two-stage training strategy that effectively aligns visual, depth, and linguistic representations with action policies. Moreover, existing VLN datasets have fundamental limitations for real-world autonomous navigation, stemming from their heavy reliance on explicit instruction-following over autonomous decision-making and insufficient real-world data. To address these issues, we construct a novel autonomous navigation dataset that shifts the paradigm from instruction-following to autonomous behavior modeling through: (1) trajectory collection emphasizing continuous obstacle avoidance, autonomous planning, and recognition workflows; (2) comprehensive real-world data integration. Experimental results demonstrate that AutoFly achieves a 3.9% higher success rate compared to state-of-the-art VLA baselines, with consistent performance across simulated and real environments.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è§†è§‰è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰éœ€è¦æ™ºèƒ½ä»£ç†é€šè¿‡è§£é‡Šè¯­è¨€æŒ‡ä»¤å’Œè§†è§‰è§‚å¯Ÿæ¥å¯¼èˆªç¯å¢ƒï¼Œè¿™æ˜¯åµŒå…¥å¼äººå·¥æ™ºèƒ½çš„åŸºçŸ³ä»»åŠ¡ã€‚ç›®å‰é’ˆå¯¹æ— äººæœº (UAV) çš„ VLN ç ”ç©¶ä¾èµ–äºè¯¦ç»†çš„ã€é¢„å…ˆæŒ‡å®šçš„æŒ‡ä»¤æ¥å¼•å¯¼æ— äººæœºæ²¿é¢„å®šè·¯çº¿è¡Œé©¶ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„æˆ·å¤–æ¢ç´¢é€šå¸¸å‘ç”Ÿåœ¨æ— æ³•è·å¾—è¯¦ç»†å¯¼èˆªè¯´æ˜çš„æœªçŸ¥ç¯å¢ƒä¸­ã€‚ç›¸åï¼Œåªèƒ½æä¾›ç²—ç²’åº¦çš„ä½ç½®æˆ–æ–¹å‘å¼•å¯¼ï¼Œè¦æ±‚æ— äººæœºé€šè¿‡è¿ç»­è§„åˆ’å’Œé¿éšœæ¥è‡ªä¸»å¯¼èˆªã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº† AutoFlyï¼Œä¸€ç§ç”¨äºè‡ªä¸»æ— äººæœºå¯¼èˆªçš„ç«¯åˆ°ç«¯è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ã€‚ AutoFly é‡‡ç”¨äº†ä¼ªæ·±åº¦ç¼–ç å™¨ï¼Œå¯ä» RGB è¾“å…¥ä¸­æ´¾ç”Ÿæ·±åº¦æ„ŸçŸ¥ç‰¹å¾ä»¥å¢å¼ºç©ºé—´æ¨ç†ï¼Œå†åŠ ä¸Šæ¸è¿›å¼ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå¯æœ‰æ•ˆåœ°å°†è§†è§‰ã€æ·±åº¦å’Œè¯­è¨€è¡¨ç¤ºä¸åŠ¨ä½œç­–ç•¥ä¿æŒä¸€è‡´ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„ VLN æ•°æ®é›†å¯¹äºç°å®ä¸–ç•Œçš„è‡ªä¸»å¯¼èˆªå­˜åœ¨æ ¹æœ¬æ€§çš„é™åˆ¶ï¼Œå› ä¸ºå®ƒä»¬ä¸¥é‡ä¾èµ–äºè‡ªä¸»å†³ç­–çš„æ˜ç¡®æŒ‡ä»¤è·Ÿè¸ªä»¥åŠç°å®ä¸–ç•Œæ•°æ®çš„ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ–°é¢–çš„è‡ªä¸»å¯¼èˆªæ•°æ®é›†ï¼Œé€šè¿‡ä»¥ä¸‹æ–¹å¼å°†èŒƒå¼ä»éµå¾ªæŒ‡ä»¤è½¬å˜ä¸ºè‡ªä¸»è¡Œä¸ºå»ºæ¨¡ï¼šï¼ˆ1ï¼‰è½¨è¿¹æ”¶é›†ï¼Œå¼ºè°ƒè¿ç»­é¿éšœã€è‡ªä¸»è§„åˆ’å’Œè¯†åˆ«å·¥ä½œæµç¨‹ï¼› (2)å…¨é¢çš„ç°å®ä¸–ç•Œæ•°æ®é›†æˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„ VLA åŸºçº¿ç›¸æ¯”ï¼ŒAutoFly çš„æˆåŠŸç‡æé«˜äº† 3.9%ï¼Œå¹¶ä¸”åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­å…·æœ‰ä¸€è‡´çš„æ€§èƒ½ã€‚

</details>

---

## 218. TeleGate: Whole-Body Humanoid Teleoperation via Gated Expert Selection with Motion Prior

**ä¸­æ–‡æ ‡é¢˜**: TeleGateï¼šé€šè¿‡é—¨æ§ä¸“å®¶é€‰æ‹©å’Œè¿åŠ¨å…ˆéªŒè¿›è¡Œå…¨èº«äººå½¢è¿œç¨‹æ“ä½œ

**Date**: 2026-02-10 | **arXiv**: [2602.09628v1](http://arxiv.org/abs/2602.09628v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09628v1)

<details><summary><b>Abstract</b></summary>

Real-time whole-body teleoperation is a critical method for humanoid robots to perform complex tasks in unstructured environments. However, developing a unified controller that robustly supports diverse human motions remains a significant challenge. Existing methods typically distill multiple expert policies into a single general policy, which often inevitably leads to performance degradation, particularly on highly dynamic motions. This paper presents TeleGate, a unified whole-body teleoperation framework for humanoid robots that achieves high-precision tracking across various motions while avoiding the performance loss inherent in knowledge distillation. Our key idea is to preserve the full capability of domain-specific expert policies by training a lightweight gating network, which dynamically activates experts in real-time based on proprioceptive states and reference trajectories. Furthermore, to compensate for the absence of future reference trajectories in real-time teleoperation, we introduce a VAE-based motion prior module that extracts implicit future motion intent from historical observations, enabling anticipatory control for motions requiring prediction such as jumping and standing up. We conducted empirical evaluations in simulation and also deployed our technique on the Unitree G1 humanoid robot. Using only 2.5 hours of motion capture data for training, our TeleGate achieves high-precision real-time teleoperation across diverse dynamic motions (e.g., running, fall recovery, and jumping), significantly outperforming the baseline methods in both tracking accuracy and success rate.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å®æ—¶å…¨èº«è¿œç¨‹æ“ä½œæ˜¯ä»¿äººæœºå™¨äººåœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­æ‰§è¡Œå¤æ‚ä»»åŠ¡çš„å…³é”®æ–¹æ³•ã€‚ç„¶è€Œï¼Œå¼€å‘ä¸€ä¸ªèƒ½å¤Ÿå¼ºæœ‰åŠ›åœ°æ”¯æŒä¸åŒäººä½“è¿åŠ¨çš„ç»Ÿä¸€æ§åˆ¶å™¨ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†å¤šä¸ªä¸“å®¶ç­–ç•¥æç‚¼ä¸ºå•ä¸ªé€šç”¨ç­–ç•¥ï¼Œè¿™é€šå¸¸ä¸å¯é¿å…åœ°å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜åº¦åŠ¨æ€çš„è¿åŠ¨ä¸­ã€‚æœ¬æ–‡æå‡ºäº† TeleGateï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºäººå½¢æœºå™¨äººçš„ç»Ÿä¸€å…¨èº«è¿œç¨‹æ“ä½œæ¡†æ¶ï¼Œå¯å®ç°å„ç§è¿åŠ¨çš„é«˜ç²¾åº¦è·Ÿè¸ªï¼ŒåŒæ—¶é¿å…çŸ¥è¯†è’¸é¦ä¸­å›ºæœ‰çš„æ€§èƒ½æŸå¤±ã€‚æˆ‘ä»¬çš„å…³é”®æ€æƒ³æ˜¯é€šè¿‡è®­ç»ƒè½»é‡çº§é—¨æ§ç½‘ç»œæ¥ä¿ç•™ç‰¹å®šé¢†åŸŸä¸“å®¶ç­–ç•¥çš„å…¨éƒ¨åŠŸèƒ½ï¼Œè¯¥ç½‘ç»œæ ¹æ®æœ¬ä½“æ„Ÿå—çŠ¶æ€å’Œå‚è€ƒè½¨è¿¹å®æ—¶åŠ¨æ€æ¿€æ´»ä¸“å®¶ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¼¥è¡¥å®æ—¶é¥æ“ä½œä¸­æœªæ¥å‚è€ƒè½¨è¿¹çš„ç¼ºå¤±ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäº VAE çš„è¿åŠ¨å…ˆéªŒæ¨¡å—ï¼Œè¯¥æ¨¡å—ä»å†å²è§‚å¯Ÿä¸­æå–éšå¼çš„æœªæ¥è¿åŠ¨æ„å›¾ï¼Œä»è€Œèƒ½å¤Ÿå¯¹éœ€è¦é¢„æµ‹çš„è¿åŠ¨ï¼ˆä¾‹å¦‚è·³è·ƒå’Œç«™ç«‹ï¼‰è¿›è¡Œé¢„æœŸæ§åˆ¶ã€‚æˆ‘ä»¬åœ¨æ¨¡æ‹Ÿä¸­è¿›è¡Œäº†å®è¯è¯„ä¼°ï¼Œå¹¶å°†æˆ‘ä»¬çš„æŠ€æœ¯éƒ¨ç½²åœ¨ Unitree G1 äººå½¢æœºå™¨äººä¸Šã€‚æˆ‘ä»¬çš„ TeleGate ä»…ä½¿ç”¨ 2.5 å°æ—¶çš„åŠ¨ä½œæ•æ‰æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå°±å®ç°äº†å„ç§åŠ¨æ€åŠ¨ä½œï¼ˆä¾‹å¦‚è·‘æ­¥ã€è·Œå€’æ¢å¤å’Œè·³è·ƒï¼‰çš„é«˜ç²¾åº¦å®æ—¶é¥æ“ä½œï¼Œåœ¨è·Ÿè¸ªç²¾åº¦å’ŒæˆåŠŸç‡æ–¹é¢å‡æ˜¾ç€ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚

</details>

---

## 219. Preference Aligned Visuomotor Diffusion Policies for Deformable Object Manipulation

**ä¸­æ–‡æ ‡é¢˜**: ç”¨äºå¯å˜å½¢ç‰©ä½“æ“çºµçš„åå¥½å¯¹é½è§†è§‰è¿åŠ¨æ‰©æ•£ç­–ç•¥

**Date**: 2026-02-10 | **arXiv**: [2602.09583v1](http://arxiv.org/abs/2602.09583v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09583v1)

<details><summary><b>Abstract</b></summary>

Humans naturally develop preferences for how manipulation tasks should be performed, which are often subtle, personal, and difficult to articulate. Although it is important for robots to account for these preferences to increase personalization and user satisfaction, they remain largely underexplored in robotic manipulation, particularly in the context of deformable objects like garments and fabrics. In this work, we study how to adapt pretrained visuomotor diffusion policies to reflect preferred behaviors using limited demonstrations. We introduce RKO, a novel preference-alignment method that combines the benefits of two recent frameworks: RPO and KTO. We evaluate RKO against common preference learning frameworks, including these two, as well as a baseline vanilla diffusion policy, on real-world cloth-folding tasks spanning multiple garments and preference settings. We show that preference-aligned policies (particularly RKO) achieve superior performance and sample efficiency compared to standard diffusion policy fine-tuning. These results highlight the importance and feasibility of structured preference learning for scaling personalized robot behavior in complex deformable object manipulation tasks.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

äººç±»è‡ªç„¶ä¼šå¯¹å¦‚ä½•æ‰§è¡Œæ“ä½œä»»åŠ¡äº§ç”Ÿåå¥½ï¼Œè¿™äº›åå¥½é€šå¸¸æ˜¯å¾®å¦™çš„ã€ä¸ªäººçš„ä¸”éš¾ä»¥æ¸…æ™°è¡¨è¾¾ã€‚å°½ç®¡å¯¹äºæœºå™¨äººæ¥è¯´ï¼Œè€ƒè™‘è¿™äº›åå¥½å¯¹äºæé«˜ä¸ªæ€§åŒ–å’Œç”¨æˆ·æ»¡æ„åº¦éå¸¸é‡è¦ï¼Œä½†å®ƒä»¬åœ¨æœºå™¨äººæ“ä½œæ–¹é¢ä»ç„¶å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«å……åˆ†æ¢ç´¢ï¼Œç‰¹åˆ«æ˜¯åœ¨æœè£…å’Œç»‡ç‰©ç­‰å¯å˜å½¢ç‰©ä½“çš„èƒŒæ™¯ä¸‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶å¦‚ä½•ä½¿ç”¨æœ‰é™çš„æ¼”ç¤ºæ¥è°ƒæ•´é¢„è®­ç»ƒçš„è§†è§‰è¿åŠ¨æ‰©æ•£ç­–ç•¥ä»¥åæ˜ åå¥½çš„è¡Œä¸ºã€‚æˆ‘ä»¬ä»‹ç» RKOï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„åå¥½è°ƒæ•´æ–¹æ³•ï¼Œå®ƒç»“åˆäº†ä¸¤ä¸ªæœ€æ–°æ¡†æ¶çš„ä¼˜ç‚¹ï¼šRPO å’Œ KTOã€‚æˆ‘ä»¬æ ¹æ®å¸¸è§çš„åå¥½å­¦ä¹ æ¡†æ¶ï¼ˆåŒ…æ‹¬è¿™ä¸¤ä¸ªæ¡†æ¶ï¼‰ä»¥åŠåŸºçº¿é¦™è‰æ‰©æ•£ç­–ç•¥ï¼Œé’ˆå¯¹è·¨è¶Šå¤šç§æœè£…å’Œåå¥½è®¾ç½®çš„ç°å®ä¸–ç•Œå¸ƒæ–™æŠ˜å ä»»åŠ¡æ¥è¯„ä¼° RKOã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œä¸æ ‡å‡†æ‰©æ•£ç­–ç•¥å¾®è°ƒç›¸æ¯”ï¼Œåå¥½ä¸€è‡´ç­–ç•¥ï¼ˆç‰¹åˆ«æ˜¯ RKOï¼‰å®ç°äº†å“è¶Šçš„æ€§èƒ½å’Œæ ·æœ¬æ•ˆç‡ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†ç»“æ„åŒ–åå¥½å­¦ä¹ å¯¹äºåœ¨å¤æ‚çš„å¯å˜å½¢ç‰©ä½“æ“çºµä»»åŠ¡ä¸­æ‰©å±•ä¸ªæ€§åŒ–æœºå™¨äººè¡Œä¸ºçš„é‡è¦æ€§å’Œå¯è¡Œæ€§ã€‚

</details>

---

## 220. Optimal Control of Microswimmers for Trajectory Tracking Using Bayesian Optimization

**ä¸­æ–‡æ ‡é¢˜**: ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–å¯¹å¾®å‹æ¸¸æ³³å™¨è¿›è¡Œè½¨è¿¹è·Ÿè¸ªä¼˜åŒ–æ§åˆ¶

**Date**: 2026-02-10 | **arXiv**: [2602.09563v1](http://arxiv.org/abs/2602.09563v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09563v1)

<details><summary><b>Abstract</b></summary>

Trajectory tracking for microswimmers remains a key challenge in microrobotics, where low-Reynolds-number dynamics make control design particularly complex. In this work, we formulate the trajectory tracking problem as an optimal control problem and solve it using a combination of B-spline parametrization with Bayesian optimization, allowing the treatment of high computational costs without requiring complex gradient computations. Applied to a flagellated magnetic swimmer, the proposed method reproduces a variety of target trajectories, including biologically inspired paths observed in experimental studies. We further evaluate the approach on a three-sphere swimmer model, demonstrating that it can adapt to and partially compensate for wall-induced hydrodynamic effects. The proposed optimization strategy can be applied consistently across models of different fidelity, from low-dimensional ODE-based models to high-fidelity PDE-based simulations, showing its robustness and generality. These results highlight the potential of Bayesian optimization as a versatile tool for optimal control strategies in microscale locomotion under complex fluid-structure interactions.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¾®å‹æ¸¸æ³³è€…çš„è½¨è¿¹è·Ÿè¸ªä»ç„¶æ˜¯å¾®å‹æœºå™¨äººé¢†åŸŸçš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå…¶ä¸­ä½é›·è¯ºæ•°åŠ¨åŠ›å­¦ä½¿å¾—æ§åˆ¶è®¾è®¡ç‰¹åˆ«å¤æ‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†è½¨è¿¹è·Ÿè¸ªé—®é¢˜è¡¨è¿°ä¸ºæœ€ä¼˜æ§åˆ¶é—®é¢˜ï¼Œå¹¶ä½¿ç”¨ B æ ·æ¡å‚æ•°åŒ–ä¸è´å¶æ–¯ä¼˜åŒ–ç›¸ç»“åˆæ¥è§£å†³å®ƒï¼Œä»è€Œæ— éœ€å¤æ‚çš„æ¢¯åº¦è®¡ç®—å³å¯å¤„ç†é«˜è®¡ç®—æˆâ€‹â€‹æœ¬ã€‚åº”ç”¨äºæœ‰é­æ¯›çš„ç£æ€§æ¸¸æ³³è€…æ—¶ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å†ç°äº†å„ç§ç›®æ ‡è½¨è¿¹ï¼ŒåŒ…æ‹¬åœ¨å®éªŒç ”ç©¶ä¸­è§‚å¯Ÿåˆ°çš„å—ç”Ÿç‰©å­¦å¯å‘çš„è·¯å¾„ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åœ¨ä¸‰çƒæ¸¸æ³³è€…æ¨¡å‹ä¸Šè¯„ä¼°è¯¥æ–¹æ³•ï¼Œè¯æ˜å®ƒå¯ä»¥é€‚åº”å¹¶éƒ¨åˆ†è¡¥å¿å£å¼•èµ·çš„æ°´åŠ¨åŠ›æ•ˆåº”ã€‚æ‰€æå‡ºçš„ä¼˜åŒ–ç­–ç•¥å¯ä»¥ä¸€è‡´åœ°åº”ç”¨äºä¸åŒä¿çœŸåº¦çš„æ¨¡å‹ï¼Œä»åŸºäºä½ç»´ ODE çš„æ¨¡å‹åˆ°åŸºäº PDE çš„é«˜ä¿çœŸæ¨¡æ‹Ÿï¼Œæ˜¾ç¤ºå‡ºå…¶é²æ£’æ€§å’Œé€šç”¨æ€§ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†è´å¶æ–¯ä¼˜åŒ–ä½œä¸ºå¤æ‚æµå›ºç›¸äº’ä½œç”¨ä¸‹å¾®å°ºåº¦è¿åŠ¨æœ€ä¼˜æ§åˆ¶ç­–ç•¥çš„å¤šåŠŸèƒ½å·¥å…·çš„æ½œåŠ›ã€‚

</details>

---

## 221. Sci-VLA: Agentic VLA Inference Plugin for Long-Horizon Tasks in Scientific Experiments

**ä¸­æ–‡æ ‡é¢˜**: Sci-VLAï¼šç”¨äºç§‘å­¦å®éªŒä¸­é•¿æœŸä»»åŠ¡çš„ä»£ç† VLA æ¨ç†æ’ä»¶

**Date**: 2026-02-10 | **arXiv**: [2602.09430v1](http://arxiv.org/abs/2602.09430v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09430v1)

<details><summary><b>Abstract</b></summary>

Robotic laboratories play a critical role in autonomous scientific discovery by enabling scalable, continuous experimental execution. Recent vision-language-action (VLA) models offer a promising foundation for robotic laboratories. However, scientific experiments typically involve long-horizon tasks composed of multiple atomic tasks, posing a fundamental challenge to existing VLA models. While VLA models fine-tuned for scientific tasks can reliably execute atomic experimental actions seen during training, they often fail to perform composite tasks formed by reordering and composing these known atomic actions. This limitation arises from a distributional mismatch between training-time atomic tasks and inference-time composite tasks, which prevents VLA models from executing necessary transitional operations between atomic tasks. To address this challenge, we propose an Agentic VLA Inference Plugin for Long-Horizon Tasks in Scientific Experiments. It introduces an LLM-based agentic inference mechanism that intervenes when executing sequential manipulation tasks. By performing explicit transition inference and generating transitional robotic action code, the proposed plugin guides VLA models through missing transitional steps, enabling reliable execution of composite scientific workflows without any additional training. This inference-only intervention makes our method computationally efficient, data-efficient, and well-suited for open-ended and long-horizon robotic laboratory tasks. We build 3D assets of scientific instruments and common scientific operating scenes within an existing simulation environment. In these scenes, we have verified that our method increases the average success rate per atomic task by 42\% during inference. Furthermore, we show that our method can be easily transferred from the simulation to real scientific laboratories.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æœºå™¨äººå®éªŒå®¤é€šè¿‡å®ç°å¯æ‰©å±•ã€è¿ç»­çš„å®éªŒæ‰§è¡Œï¼Œåœ¨è‡ªä¸»ç§‘å­¦å‘ç°ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚æœ€è¿‘çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ä¸ºæœºå™¨äººå®éªŒå®¤æä¾›äº†æœ‰å‰é€”çš„åŸºç¡€ã€‚ç„¶è€Œï¼Œç§‘å­¦å®éªŒé€šå¸¸æ¶‰åŠç”±å¤šä¸ªåŸå­ä»»åŠ¡ç»„æˆçš„é•¿è§†é‡ä»»åŠ¡ï¼Œè¿™å¯¹ç°æœ‰çš„VLAæ¨¡å‹æå‡ºäº†æ ¹æœ¬æ€§çš„æŒ‘æˆ˜ã€‚è™½ç„¶é’ˆå¯¹ç§‘å­¦ä»»åŠ¡è¿›è¡Œå¾®è°ƒçš„ VLA æ¨¡å‹å¯ä»¥å¯é åœ°æ‰§è¡Œè®­ç»ƒæœŸé—´çœ‹åˆ°çš„åŸå­å®éªŒåŠ¨ä½œï¼Œä½†å®ƒä»¬é€šå¸¸æ— æ³•æ‰§è¡Œé€šè¿‡é‡æ–°æ’åºå’Œç»„åˆè¿™äº›å·²çŸ¥åŸå­åŠ¨ä½œè€Œå½¢æˆçš„å¤åˆä»»åŠ¡ã€‚æ­¤é™åˆ¶æ˜¯ç”±äºè®­ç»ƒæ—¶åŸå­ä»»åŠ¡å’Œæ¨ç†æ—¶å¤åˆä»»åŠ¡ä¹‹é—´çš„åˆ†å¸ƒä¸åŒ¹é…è€Œå¼•èµ·çš„ï¼Œè¿™ä¼šé˜»æ­¢ VLA æ¨¡å‹åœ¨åŸå­ä»»åŠ¡ä¹‹é—´æ‰§è¡Œå¿…è¦çš„è½¬æ¢æ“ä½œã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç”¨äºç§‘å­¦å®éªŒä¸­é•¿æœŸä»»åŠ¡çš„ Agentic VLA æ¨ç†æ’ä»¶ã€‚å®ƒå¼•å…¥äº†ä¸€ç§åŸºäº LLM çš„ä»£ç†æ¨ç†æœºåˆ¶ï¼Œå¯ä»¥åœ¨æ‰§è¡Œé¡ºåºæ“ä½œä»»åŠ¡æ—¶è¿›è¡Œå¹²é¢„ã€‚é€šè¿‡æ‰§è¡Œæ˜¾å¼è½¬æ¢æ¨ç†å¹¶ç”Ÿæˆè½¬æ¢æœºå™¨äººåŠ¨ä½œä»£ç ï¼Œæ‰€æå‡ºçš„æ’ä»¶å¯å¼•å¯¼ VLA æ¨¡å‹å®Œæˆç¼ºå¤±çš„è½¬æ¢æ­¥éª¤ï¼Œä»è€Œæ— éœ€ä»»ä½•é¢å¤–è®­ç»ƒå³å¯å¯é åœ°æ‰§è¡Œå¤åˆç§‘å­¦å·¥ä½œæµç¨‹ã€‚è¿™ç§ä»…æ¨ç†çš„å¹²é¢„ä½¿æˆ‘ä»¬çš„æ–¹æ³•è®¡ç®—æ•ˆç‡é«˜ã€æ•°æ®æ•ˆç‡é«˜ï¼Œå¹¶ä¸”éå¸¸é€‚åˆå¼€æ”¾å¼å’Œé•¿è§†é‡çš„æœºå™¨äººå®éªŒå®¤ä»»åŠ¡ã€‚æˆ‘ä»¬åœ¨ç°æœ‰çš„æ¨¡æ‹Ÿç¯å¢ƒä¸­æ„å»ºç§‘å­¦ä»ªå™¨çš„ 3D èµ„äº§å’Œå¸¸è§çš„ç§‘å­¦æ“ä½œåœºæ™¯ã€‚åœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬å·²ç»éªŒè¯æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­å°†æ¯ä¸ªåŸå­ä»»åŠ¡çš„å¹³å‡æˆåŠŸç‡æé«˜äº† 42%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è½»æ¾åœ°ä»æ¨¡æ‹Ÿè½¬ç§»åˆ°çœŸå®çš„ç§‘å­¦å®éªŒå®¤ã€‚

</details>

---

## 222. Finite-time Stable Pose Estimation on TSE(3) using Point Cloud and Velocity Sensors

**ä¸­æ–‡æ ‡é¢˜**: ä½¿ç”¨ç‚¹äº‘å’Œé€Ÿåº¦ä¼ æ„Ÿå™¨å¯¹ TSE(3) è¿›è¡Œæœ‰é™æ—¶é—´ç¨³å®šå§¿æ€ä¼°è®¡

**Date**: 2026-02-10 | **arXiv**: [2602.09414v1](http://arxiv.org/abs/2602.09414v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09414v1)

<details><summary><b>Abstract</b></summary>

This work presents a finite-time stable pose estimator (FTS-PE) for rigid bodies undergoing rotational and translational motion in three dimensions, using measurements from onboard sensors that provide position vectors to inertially-fixed points and body velocities. The FTS-PE is a full-state observer for the pose (position and orientation) and velocities and is obtained through a Lyapunov analysis that shows its stability in finite time and its robustness to bounded measurement noise. Further, this observer is designed directly on the state space, the tangent bundle of the Lie group of rigid body motions, SE(3), without using local coordinates or (dual) quaternion representations. Therefore, it can estimate arbitrary rigid body motions without encountering singularities or the unwinding phenomenon and be readily applied to autonomous vehicles. A version of this observer that does not need translational velocity measurements and uses only point clouds and angular velocity measurements from rate gyros, is also obtained. It is discretized using the framework of geometric mechanics for numerical and experimental implementations. The numerical simulations compare the FTS-PE with a dual-quaternion extended Kalman filter and our previously developed variational pose estimator (VPE). The experimental results are obtained using point cloud images and rate gyro measurements obtained from a Zed 2i stereo depth camera sensor. These results validate the stability and robustness of the FTS-PE.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ç§æœ‰é™æ—¶é—´ç¨³å®šå§¿æ€ä¼°è®¡å™¨ï¼ˆFTS-PEï¼‰ï¼Œç”¨äºåœ¨ä¸‰ä¸ªç»´åº¦ä¸Šè¿›è¡Œæ—‹è½¬å’Œå¹³ç§»è¿åŠ¨çš„åˆšä½“ï¼Œä½¿ç”¨æ¿è½½ä¼ æ„Ÿå™¨çš„æµ‹é‡å€¼ï¼Œè¿™äº›ä¼ æ„Ÿå™¨ä¸ºæƒ¯æ€§å›ºå®šç‚¹å’Œè½¦èº«é€Ÿåº¦æä¾›ä½ç½®çŸ¢é‡ã€‚ FTS-PE æ˜¯ä½å§¿ï¼ˆä½ç½®å’Œæ–¹å‘ï¼‰å’Œé€Ÿåº¦çš„å…¨çŠ¶æ€è§‚æµ‹å™¨ï¼Œé€šè¿‡ Lyapunov åˆ†æè·å¾—ï¼Œæ˜¾ç¤ºäº†å…¶åœ¨æœ‰é™æ—¶é—´å†…çš„ç¨³å®šæ€§åŠå…¶å¯¹æœ‰ç•Œæµ‹é‡å™ªå£°çš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œè¯¥è§‚æµ‹å™¨ç›´æ¥åœ¨çŠ¶æ€ç©ºé—´ã€åˆšä½“è¿åŠ¨æç¾¤ SE(3) çš„åˆ‡ä¸›ä¸Šè®¾è®¡ï¼Œè€Œä¸ä½¿ç”¨å±€éƒ¨åæ ‡æˆ–ï¼ˆå¯¹å¶ï¼‰å››å…ƒæ•°è¡¨ç¤ºã€‚å› æ­¤ï¼Œå®ƒå¯ä»¥ä¼°è®¡ä»»æ„åˆšä½“è¿åŠ¨ï¼Œè€Œä¸ä¼šé‡åˆ°å¥‡ç‚¹æˆ–å±•å¼€ç°è±¡ï¼Œå¹¶ä¸”å¾ˆå®¹æ˜“åº”ç”¨äºè‡ªåŠ¨é©¾é©¶è½¦è¾†ã€‚è¿˜è·å¾—äº†è¯¥è§‚æµ‹å™¨çš„ä¸€ä¸ªç‰ˆæœ¬ï¼Œå®ƒä¸éœ€è¦å¹³ç§»é€Ÿåº¦æµ‹é‡ï¼Œå¹¶ä¸”ä»…ä½¿ç”¨æ¥è‡ªé€Ÿç‡é™€èºä»ªçš„â€‹â€‹ç‚¹äº‘å’Œè§’é€Ÿåº¦æµ‹é‡ã€‚å®ƒä½¿ç”¨å‡ ä½•åŠ›å­¦æ¡†æ¶è¿›è¡Œç¦»æ•£åŒ–ï¼Œä»¥è¿›è¡Œæ•°å€¼å’Œå®éªŒå®ç°ã€‚æ•°å€¼æ¨¡æ‹Ÿå°† FTS-PE ä¸åŒå››å…ƒæ•°æ‰©å±•å¡å°”æ›¼æ»¤æ³¢å™¨å’Œæˆ‘ä»¬ä¹‹å‰å¼€å‘çš„å˜åˆ†å§¿æ€ä¼°è®¡å™¨ (VPE) è¿›è¡Œäº†æ¯”è¾ƒã€‚å®éªŒç»“æœæ˜¯ä½¿ç”¨ç‚¹äº‘å›¾åƒå’Œä» Zed 2i ç«‹ä½“æ·±åº¦ç›¸æœºä¼ æ„Ÿå™¨è·å¾—çš„é€Ÿç‡é™€èºä»ªæµ‹é‡å€¼è·å¾—çš„ã€‚è¿™äº›ç»“æœéªŒè¯äº† FTS-PE çš„ç¨³å®šæ€§å’Œé²æ£’æ€§ã€‚

</details>

---

## 223. Phase-Aware Policy Learning for Skateboard Riding of Quadruped Robots via Feature-wise Linear Modulation

**ä¸­æ–‡æ ‡é¢˜**: é€šè¿‡ç‰¹å¾çº¿æ€§è°ƒåˆ¶è¿›è¡Œå››è¶³æœºå™¨äººæ»‘æ¿è¿åŠ¨çš„é˜¶æ®µæ„ŸçŸ¥ç­–ç•¥å­¦ä¹ 

**Date**: 2026-02-10 | **arXiv**: [2602.09370v1](http://arxiv.org/abs/2602.09370v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09370v1)

<details><summary><b>Abstract</b></summary>

Skateboards offer a compact and efficient means of transportation as a type of personal mobility device. However, controlling them with legged robots poses several challenges for policy learning due to perception-driven interactions and multi-modal control objectives across distinct skateboarding phases. To address these challenges, we introduce Phase-Aware Policy Learning (PAPL), a reinforcement-learning framework tailored for skateboarding with quadruped robots. PAPL leverages the cyclic nature of skateboarding by integrating phase-conditioned Feature-wise Linear Modulation layers into actor and critic networks, enabling a unified policy that captures phase-dependent behaviors while sharing robot-specific knowledge across phases. Our evaluations in simulation validate command-tracking accuracy and conduct ablation studies quantifying each component's contribution. We also compare locomotion efficiency against leg and wheel-leg baselines and show real-world transferability.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æ»‘æ¿ä½œä¸ºä¸€ç§ä¸ªäººç§»åŠ¨è®¾å¤‡æä¾›äº†ä¸€ç§ç´§å‡‘è€Œé«˜æ•ˆçš„äº¤é€šæ–¹å¼ã€‚ç„¶è€Œï¼Œç”±äºæ„ŸçŸ¥é©±åŠ¨çš„äº¤äº’å’Œä¸åŒæ»‘æ¿é˜¶æ®µçš„å¤šæ¨¡æ€æ§åˆ¶ç›®æ ‡ï¼Œç”¨è…¿æœºå™¨äººæ§åˆ¶å®ƒä»¬ç»™æ”¿ç­–å­¦ä¹ å¸¦æ¥äº†ä¸€äº›æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†é˜¶æ®µæ„ŸçŸ¥ç­–ç•¥å­¦ä¹ ï¼ˆPAPLï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºå››è¶³æœºå™¨äººæ»‘æ¿è€Œè®¾è®¡çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚ PAPL é€šè¿‡å°†ç›¸ä½æ¡ä»¶ç‰¹å¾çº¿æ€§è°ƒåˆ¶å±‚é›†æˆåˆ°å‚ä¸è€…å’Œæ‰¹è¯„è€…ç½‘ç»œä¸­ï¼Œåˆ©ç”¨æ»‘æ¿çš„å¾ªç¯æ€§è´¨ï¼Œå®ç°ç»Ÿä¸€çš„ç­–ç•¥ï¼Œæ•è·ç›¸ä½ç›¸å…³çš„è¡Œä¸ºï¼ŒåŒæ—¶è·¨é˜¶æ®µå…±äº«æœºå™¨äººç‰¹å®šçš„çŸ¥è¯†ã€‚æˆ‘ä»¬çš„æ¨¡æ‹Ÿè¯„ä¼°éªŒè¯äº†å‘½ä»¤è·Ÿè¸ªçš„å‡†ç¡®æ€§ï¼Œå¹¶è¿›è¡Œæ¶ˆèç ”ç©¶ï¼Œé‡åŒ–æ¯ä¸ªç»„ä»¶çš„è´¡çŒ®ã€‚æˆ‘ä»¬è¿˜å°†è¿åŠ¨æ•ˆç‡ä¸è…¿å’Œè½®è…¿åŸºçº¿è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶å±•ç¤ºç°å®ä¸–ç•Œçš„å¯è½¬ç§»æ€§ã€‚

</details>

---

## 224. CAPER: Constrained and Procedural Reasoning for Robotic Scientific Experiments

**ä¸­æ–‡æ ‡é¢˜**: CAPERï¼šæœºå™¨äººç§‘å­¦å®éªŒçš„çº¦æŸå’Œç¨‹åºæ¨ç†

**Date**: 2026-02-10 | **arXiv**: [2602.09367v1](http://arxiv.org/abs/2602.09367v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09367v1)

<details><summary><b>Abstract</b></summary>

Robotic assistance in scientific laboratories requires procedurally correct long-horizon manipulation, reliable execution under limited supervision, and robustness in low-demonstration regimes. Such conditions greatly challenge end-to-end vision-language-action (VLA) models, whose assumptions of recoverable errors and data-driven policy learning often break down in protocol-sensitive experiments. We propose CAPER, a framework for Constrained And ProcEdural Reasoning for robotic scientific experiments, which explicitly restricts where learning and reasoning occur in the planning and control pipeline. Rather than strengthening end-to-end policies, CAPER enforces a responsibility-separated structure: task-level reasoning generates procedurally valid action sequences under explicit constraints, mid-level multimodal grounding realizes subtasks without delegating spatial decision-making to large language models, and low-level control adapts to physical uncertainty via reinforcement learning with minimal demonstrations. By encoding procedural commitments through interpretable intermediate representations, CAPER prevents execution-time violations of experimental logic, improving controllability, robustness, and data efficiency. Experiments on a scientific workflow benchmark and a public long-horizon manipulation dataset demonstrate consistent improvements in success rate and procedural correctness, particularly in low-data and long-horizon settings.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç§‘å­¦å®éªŒå®¤ä¸­çš„æœºå™¨äººååŠ©éœ€è¦ç¨‹åºä¸Šæ­£ç¡®çš„é•¿è§†é‡æ“ä½œã€æœ‰é™ç›‘ç£ä¸‹çš„å¯é æ‰§è¡Œä»¥åŠä½ç¤ºèŒƒåˆ¶åº¦çš„ç¨³å¥æ€§ã€‚è¿™ç§æƒ…å†µæå¤§åœ°æŒ‘æˆ˜äº†ç«¯åˆ°ç«¯è§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ï¼Œå…¶å¯æ¢å¤é”™è¯¯å’Œæ•°æ®é©±åŠ¨ç­–ç•¥å­¦ä¹ çš„å‡è®¾åœ¨åè®®æ•æ„Ÿçš„å®éªŒä¸­ç»å¸¸è¢«æ‰“ç ´ã€‚æˆ‘ä»¬æå‡ºäº† CAPERï¼Œä¸€ä¸ªç”¨äºæœºå™¨äººç§‘å­¦å®éªŒçš„çº¦æŸå’Œè¿‡ç¨‹æ¨ç†æ¡†æ¶ï¼Œå®ƒæ˜ç¡®é™åˆ¶äº†å­¦ä¹ å’Œæ¨ç†åœ¨è§„åˆ’å’Œæ§åˆ¶ç®¡é“ä¸­å‘ç”Ÿçš„ä½ç½®ã€‚ CAPER æ²¡æœ‰åŠ å¼ºç«¯åˆ°ç«¯ç­–ç•¥ï¼Œè€Œæ˜¯å®æ–½äº†è´£ä»»åˆ†ç¦»çš„ç»“æ„ï¼šä»»åŠ¡çº§æ¨ç†åœ¨æ˜ç¡®çš„çº¦æŸä¸‹ç”Ÿæˆç¨‹åºä¸Šæœ‰æ•ˆçš„åŠ¨ä½œåºåˆ—ï¼Œä¸­çº§å¤šæ¨¡æ€åŸºç¡€å®ç°å­ä»»åŠ¡ï¼Œè€Œä¸å°†ç©ºé—´å†³ç­–å§”æ‰˜ç»™å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½çº§æ§åˆ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ä»¥æœ€å°‘çš„æ¼”ç¤ºæ¥é€‚åº”ç‰©ç†ä¸ç¡®å®šæ€§ã€‚é€šè¿‡å¯è§£é‡Šçš„ä¸­é—´è¡¨ç¤ºå¯¹ç¨‹åºæ‰¿è¯ºè¿›è¡Œç¼–ç ï¼ŒCAPER å¯ä»¥é˜²æ­¢æ‰§è¡Œæ—¶è¿åå®éªŒé€»è¾‘ï¼Œä»è€Œæé«˜å¯æ§æ€§ã€é²æ£’æ€§å’Œæ•°æ®æ•ˆç‡ã€‚å¯¹ç§‘å­¦å·¥ä½œæµç¨‹åŸºå‡†å’Œå…¬å…±é•¿æœŸæ“ä½œæ•°æ®é›†çš„å®éªŒè¯æ˜äº†æˆåŠŸç‡å’Œç¨‹åºæ­£ç¡®æ€§çš„æŒç»­æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½æ•°æ®å’Œé•¿æœŸè®¾ç½®ä¸­ã€‚

</details>

---

## 225. Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs

**ä¸­æ–‡æ ‡é¢˜**: é€šè¿‡è®¾å¤‡ä¸Š LLM çš„ Roofline å»ºæ¨¡å®ç°ç¡¬ä»¶ååŒè®¾è®¡æ‰©å±•æ³•åˆ™

**Date**: 2026-02-10 | **arXiv**: [2602.10377v1](http://arxiv.org/abs/2602.10377v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10377v1)

<details><summary><b>Abstract</b></summary>

Vision-Language-Action Models (VLAs) have emerged as a key paradigm of Physical AI and are increasingly deployed in autonomous vehicles, robots, and smart spaces. In these resource-constrained on-device settings, selecting an appropriate large language model (LLM) backbone is a critical challenge: models must balance accuracy with strict inference latency and hardware efficiency constraints. This makes hardware-software co-design a game-changing requirement for on-device LLM deployment, where each hardware platform demands a tailored architectural solution. We propose a hardware co-design law that jointly captures model accuracy and inference performance. Specifically, we model training loss as an explicit function of architectural hyperparameters and characterise inference latency via roofline modelling. We empirically evaluate 1,942 candidate architectures on NVIDIA Jetson Orin, training 170 selected models for 10B tokens each to fit a scaling law relating architecture to training loss. By coupling this scaling law with latency modelling, we establish a direct accuracy-latency correspondence and identify the Pareto frontier for hardware co-designed LLMs. We further formulate architecture search as a joint optimisation over precision and performance, deriving feasible design regions under industrial hardware and application budgets. Our approach reduces architecture selection from months to days. At the same latency as Qwen2.5-0.5B on the target hardware, our co-designed architecture achieves 19.42% lower perplexity on WikiText-2. To our knowledge, this is the first principled and operational framework for hardware co-design scaling laws in on-device LLM deployment. We will make the code and related checkpoints publicly available.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰å·²æˆä¸ºç‰©ç†äººå·¥æ™ºèƒ½çš„å…³é”®èŒƒä¾‹ï¼Œå¹¶è¶Šæ¥è¶Šå¤šåœ°éƒ¨ç½²åœ¨è‡ªåŠ¨é©¾é©¶æ±½è½¦ã€æœºå™¨äººå’Œæ™ºèƒ½ç©ºé—´ä¸­ã€‚åœ¨è¿™äº›èµ„æºå—é™çš„è®¾å¤‡ä¸Šè®¾ç½®ä¸­ï¼Œé€‰æ‹©åˆé€‚çš„å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) ä¸»å¹²æ˜¯ä¸€é¡¹å…³é”®æŒ‘æˆ˜ï¼šæ¨¡å‹å¿…é¡»åœ¨å‡†ç¡®æ€§ä¸ä¸¥æ ¼çš„æ¨ç†å»¶è¿Ÿå’Œç¡¬ä»¶æ•ˆç‡çº¦æŸä¹‹é—´å–å¾—å¹³è¡¡ã€‚è¿™ä½¿å¾—ç¡¬ä»¶-è½¯ä»¶ååŒè®¾è®¡æˆä¸ºè®¾å¤‡ä¸Š LLM éƒ¨ç½²çš„é¢ è¦†æ€§è¦æ±‚ï¼Œå…¶ä¸­æ¯ä¸ªç¡¬ä»¶å¹³å°éƒ½éœ€è¦é‡èº«å®šåˆ¶çš„æ¶æ„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¡¬ä»¶ååŒè®¾è®¡æ³•åˆ™ï¼Œå¯ä»¥å…±åŒæ•è·æ¨¡å‹å‡†ç¡®æ€§å’Œæ¨ç†æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†è®­ç»ƒæŸå¤±å»ºæ¨¡ä¸ºæ¶æ„è¶…å‚æ•°çš„æ˜¾å¼å‡½æ•°ï¼Œå¹¶é€šè¿‡å±‹é¡¶çº¿å»ºæ¨¡æ¥è¡¨å¾æ¨ç†å»¶è¿Ÿã€‚æˆ‘ä»¬åœ¨ NVIDIA Jetson Orin ä¸Šæ ¹æ®ç»éªŒè¯„ä¼°äº† 1,942 ä¸ªå€™é€‰æ¶æ„ï¼Œä¸ºæ¯ä¸ª 10B ä»¤ç‰Œè®­ç»ƒäº† 170 ä¸ªé€‰å®šæ¨¡å‹ï¼Œä»¥é€‚åº”å°†æ¶æ„ä¸è®­ç»ƒæŸå¤±ç›¸å…³çš„ç¼©æ”¾æ³•åˆ™ã€‚é€šè¿‡å°†è¿™ç§ç¼©æ”¾å®šå¾‹ä¸å»¶è¿Ÿå»ºæ¨¡ç›¸ç»“åˆï¼Œæˆ‘ä»¬å»ºç«‹äº†ç›´æ¥çš„å‡†ç¡®æ€§-å»¶è¿Ÿå¯¹åº”å…³ç³»ï¼Œå¹¶ç¡®å®šäº†ç¡¬ä»¶è”åˆè®¾è®¡çš„ LLM çš„å¸•ç´¯æ‰˜å‰æ²¿ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å°†æ¶æ„æœç´¢åˆ¶å®šä¸ºç²¾åº¦å’Œæ€§èƒ½çš„è”åˆä¼˜åŒ–ï¼Œåœ¨å·¥ä¸šç¡¬ä»¶å’Œåº”ç”¨é¢„ç®—ä¸‹å¾—å‡ºå¯è¡Œçš„è®¾è®¡åŒºåŸŸã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†æ¶æ„é€‰æ‹©ä»å‡ ä¸ªæœˆç¼©çŸ­åˆ°å‡ å¤©ã€‚åœ¨ç›®æ ‡ç¡¬ä»¶ä¸Šä¸ Qwen2.5-0.5B ç›¸åŒçš„å»¶è¿Ÿä¸‹ï¼Œæˆ‘ä»¬å…±åŒè®¾è®¡çš„æ¶æ„åœ¨ WikiText-2 ä¸Šå®ç°äº† 19.42% çš„å›°æƒ‘åº¦é™ä½ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯è®¾å¤‡ä¸Š LLM éƒ¨ç½²ä¸­ç¡¬ä»¶ååŒè®¾è®¡æ‰©å±•æ³•åˆ™çš„ç¬¬ä¸€ä¸ªåŸåˆ™æ€§å’Œå¯æ“ä½œæ¡†æ¶ã€‚æˆ‘ä»¬å°†å…¬å¼€ä»£ç å’Œç›¸å…³æ£€æŸ¥ç‚¹ã€‚

</details>

---

## 226. Simple LLM Baselines are Competitive for Model Diffing

**ä¸­æ–‡æ ‡é¢˜**: ç®€å•çš„ LLM åŸºçº¿å¯¹äºæ¨¡å‹æ¯”è¾ƒæ¥è¯´å…·æœ‰ç«äº‰åŠ›

**Date**: 2026-02-10 | **arXiv**: [2602.10371v1](http://arxiv.org/abs/2602.10371v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10371v1)

<details><summary><b>Abstract</b></summary>

Standard LLM evaluations only test capabilities or dispositions that evaluators designed them for, missing unexpected differences such as behavioral shifts between model revisions or emergent misaligned tendencies. Model diffing addresses this limitation by automatically surfacing systematic behavioral differences. Recent approaches include LLM-based methods that generate natural language descriptions and sparse autoencoder (SAE)-based methods that identify interpretable features. However, no systematic comparison of these approaches exists nor are there established evaluation criteria. We address this gap by proposing evaluation metrics for key desiderata (generalization, interestingness, and abstraction level) and use these to compare existing methods. Our results show that an improved LLM-based baseline performs comparably to the SAE-based method while typically surfacing more abstract behavioral differences.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æ ‡å‡†æ³•å­¦ç¡•å£«è¯„ä¼°ä»…æµ‹è¯•è¯„ä¼°è€…è®¾è®¡çš„èƒ½åŠ›æˆ–æ€§æ ¼ï¼Œå¿½ç•¥äº†æ„æƒ³ä¸åˆ°çš„å·®å¼‚ï¼Œä¾‹å¦‚æ¨¡å‹ä¿®è®¢ä¹‹é—´çš„è¡Œä¸ºè½¬å˜æˆ–å‡ºç°çš„ä¸ä¸€è‡´å€¾å‘ã€‚æ¨¡å‹å·®å¼‚é€šè¿‡è‡ªåŠ¨å‘ˆç°ç³»ç»Ÿè¡Œä¸ºå·®å¼‚æ¥è§£å†³æ­¤é™åˆ¶ã€‚æœ€è¿‘çš„æ–¹æ³•åŒ…æ‹¬ç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°çš„åŸºäº LLM çš„æ–¹æ³•å’Œè¯†åˆ«å¯è§£é‡Šç‰¹å¾çš„åŸºäºç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ (SAE) çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¸å­˜åœ¨ç³»ç»Ÿæ¯”è¾ƒï¼Œä¹Ÿæ²¡æœ‰æ—¢å®šçš„è¯„ä¼°æ ‡å‡†ã€‚æˆ‘ä»¬é€šè¿‡æå‡ºå…³é”®éœ€æ±‚ï¼ˆæ³›åŒ–æ€§ã€è¶£å‘³æ€§å’ŒæŠ½è±¡æ°´å¹³ï¼‰çš„è¯„ä¼°æŒ‡æ ‡æ¥è§£å†³è¿™ä¸€å·®è·ï¼Œå¹¶ä½¿ç”¨è¿™äº›æŒ‡æ ‡æ¥æ¯”è¾ƒç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ”¹è¿›çš„åŸºäº LLM çš„åŸºçº¿ä¸åŸºäº SAE çš„æ–¹æ³•çš„æ€§èƒ½ç›¸å½“ï¼ŒåŒæ—¶é€šå¸¸ä¼šè¡¨ç°å‡ºæ›´æŠ½è±¡çš„è¡Œä¸ºå·®å¼‚ã€‚

</details>

---

## 227. Efficient reduction of stellar contamination and noise in planetary transmission spectra using neural networks

**ä¸­æ–‡æ ‡é¢˜**: ä½¿ç”¨ç¥ç»ç½‘ç»œæœ‰æ•ˆå‡å°‘è¡Œæ˜Ÿä¼ è¾“å…‰è°±ä¸­çš„æ’æ˜Ÿæ±¡æŸ“å’Œå™ªå£°

**Date**: 2026-02-10 | **arXiv**: [2602.10330v1](http://arxiv.org/abs/2602.10330v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10330v1)

<details><summary><b>Abstract</b></summary>

Context: JWST has enabled transmission spectroscopy at unprecedented precision, but stellar heterogeneities (spots and faculae) remain a dominant contamination source that can bias atmospheric retrievals if uncorrected. Aims: We present a fast, unsupervised methodology to reduce stellar contamination and instrument-specific noise in exoplanet transmission spectra using denoising autoencoders, improving the reliability of retrieved atmospheric parameters. Methods: We design and train denoising autoencoder architectures on large synthetic datasets of terrestrial (TRAPPIST-1e analogues) and sub-Neptune (K2-18b analogues) planets. Reconstruction quality is evaluated with the $Ï‡^2$ statistic over a wide range of signal-to-noise ratios, and atmospheric retrieval experiments on contaminated spectra are used to compare against standard correction approaches in accuracy and computational cost. Results: The autoencoders reconstruct uncontaminated spectra while preserving key molecular features, even at low S/N. In retrieval tests, pre-processing with denoising autoencoders reduces bias in inferred abundances relative to uncorrected baselines and matches the accuracy of simultaneous stellar-contamination fitting while reducing computational time by a factor of three to six. Conclusions: Denoising autoencoders provide an efficient alternative to conventional correction strategies and are promising components of future atmospheric characterization pipelines for both rocky and gaseous exoplanets.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

èƒŒæ™¯ï¼šJWST ä½¿é€å°„å…‰è°±è¾¾åˆ°äº†å‰æ‰€æœªæœ‰çš„ç²¾åº¦ï¼Œä½†æ’æ˜Ÿå¼‚è´¨æ€§ï¼ˆæ–‘ç‚¹å’Œå…‰æ–‘ï¼‰ä»ç„¶æ˜¯ä¸»è¦çš„æ±¡æŸ“æºï¼Œå¦‚æœä¸åŠ ä»¥çº æ­£ï¼Œå¯èƒ½ä¼šå¯¼è‡´å¤§æ°”åæ¼”å‡ºç°åå·®ã€‚ç›®æ ‡ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¿«é€Ÿã€æ— ç›‘ç£çš„æ–¹æ³•ï¼Œä½¿ç”¨å»å™ªè‡ªåŠ¨ç¼–ç å™¨æ¥å‡å°‘ç³»å¤–è¡Œæ˜Ÿä¼ è¾“å…‰è°±ä¸­çš„æ’æ˜Ÿæ±¡æŸ“å’Œä»ªå™¨ç‰¹å®šå™ªå£°ï¼Œä»è€Œæé«˜æ£€ç´¢å¤§æ°”å‚æ•°çš„å¯é æ€§ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬åœ¨ç±»åœ°è¡Œæ˜Ÿï¼ˆTRAPPIST-1e ç±»ä¼¼ç‰©ï¼‰å’Œäºšæµ·ç‹æ˜Ÿè¡Œæ˜Ÿï¼ˆK2-18b ç±»ä¼¼ç‰©ï¼‰çš„å¤§å‹åˆæˆæ•°æ®é›†ä¸Šè®¾è®¡å’Œè®­ç»ƒå»å™ªè‡ªåŠ¨ç¼–ç å™¨æ¶æ„ã€‚é‡å»ºè´¨é‡ä½¿ç”¨ $Ï‡^2$ ç»Ÿè®¡é‡åœ¨å¹¿æ³›çš„ä¿¡å™ªæ¯”èŒƒå›´å†…è¿›è¡Œè¯„ä¼°ï¼Œå¹¶ä¸”ä½¿ç”¨æ±¡æŸ“å…‰è°±çš„å¤§æ°”åæ¼”å®éªŒæ¥ä¸æ ‡å‡†æ ¡æ­£æ–¹æ³•çš„å‡†ç¡®æ€§å’Œè®¡ç®—æˆæœ¬è¿›è¡Œæ¯”è¾ƒã€‚ç»“æœï¼šè‡ªåŠ¨ç¼–ç å™¨é‡å»ºæœªå—æ±¡æŸ“çš„å…‰è°±ï¼ŒåŒæ—¶ä¿ç•™å…³é”®åˆ†å­ç‰¹å¾ï¼Œå³ä½¿åœ¨ä½ä¿¡å™ªæ¯”çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚åœ¨æ£€ç´¢æµ‹è¯•ä¸­ï¼Œä½¿ç”¨å»å™ªè‡ªåŠ¨ç¼–ç å™¨è¿›è¡Œé¢„å¤„ç†å¯ä»¥å‡å°‘æ¨æ–­ä¸°åº¦ç›¸å¯¹äºæœªæ ¡æ­£åŸºçº¿çš„åå·®ï¼Œå¹¶ä¸åŒæ—¶æ’æ˜Ÿæ±¡æŸ“æ‹Ÿåˆçš„å‡†ç¡®æ€§ç›¸åŒ¹é…ï¼ŒåŒæ—¶å°†è®¡ç®—æ—¶é—´å‡å°‘ä¸‰åˆ°å…­å€ã€‚ç»“è®ºï¼šå»å™ªè‡ªåŠ¨ç¼–ç å™¨æä¾›äº†ä¼ ç»Ÿæ ¡æ­£ç­–ç•¥çš„æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶ä¸”æ˜¯æœªæ¥å²©çŸ³å’Œæ°”æ€ç³»å¤–è¡Œæ˜Ÿå¤§æ°”ç‰¹å¾ç®¡é“çš„æœ‰å‰é€”çš„ç»„æˆéƒ¨åˆ†ã€‚

</details>

---

## 228. R2RAG-Flood: A reasoning-reinforced training-free retrieval augmentation generation framework for flood damage nowcasting

**ä¸­æ–‡æ ‡é¢˜**: R2RAG-Floodï¼šç”¨äºæ´ªç¾ä¸´è¿‘é¢„æŠ¥çš„æ¨ç†å¼ºåŒ–å…è®­ç»ƒæ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶

**Date**: 2026-02-10 | **arXiv**: [2602.10312v1](http://arxiv.org/abs/2602.10312v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10312v1)

<details><summary><b>Abstract</b></summary>

R2RAG-Flood is a reasoning-reinforced, training-free retrieval-augmented generation framework for post-storm property damage nowcasting. Building on an existing supervised tabular predictor, the framework constructs a reasoning-centric knowledge base composed of labeled tabular records, where each sample includes structured predictors, a compact natural language text-mode summary, and a model-generated reasoning trajectory. During inference, R2RAG-Flood issues context-augmented prompts that retrieve and condition on relevant reasoning trajectories from nearby geospatial neighbors and canonical class prototypes, enabling the large language model backbone to emulate and adapt prior reasoning rather than learn new task-specific parameters. Predictions follow a two-stage procedure that first determines property damage occurrence and then refines severity within a three-level Property Damage Extent categorization, with a conditional downgrade step to correct over-predicted severity. In a case study of Harris County, Texas at the 12-digit Hydrologic Unit Code scale, the supervised tabular baseline trained directly on structured predictors achieves 0.714 overall accuracy and 0.859 damage class accuracy for medium and high damage classes. Across seven large language model backbones, R2RAG-Flood attains 0.613 to 0.668 overall accuracy and 0.757 to 0.896 damage class accuracy, approaching the supervised baseline while additionally producing a structured rationale for each prediction. Using a severity-per-cost efficiency metric derived from API pricing and GPU instance costs, lightweight R2RAG-Flood variants demonstrate substantially higher efficiency than both the supervised tabular baseline and larger language models, while requiring no task-specific training or fine-tuning.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

R2RAG-Flood æ˜¯ä¸€ç§æ¨ç†å¼ºåŒ–ã€å…è®­ç»ƒæ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºé£æš´åè´¢äº§æŸå¤±ä¸´è¿‘é¢„æŠ¥ã€‚è¯¥æ¡†æ¶ä»¥ç°æœ‰çš„ç›‘ç£è¡¨æ ¼é¢„æµ‹å™¨ä¸ºåŸºç¡€ï¼Œæ„å»ºäº†ä¸€ä¸ªç”±æ ‡è®°è¡¨æ ¼è®°å½•ç»„æˆçš„ä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„çŸ¥è¯†åº“ï¼Œå…¶ä¸­æ¯ä¸ªæ ·æœ¬éƒ½åŒ…å«ç»“æ„åŒ–é¢„æµ‹å™¨ã€ç´§å‡‘çš„è‡ªç„¶è¯­è¨€æ–‡æœ¬æ¨¡å¼æ‘˜è¦å’Œæ¨¡å‹ç”Ÿæˆçš„æ¨ç†è½¨è¿¹ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒR2RAG-Flood å‘å‡ºä¸Šä¸‹æ–‡å¢å¼ºæç¤ºï¼Œä»é™„è¿‘çš„åœ°ç†ç©ºé—´é‚»å±…å’Œè§„èŒƒç±»åŸå‹æ£€ç´¢ç›¸å…³æ¨ç†è½¨è¿¹å¹¶è¿›è¡Œæ¡ä»¶è°ƒèŠ‚ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹éª¨å¹²èƒ½å¤Ÿæ¨¡æ‹Ÿå’Œé€‚åº”å…ˆå‰çš„æ¨ç†ï¼Œè€Œä¸æ˜¯å­¦ä¹ æ–°çš„ç‰¹å®šäºä»»åŠ¡çš„å‚æ•°ã€‚é¢„æµ‹éµå¾ªä¸¤é˜¶æ®µç¨‹åºï¼Œé¦–å…ˆç¡®å®šè´¢äº§æŸå¤±çš„å‘ç”Ÿæƒ…å†µï¼Œç„¶ååœ¨ä¸‰çº§è´¢äº§æŸå¤±èŒƒå›´åˆ†ç±»ä¸­ç»†åŒ–ä¸¥é‡æ€§ï¼Œå¹¶é€šè¿‡æœ‰æ¡ä»¶çš„é™çº§æ­¥éª¤æ¥çº æ­£è¿‡é«˜é¢„æµ‹çš„ä¸¥é‡æ€§ã€‚åœ¨å¾·å…‹è¨æ–¯å·å“ˆé‡Œæ–¯å¿çš„ 12 ä½æ°´æ–‡å•ä½ä»£ç è§„æ¨¡çš„æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œç›´æ¥åœ¨ç»“æ„åŒ–é¢„æµ‹å™¨ä¸Šè®­ç»ƒçš„ç›‘ç£è¡¨æ ¼åŸºçº¿å®ç°äº† 0.714 çš„æ€»ä½“å‡†ç¡®åº¦å’Œ 0.859 çš„ä¸­ç­‰å’Œé«˜æŸå®³ç±»åˆ«çš„æŸå®³ç±»åˆ«å‡†ç¡®åº¦ã€‚åœ¨ä¸ƒä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸»å¹²ä¸­ï¼ŒR2RAG-Flood è·å¾—äº† 0.613 è‡³ 0.668 çš„æ•´ä½“å‡†ç¡®åº¦å’Œ 0.757 è‡³ 0.896 çš„æŸå®³ç±»åˆ«å‡†ç¡®åº¦ï¼Œæ¥è¿‘ç›‘ç£åŸºçº¿ï¼ŒåŒæ—¶è¿˜ä¸ºæ¯ä¸ªé¢„æµ‹ç”Ÿæˆäº†ç»“æ„åŒ–çš„åŸºæœ¬åŸç†ã€‚ä½¿ç”¨ä» API å®šä»·å’Œ GPU å®ä¾‹æˆæœ¬å¾—å‡ºçš„æŒ‰æˆæœ¬ä¸¥é‡æ€§æ•ˆç‡æŒ‡æ ‡ï¼Œè½»é‡çº§ R2RAG-Flood å˜ä½“è¡¨ç°å‡ºæ¯”å—ç›‘ç£è¡¨æ ¼åŸºçº¿å’Œæ›´å¤§è¯­è¨€æ¨¡å‹é«˜å¾—å¤šçš„æ•ˆç‡ï¼ŒåŒæ—¶ä¸éœ€è¦ç‰¹å®šäºä»»åŠ¡çš„è®­ç»ƒæˆ–å¾®è°ƒã€‚

</details>

---

## 229. Temper-Then-Tilt: Principled Unlearning for Generative Models through Tempering and Classifier Guidance

**ä¸­æ–‡æ ‡é¢˜**: Temper-Then-Tiltï¼šé€šè¿‡è®­ç»ƒå’Œåˆ†ç±»å™¨æŒ‡å¯¼å¯¹ç”Ÿæˆæ¨¡å‹è¿›è¡ŒåŸåˆ™æ€§çš„å¿˜å´

**Date**: 2026-02-10 | **arXiv**: [2602.10217v1](http://arxiv.org/abs/2602.10217v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10217v1)

<details><summary><b>Abstract</b></summary>

We study machine unlearning in large generative models by framing the task as density ratio estimation to a target distribution rather than supervised fine-tuning. While classifier guidance is a standard approach for approximating this ratio and can succeed in general, we show it can fail to faithfully unlearn with finite samples when the forget set represents a sharp, concentrated data distribution. To address this, we introduce Temper-Then-Tilt Unlearning (T3-Unlearning), which freezes the base model and applies a two-step inference procedure: (i) tempering the base distribution to flatten high-confidence spikes, and (ii) tilting the tempered distribution using a lightweight classifier trained to distinguish retain from forget samples. Our theoretical analysis provides finite-sample guarantees linking the surrogate classifier's risk to unlearning error, proving that tempering is necessary to successfully unlearn for concentrated distributions. Empirical evaluations on the TOFU benchmark show that T3-Unlearning improves forget quality and generative utility over existing baselines, while training only a fraction of the parameters with a minimal runtime.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æˆ‘ä»¬é€šè¿‡å°†ä»»åŠ¡æ¡†æ¶ä¸ºç›®æ ‡åˆ†å¸ƒçš„å¯†åº¦æ¯”ä¼°è®¡è€Œä¸æ˜¯ç›‘ç£å¾®è°ƒæ¥ç ”ç©¶å¤§å‹ç”Ÿæˆæ¨¡å‹ä¸­çš„æœºå™¨é—å¿˜ã€‚è™½ç„¶åˆ†ç±»å™¨æŒ‡å¯¼æ˜¯è¿‘ä¼¼è¯¥æ¯”ç‡çš„æ ‡å‡†æ–¹æ³•å¹¶ä¸”é€šå¸¸å¯ä»¥æˆåŠŸï¼Œä½†æˆ‘ä»¬è¡¨æ˜ï¼Œå½“é—å¿˜é›†ä»£è¡¨å°–é”ã€é›†ä¸­çš„æ•°æ®åˆ†å¸ƒæ—¶ï¼Œå®ƒå¯èƒ½æ— æ³•å¿ å®åœ°å¿˜è®°æœ‰é™æ ·æœ¬ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Temper-Then-Tilt Unlearningï¼ˆT3-Unlearningï¼‰ï¼Œå®ƒå†»ç»“åŸºç¡€æ¨¡å‹å¹¶åº”ç”¨ä¸¤æ­¥æ¨ç†è¿‡ç¨‹ï¼šï¼ˆiï¼‰è°ƒæ•´åŸºç¡€åˆ†å¸ƒä»¥å‹å¹³é«˜ç½®ä¿¡åº¦å°–å³°ï¼Œä»¥åŠï¼ˆiiï¼‰ä½¿ç”¨ç»è¿‡è®­ç»ƒä»¥åŒºåˆ†ä¿ç•™æ ·æœ¬å’Œé—å¿˜æ ·æœ¬çš„è½»é‡çº§åˆ†ç±»å™¨å€¾æ–œè°ƒæ•´åˆ†å¸ƒã€‚æˆ‘ä»¬çš„ç†è®ºåˆ†ææä¾›äº†æœ‰é™æ ·æœ¬ä¿è¯ï¼Œå°†ä»£ç†åˆ†ç±»å™¨çš„é£é™©ä¸é—å¿˜é”™è¯¯è”ç³»èµ·æ¥ï¼Œè¯æ˜è°ƒèŠ‚å¯¹äºæˆåŠŸé—å¿˜é›†ä¸­åˆ†å¸ƒæ˜¯å¿…è¦çš„ã€‚å¯¹ TOFU åŸºå‡†çš„å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒT3-Unlearning æ¯”ç°æœ‰åŸºçº¿æé«˜äº†é—å¿˜è´¨é‡å’Œç”Ÿæˆæ•ˆç”¨ï¼ŒåŒæ—¶ä»¥æœ€çŸ­çš„è¿è¡Œæ—¶é—´ä»…è®­ç»ƒä¸€å°éƒ¨åˆ†å‚æ•°ã€‚

</details>

---

## 230. ELROND: Exploring and decomposing intrinsic capabilities of diffusion models

**ä¸­æ–‡æ ‡é¢˜**: ELRONDï¼šæ¢ç´¢å’Œåˆ†è§£æ‰©æ•£æ¨¡å‹çš„å†…åœ¨èƒ½åŠ›

**Date**: 2026-02-10 | **arXiv**: [2602.10216v1](http://arxiv.org/abs/2602.10216v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10216v1)

<details><summary><b>Abstract</b></summary>

A single text prompt passed to a diffusion model often yields a wide range of visual outputs determined solely by stochastic process, leaving users with no direct control over which specific semantic variations appear in the image. While existing unsupervised methods attempt to analyze these variations via output features, they omit the underlying generative process. In this work, we propose a framework to disentangle these semantic directions directly within the input embedding space. To that end, we collect a set of gradients obtained by backpropagating the differences between stochastic realizations of a fixed prompt that we later decompose into meaningful steering directions with either Principal Components Analysis or Sparse Autoencoder. Our approach yields three key contributions: (1) it isolates interpretable, steerable directions for precise, fine-grained control over a single concept; (2) it effectively mitigates mode collapse in distilled models by reintroducing lost diversity; and (3) it establishes a novel estimator for concept complexity under a specific model, based on the dimensionality of the discovered subspace.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ä¼ é€’åˆ°æ‰©æ•£æ¨¡å‹çš„å•ä¸ªæ–‡æœ¬æç¤ºé€šå¸¸ä¼šäº§ç”Ÿä»…ç”±éšæœºè¿‡ç¨‹å†³å®šçš„å„ç§è§†è§‰è¾“å‡ºï¼Œä½¿ç”¨æˆ·æ— æ³•ç›´æ¥æ§åˆ¶å›¾åƒä¸­å‡ºç°çš„ç‰¹å®šè¯­ä¹‰å˜åŒ–ã€‚è™½ç„¶ç°æœ‰çš„æ— ç›‘ç£æ–¹æ³•è¯•å›¾é€šè¿‡è¾“å‡ºç‰¹å¾æ¥åˆ†æè¿™äº›å˜åŒ–ï¼Œä½†å®ƒä»¬å¿½ç•¥äº†åº•å±‚çš„ç”Ÿæˆè¿‡ç¨‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¡†æ¶æ¥ç›´æ¥åœ¨è¾“å…¥åµŒå…¥ç©ºé—´ä¸­è§£å¼€è¿™äº›è¯­ä¹‰æ–¹å‘ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ç»„æ¢¯åº¦ï¼Œè¿™äº›æ¢¯åº¦æ˜¯é€šè¿‡åå‘ä¼ æ’­å›ºå®šæç¤ºçš„éšæœºå®ç°ä¹‹é—´çš„å·®å¼‚è€Œè·å¾—çš„ï¼Œéšåæˆ‘ä»¬ä½¿ç”¨ä¸»æˆåˆ†åˆ†ææˆ–ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨å°†å…¶åˆ†è§£ä¸ºæœ‰æ„ä¹‰çš„è½¬å‘æ–¹å‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•äº§ç”Ÿäº†ä¸‰ä¸ªå…³é”®è´¡çŒ®ï¼šï¼ˆ1ï¼‰å®ƒéš”ç¦»äº†å¯è§£é‡Šã€å¯æ“çºµçš„æ–¹å‘ï¼Œä»¥å¯¹å•ä¸ªæ¦‚å¿µè¿›è¡Œç²¾ç¡®ã€ç»†ç²’åº¦çš„æ§åˆ¶ï¼› ï¼ˆ2ï¼‰é€šè¿‡é‡æ–°å¼•å…¥ä¸¢å¤±çš„å¤šæ ·æ€§ï¼Œæœ‰æ•ˆç¼“è§£è’¸é¦æ¨¡å‹ä¸­çš„æ¨¡å¼å´©æºƒï¼› (3)åŸºäºæ‰€å‘ç°çš„å­ç©ºé—´çš„ç»´æ•°ï¼Œåœ¨ç‰¹å®šæ¨¡å‹ä¸‹å»ºç«‹äº†ä¸€ç§æ–°é¢–çš„æ¦‚å¿µå¤æ‚æ€§ä¼°è®¡å™¨ã€‚

</details>

---

## 231. Features as Rewards: Scalable Supervision for Open-Ended Tasks via Interpretability

**ä¸­æ–‡æ ‡é¢˜**: ä½œä¸ºå¥–åŠ±çš„åŠŸèƒ½ï¼šé€šè¿‡å¯è§£é‡Šæ€§å¯¹å¼€æ”¾å¼ä»»åŠ¡è¿›è¡Œå¯æ‰©å±•çš„ç›‘ç£

**Date**: 2026-02-10 | **arXiv**: [2602.10067v1](http://arxiv.org/abs/2602.10067v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10067v1)

<details><summary><b>Abstract</b></summary>

Language models trained on large-scale datasets have been shown to learn features that encode abstract concepts such as factuality or intent. Such features are traditionally used for test-time monitoring or steering. We present an alternative affordance: features as scalable supervision for open-ended tasks. We consider the case of hallucination-reduction as a desirable, yet open-ended behavior and design a reinforcement learning (RL) pipeline, titled RLFR (Reinforcement Learning from Feature Rewards), that uses features as reward functions. Grounded in a novel probing framework that identifies candidate hallucinated claims, our pipeline teaches a model to intervene and correct its completions when it is uncertain of their factuality. Furthermore, the pipeline enables scalable test-time compute, guided once more by our reward features. This end-to-end process operationalized on Gemma-3-12B-IT results in a policy that is 58% less likely to hallucinate compared to the original model, while preserving performance on standard benchmarks. Taken together, by grounding supervision in the language of features, this paper introduces a novel paradigm in the use of interpretability for learning open-ended tasks.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè®­ç»ƒçš„è¯­è¨€æ¨¡å‹å·²è¢«è¯æ˜å¯ä»¥å­¦ä¹ ç¼–ç æŠ½è±¡æ¦‚å¿µï¼ˆä¾‹å¦‚äº‹å®æˆ–æ„å›¾ï¼‰çš„ç‰¹å¾ã€‚è¿™äº›åŠŸèƒ½ä¼ ç»Ÿä¸Šç”¨äºæµ‹è¯•æ—¶é—´ç›‘æ§æˆ–æŒ‡å¯¼ã€‚æˆ‘ä»¬æå‡ºäº†å¦ä¸€ç§å¯ä¾›é€‰æ‹©çš„åŠŸèƒ½ï¼šå¯¹å¼€æ”¾å¼ä»»åŠ¡è¿›è¡Œå¯æ‰©å±•çš„ç›‘ç£ã€‚æˆ‘ä»¬è®¤ä¸ºå‡å°‘å¹»è§‰çš„æƒ…å†µæ˜¯ä¸€ç§ç†æƒ³çš„ã€ä½†å¼€æ”¾å¼çš„è¡Œä¸ºï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®¡é“ï¼Œåä¸º RLFRï¼ˆç‰¹å¾å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼‰ï¼Œå®ƒä½¿ç”¨ç‰¹å¾ä½œä¸ºå¥–åŠ±å‡½æ•°ã€‚æˆ‘ä»¬çš„æµç¨‹åŸºäºä¸€ç§æ–°é¢–çš„æ¢æµ‹æ¡†æ¶ï¼Œå¯ä»¥è¯†åˆ«å€™é€‰äººçš„å¹»è§‰ä¸»å¼ ï¼Œå¹¶æ•™å¯¼æ¨¡å‹åœ¨ä¸ç¡®å®šå…¶çœŸå®æ€§æ—¶è¿›è¡Œå¹²é¢„å’Œçº æ­£å…¶å®Œæˆæƒ…å†µã€‚æ­¤å¤–ï¼Œè¯¥ç®¡é“è¿˜å¯ä»¥åœ¨æˆ‘ä»¬çš„å¥–åŠ±åŠŸèƒ½çš„æŒ‡å¯¼ä¸‹å®ç°å¯æ‰©å±•çš„æµ‹è¯•æ—¶è®¡ç®—ã€‚ä¸åŸå§‹æ¨¡å‹ç›¸æ¯”ï¼Œåœ¨ Gemma-3-12B-IT ä¸Šå®æ–½çš„è¿™ä¸€ç«¯åˆ°ç«¯æµç¨‹å¯é™ä½ 58% çš„ç­–ç•¥äº§ç”Ÿå¹»è§‰çš„å¯èƒ½æ€§ï¼ŒåŒæ—¶ä¿æŒæ ‡å‡†åŸºå‡†çš„æ€§èƒ½ã€‚æ€»è€Œè¨€ä¹‹ï¼Œé€šè¿‡ä»¥ç‰¹å¾è¯­è¨€ä¸ºåŸºç¡€çš„ç›‘ç£ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§ä½¿ç”¨å¯è§£é‡Šæ€§å­¦ä¹ å¼€æ”¾å¼ä»»åŠ¡çš„æ–°é¢–èŒƒå¼ã€‚

</details>

---

## 232. Evaluating Disentangled Representations for Controllable Music Generation

**ä¸­æ–‡æ ‡é¢˜**: è¯„ä¼°å¯æ§éŸ³ä¹ç”Ÿæˆçš„è§£ç¼ ç»“è¡¨ç¤º

**Date**: 2026-02-10 | **arXiv**: [2602.10058v1](http://arxiv.org/abs/2602.10058v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10058v1)

<details><summary><b>Abstract</b></summary>

Recent approaches in music generation rely on disentangled representations, often labeled as structure and timbre or local and global, to enable controllable synthesis. Yet the underlying properties of these embeddings remain underexplored. In this work, we evaluate such disentangled representations in a set of music audio models for controllable generation using a probing-based framework that goes beyond standard downstream tasks. The selected models reflect diverse unsupervised disentanglement strategies, including inductive biases, data augmentations, adversarial objectives, and staged training procedures. We further isolate specific strategies to analyze their effect. Our analysis spans four key axes: informativeness, equivariance, invariance, and disentanglement, which are assessed across datasets, tasks, and controlled transformations. Our findings reveal inconsistencies between intended and actual semantics of the embeddings, suggesting that current strategies fall short of producing truly disentangled representations, and prompting a re-examination of how controllability is approached in music generation.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æœ€è¿‘çš„éŸ³ä¹ç”Ÿæˆæ–¹æ³•ä¾èµ–äºè§£å¼€çš„è¡¨ç¤ºï¼Œé€šå¸¸æ ‡è®°ä¸ºç»“æ„å’ŒéŸ³è‰²æˆ–å±€éƒ¨å’Œå…¨å±€ï¼Œä»¥å®ç°å¯æ§åˆæˆã€‚ç„¶è€Œï¼Œè¿™äº›åµŒå…¥çš„åŸºæœ¬å±æ€§ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨è¶…è¶Šæ ‡å‡†ä¸‹æ¸¸ä»»åŠ¡çš„åŸºäºæ¢æµ‹çš„æ¡†æ¶æ¥è¯„ä¼°ä¸€ç»„éŸ³ä¹éŸ³é¢‘æ¨¡å‹ä¸­çš„è¿™ç§è§£å¼€çš„è¡¨ç¤ºï¼Œä»¥å®ç°å¯æ§ç”Ÿæˆã€‚æ‰€é€‰æ¨¡å‹åæ˜ äº†å¤šç§æ— ç›‘ç£è§£ç¼ ç­–ç•¥ï¼ŒåŒ…æ‹¬å½’çº³åå·®ã€æ•°æ®å¢å¼ºã€å¯¹æŠ—æ€§ç›®æ ‡å’Œåˆ†é˜¶æ®µè®­ç»ƒç¨‹åºã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åˆ†ç¦»å‡ºå…·ä½“ç­–ç•¥æ¥åˆ†æå…¶æ•ˆæœã€‚æˆ‘ä»¬çš„åˆ†æè·¨è¶Šå››ä¸ªå…³é”®è½´ï¼šä¿¡æ¯æ€§ã€ç­‰æ–¹å·®ã€ä¸å˜æ€§å’Œè§£ç¼ ç»“ï¼Œè¿™äº›è½´æ˜¯è·¨æ•°æ®é›†ã€ä»»åŠ¡å’Œå—æ§è½¬æ¢è¿›è¡Œè¯„ä¼°çš„ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ­ç¤ºäº†åµŒå…¥çš„é¢„æœŸè¯­ä¹‰å’Œå®é™…è¯­ä¹‰ä¹‹é—´çš„ä¸ä¸€è‡´ï¼Œè¿™è¡¨æ˜å½“å‰çš„ç­–ç•¥æ— æ³•äº§ç”ŸçœŸæ­£è§£å¼€çš„è¡¨ç¤ºï¼Œå¹¶ä¿ƒä½¿äººä»¬é‡æ–°å®¡è§†éŸ³ä¹ç”Ÿæˆä¸­å¦‚ä½•å®ç°å¯æ§æ€§ã€‚

</details>

---

## 233. Answer First, Reason Later: Aligning Search Relevance via Mode-Balanced Reinforcement Learning

**ä¸­æ–‡æ ‡é¢˜**: å…ˆå›ç­”ï¼Œåæ¨ç†ï¼šé€šè¿‡æ¨¡å¼å¹³è¡¡å¼ºåŒ–å­¦ä¹ è°ƒæ•´æœç´¢ç›¸å…³æ€§

**Date**: 2026-02-10 | **arXiv**: [2602.10006v1](http://arxiv.org/abs/2602.10006v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10006v1)

<details><summary><b>Abstract</b></summary>

Building a search relevance model that achieves both low latency and high performance is a long-standing challenge in the search industry. To satisfy the millisecond-level response requirements of online systems while retaining the interpretable reasoning traces of Large Language Models (LLMs), we propose a novel \textbf{Answer-First, Reason Later (AFRL)} paradigm. This paradigm requires the model to output the definitive relevance score in the very first token, followed by a structured logical explanation. Inspired by the success of reasoning models, we adopt a "Supervised Fine-Tuning (SFT) + Reinforcement Learning (RL)" pipeline to achieve AFRL. However, directly applying existing RL training often leads to \textbf{mode collapse} in the search relevance task, where the model forgets complex long-tail rules in pursuit of high rewards. From an information theory perspective: RL inherently minimizes the \textbf{Reverse KL divergence}, which tends to seek probability peaks (mode-seeking) and is prone to "reward hacking." On the other hand, SFT minimizes the \textbf{Forward KL divergence}, forcing the model to cover the data distribution (mode-covering) and effectively anchoring expert rules. Based on this insight, we propose a \textbf{Mode-Balanced Optimization} strategy, incorporating an SFT auxiliary loss into Stepwise-GRPO training to balance these two properties. Furthermore, we construct an automated instruction evolution system and a multi-stage curriculum to ensure expert-level data quality. Extensive experiments demonstrate that our 32B teacher model achieves state-of-the-art performance. Moreover, the AFRL architecture enables efficient knowledge distillation, successfully transferring expert-level logic to a 0.6B model, thereby reconciling reasoning depth with deployment latency.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æ„å»ºåŒæ—¶å®ç°ä½å»¶è¿Ÿå’Œé«˜æ€§èƒ½çš„æœç´¢ç›¸å…³æ€§æ¨¡å‹æ˜¯æœç´¢è¡Œä¸šé•¿æœŸé¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºäº†æ»¡è¶³åœ¨çº¿ç³»ç»Ÿçš„æ¯«ç§’çº§å“åº”è¦æ±‚ï¼ŒåŒæ—¶ä¿ç•™å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯è§£é‡Šæ¨ç†ç—•è¿¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ \textbf{å…ˆå›ç­”ï¼Œåæ¨ç†ï¼ˆAFRLï¼‰}èŒƒå¼ã€‚è¿™ç§èŒƒä¾‹è¦æ±‚æ¨¡å‹åœ¨ç¬¬ä¸€ä¸ªæ ‡è®°ä¸­è¾“å‡ºæ˜ç¡®çš„ç›¸å…³æ€§åˆ†æ•°ï¼Œç„¶åæ˜¯ç»“æ„åŒ–çš„é€»è¾‘è§£é‡Šã€‚å—åˆ°æ¨ç†æ¨¡å‹æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬é‡‡ç”¨â€œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰+å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰â€æµç¨‹æ¥å®ç° AFRLã€‚ç„¶è€Œï¼Œç›´æ¥åº”ç”¨ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒé€šå¸¸ä¼šå¯¼è‡´æœç´¢ç›¸å…³æ€§ä»»åŠ¡ä¸­çš„æ¨¡å¼å´©æºƒï¼Œå³æ¨¡å‹ä¸ºäº†è¿½æ±‚é«˜å¥–åŠ±è€Œå¿˜è®°å¤æ‚çš„é•¿å°¾è§„åˆ™ã€‚ä»ä¿¡æ¯è®ºçš„è§’åº¦æ¥çœ‹ï¼šRL æœ¬è´¨ä¸Šæœ€å°åŒ–äº† \textbf{Reverse KL divergence}ï¼Œå®ƒå€¾å‘äºå¯»æ±‚æ¦‚ç‡å³°å€¼ï¼ˆæ¨¡å¼å¯»æ±‚ï¼‰å¹¶ä¸”å®¹æ˜“å‡ºç°â€œå¥–åŠ±é»‘å®¢â€ã€‚å¦ä¸€æ–¹é¢ï¼ŒSFTæœ€å°åŒ–\textbf{Forward KL divergence}ï¼Œè¿«ä½¿æ¨¡å‹è¦†ç›–æ•°æ®åˆ†å¸ƒï¼ˆæ¨¡å¼è¦†ç›–ï¼‰å¹¶æœ‰æ•ˆé”šå®šä¸“å®¶è§„åˆ™ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ \textbf{æ¨¡å¼å¹³è¡¡ä¼˜åŒ–} ç­–ç•¥ï¼Œå°† SFT è¾…åŠ©æŸå¤±çº³å…¥ Stepwise-GRPO è®­ç»ƒä¸­ä»¥å¹³è¡¡è¿™ä¸¤ä¸ªå±æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†è‡ªåŠ¨åŒ–æŒ‡ä»¤è¿›åŒ–ç³»ç»Ÿå’Œå¤šé˜¶æ®µè¯¾ç¨‹ï¼Œä»¥ç¡®ä¿ä¸“å®¶çº§çš„æ•°æ®è´¨é‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ 32B æ•™å¸ˆæ¨¡å‹å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒAFRLæ¶æ„èƒ½å¤Ÿå®ç°é«˜æ•ˆçš„çŸ¥è¯†è’¸é¦ï¼ŒæˆåŠŸåœ°å°†ä¸“å®¶çº§é€»è¾‘è½¬ç§»åˆ°0.6Bæ¨¡å‹ï¼Œä»è€Œåè°ƒæ¨ç†æ·±åº¦å’Œéƒ¨ç½²å»¶è¿Ÿã€‚

</details>

---

## 234. Rethinking Global Text Conditioning in Diffusion Transformers

**ä¸­æ–‡æ ‡é¢˜**: é‡æ–°æ€è€ƒæ‰©æ•£å˜å‹å™¨ä¸­çš„å…¨å±€æ–‡æœ¬è°ƒèŠ‚

**Date**: 2026-02-09 | **arXiv**: [2602.09268v1](http://arxiv.org/abs/2602.09268v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09268v1)

<details><summary><b>Abstract</b></summary>

Diffusion transformers typically incorporate textual information via attention layers and a modulation mechanism using a pooled text embedding. Nevertheless, recent approaches discard modulation-based text conditioning and rely exclusively on attention. In this paper, we address whether modulation-based text conditioning is necessary and whether it can provide any performance advantage. Our analysis shows that, in its conventional usage, the pooled embedding contributes little to overall performance, suggesting that attention alone is generally sufficient for faithfully propagating prompt information. However, we reveal that the pooled embedding can provide significant gains when used from a different perspective-serving as guidance and enabling controllable shifts toward more desirable properties. This approach is training-free, simple to implement, incurs negligible runtime overhead, and can be applied to various diffusion models, bringing improvements across diverse tasks, including text-to-image/video generation and image editing.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æ‰©æ•£å˜å‹å™¨é€šå¸¸é€šè¿‡æ³¨æ„åŠ›å±‚å’Œä½¿ç”¨æ± æ–‡æœ¬åµŒå…¥çš„è°ƒåˆ¶æœºåˆ¶åˆå¹¶æ–‡æœ¬ä¿¡æ¯ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„æ–¹æ³•æ”¾å¼ƒäº†åŸºäºè°ƒåˆ¶çš„æ–‡æœ¬è°ƒèŠ‚å¹¶å®Œå…¨ä¾èµ–äºæ³¨æ„åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºåŸºäºè°ƒåˆ¶çš„æ–‡æœ¬è°ƒèŠ‚æ˜¯å¦å¿…è¦ä»¥åŠå®ƒæ˜¯å¦å¯ä»¥æä¾›ä»»ä½•æ€§èƒ½ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œåœ¨å¸¸è§„ç”¨æ³•ä¸­ï¼Œæ± åŒ–åµŒå…¥å¯¹æ•´ä½“æ€§èƒ½è´¡çŒ®ä¸å¤§ï¼Œè¿™è¡¨æ˜ä»…æ³¨æ„é€šå¸¸è¶³ä»¥å¿ å®åœ°ä¼ æ’­æç¤ºä¿¡æ¯ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°ï¼Œä»ä¸åŒçš„è§’åº¦ä½¿ç”¨æ—¶ï¼Œæ± åŒ–åµŒå…¥å¯ä»¥æä¾›æ˜¾ç€çš„æ”¶ç›Šâ€”â€”ä½œä¸ºæŒ‡å¯¼å¹¶å®ç°å‘æ›´ç†æƒ³çš„å±æ€§çš„å¯æ§è½¬å˜ã€‚è¿™ç§æ–¹æ³•æ— éœ€åŸ¹è®­ï¼Œæ˜“äºå®ç°ï¼Œè¿è¡Œæ—¶å¼€é”€å¯ä»¥å¿½ç•¥ä¸è®¡ï¼Œå¹¶ä¸”å¯ä»¥åº”ç”¨äºå„ç§æ‰©æ•£æ¨¡å‹ï¼Œä»è€Œæ”¹è¿›å„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒ/è§†é¢‘ç”Ÿæˆå’Œå›¾åƒç¼–è¾‘ã€‚

</details>

---

## 235. VLM-Guided Iterative Refinement for Surgical Image Segmentation with Foundation Models

**ä¸­æ–‡æ ‡é¢˜**: VLM å¼•å¯¼çš„åŸºäºåŸºç¡€æ¨¡å‹çš„æ‰‹æœ¯å›¾åƒåˆ†å‰²è¿­ä»£ç»†åŒ–

**Date**: 2026-02-09 | **arXiv**: [2602.09252v1](http://arxiv.org/abs/2602.09252v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09252v1)

<details><summary><b>Abstract</b></summary>

Surgical image segmentation is essential for robot-assisted surgery and intraoperative guidance. However, existing methods are constrained to predefined categories, produce one-shot predictions without adaptive refinement, and lack mechanisms for clinician interaction. We propose IR-SIS, an iterative refinement system for surgical image segmentation that accepts natural language descriptions. IR-SIS leverages a fine-tuned SAM3 for initial segmentation, employs a Vision-Language Model to detect instruments and assess segmentation quality, and applies an agentic workflow that adaptively selects refinement strategies. The system supports clinician-in-the-loop interaction through natural language feedback. We also construct a multi-granularity language-annotated dataset from EndoVis2017 and EndoVis2018 benchmarks. Experiments demonstrate state-of-the-art performance on both in-domain and out-of-distribution data, with clinician interaction providing additional improvements. Our work establishes the first language-based surgical segmentation framework with adaptive self-refinement capabilities.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æ‰‹æœ¯å›¾åƒåˆ†å‰²å¯¹äºæœºå™¨äººè¾…åŠ©æ‰‹æœ¯å’Œæœ¯ä¸­æŒ‡å¯¼è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•ä»…é™äºé¢„å®šä¹‰çš„ç±»åˆ«ï¼Œäº§ç”Ÿæ²¡æœ‰è‡ªé€‚åº”ç»†åŒ–çš„ä¸€æ¬¡æ€§é¢„æµ‹ï¼Œå¹¶ä¸”ç¼ºä¹ä¸´åºŠåŒ»ç”Ÿäº¤äº’çš„æœºåˆ¶ã€‚æˆ‘ä»¬æå‡ºäº† IR-SISï¼Œè¿™æ˜¯ä¸€ç§æ¥å—è‡ªç„¶è¯­è¨€æè¿°çš„å¤–ç§‘å›¾åƒåˆ†å‰²è¿­ä»£ç»†åŒ–ç³»ç»Ÿã€‚ IR-SIS åˆ©ç”¨å¾®è°ƒçš„ SAM3 è¿›è¡Œåˆå§‹åˆ†å‰²ï¼Œé‡‡ç”¨è§†è§‰è¯­è¨€æ¨¡å‹æ¥æ£€æµ‹ä»ªå™¨å¹¶è¯„ä¼°åˆ†å‰²è´¨é‡ï¼Œå¹¶åº”ç”¨è‡ªé€‚åº”é€‰æ‹©ç»†åŒ–ç­–ç•¥çš„ä»£ç†å·¥ä½œæµç¨‹ã€‚è¯¥ç³»ç»Ÿé€šè¿‡è‡ªç„¶è¯­è¨€åé¦ˆæ”¯æŒä¸´åºŠåŒ»ç”Ÿåœ¨å¾ªç¯ä¸­çš„äº¤äº’ã€‚æˆ‘ä»¬è¿˜æ ¹æ® EndoVis2017 å’Œ EndoVis2018 åŸºå‡†æ„å»ºäº†å¤šç²’åº¦è¯­è¨€æ³¨é‡Šæ•°æ®é›†ã€‚å®éªŒè¯æ˜äº†åŸŸå†…å’Œåˆ†å¸ƒå¤–æ•°æ®çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œä¸´åºŠåŒ»ç”Ÿçš„äº’åŠ¨æä¾›äº†é¢å¤–çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„å·¥ä½œå»ºç«‹äº†ç¬¬ä¸€ä¸ªåŸºäºè¯­è¨€çš„å…·æœ‰è‡ªé€‚åº”è‡ªæˆ‘å®Œå–„åŠŸèƒ½çš„æ‰‹æœ¯åˆ†å‰²æ¡†æ¶ã€‚

</details>

---

## 236. A Systematic Review on Data-Driven Brain Deformation Modeling for Image-Guided Neurosurgery

**ä¸­æ–‡æ ‡é¢˜**: å›¾åƒå¼•å¯¼ç¥ç»å¤–ç§‘æ•°æ®é©±åŠ¨è„‘å˜å½¢å»ºæ¨¡çš„ç³»ç»Ÿç»¼è¿°

**Date**: 2026-02-09 | **arXiv**: [2602.10155v1](http://arxiv.org/abs/2602.10155v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10155v1)

<details><summary><b>Abstract</b></summary>

Accurate compensation of brain deformation is a critical challenge for reliable image-guided neurosurgery, as surgical manipulation and tumor resection induce tissue motion that misaligns preoperative planning images with intraoperative anatomy and longitudinal studies. In this systematic review, we synthesize recent AI-driven approaches developed between January 2020 and April 2025 for modeling and correcting brain deformation. A comprehensive literature search was conducted in PubMed, IEEE Xplore, Scopus, and Web of Science, with predefined inclusion and exclusion criteria focused on computational methods applied to brain deformation compensation for neurosurgical imaging, resulting in 41 studies meeting these criteria. We provide a unified analysis of methodological strategies, including deep learning-based image registration, direct deformation field regression, synthesis-driven multimodal alignment, resection-aware architectures addressing missing correspondences, and hybrid models that integrate biomechanical priors. We also examine dataset utilization, reported evaluation metrics, validation protocols, and how uncertainty and generalization have been assessed across studies. While AI-based deformation models demonstrate promising performance and computational efficiency, current approaches exhibit limitations in out-of-distribution robustness, standardized benchmarking, interpretability, and readiness for clinical deployment. Our review highlights these gaps and outlines opportunities for future research aimed at achieving more robust, generalizable, and clinically translatable deformation compensation solutions for neurosurgical guidance. By organizing recent advances and critically evaluating evaluation practices, this work provides a comprehensive foundation for researchers and clinicians engaged in developing and applying AI-based brain deformation methods.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¤§è„‘å˜å½¢çš„å‡†ç¡®è¡¥å¿æ˜¯å¯é çš„å›¾åƒå¼•å¯¼ç¥ç»å¤–ç§‘æ‰‹æœ¯çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå› ä¸ºæ‰‹æœ¯æ“ä½œå’Œè‚¿ç˜¤åˆ‡é™¤ä¼šå¼•èµ·ç»„ç»‡è¿åŠ¨ï¼Œä½¿æœ¯å‰è®¡åˆ’å›¾åƒä¸æœ¯ä¸­è§£å‰–å’Œçºµå‘ç ”ç©¶ä¸ä¸€è‡´ã€‚åœ¨è¿™ç¯‡ç³»ç»Ÿç»¼è¿°ä¸­ï¼Œæˆ‘ä»¬ç»¼åˆäº† 2020 å¹´ 1 æœˆè‡³ 2025 å¹´ 4 æœˆæœŸé—´å¼€å‘çš„æœ€æ–°äººå·¥æ™ºèƒ½é©±åŠ¨æ–¹æ³•ï¼Œç”¨äºå»ºæ¨¡å’Œçº æ­£å¤§è„‘å˜å½¢ã€‚åœ¨ PubMedã€IEEE Xploreã€Scopus å’Œ Web of Science ä¸Šè¿›è¡Œäº†å…¨é¢çš„æ–‡çŒ®æ£€ç´¢ï¼Œé¢„å®šä¹‰çš„çº³å…¥å’Œæ’é™¤æ ‡å‡†ä¾§é‡äºåº”ç”¨äºç¥ç»å¤–ç§‘æˆåƒè„‘å˜å½¢è¡¥å¿çš„è®¡ç®—æ–¹æ³•ï¼Œç»“æœæœ‰ 41 é¡¹ç ”ç©¶ç¬¦åˆè¿™äº›æ ‡å‡†ã€‚æˆ‘ä»¬æä¾›æ–¹æ³•ç­–ç•¥çš„ç»Ÿä¸€åˆ†æï¼ŒåŒ…æ‹¬åŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒé…å‡†ã€ç›´æ¥å˜å½¢åœºå›å½’ã€åˆæˆé©±åŠ¨çš„å¤šæ¨¡æ€å¯¹é½ã€è§£å†³ç¼ºå¤±å¯¹åº”å…³ç³»çš„åˆ‡é™¤æ„ŸçŸ¥æ¶æ„ä»¥åŠé›†æˆç”Ÿç‰©åŠ›å­¦å…ˆéªŒçš„æ··åˆæ¨¡å‹ã€‚æˆ‘ä»¬è¿˜æ£€æŸ¥æ•°æ®é›†åˆ©ç”¨ç‡ã€æŠ¥å‘Šçš„è¯„ä¼°æŒ‡æ ‡ã€éªŒè¯åè®®ä»¥åŠå¦‚ä½•è¯„ä¼°ç ”ç©¶ä¸­çš„ä¸ç¡®å®šæ€§å’Œæ¦‚æ‹¬æ€§ã€‚è™½ç„¶åŸºäºäººå·¥æ™ºèƒ½çš„å˜å½¢æ¨¡å‹è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ï¼Œä½†å½“å‰çš„æ–¹æ³•åœ¨åˆ†å¸ƒå¤–ç¨³å¥æ€§ã€æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•ã€å¯è§£é‡Šæ€§å’Œä¸´åºŠéƒ¨ç½²å‡†å¤‡æ–¹é¢è¡¨ç°å‡ºå±€é™æ€§ã€‚æˆ‘ä»¬çš„ç»¼è¿°å¼ºè°ƒäº†è¿™äº›å·®è·ï¼Œå¹¶æ¦‚è¿°äº†æœªæ¥ç ”ç©¶çš„æœºä¼šï¼Œæ—¨åœ¨ä¸ºç¥ç»å¤–ç§‘æŒ‡å¯¼å®ç°æ›´ç¨³å¥ã€å¯æ¨å¹¿å’Œä¸´åºŠå¯è½¬åŒ–çš„å˜å½¢è¡¥å¿è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡æ•´ç†æœ€æ–°è¿›å±•å¹¶ä¸¥æ ¼è¯„ä¼°è¯„ä¼°å®è·µï¼Œè¿™é¡¹å·¥ä½œä¸ºä»äº‹å¼€å‘å’Œåº”ç”¨åŸºäºäººå·¥æ™ºèƒ½çš„å¤§è„‘å˜å½¢æ–¹æ³•çš„ç ”ç©¶äººå‘˜å’Œä¸´åºŠåŒ»ç”Ÿæä¾›äº†å…¨é¢çš„åŸºç¡€ã€‚

</details>

---

## 237. VLM-UQBench: A Benchmark for Modality-Specific and Cross-Modality Uncertainties in Vision Language Models

**ä¸­æ–‡æ ‡é¢˜**: VLM-UQBenchï¼šè§†è§‰è¯­è¨€æ¨¡å‹ä¸­ç‰¹å®šæ¨¡æ€å’Œè·¨æ¨¡æ€ä¸ç¡®å®šæ€§çš„åŸºå‡†

**Date**: 2026-02-09 | **arXiv**: [2602.09214v1](http://arxiv.org/abs/2602.09214v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09214v1)

<details><summary><b>Abstract</b></summary>

Uncertainty quantification (UQ) is vital for ensuring that vision-language models (VLMs) behave safely and reliably. A central challenge is to localize uncertainty to its source, determining whether it arises from the image, the text, or misalignment between the two. We introduce VLM-UQBench, a benchmark for modality-specific and cross-modal data uncertainty in VLMs, It consists of 600 real-world samples drawn from the VizWiz dataset, curated into clean, image-, text-, and cross-modal uncertainty subsets, and a scalable perturbation pipeline with 8 visual, 5 textual, and 3 cross-modal perturbations. We further propose two simple metrics that quantify the sensitivity of UQ scores to these perturbations and their correlation with hallucinations, and use them to evaluate a range of UQ methods across four VLMs and three datasets. Empirically, we find that: (i) existing UQ methods exhibit strong modality-specific specialization and substantial dependence on the underlying VLM, (ii) modality-specific uncertainty frequently co-occurs with hallucinations while current UQ scores provide only weak and inconsistent risk signals, and (iii) although UQ methods can rival reasoning-based chain-of-thought baselines on overt, group-level ambiguity, they largely fail to detect the subtle, instance-level ambiguity introduced by our perturbation pipeline. These results highlight a significant gap between current UQ practices and the fine-grained, modality-aware uncertainty required for reliable VLM deployment.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ä¸ç¡®å®šæ€§é‡åŒ– (UQ) å¯¹äºç¡®ä¿è§†è§‰è¯­è¨€æ¨¡å‹ (VLM) å®‰å…¨å¯é åœ°è¿è¡Œè‡³å…³é‡è¦ã€‚ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜æ˜¯å®šä½ä¸ç¡®å®šæ€§çš„æ¥æºï¼Œç¡®å®šå®ƒæ˜¯ç”±å›¾åƒã€æ–‡æœ¬è¿˜æ˜¯ä¸¤è€…ä¹‹é—´çš„é”™ä½å¼•èµ·çš„ã€‚æˆ‘ä»¬å¼•å…¥äº† VLM-UQBenchï¼Œå®ƒæ˜¯ VLM ä¸­ç‰¹å®šæ¨¡æ€å’Œè·¨æ¨¡æ€æ•°æ®ä¸ç¡®å®šæ€§çš„åŸºå‡†ï¼Œå®ƒç”±ä» VizWiz æ•°æ®é›†æå–çš„ 600 ä¸ªçœŸå®ä¸–ç•Œæ ·æœ¬ç»„æˆï¼Œæ•´ç†ä¸ºå¹²å‡€çš„å›¾åƒã€æ–‡æœ¬å’Œè·¨æ¨¡æ€ä¸ç¡®å®šæ€§å­é›†ï¼Œä»¥åŠå…·æœ‰ 8 ä¸ªè§†è§‰æ‰°åŠ¨ã€5 ä¸ªæ–‡æœ¬æ‰°åŠ¨å’Œ 3 ä¸ªè·¨æ¨¡æ€æ‰°åŠ¨çš„å¯æ‰©å±•æ‰°åŠ¨ç®¡é“ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸¤ä¸ªç®€å•çš„æŒ‡æ ‡æ¥é‡åŒ– UQ åˆ†æ•°å¯¹è¿™äº›æ‰°åŠ¨çš„æ•æ„Ÿæ€§åŠå…¶ä¸å¹»è§‰çš„ç›¸å…³æ€§ï¼Œå¹¶ä½¿ç”¨å®ƒä»¬æ¥è¯„ä¼°è·¨å››ä¸ª VLM å’Œä¸‰ä¸ªæ•°æ®é›†çš„ä¸€ç³»åˆ— UQ æ–¹æ³•ã€‚æ ¹æ®ç»éªŒï¼Œæˆ‘ä»¬å‘ç°ï¼šï¼ˆiï¼‰ç°æœ‰çš„æ˜†å£«å…°å¤§å­¦æ–¹æ³•è¡¨ç°å‡ºå¼ºçƒˆçš„ç‰¹å®šäºæ¨¡æ€çš„ä¸“ä¸šåŒ–å’Œå¯¹åº•å±‚ VLM çš„å®è´¨æ€§ä¾èµ–ï¼Œï¼ˆiiï¼‰ç‰¹å®šäºæ¨¡æ€çš„ä¸ç¡®å®šæ€§ç»å¸¸ä¸å¹»è§‰åŒæ—¶å‡ºç°ï¼Œè€Œå½“å‰çš„ UQ åˆ†æ•°ä»…æä¾›å¾®å¼±ä¸”ä¸ä¸€è‡´çš„é£é™©ä¿¡å·ï¼Œä»¥åŠï¼ˆiiiï¼‰å°½ç®¡ UQ æ–¹æ³•å¯ä»¥ä¸åŸºäºæ¨ç†çš„æ€ç»´é“¾åŸºçº¿åœ¨æ˜æ˜¾çš„ç¾¤ä½“çº§æ¨¡ç³Šæ€§ä¸Šç›¸åª²ç¾ï¼Œä½†å®ƒä»¬åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ— æ³•æ£€æµ‹åˆ°æˆ‘ä»¬å¼•å…¥çš„å¾®å¦™çš„å®ä¾‹çº§æ¨¡ç³Šæ€§ã€‚æ‰°åŠ¨ç®¡é“ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†æ˜†å£«å…°å¤§å­¦å½“å‰çš„å®è·µä¸å¯é çš„ VLM éƒ¨ç½²æ‰€éœ€çš„ç»†ç²’åº¦ã€æ¨¡æ€æ„ŸçŸ¥çš„ä¸ç¡®å®šæ€§ä¹‹é—´å­˜åœ¨æ˜¾ç€å·®è·ã€‚

</details>

---

## 238. Wearable environmental sensing to forecast how legged systems will interact with upcoming terrain

**ä¸­æ–‡æ ‡é¢˜**: å¯ç©¿æˆ´ç¯å¢ƒä¼ æ„Ÿå¯é¢„æµ‹è…¿å¼ç³»ç»Ÿå¦‚ä½•ä¸å³å°†åˆ°æ¥çš„åœ°å½¢ç›¸äº’ä½œç”¨

**Date**: 2026-02-09 | **arXiv**: [2602.09209v1](http://arxiv.org/abs/2602.09209v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09209v1)

<details><summary><b>Abstract</b></summary>

Computer-vision (CV) has been used for environmental classification during gait and is often used to inform control in assistive systems; however, the ability to predict how the foot will contact a changing environment is underexplored. We evaluated the feasibility of forecasting the anterior-posterior (AP) foot center-of-pressure (COP) and time-of-impact (TOI) prior to foot-strike on a level-ground to stair-ascent transition. Eight subjects wore an RGB-D camera on their right shank and instrumented insoles while performing the task of stepping onto the stairs. We trained a CNN-RNN to forecast the COP and TOI continuously within a 250ms window prior to foot-strike, termed the forecast horizon (FH). The COP mean-absolute-error (MAE) at 150, 100, and 50ms FH was 29.42mm, 26.82, and 23.72mm respectively. The TOI MAE was 21.14, 20.08, and 17.73ms for 150, 100, and 50ms respectively. While torso velocity had no effect on the error in either task, faster toe-swing speeds prior to foot-strike were found to improve the prediction accuracy in the COP case, however, was insignificant in the TOI case. Further, more anterior foot-strikes were found to reduce COP prediction accuracy but did not affect the TOI prediction accuracy. We also found that our lightweight model was capable at running at 60 FPS on either a consumer grade laptop or an edge computing device. This study demonstrates that forecasting COP and TOI from visual data was feasible using a lightweight model, which may have important implications for anticipatory control in assistive systems.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰å·²ç”¨äºæ­¥æ€æœŸé—´çš„ç¯å¢ƒåˆ†ç±»ï¼Œå¹¶ä¸”é€šå¸¸ç”¨äºé€šçŸ¥è¾…åŠ©ç³»ç»Ÿä¸­çš„æ§åˆ¶ï¼›ç„¶è€Œï¼Œé¢„æµ‹è¶³éƒ¨å¦‚ä½•æ¥è§¦ä¸æ–­å˜åŒ–çš„ç¯å¢ƒçš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬è¯„ä¼°äº†åœ¨å¹³åœ°åˆ°çˆ¬æ¥¼æ¢¯è¿‡æ¸¡æ—¶é¢„æµ‹è¶³éƒ¨è§¦åœ°å‰çš„å‰å (â€‹â€‹AP) è¶³éƒ¨å‹åŠ›ä¸­å¿ƒ (COP) å’Œå†²å‡»æ—¶é—´ (TOI) çš„å¯è¡Œæ€§ã€‚å…«åå—è¯•è€…åœ¨æ‰§è¡Œä¸Šæ¥¼æ¢¯ä»»åŠ¡æ—¶ï¼Œå³è…¿ä¸Šä½©æˆ´æœ‰ RGB-D ç›¸æœºï¼Œé‹å«ä¸Šè£…æœ‰ä»ªå™¨ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ª CNN-RNNï¼Œåœ¨æ­¥è¡Œä¹‹å‰çš„ 250 æ¯«ç§’çª—å£å†…è¿ç»­é¢„æµ‹ COP å’Œ TOIï¼Œç§°ä¸ºé¢„æµ‹èŒƒå›´ (FH)ã€‚ 150ã€100 å’Œ 50ms FH æ—¶çš„ COP å¹³å‡ç»å¯¹è¯¯å·® (MAE) åˆ†åˆ«ä¸º 29.42mmã€26.82 å’Œ 23.72mmã€‚ 150ã€100 å’Œ 50 æ¯«ç§’æ—¶ï¼ŒTOI MAE åˆ†åˆ«ä¸º 21.14ã€20.08 å’Œ 17.73 æ¯«ç§’ã€‚è™½ç„¶èº¯å¹²é€Ÿåº¦å¯¹è¿™ä¸¤é¡¹ä»»åŠ¡çš„è¯¯å·®æ²¡æœ‰å½±å“ï¼Œä½†åœ¨è„šç€åœ°ä¹‹å‰æ›´å¿«çš„è„šè¶¾æ‘†åŠ¨é€Ÿåº¦è¢«å‘ç°å¯ä»¥æé«˜ COP æƒ…å†µä¸‹çš„é¢„æµ‹å‡†ç¡®æ€§ï¼Œç„¶è€Œï¼Œåœ¨ TOI æƒ…å†µä¸‹å´å¾®ä¸è¶³é“ã€‚æ­¤å¤–ï¼Œæ›´å¤šçš„å‰è„šç€åœ°ä¼šé™ä½ COP é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œä½†ä¸ä¼šå½±å“ TOI é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œæˆ‘ä»¬çš„è½»é‡çº§æ¨¡å‹èƒ½å¤Ÿåœ¨æ¶ˆè´¹çº§ç¬”è®°æœ¬ç”µè„‘æˆ–è¾¹ç¼˜è®¡ç®—è®¾å¤‡ä¸Šä»¥ 60 FPS çš„é€Ÿåº¦è¿è¡Œã€‚è¿™é¡¹ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨è½»é‡çº§æ¨¡å‹ä»è§†è§‰æ•°æ®é¢„æµ‹ COP å’Œ TOI æ˜¯å¯è¡Œçš„ï¼Œè¿™å¯èƒ½å¯¹è¾…åŠ©ç³»ç»Ÿçš„é¢„æœŸæ§åˆ¶äº§ç”Ÿé‡è¦å½±å“ã€‚

</details>

---

## 239. All-in-One Conditioning for Text-to-Image Synthesis

**ä¸­æ–‡æ ‡é¢˜**: ç”¨äºæ–‡æœ¬åˆ°å›¾åƒåˆæˆçš„å¤šåˆä¸€è°ƒèŠ‚

**Date**: 2026-02-09 | **arXiv**: [2602.09165v1](http://arxiv.org/abs/2602.09165v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09165v1)

<details><summary><b>Abstract</b></summary>

Accurate interpretation and visual representation of complex prompts involving multiple objects, attributes, and spatial relationships is a critical challenge in text-to-image synthesis. Despite recent advancements in generating photorealistic outputs, current models often struggle with maintaining semantic fidelity and structural coherence when processing intricate textual inputs. We propose a novel approach that grounds text-to-image synthesis within the framework of scene graph structures, aiming to enhance the compositional abilities of existing models. Eventhough, prior approaches have attempted to address this by using pre-defined layout maps derived from prompts, such rigid constraints often limit compositional flexibility and diversity. In contrast, we introduce a zero-shot, scene graph-based conditioning mechanism that generates soft visual guidance during inference. At the core of our method is the Attribute-Size-Quantity-Location (ASQL) Conditioner, which produces visual conditions via a lightweight language model and guides diffusion-based generation through inference-time optimization. This enables the model to maintain text-image alignment while supporting lightweight, coherent, and diverse image synthesis.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æ¶‰åŠå¤šä¸ªå¯¹è±¡ã€å±æ€§å’Œç©ºé—´å…³ç³»çš„å¤æ‚æç¤ºçš„å‡†ç¡®è§£é‡Šå’Œè§†è§‰è¡¨ç¤ºæ˜¯æ–‡æœ¬åˆ°å›¾åƒåˆæˆçš„å…³é”®æŒ‘æˆ˜ã€‚å°½ç®¡æœ€è¿‘åœ¨ç”Ÿæˆé€¼çœŸè¾“å‡ºæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å½“å‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚çš„æ–‡æœ¬è¾“å…¥æ—¶å¸¸å¸¸éš¾ä»¥ç»´æŒè¯­ä¹‰ä¿çœŸåº¦å’Œç»“æ„è¿è´¯æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå°†æ–‡æœ¬åˆ°å›¾åƒçš„åˆæˆå»ºç«‹åœ¨åœºæ™¯å›¾ç»“æ„çš„æ¡†æ¶å†…ï¼Œæ—¨åœ¨å¢å¼ºç°æœ‰æ¨¡å‹çš„ç»„åˆèƒ½åŠ›ã€‚å°½ç®¡å…ˆå‰çš„æ–¹æ³•å·²ç»å°è¯•é€šè¿‡ä½¿ç”¨ä»æç¤ºå¯¼å‡ºçš„é¢„å®šä¹‰å¸ƒå±€å›¾æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†æ˜¯è¿™ç§ä¸¥æ ¼çš„çº¦æŸé€šå¸¸é™åˆ¶äº†æ„å›¾çš„çµæ´»æ€§å’Œå¤šæ ·æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é›¶æ ·æœ¬ã€åŸºäºåœºæ™¯å›¾çš„æ¡ä»¶æœºåˆ¶ï¼Œå¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”Ÿæˆè½¯è§†è§‰æŒ‡å¯¼ã€‚æˆ‘ä»¬æ–¹æ³•çš„æ ¸å¿ƒæ˜¯å±æ€§-å¤§å°-æ•°é‡-ä½ç½® (ASQL) æ¡ä»¶å™¨ï¼Œå®ƒé€šè¿‡è½»é‡çº§è¯­è¨€æ¨¡å‹ç”Ÿæˆè§†è§‰æ¡ä»¶ï¼Œå¹¶é€šè¿‡æ¨ç†æ—¶é—´ä¼˜åŒ–æŒ‡å¯¼åŸºäºæ‰©æ•£çš„ç”Ÿæˆã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿä¿æŒæ–‡æœ¬å›¾åƒå¯¹é½ï¼ŒåŒæ—¶æ”¯æŒè½»é‡çº§ã€è¿è´¯ä¸”å¤šæ ·åŒ–çš„å›¾åƒåˆæˆã€‚

</details>

---

## 240. SemanticMoments: Training-Free Motion Similarity via Third Moment Features

**ä¸­æ–‡æ ‡é¢˜**: SemanticMomentsï¼šé€šè¿‡ç¬¬ä¸‰çŸ©ç‰¹å¾è¿›è¡Œå…è®­ç»ƒè¿åŠ¨ç›¸ä¼¼æ€§

**Date**: 2026-02-09 | **arXiv**: [2602.09146v1](http://arxiv.org/abs/2602.09146v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09146v1)

<details><summary><b>Abstract</b></summary>

Retrieving videos based on semantic motion is a fundamental, yet unsolved, problem. Existing video representation approaches overly rely on static appearance and scene context rather than motion dynamics, a bias inherited from their training data and objectives. Conversely, traditional motion-centric inputs like optical flow lack the semantic grounding needed to understand high-level motion. To demonstrate this inherent bias, we introduce the SimMotion benchmarks, combining controlled synthetic data with a new human-annotated real-world dataset. We show that existing models perform poorly on these benchmarks, often failing to disentangle motion from appearance. To address this gap, we propose SemanticMoments, a simple, training-free method that computes temporal statistics (specifically, higher-order moments) over features from pre-trained semantic models. Across our benchmarks, SemanticMoments consistently outperforms existing RGB, flow, and text-supervised methods. This demonstrates that temporal statistics in a semantic feature space provide a scalable and perceptually grounded foundation for motion-centric video understanding.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åŸºäºè¯­ä¹‰è¿åŠ¨æ£€ç´¢è§†é¢‘æ˜¯ä¸€ä¸ªåŸºæœ¬ä½†å°šæœªè§£å†³çš„é—®é¢˜ã€‚ç°æœ‰çš„è§†é¢‘è¡¨ç¤ºæ–¹æ³•è¿‡åº¦ä¾èµ–é™æ€å¤–è§‚å’Œåœºæ™¯ä¸Šä¸‹æ–‡ï¼Œè€Œä¸æ˜¯è¿åŠ¨åŠ¨æ€ï¼Œè¿™æ˜¯ä»å…¶è®­ç»ƒæ•°æ®å’Œç›®æ ‡ç»§æ‰¿çš„åè§ã€‚ç›¸åï¼Œä¼ ç»Ÿçš„ä»¥è¿åŠ¨ä¸ºä¸­å¿ƒçš„è¾“å…¥ï¼ˆä¾‹å¦‚å…‰æµï¼‰ç¼ºä¹ç†è§£é«˜çº§è¿åŠ¨æ‰€éœ€çš„è¯­ä¹‰åŸºç¡€ã€‚ä¸ºäº†è¯æ˜è¿™ç§å›ºæœ‰åå·®ï¼Œæˆ‘ä»¬å¼•å…¥äº† SimMotion åŸºå‡†ï¼Œå°†å—æ§åˆæˆæ•°æ®ä¸æ–°çš„äººå·¥æ³¨é‡Šçš„çœŸå®ä¸–ç•Œæ•°æ®é›†ç›¸ç»“åˆã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œç°æœ‰æ¨¡å‹åœ¨è¿™äº›åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¸ä½³ï¼Œé€šå¸¸æ— æ³•å°†è¿åŠ¨ä¸å¤–è§‚åˆ†å¼€ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº† SemanticMomentsï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„ã€å…è®­ç»ƒçš„æ–¹æ³•ï¼Œå¯ä»¥æ ¹æ®é¢„å…ˆè®­ç»ƒçš„è¯­ä¹‰æ¨¡å‹çš„ç‰¹å¾è®¡ç®—æ—¶é—´ç»Ÿè®¡æ•°æ®ï¼ˆç‰¹åˆ«æ˜¯é«˜é˜¶çŸ©ï¼‰ã€‚åœ¨æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSemanticMoments å§‹ç»ˆä¼˜äºç°æœ‰çš„ RGBã€æµå’Œæ–‡æœ¬ç›‘ç£æ–¹æ³•ã€‚è¿™è¡¨æ˜è¯­ä¹‰ç‰¹å¾ç©ºé—´ä¸­çš„æ—¶é—´ç»Ÿè®¡ä¸ºä»¥è¿åŠ¨ä¸ºä¸­å¿ƒçš„è§†é¢‘ç†è§£æä¾›äº†å¯æ‰©å±•ä¸”åŸºäºæ„ŸçŸ¥çš„åŸºç¡€ã€‚

</details>

---

## 241. Autoregressive Image Generation with Masked Bit Modeling

**ä¸­æ–‡æ ‡é¢˜**: ä½¿ç”¨æ©è”½ä½å»ºæ¨¡çš„è‡ªå›å½’å›¾åƒç”Ÿæˆ

**Date**: 2026-02-09 | **arXiv**: [2602.09024v1](http://arxiv.org/abs/2602.09024v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09024v1)

<details><summary><b>Abstract</b></summary>

This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook sizes. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at https://bar-gen.github.io/

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æœ¬æ–‡æŒ‘æˆ˜äº†è¿ç»­ç®¡é“åœ¨è§†è§‰ç”Ÿæˆä¸­çš„ä¸»å¯¼åœ°ä½ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶ç¦»æ•£æ–¹æ³•å’Œè¿ç»­æ–¹æ³•ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚ä¸ç¦»æ•£åˆ†è¯å™¨æœ¬è´¨ä¸Šè¾ƒå·®çš„è§‚ç‚¹ç›¸åï¼Œæˆ‘ä»¬è¯æ˜è¿™ç§å·®å¼‚ä¸»è¦æºäºæ½œåœ¨ç©ºé—´ä¸­åˆ†é…çš„ä½æ•°ï¼ˆå³å‹ç¼©æ¯”ï¼‰ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæ‰©å¤§ç æœ¬å¤§å°å¯ä»¥æœ‰æ•ˆåœ°å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œä½¿ç¦»æ•£åˆ†è¯å™¨èƒ½å¤ŸåŒ¹é…æˆ–è¶…è¶Šå…¶è¿ç»­åˆ†è¯å™¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç¦»æ•£ç”Ÿæˆæ–¹æ³•å¾ˆéš¾åˆ©ç”¨è¿™ç§æ´å¯ŸåŠ›ï¼Œå› ä¸ºæ€§èƒ½ä¸‹é™æˆ–ç¼©æ”¾ç æœ¬çš„è®­ç»ƒæˆæœ¬è¿‡é«˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å±è”½ä½è‡ªå›å½’å»ºæ¨¡ï¼ˆBARï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ”¯æŒä»»æ„ç æœ¬å¤§å°çš„å¯æ‰©å±•æ¡†æ¶ã€‚é€šè¿‡ä¸ºè‡ªå›å½’å˜å‹å™¨é…å¤‡æ©ç ä½å»ºæ¨¡å¤´ï¼ŒBAR é€šè¿‡é€æ­¥ç”Ÿæˆç¦»æ•£æ ‡è®°çš„ç»„æˆä½æ¥é¢„æµ‹ç¦»æ•£æ ‡è®°ã€‚ BAR åœ¨ ImageNet-256 ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ gFID 0.99ï¼Œåœ¨è¿ç»­å’Œç¦»æ•£èŒƒå¼ä¸Šéƒ½ä¼˜äºé¢†å…ˆæ–¹æ³•ï¼ŒåŒæ—¶æ˜¾ç€é™ä½äº†é‡‡æ ·æˆæœ¬ï¼Œå¹¶ä¸”æ¯”ä¹‹å‰çš„è¿ç»­æ–¹æ³•æ”¶æ•›å¾—æ›´å¿«ã€‚é¡¹ç›®é¡µé¢ä½äº https://bar-gen.github.io/

</details>

---

## 242. WorldCompass: Reinforcement Learning for Long-Horizon World Models

**ä¸­æ–‡æ ‡é¢˜**: WorldCompassï¼šé•¿æœŸä¸–ç•Œæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ 

**Date**: 2026-02-09 | **arXiv**: [2602.09022v1](http://arxiv.org/abs/2602.09022v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09022v1)

<details><summary><b>Abstract</b></summary>

This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively "steer" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è¿™é¡¹å·¥ä½œæå‡ºäº† WorldCompassï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹  (RL) åè®­ç»ƒæ¡†æ¶ï¼Œé€‚ç”¨äºé•¿è§†é‡ã€åŸºäºäº¤äº’å¼è§†é¢‘çš„ä¸–ç•Œæ¨¡å‹ï¼Œä½¿ä»–ä»¬èƒ½å¤Ÿæ ¹æ®äº¤äº’ä¿¡å·æ›´å‡†ç¡®ã€ä¸€è‡´åœ°æ¢ç´¢ä¸–ç•Œã€‚ä¸ºäº†æœ‰æ•ˆåœ°â€œå¼•å¯¼â€ä¸–ç•Œæ¨¡å‹çš„æ¢ç´¢ï¼Œæˆ‘ä»¬å¼•å…¥äº†é’ˆå¯¹è‡ªå›å½’è§†é¢‘ç”ŸæˆèŒƒå¼é‡èº«å®šåˆ¶çš„ä¸‰é¡¹æ ¸å¿ƒåˆ›æ–°ï¼š1ï¼‰å‰ªè¾‘çº§æ¨å‡ºç­–ç•¥ï¼šæˆ‘ä»¬åœ¨å•ä¸ªç›®æ ‡å‰ªè¾‘ä¸Šç”Ÿæˆå¹¶è¯„ä¼°å¤šä¸ªæ ·æœ¬ï¼Œè¿™æ˜¾ç€æé«˜äº†æ¨å‡ºæ•ˆç‡å¹¶æä¾›ç»†ç²’åº¦çš„å¥–åŠ±ä¿¡å·ã€‚ 2ï¼‰è¡¥å……å¥–åŠ±å‡½æ•°ï¼šæˆ‘ä»¬è®¾è®¡äº†é’ˆå¯¹äº¤äº’è·Ÿè¸ªå‡†ç¡®æ€§å’Œè§†è§‰è´¨é‡çš„å¥–åŠ±å‡½æ•°ï¼Œæä¾›ç›´æ¥ç›‘ç£å¹¶æœ‰æ•ˆæŠ‘åˆ¶å¥–åŠ±é»‘å®¢è¡Œä¸ºã€‚ 3ï¼‰é«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼šæˆ‘ä»¬é‡‡ç”¨è´Ÿæ„ŸçŸ¥å¾®è°ƒç­–ç•¥ç»“åˆå„ç§æ•ˆç‡ä¼˜åŒ–æ¥é«˜æ•ˆä¸”æœ‰æ•ˆåœ°å¢å¼ºæ¨¡å‹å®¹é‡ã€‚å¯¹SoTAå¼€æºä¸–ç•Œæ¨¡å‹WorldPlayçš„è¯„ä¼°è¡¨æ˜ï¼ŒWorldCompassæ˜¾ç€æé«˜äº†å„ç§åœºæ™¯ä¸‹çš„äº¤äº’å‡†ç¡®æ€§å’Œè§†è§‰ä¿çœŸåº¦ã€‚

</details>

---

## 243. $Ï‡_{0}$: Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies

**ä¸­æ–‡æ ‡é¢˜**: $Ï‡_{0}$ï¼šé€šè¿‡é©¯æœåˆ†å¸ƒä¸ä¸€è‡´è¿›è¡Œèµ„æºæ„ŸçŸ¥çš„é²æ£’æ“ä½œ

**Date**: 2026-02-09 | **arXiv**: [2602.09021v1](http://arxiv.org/abs/2602.09021v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09021v1)

<details><summary><b>Abstract</b></summary>

High-reliability long-horizon robotic manipulation has traditionally relied on large-scale data and compute to understand complex real-world dynamics. However, we identify that the primary bottleneck to real-world robustness is not resource scale alone, but the distributional shift among the human demonstration distribution, the inductive bias learned by the policy, and the test-time execution distribution -- a systematic inconsistency that causes compounding errors in multi-stage tasks. To mitigate these inconsistencies, we propose $Ï‡_{0}$, a resource-efficient framework with effective modules designated to achieve production-level robustness in robotic manipulation. Our approach builds off three technical pillars: (i) Model Arithmetic, a weight-space merging strategy that efficiently soaks up diverse distributions of different demonstrations, varying from object appearance to state variations; (ii) Stage Advantage, a stage-aware advantage estimator that provides stable, dense progress signals, overcoming the numerical instability of prior non-stage approaches; and (iii) Train-Deploy Alignment, which bridges the distribution gap via spatio-temporal augmentation, heuristic DAgger corrections, and temporal chunk-wise smoothing. $Ï‡_{0}$ enables two sets of dual-arm robots to collaboratively orchestrate long-horizon garment manipulation, spanning tasks from flattening, folding, to hanging different clothes. Our method exhibits high-reliability autonomy; we are able to run the system from arbitrary initial state for consecutive 24 hours non-stop. Experiments validate that $Ï‡_{0}$ surpasses the state-of-the-art $Ï€_{0.5}$ in success rate by nearly 250%, with only 20-hour data and 8 A100 GPUs. Code, data and models will be released to facilitate the community.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

é«˜å¯é æ€§é•¿è§†è·æœºå™¨äººæ“çºµä¼ ç»Ÿä¸Šä¾èµ–å¤§è§„æ¨¡æ•°æ®å’Œè®¡ç®—æ¥ç†è§£å¤æ‚çš„ç°å®ä¸–ç•ŒåŠ¨æ€ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°ç°å®ä¸–ç•Œé²æ£’æ€§çš„ä¸»è¦ç“¶é¢ˆä¸ä»…ä»…æ˜¯èµ„æºè§„æ¨¡ï¼Œè€Œæ˜¯äººç±»ç¤ºèŒƒåˆ†å¸ƒã€ç­–ç•¥å­¦ä¹ åˆ°çš„å½’çº³åå·®å’Œæµ‹è¯•æ—¶æ‰§è¡Œåˆ†å¸ƒä¹‹é—´çš„åˆ†å¸ƒå˜åŒ–â€”â€”è¿™æ˜¯ä¸€ç§ç³»ç»Ÿä¸ä¸€è‡´ï¼Œä¼šå¯¼è‡´å¤šé˜¶æ®µä»»åŠ¡ä¸­çš„å¤åˆé”™è¯¯ã€‚ä¸ºäº†ç¼“è§£è¿™äº›ä¸ä¸€è‡´é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº† $Ï‡_{0}$ï¼Œè¿™æ˜¯ä¸€ä¸ªèµ„æºé«˜æ•ˆçš„æ¡†æ¶ï¼Œå…·æœ‰æœ‰æ•ˆçš„æ¨¡å—ï¼Œæ—¨åœ¨å®ç°æœºå™¨äººæ“ä½œçš„ç”Ÿäº§çº§é²æ£’æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å»ºç«‹åœ¨ä¸‰ä¸ªæŠ€æœ¯æ”¯æŸ±ä¹‹ä¸Šï¼šï¼ˆiï¼‰æ¨¡å‹ç®—æœ¯ï¼Œä¸€ç§æƒé‡ç©ºé—´åˆå¹¶ç­–ç•¥ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¸æ”¶ä¸åŒæ¼”ç¤ºçš„ä¸åŒåˆ†å¸ƒï¼Œä»å¯¹è±¡å¤–è§‚åˆ°çŠ¶æ€å˜åŒ–ï¼› (ii) Stage Advantageï¼Œä¸€ç§é˜¶æ®µæ„ŸçŸ¥ä¼˜åŠ¿ä¼°è®¡å™¨ï¼Œå¯æä¾›ç¨³å®šã€å¯†é›†çš„è¿›åº¦ä¿¡å·ï¼Œå…‹æœå…ˆå‰éé˜¶æ®µæ–¹æ³•çš„æ•°å€¼ä¸ç¨³å®šæ€§ï¼› (iii) è®­ç»ƒéƒ¨ç½²å¯¹é½ï¼Œé€šè¿‡æ—¶ç©ºå¢å¼ºã€å¯å‘å¼ DAgger æ ¡æ­£å’Œæ—¶é—´å—å¹³æ»‘æ¥å¼¥åˆåˆ†å¸ƒå·®è·ã€‚ $Ï‡_{0}$ ä½¿ä¸¤ç»„åŒè‡‚æœºå™¨äººèƒ½å¤Ÿåä½œç¼–æ’é•¿æœŸæœè£…æ“ä½œï¼Œæ¶µç›–ä»å±•å¹³ã€æŠ˜å åˆ°æ‚¬æŒ‚ä¸åŒè¡£æœçš„ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°å‡ºé«˜å¯é æ€§çš„è‡ªä¸»æ€§ï¼›æˆ‘ä»¬èƒ½å¤Ÿä»ä»»æ„åˆå§‹çŠ¶æ€è¿ç»­24å°æ—¶ä¸é—´æ–­åœ°è¿è¡Œç³»ç»Ÿã€‚å®éªŒéªŒè¯ï¼Œä»…ç”¨ 20 å°æ—¶çš„æ•°æ®å’Œ 8 ä¸ª A100 GPUï¼Œ$Ï‡_{0}$ çš„æˆåŠŸç‡å°±è¶…è¿‡äº†æœ€å…ˆè¿›çš„ $Ï€_{0.5}$ è¿‘ 250%ã€‚ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å°†è¢«å‘å¸ƒä»¥æ–¹ä¾¿ç¤¾åŒºã€‚

</details>

---

## 244. Robustness Is a Function, Not a Number: A Factorized Comprehensive Study of OOD Robustness in Vision-Based Driving

**ä¸­æ–‡æ ‡é¢˜**: é²æ£’æ€§æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªæ•°å­—ï¼šåŸºäºè§†è§‰çš„é©¾é©¶ä¸­ OOD é²æ£’æ€§çš„åˆ†è§£ç»¼åˆç ”ç©¶

**Date**: 2026-02-09 | **arXiv**: [2602.09018v1](http://arxiv.org/abs/2602.09018v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09018v1)

<details><summary><b>Abstract</b></summary>

Out of distribution (OOD) robustness in autonomous driving is often reduced to a single number, hiding what breaks a policy. We decompose environments along five axes: scene (rural/urban), season, weather, time (day/night), and agent mix; and measure performance under controlled $k$-factor perturbations ($k \in \{0,1,2,3\}$). Using closed loop control in VISTA, we benchmark FC, CNN, and ViT policies, train compact ViT heads on frozen foundation-model (FM) features, and vary ID support in scale, diversity, and temporal context. (1) ViT policies are markedly more OOD-robust than comparably sized CNN/FC, and FM features yield state-of-the-art success at a latency cost. (2) Naive temporal inputs (multi-frame) do not beat the best single-frame baseline. (3) The largest single factor drops are rural $\rightarrow$ urban and day $\rightarrow$ night ($\sim 31\%$ each); actor swaps $\sim 10\%$, moderate rain $\sim 7\%$; season shifts can be drastic, and combining a time flip with other changes further degrades performance. (4) FM-feature policies stay above $85\%$ under three simultaneous changes; non-FM single-frame policies take a large first-shift hit, and all no-FM models fall below $50\%$ by three changes. (5) Interactions are non-additive: some pairings partially offset, whereas season-time combinations are especially harmful. (6) Training on winter/snow is most robust to single-factor shifts, while a rural+summer baseline gives the best overall OOD performance. (7) Scaling traces/views improves robustness ($+11.8$ points from $5$ to $14$ traces), yet targeted exposure to hard conditions can substitute for scale. (8) Using multiple ID environments broadens coverage and strengthens weak cases (urban OOD $60.6\% \rightarrow 70.1\%$) with a small ID drop; single-ID preserves peak performance but in a narrow domain. These results yield actionable design rules for OOD-robust driving policies.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è‡ªåŠ¨é©¾é©¶ä¸­çš„åˆ†å¸ƒå¤– (OOD) é²æ£’æ€§é€šå¸¸ä¼šç®€åŒ–ä¸ºä¸€ä¸ªæ•°å­—ï¼Œä»è€Œéšè—äº†è¿åç­–ç•¥çš„å†…å®¹ã€‚æˆ‘ä»¬æ²¿ç€äº”ä¸ªè½´åˆ†è§£ç¯å¢ƒï¼šåœºæ™¯ï¼ˆå†œæ‘/åŸå¸‚ï¼‰ã€å­£èŠ‚ã€å¤©æ°”ã€æ—¶é—´ï¼ˆç™½å¤©/å¤œæ™šï¼‰å’Œä»£ç†ç»„åˆï¼›å¹¶æµ‹é‡å—æ§ $k$ å› å­æ‰°åŠ¨ä¸‹çš„æ€§èƒ½ ($k \in \{0,1,2,3\}$)ã€‚ä½¿ç”¨ VISTA ä¸­çš„é—­ç¯æ§åˆ¶ï¼Œæˆ‘ä»¬å¯¹ FCã€CNN å’Œ ViT ç­–ç•¥è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œåœ¨å†»ç»“çš„åŸºç¡€æ¨¡å‹ (FM) ç‰¹å¾ä¸Šè®­ç»ƒç´§å‡‘çš„ ViT å¤´ï¼Œå¹¶åœ¨è§„æ¨¡ã€å¤šæ ·æ€§å’Œæ—¶é—´ä¸Šä¸‹æ–‡ä¸­æ”¹å˜ ID æ”¯æŒã€‚ (1) ViT ç­–ç•¥æ˜æ˜¾æ¯”åŒç­‰è§„æ¨¡çš„ CNN/FC æ›´å…·æœ‰ OOD é²æ£’æ€§ï¼Œå¹¶ä¸” FM åŠŸèƒ½ä»¥å»¶è¿Ÿæˆæœ¬å–å¾—äº†æœ€å…ˆè¿›çš„æˆåŠŸã€‚ (2) æœ´ç´ æ—¶é—´è¾“å…¥ï¼ˆå¤šå¸§ï¼‰æ— æ³•å‡»è´¥æœ€ä½³å•å¸§åŸºçº¿ã€‚ ï¼ˆ3ï¼‰å•å› ç´ ä¸‹é™æœ€å¤§çš„æ˜¯å†œæ‘$\rightarrow$åŸå¸‚å’Œç™½å¤©$\rightarrow$å¤œé—´ï¼ˆå„$\sim 31\%$ï¼‰ï¼›æ¼”å‘˜äº¤æ¢$\sim 10\%$ï¼Œä¸­é›¨$\sim 7\%$ï¼›å­£èŠ‚å˜åŒ–å¯èƒ½ä¼šå¾ˆå‰§çƒˆï¼Œå¹¶ä¸”å°†æ—¶é—´ç¿»è½¬ä¸å…¶ä»–å˜åŒ–ç»“åˆèµ·æ¥ä¼šè¿›ä¸€æ­¥é™ä½æ€§èƒ½ã€‚ (4) FMç‰¹è‰²ä¿å•åœ¨ä¸‰é¡¹åŒæ—¶å˜åŒ–ä¸‹ä¿æŒåœ¨$85\%$ä¹‹ä¸Šï¼›é FM å•å¸§ç­–ç•¥å—åˆ°è¾ƒå¤§çš„ç¬¬ä¸€è½®æ‰“å‡»ï¼Œæ‰€æœ‰é FM æ¨¡å‹å‡é€šè¿‡ä¸‰ä¸ªå˜åŒ–è·Œè‡³ 50\%$ ä»¥ä¸‹ã€‚ (5) ç›¸äº’ä½œç”¨æ˜¯éç´¯åŠ æ€§çš„ï¼šä¸€äº›é…å¯¹ä¼šéƒ¨åˆ†æŠµæ¶ˆï¼Œè€Œå­£èŠ‚ç»„åˆå°¤å…¶æœ‰å®³ã€‚ (6) å†¬å­£/é›ªåœ°è®­ç»ƒå¯¹äºå•å› ç´ å˜åŒ–æœ€ä¸ºç¨³å¥ï¼Œè€Œä¹¡æ‘+å¤å­£åŸºçº¿åˆ™æä¾›æœ€ä½³çš„æ•´ä½“ OOD æ€§èƒ½ã€‚ (7) ç¼©æ”¾è½¨è¿¹/è§†å›¾å¯æé«˜é²æ£’æ€§ï¼ˆä» 5 ç¾å…ƒåˆ° 14 ç¾å…ƒè½¨è¿¹ï¼Œ$+11.8 ç‚¹ï¼‰ï¼Œä½†æœ‰é’ˆå¯¹æ€§åœ°æš´éœ²åœ¨æ¶åŠ£æ¡ä»¶ä¸‹å¯ä»¥æ›¿ä»£ç¼©æ”¾ã€‚ ï¼ˆ8ï¼‰ä½¿ç”¨å¤šä¸ªIDç¯å¢ƒæ‰©å¤§äº†è¦†ç›–èŒƒå›´å¹¶åŠ å¼ºäº†å¼±æ¡ˆä¾‹ï¼ˆåŸå¸‚OOD $60.6\% \rightarrow 70.1\%$ï¼‰ï¼ŒIDä¸‹é™å¹…åº¦è¾ƒå°ï¼›å• ID å¯ä»¥ä¿æŒå³°å€¼æ€§èƒ½ï¼Œä½†èŒƒå›´å¾ˆçª„ã€‚è¿™äº›ç»“æœä¸º OOD ç¨³å¥çš„é©¾é©¶ç­–ç•¥æä¾›äº†å¯è¡Œçš„è®¾è®¡è§„åˆ™ã€‚

</details>

---

## 245. ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation

**ä¸­æ–‡æ ‡é¢˜**: ArcFlowï¼šé€šè¿‡é«˜ç²¾åº¦éçº¿æ€§æµåŠ¨è’¸é¦å®ç°ä¸¤æ­¥æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ

**Date**: 2026-02-09 | **arXiv**: [2602.09014v1](http://arxiv.org/abs/2602.09014v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09014v1)

<details><summary><b>Abstract</b></summary>

Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æ‰©æ•£æ¨¡å‹å·²ç»å®ç°äº†å“è¶Šçš„ç”Ÿæˆè´¨é‡ï¼Œä½†ç”±äºä¾èµ–å¤šä¸ªè¿ç»­çš„å»å™ªæ­¥éª¤ï¼Œå®ƒä»¬çš„æ¨ç†æˆæœ¬å¾ˆé«˜ï¼Œè¿™ä¿ƒä½¿äººä»¬æœ€è¿‘åŠªåŠ›å°†è¿™ç§æ¨ç†è¿‡ç¨‹æç‚¼ä¸ºå‡ ä¸ªæ­¥éª¤ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è’¸é¦æ–¹æ³•é€šå¸¸ä½¿ç”¨çº¿æ€§æ·å¾„æ¥è¿‘ä¼¼æ•™å¸ˆè½¨è¿¹ï¼Œè¿™ä½¿å¾—éšç€é€Ÿåº¦è·¨æ—¶é—´æ­¥é•¿çš„å˜åŒ–è€Œéš¾ä»¥åŒ¹é…å…¶ä¸æ–­å˜åŒ–çš„åˆ‡çº¿æ–¹å‘ï¼Œä»è€Œå¯¼è‡´è´¨é‡ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº† ArcFlowï¼Œè¿™æ˜¯ä¸€ç§åˆ†æ­¥è’¸é¦æ¡†æ¶ï¼Œå®ƒæ˜ç¡®åœ°é‡‡ç”¨éçº¿æ€§æµåŠ¨è½¨è¿¹æ¥è¿‘ä¼¼é¢„å…ˆè®­ç»ƒçš„æ•™å¸ˆè½¨è¿¹ã€‚å…·ä½“æ¥è¯´ï¼ŒArcFlow å°†æ¨ç†è½¨è¿¹ä¸‹çš„é€Ÿåº¦åœºå‚æ•°åŒ–ä¸ºè¿ç»­åŠ¨é‡è¿‡ç¨‹çš„æ··åˆã€‚è¿™ä½¿å¾— ArcFlow èƒ½å¤Ÿæ•è·é€Ÿåº¦æ¼”åŒ–å¹¶æ¨æ–­ç›¸å¹²é€Ÿåº¦ï¼Œä»¥åœ¨æ¯ä¸ªå»å™ªæ­¥éª¤å†…å½¢æˆè¿ç»­çš„éçº¿æ€§è½¨è¿¹ã€‚é‡è¦çš„æ˜¯ï¼Œè¿™ç§å‚æ•°åŒ–å…è®¸å¯¹éçº¿æ€§è½¨è¿¹è¿›è¡Œåˆ†æç§¯åˆ†ï¼Œä»è€Œé¿å…æ•°å€¼ç¦»æ•£è¯¯å·®å¹¶å¯¼è‡´æ•™å¸ˆè½¨è¿¹çš„é«˜ç²¾åº¦è¿‘ä¼¼ã€‚ä¸ºäº†å°†è¿™ç§å‚æ•°åŒ–è®­ç»ƒæˆå‡ æ­¥ç”Ÿæˆå™¨ï¼Œæˆ‘ä»¬ä½¿ç”¨è½»é‡çº§é€‚é…å™¨åœ¨é¢„å…ˆè®­ç»ƒçš„æ•™å¸ˆæ¨¡å‹ä¸Šé€šè¿‡è½¨è¿¹è’¸é¦æ¥å®ç° ArcFlowã€‚è¯¥ç­–ç•¥ç¡®ä¿å¿«é€Ÿã€ç¨³å®šçš„æ”¶æ•›ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆå¤šæ ·æ€§å’Œè´¨é‡ã€‚ ArcFlow åŸºäºå¤§å‹æ¨¡å‹ï¼ˆQwen-Image-20B å’Œ FLUX.1-devï¼‰æ„å»ºï¼Œä»…å¯¹ä¸åˆ° 5% çš„åŸå§‹å‚æ•°è¿›è¡Œå¾®è°ƒï¼Œä¸åŸå§‹å¤šæ­¥æ•™å¸ˆç›¸æ¯”ï¼Œé€šè¿‡ 2 ä¸ª NFE å®ç°äº† 40 å€çš„åŠ é€Ÿï¼Œè€Œè´¨é‡æ²¡æœ‰æ˜¾ç€ä¸‹é™ã€‚åŸºå‡†å®éªŒä»å®šæ€§å’Œå®šé‡ä¸¤ä¸ªæ–¹é¢è¯æ˜äº† ArcFlow çš„æœ‰æ•ˆæ€§ã€‚

</details>

---

## 246. Dexterous Manipulation Policies from RGB Human Videos via 4D Hand-Object Trajectory Reconstruction

**ä¸­æ–‡æ ‡é¢˜**: é€šè¿‡ 4D æ‰‹éƒ¨ç‰©ä½“è½¨è¿¹é‡å»ºä» RGB äººç±»è§†é¢‘ä¸­è·å¾—çµå·§æ“çºµç­–ç•¥

**Date**: 2026-02-09 | **arXiv**: [2602.09013v1](http://arxiv.org/abs/2602.09013v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09013v1)

<details><summary><b>Abstract</b></summary>

Multi-finger robotic hand manipulation and grasping are challenging due to the high-dimensional action space and the difficulty of acquiring large-scale training data. Existing approaches largely rely on human teleoperation with wearable devices or specialized sensing equipment to capture hand-object interactions, which limits scalability. In this work, we propose VIDEOMANIP, a device-free framework that learns dexterous manipulation directly from RGB human videos. Leveraging recent advances in computer vision, VIDEOMANIP reconstructs explicit 4D robot-object trajectories from monocular videos by estimating human hand poses, object meshes, and retargets the reconstructed human motions to robotic hands for manipulation learning. To make the reconstructed robot data suitable for dexterous manipulation training, we introduce hand-object contact optimization with interaction-centric grasp modeling, as well as a demonstration synthesis strategy that generates diverse training trajectories from a single video, enabling generalizable policy learning without additional robot demonstrations. In simulation, the learned grasping model achieves a 70.25% success rate across 20 diverse objects using the Inspire Hand. In the real world, manipulation policies trained from RGB videos achieve an average 62.86% success rate across seven tasks using the LEAP Hand, outperforming retargeting-based methods by 15.87%. Project videos are available at videomanip.github.io.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç”±äºé«˜ç»´åŠ¨ä½œç©ºé—´å’Œè·å–å¤§è§„æ¨¡è®­ç»ƒæ•°æ®çš„å›°éš¾ï¼Œå¤šæŒ‡æœºå™¨äººæ‰‹çš„æ“çºµå’ŒæŠ“å–å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰æ–¹æ³•å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºäººç±»é€šè¿‡å¯ç©¿æˆ´è®¾å¤‡æˆ–ä¸“ç”¨ä¼ æ„Ÿè®¾å¤‡è¿›è¡Œè¿œç¨‹æ“ä½œæ¥æ•è·æ‰‹éƒ¨ç‰©ä½“äº¤äº’ï¼Œè¿™é™åˆ¶äº†å¯æ‰©å±•æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† VIDEOMANIPï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®¾å¤‡çš„æ¡†æ¶ï¼Œå¯ä»¥ç›´æ¥ä» RGB äººç±»è§†é¢‘ä¸­å­¦ä¹ çµå·§çš„æ“ä½œã€‚åˆ©ç”¨è®¡ç®—æœºè§†è§‰çš„æœ€æ–°è¿›å±•ï¼ŒVIDEOMANIP é€šè¿‡ä¼°è®¡äººç±»æ‰‹éƒ¨å§¿åŠ¿ã€ç‰©ä½“ç½‘æ ¼ï¼Œä»å•ç›®è§†é¢‘ä¸­é‡å»ºæ˜ç¡®çš„ 4D æœºå™¨äººç‰©ä½“è½¨è¿¹ï¼Œå¹¶å°†é‡å»ºçš„äººç±»è¿åŠ¨é‡æ–°å®šä½åˆ°æœºå™¨äººæ‰‹ä»¥è¿›è¡Œæ“ä½œå­¦ä¹ ã€‚ä¸ºäº†ä½¿é‡å»ºçš„æœºå™¨äººæ•°æ®é€‚åˆçµå·§æ“ä½œè®­ç»ƒï¼Œæˆ‘ä»¬å¼•å…¥äº†ä»¥äº¤äº’ä¸ºä¸­å¿ƒçš„æŠ“å–å»ºæ¨¡çš„æ‰‹éƒ¨ç‰©ä½“æ¥è§¦ä¼˜åŒ–ï¼Œä»¥åŠä»å•ä¸ªè§†é¢‘ç”Ÿæˆä¸åŒè®­ç»ƒè½¨è¿¹çš„æ¼”ç¤ºç»¼åˆç­–ç•¥ï¼Œæ— éœ€é¢å¤–çš„æœºå™¨äººæ¼”ç¤ºå³å¯å®ç°å¯æ¨å¹¿çš„ç­–ç•¥å­¦ä¹ ã€‚åœ¨æ¨¡æ‹Ÿä¸­ï¼Œå­¦ä¹ æŠ“å–æ¨¡å‹ä½¿ç”¨ Inspire Hand åœ¨ 20 ä¸ªä¸åŒç‰©ä½“ä¸Šå®ç°äº† 70.25% çš„æˆåŠŸç‡ã€‚åœ¨ç°å®ä¸–ç•Œä¸­ï¼Œä½¿ç”¨ LEAP Hand ä» RGB è§†é¢‘è®­ç»ƒçš„æ“çºµç­–ç•¥åœ¨ä¸ƒé¡¹ä»»åŠ¡ä¸­å¹³å‡æˆåŠŸç‡ä¸º 62.86%ï¼Œæ¯”åŸºäºé‡å®šå‘çš„æ–¹æ³•é«˜å‡º 15.87%ã€‚é¡¹ç›®è§†é¢‘å¯åœ¨ videomanip.github.io ä¸Šè·å–ã€‚

</details>

---

## 247. GEBench: Benchmarking Image Generation Models as GUI Environments

**ä¸­æ–‡æ ‡é¢˜**: GEBenchï¼šå°†å›¾åƒç”Ÿæˆæ¨¡å‹ä½œä¸º GUI ç¯å¢ƒè¿›è¡ŒåŸºå‡†æµ‹è¯•

**Date**: 2026-02-09 | **arXiv**: [2602.09007v2](http://arxiv.org/abs/2602.09007v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.09007v2)

**Code**: https://github.com/stepfun-ai/GEBench.

<details><summary><b>Abstract</b></summary>

Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å›¾åƒç”Ÿæˆæ¨¡å‹çš„æœ€æ–°è¿›å±•ä½¿å¾—èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·æŒ‡ä»¤é¢„æµ‹æœªæ¥çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰çŠ¶æ€ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†ä¸»è¦å…³æ³¨ä¸€èˆ¬é¢†åŸŸçš„è§†è§‰ä¿çœŸåº¦ï¼Œè€Œå¯¹ç‰¹å®šäº GUI çš„ä¸Šä¸‹æ–‡ä¸­çš„çŠ¶æ€è½¬æ¢å’Œæ—¶é—´ä¸€è‡´æ€§çš„è¯„ä¼°å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº† GEBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼° GUI ç”Ÿæˆä¸­çš„åŠ¨æ€äº¤äº’å’Œæ—¶é—´ä¸€è‡´æ€§çš„ç»¼åˆåŸºå‡†ã€‚ GEBench åŒ…å« 700 ä¸ªç²¾å¿ƒç­–åˆ’çš„æ ·æœ¬ï¼Œæ¶µç›–äº”ä¸ªä»»åŠ¡ç±»åˆ«ï¼Œæ¶µç›–ç°å®ä¸–ç•Œå’Œè™šæ„åœºæ™¯ä¸­çš„å•æ­¥äº¤äº’å’Œå¤šæ­¥è½¨è¿¹ï¼Œä»¥åŠæ¥åœ°ç‚¹å®šä½ã€‚ä¸ºäº†æ”¯æŒç³»ç»Ÿè¯„ä¼°ï¼Œæˆ‘ä»¬æå‡ºäº† GE-Scoreï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„äº”ç»´æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°ç›®æ ‡å®ç°ã€äº¤äº’é€»è¾‘ã€å†…å®¹ä¸€è‡´æ€§ã€UI åˆç†æ€§å’Œè§†è§‰è´¨é‡ã€‚å¯¹å½“å‰æ¨¡å‹çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œè™½ç„¶å®ƒä»¬åœ¨å•æ­¥è½¬æ¢ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨è¾ƒé•¿äº¤äº’åºåˆ—ä¸Šç»´æŒæ—¶é—´è¿è´¯æ€§å’Œç©ºé—´åŸºç¡€æ–¹é¢å­˜åœ¨å¾ˆå¤§å›°éš¾ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜å›¾æ ‡è§£é‡Šã€æ–‡æœ¬æ¸²æŸ“å’Œå®šä½ç²¾åº¦æ˜¯å…³é”®ç“¶é¢ˆã€‚è¿™é¡¹å·¥ä½œä¸ºç³»ç»Ÿè¯„ä¼°å¥ å®šäº†åŸºç¡€ï¼Œå¹¶ä¸ºæ„å»ºé«˜ä¿çœŸç”Ÿæˆ GUI ç¯å¢ƒçš„æœªæ¥ç ”ç©¶æå‡ºäº†æœ‰å¸Œæœ›çš„æ–¹å‘ã€‚ä»£ç ä½äºï¼šhttps://github.com/stepfun-ai/GEBenchã€‚

</details>

---

## 248. WorldArena: A Unified Benchmark for Evaluating Perception and Functional Utility of Embodied World Models

**ä¸­æ–‡æ ‡é¢˜**: WorldArenaï¼šè¯„ä¼°å…·ä½“ä¸–ç•Œæ¨¡å‹çš„æ„ŸçŸ¥å’ŒåŠŸèƒ½æ•ˆç”¨çš„ç»Ÿä¸€åŸºå‡†

**Date**: 2026-02-09 | **arXiv**: [2602.08971v2](http://arxiv.org/abs/2602.08971v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.08971v2)

<details><summary><b>Abstract</b></summary>

While world models have emerged as a cornerstone of embodied intelligence by enabling agents to reason about environmental dynamics through action-conditioned prediction, their evaluation remains fragmented. Current evaluation of embodied world models has largely focused on perceptual fidelity (e.g., video generation quality), overlooking the functional utility of these models in downstream decision-making tasks. In this work, we introduce WorldArena, a unified benchmark designed to systematically evaluate embodied world models across both perceptual and functional dimensions. WorldArena assesses models through three dimensions: video perception quality, measured with 16 metrics across six sub-dimensions; embodied task functionality, which evaluates world models as data engines, policy evaluators, and action planners integrating with subjective human evaluation. Furthermore, we propose EWMScore, a holistic metric integrating multi-dimensional performance into a single interpretable index. Through extensive experiments on 14 representative models, we reveal a significant perception-functionality gap, showing that high visual quality does not necessarily translate into strong embodied task capability. WorldArena benchmark with the public leaderboard is released at https://world-arena.ai, providing a framework for tracking progress toward truly functional world models in embodied AI.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è™½ç„¶ä¸–ç•Œæ¨¡å‹å·²æˆä¸ºä½“ç°æ™ºèƒ½çš„åŸºçŸ³ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿé€šè¿‡è¡ŒåŠ¨æ¡ä»¶é¢„æµ‹æ¥æ¨ç†ç¯å¢ƒåŠ¨æ€ï¼Œä½†å®ƒä»¬çš„è¯„ä¼°ä»ç„¶æ”¯ç¦»ç ´ç¢ã€‚ç›®å‰å¯¹å…·ä½“ä¸–ç•Œæ¨¡å‹çš„è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨æ„ŸçŸ¥ä¿çœŸåº¦ï¼ˆä¾‹å¦‚è§†é¢‘ç”Ÿæˆè´¨é‡ï¼‰ï¼Œè€Œå¿½è§†äº†è¿™äº›æ¨¡å‹åœ¨ä¸‹æ¸¸å†³ç­–ä»»åŠ¡ä¸­çš„åŠŸèƒ½æ•ˆç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† WorldArenaï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„åŸºå‡†ï¼Œæ—¨åœ¨è·¨æ„ŸçŸ¥å’ŒåŠŸèƒ½ç»´åº¦ç³»ç»Ÿåœ°è¯„ä¼°å…·ä½“ä¸–ç•Œæ¨¡å‹ã€‚ WorldArena é€šè¿‡ä¸‰ä¸ªç»´åº¦è¯„ä¼°æ¨¡å‹ï¼šè§†é¢‘æ„ŸçŸ¥è´¨é‡ï¼Œé€šè¿‡ 6 ä¸ªå­ç»´åº¦çš„ 16 ä¸ªæŒ‡æ ‡è¿›è¡Œè¡¡é‡ï¼›ä½“ç°ä»»åŠ¡åŠŸèƒ½ï¼Œå°†ä¸–ç•Œæ¨¡å‹è¯„ä¼°ä¸ºæ•°æ®å¼•æ“ã€æ”¿ç­–è¯„ä¼°è€…å’Œä¸ä¸»è§‚äººç±»è¯„ä¼°ç›¸ç»“åˆçš„è¡ŒåŠ¨è§„åˆ’è€…ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº† EWMScoreï¼Œè¿™æ˜¯ä¸€ç§å°†å¤šç»´æ€§èƒ½é›†æˆåˆ°å•ä¸ªå¯è§£é‡ŠæŒ‡æ•°ä¸­çš„æ•´ä½“æŒ‡æ ‡ã€‚é€šè¿‡å¯¹ 14 ä¸ªä»£è¡¨æ€§æ¨¡å‹çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬æ­ç¤ºäº†æ˜¾ç€çš„æ„ŸçŸ¥åŠŸèƒ½å·®è·ï¼Œè¡¨æ˜é«˜è§†è§‰è´¨é‡å¹¶ä¸ä¸€å®šè½¬åŒ–ä¸ºå¼ºå¤§çš„å…·ä½“ä»»åŠ¡èƒ½åŠ›ã€‚ WorldArena åŸºå‡†æµ‹è¯•å’Œå…¬å…±æ’è¡Œæ¦œåœ¨ https://world-arena.ai ä¸Šå‘å¸ƒï¼Œæä¾›äº†ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºè·Ÿè¸ªå…·ä½“äººå·¥æ™ºèƒ½ä¸­çœŸæ­£åŠŸèƒ½æ€§ä¸–ç•Œæ¨¡å‹çš„è¿›å±•ã€‚

</details>

---

## 249. Modeling 3D Pedestrian-Vehicle Interactions for Vehicle-Conditioned Pose Forecasting

**ä¸­æ–‡æ ‡é¢˜**: å¯¹ 3D è¡Œäºº-è½¦è¾†äº¤äº’å»ºæ¨¡ä»¥è¿›è¡Œè½¦è¾†æ¡ä»¶å§¿æ€é¢„æµ‹

**Date**: 2026-02-09 | **arXiv**: [2602.08962v1](http://arxiv.org/abs/2602.08962v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08962v1)

**Code**: https://github.com/GuangxunZhu/VehCondPose3D

<details><summary><b>Abstract</b></summary>

Accurately predicting pedestrian motion is crucial for safe and reliable autonomous driving in complex urban environments. In this work, we present a 3D vehicle-conditioned pedestrian pose forecasting framework that explicitly incorporates surrounding vehicle information. To support this, we enhance the Waymo-3DSkelMo dataset with aligned 3D vehicle bounding boxes, enabling realistic modeling of multi-agent pedestrian-vehicle interactions. We introduce a sampling scheme to categorize scenes by pedestrian and vehicle count, facilitating training across varying interaction complexities. Our proposed network adapts the TBIFormer architecture with a dedicated vehicle encoder and pedestrian-vehicle interaction cross-attention module to fuse pedestrian and vehicle features, allowing predictions to be conditioned on both historical pedestrian motion and surrounding vehicles. Extensive experiments demonstrate substantial improvements in forecasting accuracy and validate different approaches for modeling pedestrian-vehicle interactions, highlighting the importance of vehicle-aware 3D pose prediction for autonomous driving. Code is available at: https://github.com/GuangxunZhu/VehCondPose3D

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å‡†ç¡®é¢„æµ‹è¡Œäººè¿åŠ¨å¯¹äºå¤æ‚åŸå¸‚ç¯å¢ƒä¸­å®‰å…¨å¯é çš„è‡ªåŠ¨é©¾é©¶è‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ª 3D è½¦è¾†è°ƒèŠ‚è¡Œäººå§¿åŠ¿é¢„æµ‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ˜ç¡®åœ°ç»“åˆäº†å‘¨å›´è½¦è¾†ä¿¡æ¯ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä½¿ç”¨å¯¹é½çš„ 3D è½¦è¾†è¾¹ç•Œæ¡†å¢å¼ºäº† Waymo-3DSkelMo æ•°æ®é›†ï¼Œä»è€Œå®ç°äº†å¤šæ™ºèƒ½ä½“è¡Œäºº-è½¦è¾†äº¤äº’çš„çœŸå®å»ºæ¨¡ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é‡‡æ ·æ–¹æ¡ˆï¼Œæ ¹æ®è¡Œäººå’Œè½¦è¾†æ•°é‡å¯¹åœºæ™¯è¿›è¡Œåˆ†ç±»ï¼Œä»è€Œä¿ƒè¿›ä¸åŒäº¤äº’å¤æ‚æ€§çš„è®­ç»ƒã€‚æˆ‘ä»¬æå‡ºçš„ç½‘ç»œé‡‡ç”¨ä¸“ç”¨è½¦è¾†ç¼–ç å™¨å’Œè¡Œäººè½¦è¾†äº¤äº’äº¤å‰æ³¨æ„æ¨¡å—æ¥é€‚åº” TBIFormer æ¶æ„ï¼Œä»¥èåˆè¡Œäººå’Œè½¦è¾†ç‰¹å¾ï¼Œä»è€Œå…è®¸æ ¹æ®å†å²è¡Œäººè¿åŠ¨å’Œå‘¨å›´è½¦è¾†è¿›è¡Œé¢„æµ‹ã€‚å¤§é‡å®éªŒè¯æ˜äº†é¢„æµ‹å‡†ç¡®æ€§çš„æ˜¾ç€æé«˜ï¼Œå¹¶éªŒè¯äº†è¡Œäºº-è½¦è¾†äº¤äº’å»ºæ¨¡çš„ä¸åŒæ–¹æ³•ï¼Œå‡¸æ˜¾äº†è½¦è¾†æ„ŸçŸ¥ 3D å§¿æ€é¢„æµ‹å¯¹äºè‡ªåŠ¨é©¾é©¶çš„é‡è¦æ€§ã€‚ä»£ç å¯è§ï¼šhttps://github.com/GuangxunZhu/VehCondPose3D

</details>

---

## 250. MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE

**ä¸­æ–‡æ ‡é¢˜**: MotionCrafterï¼šä½¿ç”¨ 4D VAE è¿›è¡Œå¯†é›†å‡ ä½•å’Œè¿åŠ¨é‡å»º

**Date**: 2026-02-09 | **arXiv**: [2602.08961v1](http://arxiv.org/abs/2602.08961v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08961v1)

**Project**: https://ruijiezhu94.github.io/MotionCrafter_Page  <details><summary><b>Abstract</b></summary>

We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æˆ‘ä»¬å¼•å…¥äº† MotionCrafterï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè§†é¢‘æ‰©æ•£çš„æ¡†æ¶ï¼Œå¯ä»¥è”åˆé‡å»º 4D å‡ ä½•ç»“æ„å¹¶ä¼°è®¡å•ç›®è§†é¢‘ä¸­çš„å¯†é›†è¿åŠ¨ã€‚æˆ‘ä»¬æ–¹æ³•çš„æ ¸å¿ƒæ˜¯å…±äº«åæ ‡ç³»ä¸­å¯†é›† 3D ç‚¹å›¾å’Œ 3D åœºæ™¯æµçš„æ–°é¢–è”åˆè¡¨ç¤ºï¼Œä»¥åŠæœ‰æ•ˆå­¦ä¹ è¿™ç§è¡¨ç¤ºçš„æ–°é¢– 4D VAEã€‚ä¸ä¹‹å‰å¼ºåˆ¶ 3D å€¼å’Œæ½œåœ¨å˜é‡ä¸ RGB VAE æ½œåœ¨å˜é‡ä¸¥æ ¼å¯¹é½çš„å·¥ä½œä¸åŒï¼ˆå°½ç®¡å®ƒä»¬çš„åˆ†å¸ƒæ ¹æœ¬ä¸åŒï¼‰ï¼Œæˆ‘ä»¬è¡¨æ˜è¿™ç§å¯¹é½æ˜¯ä¸å¿…è¦çš„ï¼Œå¹¶ä¸”ä¼šå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚ç›¸åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„æ•°æ®å½’ä¸€åŒ–å’Œ VAE è®­ç»ƒç­–ç•¥ï¼Œå¯ä»¥æ›´å¥½åœ°ä¼ è¾“æ‰©æ•£å…ˆéªŒå¹¶å¤§å¤§æé«˜é‡å»ºè´¨é‡ã€‚è·¨å¤šä¸ªæ•°æ®é›†çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMotionCrafter åœ¨å‡ ä½•é‡å»ºå’Œå¯†é›†åœºæ™¯æµä¼°è®¡æ–¹é¢å‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨å‡ ä½•å’Œè¿åŠ¨é‡å»ºæ–¹é¢åˆ†åˆ«å®ç°äº† 38.64% å’Œ 25.0% çš„æ”¹è¿›ï¼Œå¹¶ä¸”å…¨éƒ¨æ— éœ€ä»»ä½•åæœŸä¼˜åŒ–ã€‚é¡¹ç›®é¡µé¢ï¼šhttps://ruijiezhu94.github.io/MotionCrafter_Page

</details>

---

## 251. Grow with the Flow: 4D Reconstruction of Growing Plants with Gaussian Flow Fields

**ä¸­æ–‡æ ‡é¢˜**: éšæµç”Ÿé•¿ï¼šåˆ©ç”¨é«˜æ–¯æµåœº 4D é‡å»ºæ­£åœ¨ç”Ÿé•¿çš„æ¤ç‰©

**Date**: 2026-02-09 | **arXiv**: [2602.08958v2](http://arxiv.org/abs/2602.08958v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.08958v2)

<details><summary><b>Abstract</b></summary>

Modeling the time-varying 3D appearance of plants during their growth poses unique challenges: unlike many dynamic scenes, plants generate new geometry over time as they expand, branch, and differentiate. Recent motion modeling techniques are ill-suited to this problem setting. For example, deformation fields cannot introduce new geometry, and 4D Gaussian splatting constrains motion to a linear trajectory in space and time and cannot track the same set of Gaussians over time. Here, we introduce a 3D Gaussian flow field representation that models plant growth as a time-varying derivative over Gaussian parameters -- position, scale, orientation, color, and opacity -- enabling nonlinear and continuous-time growth dynamics. To initialize a sufficient set of Gaussian primitives, we reconstruct the mature plant and learn a process of reverse growth, effectively simulating the plant's developmental history in reverse. Our approach achieves superior image quality and geometric accuracy compared to prior methods on multi-view timelapse datasets of plant growth, providing a new approach for appearance modeling of growing 3D structures.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¯¹æ¤ç‰©ç”Ÿé•¿è¿‡ç¨‹ä¸­éšæ—¶é—´å˜åŒ–çš„ 3D å¤–è§‚è¿›è¡Œå»ºæ¨¡æå‡ºäº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼šä¸è®¸å¤šåŠ¨æ€åœºæ™¯ä¸åŒï¼Œæ¤ç‰©éšç€æ—¶é—´çš„æ¨ç§»ï¼Œåœ¨æ‰©å±•ã€åˆ†æ”¯å’Œåˆ†åŒ–æ—¶ä¼šç”Ÿæˆæ–°çš„å‡ ä½•å½¢çŠ¶ã€‚æœ€è¿‘çš„è¿åŠ¨å»ºæ¨¡æŠ€æœ¯ä¸é€‚åˆè¿™ä¸ªé—®é¢˜è®¾ç½®ã€‚ä¾‹å¦‚ï¼Œå˜å½¢åœºæ— æ³•å¼•å…¥æ–°çš„å‡ ä½•å½¢çŠ¶ï¼Œ4D é«˜æ–¯å–·å°„å°†è¿åŠ¨é™åˆ¶ä¸ºç©ºé—´å’Œæ—¶é—´ä¸­çš„çº¿æ€§è½¨è¿¹ï¼Œå¹¶ä¸”æ— æ³•éšæ—¶é—´è·Ÿè¸ªåŒä¸€ç»„é«˜æ–¯ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¼•å…¥äº† 3D é«˜æ–¯æµåœºè¡¨ç¤ºï¼Œå°†æ¤ç‰©ç”Ÿé•¿å»ºæ¨¡ä¸ºé«˜æ–¯å‚æ•°ï¼ˆä½ç½®ã€å°ºåº¦ã€æ–¹å‘ã€é¢œè‰²å’Œä¸é€æ˜åº¦ï¼‰çš„æ—¶å˜å¯¼æ•°ï¼Œä»è€Œå®ç°éçº¿æ€§å’Œè¿ç»­æ—¶é—´ç”Ÿé•¿åŠ¨æ€ã€‚ä¸ºäº†åˆå§‹åŒ–ä¸€ç»„è¶³å¤Ÿçš„é«˜æ–¯åŸè¯­ï¼Œæˆ‘ä»¬é‡å»ºäº†æˆç†Ÿçš„æ¤ç‰©å¹¶å­¦ä¹ äº†åå‘ç”Ÿé•¿çš„è¿‡ç¨‹ï¼Œæœ‰æ•ˆåœ°åå‘æ¨¡æ‹Ÿäº†æ¤ç‰©çš„å‘è‚²å†å²ã€‚ä¸æ¤ç‰©ç”Ÿé•¿å¤šè§†å›¾å»¶æ—¶æ•°æ®é›†ä¸Šçš„å…ˆå‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†å“è¶Šçš„å›¾åƒè´¨é‡å’Œå‡ ä½•ç²¾åº¦ï¼Œä¸ºç”Ÿé•¿ 3D ç»“æ„çš„å¤–è§‚å»ºæ¨¡æä¾›äº†ä¸€ç§æ–°æ–¹æ³•ã€‚

</details>

---

## 252. PRISM-XR: Empowering Privacy-Aware XR Collaboration with Multimodal Large Language Models

**ä¸­æ–‡æ ‡é¢˜**: PRISM-XRï¼šé€šè¿‡å¤šæ¨¡å¼å¤§è¯­è¨€æ¨¡å‹å¢å¼ºéšç§æ„è¯† XR åä½œ

**Date**: 2026-02-09 | **arXiv**: [2602.10154v1](http://arxiv.org/abs/2602.10154v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10154v1)

<details><summary><b>Abstract</b></summary>

Multimodal Large Language Models (MLLMs) enhance collaboration in Extended Reality (XR) environments by enabling flexible object and animation creation through the combination of natural language and visual inputs. However, visual data captured by XR headsets includes real-world backgrounds that may contain irrelevant or sensitive user information, such as credit cards left on the table or facial identities of other users. Uploading those frames to cloud-based MLLMs poses serious privacy risks, particularly when such data is processed without explicit user consent. Additionally, existing colocation and synchronization mechanisms in commercial XR APIs rely on time-consuming, privacy-invasive environment scanning and struggle to adapt to the highly dynamic nature of MLLM-integrated XR environments. In this paper, we propose PRISM-XR, a novel framework that facilitates multi-user collaboration in XR by providing privacy-aware MLLM integration. PRISM-XR employs intelligent frame preprocessing on the edge server to filter sensitive data and remove irrelevant context before communicating with cloud generative AI models. Additionally, we introduce a lightweight registration process and a fully customizable content-sharing mechanism to enable efficient, accurate, and privacy-preserving content synchronization among users. Our numerical evaluation results indicate that the proposed platform achieves nearly 90% accuracy in fulfilling user requests and less than 0.27 seconds registration time while maintaining spatial inconsistencies of less than 3.5 cm. Furthermore, we conducted an IRB-approved user study with 28 participants, demonstrating that our system could automatically filter highly sensitive objects in over 90% of scenarios while maintaining strong overall usability.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLM) é€šè¿‡ç»“åˆè‡ªç„¶è¯­è¨€å’Œè§†è§‰è¾“å…¥æ¥å®ç°çµæ´»çš„å¯¹è±¡å’ŒåŠ¨ç”»åˆ›å»ºï¼Œä»è€Œå¢å¼ºæ‰©å±•ç°å® (XR) ç¯å¢ƒä¸­çš„åä½œã€‚ç„¶è€Œï¼ŒXR è€³æœºæ•è·çš„è§†è§‰æ•°æ®åŒ…æ‹¬ç°å®ä¸–ç•Œçš„èƒŒæ™¯ï¼Œå…¶ä¸­å¯èƒ½åŒ…å«ä¸ç›¸å…³æˆ–æ•æ„Ÿçš„ç”¨æˆ·ä¿¡æ¯ï¼Œä¾‹å¦‚ç•™åœ¨æ¡Œå­ä¸Šçš„ä¿¡ç”¨å¡æˆ–å…¶ä»–ç”¨æˆ·çš„é¢éƒ¨èº«ä»½ã€‚å°†è¿™äº›å¸§ä¸Šä¼ åˆ°åŸºäºäº‘çš„ MLLM ä¼šå¸¦æ¥ä¸¥é‡çš„éšç§é£é™©ï¼Œç‰¹åˆ«æ˜¯åœ¨æœªç»ç”¨æˆ·æ˜ç¡®åŒæ„çš„æƒ…å†µä¸‹å¤„ç†æ­¤ç±»æ•°æ®æ—¶ã€‚æ­¤å¤–ï¼Œå•†ä¸š XR API ä¸­ç°æœ‰çš„æ‰˜ç®¡å’ŒåŒæ­¥æœºåˆ¶ä¾èµ–äºè€—æ—¶ã€ä¾µçŠ¯éšç§çš„ç¯å¢ƒæ‰«æï¼Œå¹¶ä¸”éš¾ä»¥é€‚åº” MLLM é›†æˆ XR ç¯å¢ƒçš„é«˜åº¦åŠ¨æ€ç‰¹æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† PRISM-XRï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡æä¾›éšç§æ„ŸçŸ¥çš„ MLLM é›†æˆæ¥ä¿ƒè¿› XR ä¸­çš„å¤šç”¨æˆ·åä½œã€‚ PRISM-XR åœ¨è¾¹ç¼˜æœåŠ¡å™¨ä¸Šé‡‡ç”¨æ™ºèƒ½å¸§é¢„å¤„ç†ï¼Œåœ¨ä¸äº‘ç”Ÿæˆäººå·¥æ™ºèƒ½æ¨¡å‹é€šä¿¡ä¹‹å‰è¿‡æ»¤æ•æ„Ÿæ•°æ®å¹¶åˆ é™¤ä¸ç›¸å…³çš„ä¸Šä¸‹æ–‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†è½»é‡çº§æ³¨å†Œæµç¨‹å’Œå®Œå…¨å¯å®šåˆ¶çš„å†…å®¹å…±äº«æœºåˆ¶ï¼Œä»¥å®ç°ç”¨æˆ·ä¹‹é—´é«˜æ•ˆã€å‡†ç¡®ä¸”ä¿æŠ¤éšç§çš„å†…å®¹åŒæ­¥ã€‚æˆ‘ä»¬çš„æ•°å€¼è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„å¹³å°åœ¨æ»¡è¶³ç”¨æˆ·è¯·æ±‚æ–¹é¢å®ç°äº†è¿‘ 90% çš„å‡†ç¡®åº¦ï¼Œæ³¨å†Œæ—¶é—´å°äº 0.27 ç§’ï¼ŒåŒæ—¶ä¿æŒç©ºé—´ä¸ä¸€è‡´æ€§å°äº 3.5 å˜ç±³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹ 28 åå‚ä¸è€…è¿›è¡Œäº† IRB æ‰¹å‡†çš„ç”¨æˆ·ç ”ç©¶ï¼Œè¯æ˜æˆ‘ä»¬çš„ç³»ç»Ÿå¯ä»¥åœ¨è¶…è¿‡ 90% çš„åœºæ™¯ä¸­è‡ªåŠ¨è¿‡æ»¤é«˜åº¦æ•æ„Ÿçš„å¯¹è±¡ï¼ŒåŒæ—¶ä¿æŒå¼ºå¤§çš„æ•´ä½“å¯ç”¨æ€§ã€‚

</details>

---

## 253. GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing

**ä¸­æ–‡æ ‡é¢˜**: GOT-Editï¼šé€šè¿‡åœ¨çº¿æ¨¡å‹ç¼–è¾‘è¿›è¡Œå‡ ä½•æ„ŸçŸ¥é€šç”¨å¯¹è±¡è·Ÿè¸ª

**Date**: 2026-02-09 | **arXiv**: [2602.08550v1](http://arxiv.org/abs/2602.08550v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08550v1)

<details><summary><b>Abstract</b></summary>

Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geometric cues, which makes them susceptible to partial occlusion, distractors, and variations in geometry and appearance. To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to enable geometric cue inference from only a few 2D images. To tackle the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing with null-space constrained updates that incorporate geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios. Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

äººç±»å¯¹ 2D è§†é¢‘æµä¸­æœ‰æ•ˆå¯¹è±¡è·Ÿè¸ªçš„æ„ŸçŸ¥æºè‡ªå¯¹å…ˆéªŒ 3D çŸ¥è¯†ä¸è¯­ä¹‰æ¨ç†ç›¸ç»“åˆçš„éšå¼ä½¿ç”¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤§å¤šæ•°é€šç”¨å¯¹è±¡è·Ÿè¸ª (GOT) æ–¹æ³•ä¸»è¦ä¾èµ–äºç›®æ ‡åŠå…¶å‘¨å›´ç¯å¢ƒçš„ 2D ç‰¹å¾ï¼Œè€Œå¿½ç•¥ 3D å‡ ä½•çº¿ç´¢ï¼Œè¿™ä½¿å¾—å®ƒä»¬å®¹æ˜“å—åˆ°éƒ¨åˆ†é®æŒ¡ã€å¹²æ‰°ä»¥åŠå‡ ä½•å’Œå¤–è§‚å˜åŒ–çš„å½±å“ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº† GOT-Editï¼Œè¿™æ˜¯ä¸€ç§åœ¨çº¿è·¨æ¨¡æ€æ¨¡å‹ç¼–è¾‘æ–¹æ³•ï¼Œå®ƒå°†å‡ ä½•æ„ŸçŸ¥çº¿ç´¢é›†æˆåˆ°æ¥è‡ª 2D è§†é¢‘æµçš„é€šç”¨å¯¹è±¡è·Ÿè¸ªå™¨ä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨é¢„å…ˆè®­ç»ƒçš„ Visual Geometry Grounded Transformer çš„åŠŸèƒ½ï¼Œä»…ä»å°‘é‡ 2D å›¾åƒå³å¯è¿›è¡Œå‡ ä½•çº¿ç´¢æ¨æ–­ã€‚ä¸ºäº†åº”å¯¹æ— ç¼ç»“åˆå‡ ä½•å’Œè¯­ä¹‰çš„æŒ‘æˆ˜ï¼ŒGOT-Edit é€šè¿‡é›¶ç©ºé—´çº¦æŸæ›´æ–°æ‰§è¡Œåœ¨çº¿æ¨¡å‹ç¼–è¾‘ï¼Œåœ¨ä¿ç•™è¯­ä¹‰åŒºåˆ†çš„åŒæ—¶åˆå¹¶å‡ ä½•ä¿¡æ¯ï¼Œä»è€Œåœ¨ä¸åŒåœºæ™¯ä¸­å§‹ç»ˆè·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚å¯¹å¤šä¸ª GOT åŸºå‡†çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGOT-Edit å®ç°äº†å“è¶Šçš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é®æŒ¡å’Œæ‚ä¹±çš„æƒ…å†µä¸‹ï¼Œå»ºç«‹äº†å°† 2D è¯­ä¹‰ä¸ 3D å‡ ä½•æ¨ç†ç›¸ç»“åˆä»¥è¿›è¡Œé€šç”¨å¯¹è±¡è·Ÿè¸ªçš„æ–°èŒƒä¾‹ã€‚

</details>

---

## 254. T2VTree: User-Centered Visual Analytics for Agent-Assisted Thought-to-Video Authoring

**ä¸­æ–‡æ ‡é¢˜**: T2VTreeï¼šä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„å¯è§†åŒ–åˆ†æï¼Œç”¨äºä»£ç†è¾…åŠ©çš„æ€æƒ³åˆ°è§†é¢‘åˆ›ä½œ

**Date**: 2026-02-09 | **arXiv**: [2602.08368v1](http://arxiv.org/abs/2602.08368v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08368v1)

**Code**: https://github.com/tezuka0210/T2VTree.

<details><summary><b>Abstract</b></summary>

Generative models have substantially expanded video generation capabilities, yet practical thought-to-video creation remains a multi-stage, multi-modal, and decision-intensive process. However, existing tools either hide intermediate decisions behind repeated reruns or expose operator-level workflows that make exploration traces difficult to manage, compare, and reuse. We present T2VTree, a user-centered visual analytics approach for agent-assisted thought-to-video authoring. T2VTree represents the authoring process as a tree visualization. Each node in the tree binds an editable specification (intent, referenced inputs, workflow choice, prompts, and parameters) with the resulting multimodal outputs, making refinement, branching, and provenance inspection directly operable. To reduce the burden of deciding what to do next, a set of collaborating agents translates step-level intent into an executable plan that remains visible and user-editable before execution. We further implement a visual analytics system that integrates branching authoring with in-place preview and stitching for convergent assembly, enabling end-to-end multi-scene creation without leaving the authoring context. We demonstrate T2VTreeVA through two multi-scene case studies and a comparative user study, showing how the T2VTree visualization and editable agent planning support reliable refinement, localized comparison, and practical reuse in real authoring workflows. T2VTree is available at: https://github.com/tezuka0210/T2VTree.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç”Ÿæˆæ¨¡å‹å¤§å¤§æ‰©å±•äº†è§†é¢‘ç”ŸæˆåŠŸèƒ½ï¼Œä½†å®é™…çš„æ€æƒ³åˆ°è§†é¢‘åˆ›ä½œä»ç„¶æ˜¯ä¸€ä¸ªå¤šé˜¶æ®µã€å¤šæ¨¡å¼å’Œå†³ç­–å¯†é›†å‹çš„è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œç°æœ‰å·¥å…·è¦ä¹ˆéšè—é‡å¤é‡æ–°è¿è¡ŒèƒŒåçš„ä¸­é—´å†³ç­–ï¼Œè¦ä¹ˆæš´éœ²æ“ä½œå‘˜çº§åˆ«çš„å·¥ä½œæµç¨‹ï¼Œè¿™ä½¿å¾—æ¢ç´¢è·Ÿè¸ªéš¾ä»¥ç®¡ç†ã€æ¯”è¾ƒå’Œé‡ç”¨ã€‚æˆ‘ä»¬æå‡ºäº† T2VTreeï¼Œä¸€ç§ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„å¯è§†åŒ–åˆ†ææ–¹æ³•ï¼Œç”¨äºä»£ç†è¾…åŠ©çš„æ€æƒ³åˆ°è§†é¢‘åˆ›ä½œã€‚ T2VTree å°†åˆ›ä½œè¿‡ç¨‹è¡¨ç¤ºä¸ºæ ‘å¯è§†åŒ–ã€‚æ ‘ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹éƒ½å°†å¯ç¼–è¾‘çš„è§„èŒƒï¼ˆæ„å›¾ã€å¼•ç”¨çš„è¾“å…¥ã€å·¥ä½œæµé€‰æ‹©ã€æç¤ºå’Œå‚æ•°ï¼‰ä¸ç”Ÿæˆçš„å¤šæ¨¡å¼è¾“å‡ºç»‘å®šåœ¨ä¸€èµ·ï¼Œä»è€Œä½¿ç»†åŒ–ã€åˆ†æ”¯å’Œæ¥æºæ£€æŸ¥å¯ç›´æ¥æ“ä½œã€‚ä¸ºäº†å‡è½»å†³å®šä¸‹ä¸€æ­¥åšä»€ä¹ˆçš„è´Ÿæ‹…ï¼Œä¸€ç»„åä½œä»£ç†å°†æ­¥éª¤çº§æ„å›¾è½¬æ¢ä¸ºå¯æ‰§è¡Œè®¡åˆ’ï¼Œè¯¥è®¡åˆ’åœ¨æ‰§è¡Œå‰ä¿æŒå¯è§ä¸”ç”¨æˆ·å¯ç¼–è¾‘ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å®ç°äº†ä¸€ä¸ªå¯è§†åŒ–åˆ†æç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå°†åˆ†æ”¯åˆ›ä½œä¸å°±åœ°é¢„è§ˆå’Œæ‹¼æ¥é›†æˆåœ¨ä¸€èµ·ä»¥è¿›è¡Œèšåˆç»„è£…ï¼Œä»è€Œæ— éœ€ç¦»å¼€åˆ›ä½œä¸Šä¸‹æ–‡å³å¯å®ç°ç«¯åˆ°ç«¯çš„å¤šåœºæ™¯åˆ›å»ºã€‚æˆ‘ä»¬é€šè¿‡ä¸¤ä¸ªå¤šåœºæ™¯æ¡ˆä¾‹ç ”ç©¶å’Œæ¯”è¾ƒç”¨æˆ·ç ”ç©¶æ¥æ¼”ç¤º T2VTreeVAï¼Œå±•ç¤º T2VTree å¯è§†åŒ–å’Œå¯ç¼–è¾‘ä»£ç†è§„åˆ’å¦‚ä½•æ”¯æŒçœŸå®åˆ›ä½œå·¥ä½œæµç¨‹ä¸­çš„å¯é ç»†åŒ–ã€æœ¬åœ°åŒ–æ¯”è¾ƒå’Œå®é™…é‡ç”¨ã€‚ T2VTree ä½äºï¼šhttps://github.com/tezuka0210/T2VTreeã€‚

</details>

---

## 255. Data-centric Design of Learning-based Surgical Gaze Perception Models in Multi-Task Simulation

**ä¸­æ–‡æ ‡é¢˜**: å¤šä»»åŠ¡æ¨¡æ‹Ÿä¸­åŸºäºå­¦ä¹ çš„æ‰‹æœ¯æ³¨è§†æ„ŸçŸ¥æ¨¡å‹çš„ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„è®¾è®¡

**Date**: 2026-02-09 | **arXiv**: [2602.09259v1](http://arxiv.org/abs/2602.09259v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09259v1)

<details><summary><b>Abstract</b></summary>

In robot-assisted minimally invasive surgery (RMIS), reduced haptic feedback and depth cues increase reliance on expert visual perception, motivating gaze-guided training and learning-based surgical perception models. However, operative expert gaze is costly to collect, and it remains unclear how the source of gaze supervision, both expertise level (intermediate vs. novice) and perceptual modality (active execution vs. passive viewing), shapes what attention models learn. We introduce a paired active-passive, multi-task surgical gaze dataset collected on the da Vinci SimNow simulator across four drills. Active gaze was recorded during task execution using a VR headset with eye tracking, and the corresponding videos were reused as stimuli to collect passive gaze from observers, enabling controlled same-video comparisons. We quantify skill- and modality-dependent differences in gaze organization and evaluate the substitutability of passive gaze for operative supervision using fixation density overlap analyses and single-frame saliency modeling. Across settings, MSI-Net produced stable, interpretable predictions, whereas SalGAN was unstable and often poorly aligned with human fixations. Models trained on passive gaze recovered a substantial portion of intermediate active attention, but with predictable degradation, and transfer was asymmetric between active and passive targets. Notably, novice passive labels approximated intermediate-passive targets with limited loss on higher-quality demonstrations, suggesting a practical path for scalable, crowd-sourced gaze supervision in surgical coaching and perception modeling.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åœ¨æœºå™¨äººè¾…åŠ©å¾®åˆ›æ‰‹æœ¯ï¼ˆRMISï¼‰ä¸­ï¼Œè§¦è§‰åé¦ˆå’Œæ·±åº¦æç¤ºçš„å‡å°‘å¢åŠ äº†å¯¹ä¸“å®¶è§†è§‰æ„ŸçŸ¥çš„ä¾èµ–ï¼Œæ¿€å‘äº†å‡è§†å¼•å¯¼è®­ç»ƒå’ŒåŸºäºå­¦ä¹ çš„æ‰‹æœ¯æ„ŸçŸ¥æ¨¡å‹ã€‚ç„¶è€Œï¼Œæ“ä½œä¸“å®¶å‡è§†çš„æ”¶é›†æˆæœ¬å¾ˆé«˜ï¼Œè€Œä¸”ç›®å‰è¿˜ä¸æ¸…æ¥šå‡è§†ç›‘ç£çš„æ¥æºï¼ŒåŒ…æ‹¬ä¸“ä¸šæ°´å¹³ï¼ˆä¸­çº§ä¸æ–°æ‰‹ï¼‰å’Œæ„ŸçŸ¥æ–¹å¼ï¼ˆä¸»åŠ¨æ‰§è¡Œä¸è¢«åŠ¨è§‚çœ‹ï¼‰å¦‚ä½•å¡‘é€ æ³¨æ„åŠ›æ¨¡å‹å­¦ä¹ çš„å†…å®¹ã€‚æˆ‘ä»¬ä»‹ç»äº†åœ¨è¾¾èŠ¬å¥‡ SimNow æ¨¡æ‹Ÿå™¨ä¸Šé€šè¿‡å››æ¬¡æ¼”ä¹ æ”¶é›†çš„é…å¯¹ä¸»åŠ¨-è¢«åŠ¨ã€å¤šä»»åŠ¡æ‰‹æœ¯æ³¨è§†æ•°æ®é›†ã€‚åœ¨ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨å…·æœ‰çœ¼åŠ¨è¿½è¸ªåŠŸèƒ½çš„ VR è€³æœºè®°å½•ä¸»åŠ¨å‡è§†ï¼Œå¹¶å°†ç›¸åº”çš„è§†é¢‘é‡æ–°ç”¨ä½œåˆºæ¿€ï¼Œä»¥æ”¶é›†è§‚å¯Ÿè€…çš„è¢«åŠ¨å‡è§†ï¼Œä»è€Œå®ç°å—æ§çš„ç›¸åŒè§†é¢‘æ¯”è¾ƒã€‚æˆ‘ä»¬é‡åŒ–æ³¨è§†ç»„ç»‡ä¸­ä¾èµ–äºæŠ€èƒ½å’Œæ¨¡æ€çš„å·®å¼‚ï¼Œå¹¶ä½¿ç”¨æ³¨è§†å¯†åº¦é‡å åˆ†æå’Œå•å¸§æ˜¾ç€æ€§æ¨¡å‹è¯„ä¼°è¢«åŠ¨æ³¨è§†å¯¹æ“ä½œç›‘ç£çš„å¯æ›¿ä»£æ€§ã€‚åœ¨ä¸åŒçš„è®¾ç½®ä¸­ï¼ŒMSI-Net äº§ç”Ÿç¨³å®šã€å¯è§£é‡Šçš„é¢„æµ‹ï¼Œè€Œ SalGAN åˆ™ä¸ç¨³å®šï¼Œå¹¶ä¸”é€šå¸¸ä¸äººç±»æ³¨è§†ç‚¹ä¸ä¸€è‡´ã€‚åœ¨è¢«åŠ¨å‡è§†ä¸Šè®­ç»ƒçš„æ¨¡å‹æ¢å¤äº†å¤§éƒ¨åˆ†ä¸­é—´ä¸»åŠ¨æ³¨æ„åŠ›ï¼Œä½†ä¼šå‡ºç°å¯é¢„æµ‹çš„é€€åŒ–ï¼Œå¹¶ä¸”ä¸»åŠ¨ç›®æ ‡å’Œè¢«åŠ¨ç›®æ ‡ä¹‹é—´çš„è½¬ç§»æ˜¯ä¸å¯¹ç§°çš„ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ–°æ‰‹è¢«åŠ¨æ ‡ç­¾è¿‘ä¼¼äºä¸­è¢«åŠ¨ç›®æ ‡ï¼Œåœ¨é«˜è´¨é‡æ¼”ç¤ºä¸­æŸå¤±æœ‰é™ï¼Œè¿™ä¸ºæ‰‹æœ¯æŒ‡å¯¼å’Œæ„ŸçŸ¥å»ºæ¨¡ä¸­å¯æ‰©å±•çš„ã€ä¼—åŒ…çš„æ³¨è§†ç›‘ç£æä¾›äº†ä¸€æ¡å®ç”¨é€”å¾„ã€‚

</details>

---

## 256. STaR: Scalable Task-Conditioned Retrieval for Long-Horizon Multimodal Robot Memory

**ä¸­æ–‡æ ‡é¢˜**: STaRï¼šé•¿è§†é‡å¤šæ¨¡æ€æœºå™¨äººå†…å­˜çš„å¯æ‰©å±•ä»»åŠ¡æ¡ä»¶æ£€ç´¢

**Date**: 2026-02-09 | **arXiv**: [2602.09255v1](http://arxiv.org/abs/2602.09255v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09255v1)

<details><summary><b>Abstract</b></summary>

Mobile robots are often deployed over long durations in diverse open, dynamic scenes, including indoor setting such as warehouses and manufacturing facilities, and outdoor settings such as agricultural and roadway operations. A core challenge is to build a scalable long-horizon memory that supports an agentic workflow for planning, retrieval, and reasoning over open-ended instructions at variable granularity, while producing precise, actionable answers for navigation. We present STaR, an agentic reasoning framework that (i) constructs a task-agnostic, multimodal long-term memory that generalizes to unseen queries while preserving fine-grained environmental semantics (object attributes, spatial relations, and dynamic events), and (ii) introduces a Scalable TaskConditioned Retrieval algorithm based on the Information Bottleneck principle to extract from long-term memory a compact, non-redundant, information-rich set of candidate memories for contextual reasoning. We evaluate STaR on NaVQA (mixed indoor/outdoor campus scenes) and WH-VQA, a customized warehouse benchmark with many visually similar objects built with Isaac Sim, emphasizing contextual reasoning. Across the two datasets, STaR consistently outperforms strong baselines, achieving higher success rates and markedly lower spatial error. We further deploy STaR on a real Husky wheeled robot in both indoor and outdoor environments, demonstrating robust longhorizon reasoning, scalability, and practical utility.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç§»åŠ¨æœºå™¨äººé€šå¸¸é•¿æ—¶é—´éƒ¨ç½²åœ¨å„ç§å¼€æ”¾ã€åŠ¨æ€çš„åœºæ™¯ä¸­ï¼ŒåŒ…æ‹¬ä»“åº“å’Œåˆ¶é€ è®¾æ–½ç­‰å®¤å†…ç¯å¢ƒï¼Œä»¥åŠå†œä¸šå’Œé“è·¯ä½œä¸šç­‰å®¤å¤–ç¯å¢ƒã€‚æ ¸å¿ƒæŒ‘æˆ˜æ˜¯æ„å»ºä¸€ä¸ªå¯æ‰©å±•çš„é•¿è§†é‡å†…å­˜ï¼Œæ”¯æŒä»£ç†å·¥ä½œæµç¨‹ï¼Œä»¥å¯å˜ç²’åº¦å¯¹å¼€æ”¾å¼æŒ‡ä»¤è¿›è¡Œè§„åˆ’ã€æ£€ç´¢å’Œæ¨ç†ï¼ŒåŒæ—¶ç”Ÿæˆç²¾ç¡®çš„ã€å¯æ“ä½œçš„å¯¼èˆªç­”æ¡ˆã€‚æˆ‘ä»¬æå‡ºäº† STaRï¼Œä¸€ç§ä»£ç†æ¨ç†æ¡†æ¶ï¼Œå®ƒï¼ˆiï¼‰æ„å»ºäº†ä¸€ä¸ªä¸ä»»åŠ¡æ— å…³çš„å¤šæ¨¡æ€é•¿æœŸè®°å¿†ï¼Œå¯æ³›åŒ–åˆ°æœªè§è¿‡çš„æŸ¥è¯¢ï¼ŒåŒæ—¶ä¿ç•™ç»†ç²’åº¦çš„ç¯å¢ƒè¯­ä¹‰ï¼ˆå¯¹è±¡å±æ€§ã€ç©ºé—´å…³ç³»å’ŒåŠ¨æ€äº‹ä»¶ï¼‰ï¼Œä»¥åŠï¼ˆiiï¼‰å¼•å…¥åŸºäºä¿¡æ¯ç“¶é¢ˆåŸç†çš„å¯æ‰©å±•ä»»åŠ¡æ¡ä»¶æ£€ç´¢ç®—æ³•ï¼Œä»é•¿æœŸè®°å¿†ä¸­æå–ç´§å‡‘ã€éå†—ä½™ã€ä¿¡æ¯ä¸°å¯Œçš„å€™é€‰è®°å¿†é›†ï¼Œç”¨äºä¸Šä¸‹æ–‡æ¨ç†ã€‚æˆ‘ä»¬åœ¨ NaVQAï¼ˆå®¤å†…/å®¤å¤–æ··åˆæ ¡å›­åœºæ™¯ï¼‰å’Œ WH-VQA ä¸Šè¯„ä¼° STaRï¼ŒWH-VQA æ˜¯ä¸€ä¸ªå®šåˆ¶çš„ä»“åº“åŸºå‡†ï¼Œå…·æœ‰è®¸å¤šç”¨ Isaac Sim æ„å»ºçš„è§†è§‰ç›¸ä¼¼çš„å¯¹è±¡ï¼Œå¼ºè°ƒä¸Šä¸‹æ–‡æ¨ç†ã€‚åœ¨è¿™ä¸¤ä¸ªæ•°æ®é›†ä¸­ï¼ŒSTARR å§‹ç»ˆä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼Œå®ç°äº†æ›´é«˜çš„æˆåŠŸç‡å’Œæ˜¾ç€æ›´ä½çš„ç©ºé—´è¯¯å·®ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åœ¨å®¤å†…å’Œå®¤å¤–ç¯å¢ƒä¸­çš„çœŸå® Husky è½®å¼æœºå™¨äººä¸Šéƒ¨ç½² STaRï¼Œå±•ç¤ºäº†å¼ºå¤§çš„é•¿è§†é‡æ¨ç†ã€å¯æ‰©å±•æ€§å’Œå®ç”¨æ€§ã€‚

</details>

---

## 257. From Legible to Inscrutable Trajectories: (Il)legible Motion Planning Accounting for Multiple Observers

**ä¸­æ–‡æ ‡é¢˜**: ä»æ¸…æ™°çš„è½¨è¿¹åˆ°éš¾ä»¥ç†è§£çš„è½¨è¿¹ï¼šï¼ˆä¸ï¼‰æ¸…æ™°çš„å¤šä¸ªè§‚å¯Ÿè€…çš„è¿åŠ¨è§„åˆ’è¯´æ˜

**Date**: 2026-02-09 | **arXiv**: [2602.09227v1](http://arxiv.org/abs/2602.09227v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09227v1)

<details><summary><b>Abstract</b></summary>

In cooperative environments, such as in factories or assistive scenarios, it is important for a robot to communicate its intentions to observers, who could be either other humans or robots. A legible trajectory allows an observer to quickly and accurately predict an agent's intention. In adversarial environments, such as in military operations or games, it is important for a robot to not communicate its intentions to observers. An illegible trajectory leads an observer to incorrectly predict the agent's intention or delays when an observer is able to make a correct prediction about the agent's intention. However, in some environments there are multiple observers, each of whom may be able to see only part of the environment, and each of whom may have different motives. In this work, we introduce the Mixed-Motive Limited-Observability Legible Motion Planning (MMLO-LMP) problem, which requires a motion planner to generate a trajectory that is legible to observers with positive motives and illegible to observers with negative motives while also considering the visibility limitations of each observer. We highlight multiple strategies an agent can take while still achieving the problem objective. We also present DUBIOUS, a trajectory optimizer that solves MMLO-LMP. Our results show that DUBIOUS can generate trajectories that balance legibility with the motives and limited visibility regions of the observers. Future work includes many variations of MMLO-LMP, including moving observers and observer teaming.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åœ¨åˆä½œç¯å¢ƒä¸­ï¼Œä¾‹å¦‚åœ¨å·¥å‚æˆ–è¾…åŠ©åœºæ™¯ä¸­ï¼Œæœºå™¨äººå°†å…¶æ„å›¾ä¼ è¾¾ç»™è§‚å¯Ÿè€…ï¼ˆå¯ä»¥æ˜¯å…¶ä»–äººç±»æˆ–æœºå™¨äººï¼‰éå¸¸é‡è¦ã€‚æ¸…æ™°çš„è½¨è¿¹ä½¿è§‚å¯Ÿè€…èƒ½å¤Ÿå¿«é€Ÿå‡†ç¡®åœ°é¢„æµ‹ç‰¹å·¥çš„æ„å›¾ã€‚åœ¨æ•Œå¯¹ç¯å¢ƒä¸­ï¼Œä¾‹å¦‚åœ¨å†›äº‹è¡ŒåŠ¨æˆ–æ¸¸æˆä¸­ï¼Œæœºå™¨äººä¸è¦å‘è§‚å¯Ÿè€…ä¼ è¾¾å…¶æ„å›¾ï¼Œè¿™ä¸€ç‚¹å¾ˆé‡è¦ã€‚éš¾ä»¥è¾¨è®¤çš„è½¨è¿¹ä¼šå¯¼è‡´è§‚å¯Ÿè€…é”™è¯¯åœ°é¢„æµ‹æ™ºèƒ½ä½“çš„æ„å›¾ï¼Œæˆ–è€…åœ¨è§‚å¯Ÿè€…èƒ½å¤Ÿå¯¹æ™ºèƒ½ä½“çš„æ„å›¾åšå‡ºæ­£ç¡®é¢„æµ‹æ—¶å‡ºç°å»¶è¿Ÿã€‚ç„¶è€Œï¼Œåœ¨æŸäº›ç¯å¢ƒä¸­å­˜åœ¨å¤šä¸ªè§‚å¯Ÿè€…ï¼Œæ¯ä¸ªè§‚å¯Ÿè€…å¯èƒ½åªèƒ½çœ‹åˆ°ç¯å¢ƒçš„ä¸€éƒ¨åˆ†ï¼Œå¹¶ä¸”æ¯ä¸ªè§‚å¯Ÿè€…å¯èƒ½æœ‰ä¸åŒçš„åŠ¨æœºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ··åˆåŠ¨æœºæœ‰é™å¯è§‚å¯Ÿæ€§æ¸…æ™°è¿åŠ¨è§„åˆ’ï¼ˆMMLO-LMPï¼‰é—®é¢˜ï¼Œè¯¥é—®é¢˜è¦æ±‚è¿åŠ¨è§„åˆ’å™¨ç”Ÿæˆä¸€æ¡è½¨è¿¹ï¼Œè¯¥è½¨è¿¹å¯¹äºå…·æœ‰ç§¯æåŠ¨æœºçš„è§‚å¯Ÿè€…æ¥è¯´æ˜¯æ¸…æ™°çš„ï¼Œå¯¹äºå…·æœ‰æ¶ˆæåŠ¨æœºçš„è§‚å¯Ÿè€…æ¥è¯´æ˜¯éš¾ä»¥è¾¨è®¤çš„ï¼ŒåŒæ—¶è¿˜è€ƒè™‘åˆ°æ¯ä¸ªè§‚å¯Ÿè€…çš„å¯è§æ€§é™åˆ¶ã€‚æˆ‘ä»¬å¼ºè°ƒäº†æ™ºèƒ½ä½“åœ¨å®ç°é—®é¢˜ç›®æ ‡çš„åŒæ—¶å¯ä»¥é‡‡å–çš„å¤šç§ç­–ç•¥ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº† DUBIOUSï¼Œä¸€ç§æ±‚è§£ MMLO-LMP çš„è½¨è¿¹ä¼˜åŒ–å™¨ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒDUBIOUS å¯ä»¥ç”Ÿæˆå¹³è¡¡æ˜“è¯»æ€§ä¸è§‚å¯Ÿè€…çš„åŠ¨æœºå’Œæœ‰é™å¯è§åŒºåŸŸçš„è½¨è¿¹ã€‚æœªæ¥çš„å·¥ä½œåŒ…æ‹¬ MMLO-LMP çš„è®¸å¤šå˜ä½“ï¼ŒåŒ…æ‹¬ç§»åŠ¨è§‚å¯Ÿè€…å’Œè§‚å¯Ÿè€…åˆ†ç»„ã€‚

</details>

---

## 258. Risk-Aware Obstacle Avoidance Algorithm for Real-Time Applications

**ä¸­æ–‡æ ‡é¢˜**: å®æ—¶åº”ç”¨çš„é£é™©æ„ŸçŸ¥é¿éšœç®—æ³•

**Date**: 2026-02-09 | **arXiv**: [2602.09204v1](http://arxiv.org/abs/2602.09204v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09204v1)

<details><summary><b>Abstract</b></summary>

Robust navigation in changing marine environments requires autonomous systems capable of perceiving, reasoning, and acting under uncertainty. This study introduces a hybrid risk-aware navigation architecture that integrates probabilistic modeling of obstacles along the vehicle path with smooth trajectory optimization for autonomous surface vessels. The system constructs probabilistic risk maps that capture both obstacle proximity and the behavior of dynamic objects. A risk-biased Rapidly Exploring Random Tree (RRT) planner leverages these maps to generate collision-free paths, which are subsequently refined using B-spline algorithms to ensure trajectory continuity. Three distinct RRT* rewiring modes are implemented based on the cost function: minimizing the path length, minimizing risk, and optimizing a combination of the path length and total risk. The framework is evaluated in experimental scenarios containing both static and dynamic obstacles. The results demonstrate the system's ability to navigate safely, maintain smooth trajectories, and dynamically adapt to changing environmental risks. Compared with conventional LIDAR or vision-only navigation approaches, the proposed method shows improvements in operational safety and autonomy, establishing it as a promising solution for risk-aware autonomous vehicle missions in uncertain and dynamic environments.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åœ¨ä¸æ–­å˜åŒ–çš„æµ·æ´‹ç¯å¢ƒä¸­å®ç°ç¨³å¥çš„å¯¼èˆªéœ€è¦èƒ½å¤Ÿåœ¨ä¸ç¡®å®šæ€§ä¸‹æ„ŸçŸ¥ã€æ¨ç†å’Œè¡ŒåŠ¨çš„è‡ªä¸»ç³»ç»Ÿã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ··åˆé£é™©æ„ŸçŸ¥å¯¼èˆªæ¶æ„ï¼Œè¯¥æ¶æ„å°†è½¦è¾†è·¯å¾„æ²¿çº¿éšœç¢ç‰©çš„æ¦‚ç‡å»ºæ¨¡ä¸è‡ªä¸»æ°´é¢èˆ¹èˆ¶çš„å¹³æ»‘è½¨è¿¹ä¼˜åŒ–ç›¸ç»“åˆã€‚è¯¥ç³»ç»Ÿæ„å»ºæ¦‚ç‡é£é™©å›¾ï¼Œæ•æ‰éšœç¢ç‰©æ¥è¿‘åº¦å’ŒåŠ¨æ€ç‰©ä½“çš„è¡Œä¸ºã€‚æœ‰é£é™©çš„å¿«é€Ÿæ¢ç´¢éšæœºæ ‘ (RRT) è§„åˆ’å™¨åˆ©ç”¨è¿™äº›åœ°å›¾ç”Ÿæˆæ— ç¢°æ’è·¯å¾„ï¼Œéšåä½¿ç”¨ B æ ·æ¡ç®—æ³•å¯¹å…¶è¿›è¡Œç»†åŒ–ï¼Œä»¥ç¡®ä¿è½¨è¿¹è¿ç»­æ€§ã€‚åŸºäºæˆæœ¬å‡½æ•°å®ç°äº†ä¸‰ç§ä¸åŒçš„ RRT* é‡å¸ƒçº¿æ¨¡å¼ï¼šæœ€å°åŒ–è·¯å¾„é•¿åº¦ã€æœ€å°åŒ–é£é™©ä»¥åŠä¼˜åŒ–è·¯å¾„é•¿åº¦å’Œæ€»é£é™©çš„ç»„åˆã€‚è¯¥æ¡†æ¶åœ¨åŒ…å«é™æ€å’ŒåŠ¨æ€éšœç¢ç‰©çš„å®éªŒåœºæ™¯ä¸­è¿›è¡Œè¯„ä¼°ã€‚ç»“æœè¯æ˜äº†è¯¥ç³»ç»Ÿèƒ½å¤Ÿå®‰å…¨å¯¼èˆªã€ä¿æŒå¹³ç¨³è½¨è¿¹å¹¶åŠ¨æ€é€‚åº”ä¸æ–­å˜åŒ–çš„ç¯å¢ƒé£é™©ã€‚ä¸ä¼ ç»Ÿçš„æ¿€å…‰é›·è¾¾æˆ–ä»…è§†è§‰å¯¼èˆªæ–¹æ³•ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•æ˜¾ç¤ºäº†æ“ä½œå®‰å…¨æ€§å’Œè‡ªä¸»æ€§æ–¹é¢çš„æ”¹è¿›ï¼Œä½¿å…¶æˆä¸ºä¸ç¡®å®šå’ŒåŠ¨æ€ç¯å¢ƒä¸­å…·æœ‰é£é™©æ„è¯†çš„è‡ªåŠ¨é©¾é©¶è½¦è¾†ä»»åŠ¡çš„æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚

</details>

---

## 259. Elements of Robot Morphology: Supporting Designers in Robot Form Exploration

**ä¸­æ–‡æ ‡é¢˜**: æœºå™¨äººå½¢æ€å­¦çš„è¦ç´ ï¼šæ”¯æŒè®¾è®¡å¸ˆæ¢ç´¢æœºå™¨äººå½¢æ€

**Date**: 2026-02-09 | **arXiv**: [2602.09203v1](http://arxiv.org/abs/2602.09203v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09203v1)

<details><summary><b>Abstract</b></summary>

Robot morphology, the form, shape, and structure of robots, is a key design space in human-robot interaction (HRI), shaping how robots function, express themselves, and interact with people. Yet, despite its importance, little is known about how design frameworks can guide systematic form exploration. To address this gap, we introduce Elements of Robot Morphology, a framework that identifies five fundamental elements: perception, articulation, end effectors, locomotion, and structure. Derived from an analysis of existing robots, the framework supports structured exploration of diverse robot forms. To operationalize the framework, we developed Morphology Exploration Blocks (MEB), a set of tangible blocks that enable hands-on, collaborative experimentation with robot morphologies. We evaluate the framework and toolkit through a case study and design workshops, showing how they support analysis, ideation, reflection, and collaborative robot design.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æœºå™¨äººå½¢æ€ï¼Œå³æœºå™¨äººçš„å½¢å¼ã€å½¢çŠ¶å’Œç»“æ„ï¼Œæ˜¯äººæœºäº¤äº’ï¼ˆHRIï¼‰ä¸­çš„å…³é”®è®¾è®¡ç©ºé—´ï¼Œå†³å®šç€æœºå™¨äººå¦‚ä½•å‘æŒ¥ä½œç”¨ã€è¡¨è¾¾è‡ªå·±ä»¥åŠä¸äººäº’åŠ¨ã€‚ç„¶è€Œï¼Œå°½ç®¡è®¾è®¡æ¡†æ¶å¾ˆé‡è¦ï¼Œä½†äººä»¬å¯¹å¦‚ä½•æŒ‡å¯¼ç³»ç»Ÿå½¢å¼æ¢ç´¢å´çŸ¥ä¹‹ç”šå°‘ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†æœºå™¨äººå½¢æ€å­¦çš„å…ƒç´ ï¼Œè¯¥æ¡†æ¶ç¡®å®šäº†äº”ä¸ªåŸºæœ¬å…ƒç´ ï¼šæ„ŸçŸ¥ã€å…³èŠ‚ã€æœ«ç«¯æ‰§è¡Œå™¨ã€è¿åŠ¨å’Œç»“æ„ã€‚è¯¥æ¡†æ¶æºè‡ªå¯¹ç°æœ‰æœºå™¨äººçš„åˆ†æï¼Œæ”¯æŒå¯¹ä¸åŒæœºå™¨äººå½¢å¼çš„ç»“æ„åŒ–æ¢ç´¢ã€‚ä¸ºäº†å®æ–½è¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬å¼€å‘äº†å½¢æ€æ¢ç´¢æ¨¡å—ï¼ˆMEBï¼‰ï¼Œè¿™æ˜¯ä¸€ç»„æœ‰å½¢çš„æ¨¡å—ï¼Œå¯ä»¥å¯¹æœºå™¨äººå½¢æ€è¿›è¡ŒåŠ¨æ‰‹ã€åä½œå®éªŒã€‚æˆ‘ä»¬é€šè¿‡æ¡ˆä¾‹ç ”ç©¶å’Œè®¾è®¡ç ”è®¨ä¼šè¯„ä¼°æ¡†æ¶å’Œå·¥å…·åŒ…ï¼Œå±•ç¤ºå®ƒä»¬å¦‚ä½•æ”¯æŒåˆ†æã€æ„æ€ã€åæ€å’Œåä½œæœºå™¨äººè®¾è®¡ã€‚

</details>

---

## 260. Agile asymmetric multi-legged locomotion: contact planning via geometric mechanics and spin model duality

**ä¸­æ–‡æ ‡é¢˜**: æ•æ·çš„ä¸å¯¹ç§°å¤šè¶³è¿åŠ¨ï¼šé€šè¿‡å‡ ä½•åŠ›å­¦å’Œæ—‹è½¬æ¨¡å‹å¯¹å¶æ€§è¿›è¡Œæ¥è§¦è§„åˆ’

**Date**: 2026-02-09 | **arXiv**: [2602.09123v1](http://arxiv.org/abs/2602.09123v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09123v1)

<details><summary><b>Abstract</b></summary>

Legged robot research is presently focused on bipedal or quadrupedal robots, despite capabilities to build robots with many more legs to potentially improve locomotion performance. This imbalance is not necessarily due to hardware limitations, but rather to the absence of principled control frameworks that explain when and how additional legs improve locomotion performance. In multi-legged systems, coordinating many simultaneous contacts introduces a severe curse of dimensionality that challenges existing modeling and control approaches. As an alternative, multi-legged robots are typically controlled using low-dimensional gaits originally developed for bipeds or quadrupeds. These strategies fail to exploit the new symmetries and control opportunities that emerge in higher-dimensional systems. In this work, we develop a principled framework for discovering new control structures in multi-legged locomotion. We use geometric mechanics to reduce contact-rich locomotion planning to a graph optimization problem, and propose a spin model duality framework from statistical mechanics to exploit symmetry breaking and guide optimal gait reorganization. Using this approach, we identify an asymmetric locomotion strategy for a hexapod robot that achieves a forward speed of 0.61 body lengths per cycle (a 50% improvement over conventional gaits). The resulting asymmetry appears at both the control and hardware levels. At the control level, the body orientation oscillates asymmetrically between fast clockwise and slow counterclockwise turning phases for forward locomotion. At the hardware level, two legs on the same side remain unactuated and can be replaced with rigid parts without degrading performance. Numerical simulations and robophysical experiments validate the framework and reveal novel locomotion behaviors that emerge from symmetry reforming in high-dimensional embodied systems.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è…¿å¼æœºå™¨äººç ”ç©¶ç›®å‰ä¸»è¦é›†ä¸­åœ¨åŒè¶³æˆ–å››è¶³æœºå™¨äººï¼Œå°½ç®¡æœ‰èƒ½åŠ›åˆ¶é€ å…·æœ‰æ›´å¤šè…¿çš„æœºå™¨äººä»¥æ½œåœ¨åœ°æé«˜è¿åŠ¨æ€§èƒ½ã€‚è¿™ç§ä¸å¹³è¡¡ä¸ä¸€å®šæ˜¯ç”±äºç¡¬ä»¶é™åˆ¶ï¼Œè€Œæ˜¯ç”±äºç¼ºä¹åŸåˆ™æ€§çš„æ§åˆ¶æ¡†æ¶æ¥è§£é‡Šé¢å¤–çš„è…¿ä½•æ—¶ä»¥åŠå¦‚ä½•æé«˜è¿åŠ¨æ€§èƒ½ã€‚åœ¨å¤šè¶³ç³»ç»Ÿä¸­ï¼Œåè°ƒè®¸å¤šåŒæ—¶æ¥è§¦ä¼šå¸¦æ¥ä¸¥é‡çš„ç»´æ•°ç¾éš¾ï¼Œè¿™å¯¹ç°æœ‰çš„å»ºæ¨¡å’Œæ§åˆ¶æ–¹æ³•æå‡ºäº†æŒ‘æˆ˜ã€‚ä½œä¸ºæ›¿ä»£æ–¹æ¡ˆï¼Œå¤šè¶³æœºå™¨äººé€šå¸¸ä½¿ç”¨æœ€åˆä¸ºä¸¤è¶³åŠ¨ç‰©æˆ–å››è¶³åŠ¨ç‰©å¼€å‘çš„ä½ç»´æ­¥æ€è¿›è¡Œæ§åˆ¶ã€‚è¿™äº›ç­–ç•¥æœªèƒ½åˆ©ç”¨é«˜ç»´ç³»ç»Ÿä¸­å‡ºç°çš„æ–°å¯¹ç§°æ€§å’Œæ§åˆ¶æœºä¼šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåŸåˆ™æ¡†æ¶ï¼Œç”¨äºå‘ç°å¤šè¶³è¿åŠ¨ä¸­çš„æ–°æ§åˆ¶ç»“æ„ã€‚æˆ‘ä»¬ä½¿ç”¨å‡ ä½•åŠ›å­¦å°†å¯Œå«æ¥è§¦çš„è¿åŠ¨è§„åˆ’ç®€åŒ–ä¸ºå›¾ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶æå‡ºäº†ç»Ÿè®¡åŠ›å­¦çš„è‡ªæ—‹æ¨¡å‹å¯¹å¶æ¡†æ¶ï¼Œä»¥åˆ©ç”¨å¯¹ç§°æ€§ç ´ç¼ºå¹¶æŒ‡å¯¼æœ€ä½³æ­¥æ€é‡ç»„ã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬ç¡®å®šäº†å…­è¶³æœºå™¨äººçš„ä¸å¯¹ç§°è¿åŠ¨ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å®ç°äº†æ¯ä¸ªå‘¨æœŸ 0.61 ä¸ªèº«ä½“é•¿åº¦çš„å‰è¿›é€Ÿåº¦ï¼ˆæ¯”ä¼ ç»Ÿæ­¥æ€æé«˜äº† 50%ï¼‰ã€‚ç”±æ­¤äº§ç”Ÿçš„ä¸å¯¹ç§°æ€§åŒæ—¶å‡ºç°åœ¨æ§åˆ¶å’Œç¡¬ä»¶çº§åˆ«ã€‚åœ¨æ§åˆ¶å±‚é¢ï¼Œèº«ä½“æ–¹å‘åœ¨å¿«é€Ÿé¡ºæ—¶é’ˆè½¬åŠ¨é˜¶æ®µå’Œç¼“æ…¢é€†æ—¶é’ˆè½¬åŠ¨é˜¶æ®µä¹‹é—´ä¸å¯¹ç§°æŒ¯è¡ï¼Œä»¥å®ç°å‘å‰è¿åŠ¨ã€‚åœ¨ç¡¬ä»¶å±‚é¢ï¼ŒåŒä¸€ä¾§çš„ä¸¤æ¡è…¿ä¿æŒæœªé©±åŠ¨çŠ¶æ€ï¼Œå¯ä»¥ç”¨åˆšæ€§éƒ¨ä»¶æ›¿æ¢ï¼Œè€Œä¸ä¼šé™ä½æ€§èƒ½ã€‚æ•°å€¼æ¨¡æ‹Ÿå’Œæœºå™¨äººç‰©ç†å®éªŒéªŒè¯äº†è¯¥æ¡†æ¶ï¼Œå¹¶æ­ç¤ºäº†é«˜ç»´ä½“ç°ç³»ç»Ÿä¸­å¯¹ç§°æ€§é‡ç»„æ‰€äº§ç”Ÿçš„æ–°é¢–è¿åŠ¨è¡Œä¸ºã€‚

</details>

---

## 261. TwinRL-VLA: Digital Twin-Driven Reinforcement Learning for Real-World Robotic Manipulation

**ä¸­æ–‡æ ‡é¢˜**: TwinRL-VLAï¼šç”¨äºç°å®ä¸–ç•Œæœºå™¨äººæ“ä½œçš„æ•°å­—å­ªç”Ÿé©±åŠ¨å¼ºåŒ–å­¦ä¹ 

**Date**: 2026-02-09 | **arXiv**: [2602.09023v1](http://arxiv.org/abs/2602.09023v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09023v1)

<details><summary><b>Abstract</b></summary>

Despite strong generalization capabilities, Vision-Language-Action (VLA) models remain constrained by the high cost of expert demonstrations and insufficient real-world interaction. While online reinforcement learning (RL) has shown promise in improving general foundation models, applying RL to VLA manipulation in real-world settings is still hindered by low exploration efficiency and a restricted exploration space. Through systematic real-world experiments, we observe that the effective exploration space of online RL is closely tied to the data distribution of supervised fine-tuning (SFT). Motivated by this observation, we propose TwinRL, a digital twin-real-world collaborative RL framework designed to scale and guide exploration for VLA models. First, a high-fidelity digital twin is efficiently reconstructed from smartphone-captured scenes, enabling realistic bidirectional transfer between real and simulated environments. During the SFT warm-up stage, we introduce an exploration space expansion strategy using digital twins to broaden the support of the data trajectory distribution. Building on this enhanced initialization, we propose a sim-to-real guided exploration strategy to further accelerate online RL. Specifically, TwinRL performs efficient and parallel online RL in the digital twin prior to deployment, effectively bridging the gap between offline and online training stages. Subsequently, we exploit efficient digital twin sampling to identify failure-prone yet informative configurations, which are used to guide targeted human-in-the-loop rollouts on the real robot. In our experiments, TwinRL approaches 100% success in both in-distribution regions covered by real-world demonstrations and out-of-distribution regions, delivering at least a 30% speedup over prior real-world RL methods and requiring only about 20 minutes on average across four tasks.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å°½ç®¡æ³›åŒ–èƒ½åŠ›å¾ˆå¼ºï¼Œä½†è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ä»ç„¶å—åˆ°ä¸“å®¶æ¼”ç¤ºæˆæœ¬é«˜æ˜‚å’Œç°å®ä¸–ç•Œäº¤äº’ä¸è¶³çš„é™åˆ¶ã€‚è™½ç„¶åœ¨çº¿å¼ºåŒ–å­¦ä¹  (RL) åœ¨æ”¹è¿›é€šç”¨åŸºç¡€æ¨¡å‹æ–¹é¢è¡¨ç°å‡ºäº†è‰¯å¥½çš„å‰æ™¯ï¼Œä½†åœ¨ç°å®ç¯å¢ƒä¸­å°† RL åº”ç”¨äº VLA æ“ä½œä»ç„¶å—åˆ°æ¢ç´¢æ•ˆç‡ä½å’Œæ¢ç´¢ç©ºé—´æœ‰é™çš„é˜»ç¢ã€‚é€šè¿‡ç³»ç»Ÿçš„ç°å®ä¸–ç•Œå®éªŒï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°åœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„æœ‰æ•ˆæ¢ç´¢ç©ºé—´ä¸ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„æ•°æ®åˆ†å¸ƒå¯†åˆ‡ç›¸å…³ã€‚å—è¿™ä¸€è§‚å¯Ÿçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº† TwinRLï¼Œè¿™æ˜¯ä¸€ç§æ•°å­—å­ªç”Ÿç°å®ä¸–ç•Œåä½œ RL æ¡†æ¶ï¼Œæ—¨åœ¨æ‰©å±•å’ŒæŒ‡å¯¼ VLA æ¨¡å‹çš„æ¢ç´¢ã€‚é¦–å…ˆï¼Œä»æ™ºèƒ½æ‰‹æœºæ•è·çš„åœºæ™¯ä¸­æœ‰æ•ˆåœ°é‡å»ºé«˜ä¿çœŸæ•°å­—å­ªç”Ÿï¼Œä»è€Œå®ç°çœŸå®ç¯å¢ƒå’Œæ¨¡æ‹Ÿç¯å¢ƒä¹‹é—´çš„çœŸå®åŒå‘ä¼ è¾“ã€‚åœ¨SFTé¢„çƒ­é˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä½¿ç”¨æ•°å­—å­ªç”Ÿçš„æ¢ç´¢ç©ºé—´æ‰©å±•ç­–ç•¥ï¼Œä»¥æ‹“å®½æ•°æ®è½¨è¿¹åˆ†å¸ƒçš„æ”¯æŒã€‚åŸºäºè¿™ç§å¢å¼ºçš„åˆå§‹åŒ–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¨¡æ‹Ÿåˆ°çœŸå®çš„å¼•å¯¼æ¢ç´¢ç­–ç•¥ï¼Œä»¥è¿›ä¸€æ­¥åŠ é€Ÿåœ¨çº¿å¼ºåŒ–å­¦ä¹ ã€‚å…·ä½“æ¥è¯´ï¼ŒTwinRL åœ¨éƒ¨ç½²ä¹‹å‰åœ¨æ•°å­—å­ªç”Ÿä¸­æ‰§è¡Œé«˜æ•ˆä¸”å¹¶è¡Œçš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œæœ‰æ•ˆåœ°å¼¥åˆäº†ç¦»çº¿å’Œåœ¨çº¿è®­ç»ƒé˜¶æ®µä¹‹é—´çš„å·®è·ã€‚éšåï¼Œæˆ‘ä»¬åˆ©ç”¨é«˜æ•ˆçš„æ•°å­—å­ªç”Ÿé‡‡æ ·æ¥è¯†åˆ«å®¹æ˜“å‘ç”Ÿæ•…éšœä½†ä¿¡æ¯ä¸°å¯Œçš„é…ç½®ï¼Œè¿™äº›é…ç½®ç”¨äºæŒ‡å¯¼åœ¨çœŸå®æœºå™¨äººä¸Šè¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„äººæœºäº¤äº’ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼ŒTwinRL åœ¨ç°å®ä¸–ç•Œæ¼”ç¤ºè¦†ç›–çš„åˆ†å¸ƒå†…åŒºåŸŸå’Œåˆ†å¸ƒå¤–åŒºåŸŸå‡å–å¾—äº† 100% çš„æˆåŠŸï¼Œæ¯”ä¹‹å‰çš„ç°å®ä¸–ç•Œ RL æ–¹æ³•è‡³å°‘æé«˜äº† 30% çš„é€Ÿåº¦ï¼Œå¹¶ä¸”å››é¡¹ä»»åŠ¡å¹³å‡åªéœ€è¦å¤§çº¦ 20 åˆ†é’Ÿã€‚

</details>

---

## 262. From Obstacles to Etiquette: Robot Social Navigation with VLM-Informed Path Selection

**ä¸­æ–‡æ ‡é¢˜**: ä»éšœç¢åˆ°ç¤¼èŠ‚ï¼šå…·æœ‰ VLM é€šçŸ¥è·¯å¾„é€‰æ‹©çš„æœºå™¨äººç¤¾äº¤å¯¼èˆª

**Date**: 2026-02-09 | **arXiv**: [2602.09002v1](http://arxiv.org/abs/2602.09002v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09002v1)

<details><summary><b>Abstract</b></summary>

Navigating socially in human environments requires more than satisfying geometric constraints, as collision-free paths may still interfere with ongoing activities or conflict with social norms. Addressing this challenge calls for analyzing interactions between agents and incorporating common-sense reasoning into planning. This paper presents a social robot navigation framework that integrates geometric planning with contextual social reasoning. The system first extracts obstacles and human dynamics to generate geometrically feasible candidate paths, then leverages a fine-tuned vision-language model (VLM) to evaluate these paths, informed by contextually grounded social expectations, selecting a socially optimized path for the controller. This task-specific VLM distills social reasoning from large foundation models into a smaller and efficient model, allowing the framework to perform real-time adaptation in diverse human-robot interaction contexts. Experiments in four social navigation contexts demonstrate that our method achieves the best overall performance with the lowest personal space violation duration, the minimal pedestrian-facing time, and no social zone intrusions. Project page: https://path-etiquette.github.io

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åœ¨äººç±»ç¯å¢ƒä¸­è¿›è¡Œç¤¾äº¤å¯¼èˆªéœ€è¦çš„ä¸ä»…ä»…æ˜¯æ»¡è¶³å‡ ä½•çº¦æŸï¼Œå› ä¸ºæ— ç¢°æ’è·¯å¾„ä»ç„¶å¯èƒ½å¹²æ‰°æ­£åœ¨è¿›è¡Œçš„æ´»åŠ¨æˆ–ä¸ç¤¾ä¼šè§„èŒƒå‘ç”Ÿå†²çªã€‚åº”å¯¹è¿™ä¸€æŒ‘æˆ˜éœ€è¦åˆ†æä»£ç†ä¹‹é—´çš„äº¤äº’å¹¶å°†å¸¸è¯†æ¨ç†çº³å…¥è§„åˆ’ä¸­ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†å‡ ä½•è§„åˆ’ä¸æƒ…å¢ƒç¤¾äº¤æ¨ç†ç›¸ç»“åˆçš„ç¤¾äº¤æœºå™¨äººå¯¼èˆªæ¡†æ¶ã€‚è¯¥ç³»ç»Ÿé¦–å…ˆæå–éšœç¢ç‰©å’Œäººä½“åŠ¨æ€ï¼Œä»¥ç”Ÿæˆå‡ ä½•ä¸Šå¯è¡Œçš„å€™é€‰è·¯å¾„ï¼Œç„¶ååˆ©ç”¨å¾®è°ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥è¯„ä¼°è¿™äº›è·¯å¾„ï¼Œå¹¶æ ¹æ®åŸºäºä¸Šä¸‹æ–‡çš„ç¤¾ä¼šæœŸæœ›æ¥è¯„ä¼°è¿™äº›è·¯å¾„ï¼Œä¸ºæ§åˆ¶å™¨é€‰æ‹©ä¸€æ¡ç¤¾ä¼šä¼˜åŒ–è·¯å¾„ã€‚è¿™ç§ç‰¹å®šäºä»»åŠ¡çš„ VLM å°†å¤§å‹åŸºç¡€æ¨¡å‹ä¸­çš„ç¤¾ä¼šæ¨ç†æç‚¼ä¸ºæ›´å°ä¸”é«˜æ•ˆçš„æ¨¡å‹ï¼Œä½¿æ¡†æ¶èƒ½å¤Ÿåœ¨ä¸åŒçš„äººæœºäº¤äº’ç¯å¢ƒä¸­æ‰§è¡Œå®æ—¶é€‚åº”ã€‚åœ¨å››ç§ç¤¾äº¤å¯¼èˆªç¯å¢ƒä¸­çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æœ€ä½³çš„æ•´ä½“æ€§èƒ½ï¼Œå…·æœ‰æœ€ä½çš„ä¸ªäººç©ºé—´ä¾µçŠ¯æŒç»­æ—¶é—´ã€æœ€çŸ­çš„è¡Œäººé¢å¯¹æ—¶é—´å¹¶ä¸”æ²¡æœ‰ç¤¾äº¤åŒºåŸŸå…¥ä¾µã€‚é¡¹ç›®é¡µé¢ï¼šhttps://path-etiquette.github.io

</details>

---

## 263. CLUE: Crossmodal disambiguation via Language-vision Understanding with attEntion

**ä¸­æ–‡æ ‡é¢˜**: çº¿ç´¢ï¼šé€šè¿‡è¯­è¨€è§†è§‰ç†è§£å’Œæ³¨æ„è¿›è¡Œè·¨æ¨¡æ€æ¶ˆæ­§

**Date**: 2026-02-09 | **arXiv**: [2602.08999v1](http://arxiv.org/abs/2602.08999v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08999v1)

<details><summary><b>Abstract</b></summary>

With the increasing integration of robots into daily life, human-robot interaction has become more complex and multifaceted. A critical component of this interaction is Interactive Visual Grounding (IVG), through which robots must interpret human intentions and resolve ambiguity. Existing IVG models generally lack a mechanism to determine when to ask clarification questions, as they implicitly rely on their learned representations. CLUE addresses this gap by converting the VLM's cross-modal attention into an explicit, spatially grounded signal for deciding when to ask. We extract text to image attention maps and pass them to a lightweight CNN to detect referential ambiguity, while a LoRA fine-tuned decoder conducts the dialog and emits grounding location tokens. We train on a real-world interactive dataset for IVG, and a mixed ambiguity set for the detector. With InViG-only supervision, our model surpasses a state-of-the-art method while using parameter-efficient fine-tuning. Similarly, the ambiguity detector outperforms prior baselines. Overall, CLUE turns the internal cross-modal attention of a VLM into an explicit, spatially grounded signal for deciding when to ask. The data and code are publicly available at: mouadabrini.github.io/clue

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

éšç€æœºå™¨äººè¶Šæ¥è¶Šèå…¥æ—¥å¸¸ç”Ÿæ´»ï¼Œäººæœºäº¤äº’å˜å¾—æ›´åŠ å¤æ‚å’Œå¤šå±‚é¢ã€‚è¿™ç§äº¤äº’çš„ä¸€ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†æ˜¯äº¤äº’å¼è§†è§‰åŸºç¡€ï¼ˆIVGï¼‰ï¼Œæœºå™¨äººå¿…é¡»é€šè¿‡å®ƒè§£é‡Šäººç±»çš„æ„å›¾å¹¶è§£å†³æ­§ä¹‰ã€‚ç°æœ‰çš„ IVG æ¨¡å‹é€šå¸¸ç¼ºä¹ç¡®å®šä½•æ—¶æå‡ºæ¾„æ¸…é—®é¢˜çš„æœºåˆ¶ï¼Œå› ä¸ºå®ƒä»¬éšå«åœ°ä¾èµ–äºå…¶å­¦ä¹ åˆ°çš„è¡¨ç¤ºã€‚ CLUE é€šè¿‡å°† VLM çš„è·¨æ¨¡å¼æ³¨æ„åŠ›è½¬æ¢ä¸ºæ˜ç¡®çš„ã€åŸºäºç©ºé—´çš„ä¿¡å·æ¥å†³å®šä½•æ—¶è¯¢é—®ï¼Œä»è€Œè§£å†³äº†è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬å°†æ–‡æœ¬æå–åˆ°å›¾åƒæ³¨æ„åŠ›å›¾ï¼Œå¹¶å°†å®ƒä»¬ä¼ é€’ç»™è½»é‡çº§ CNN æ¥æ£€æµ‹å¼•ç”¨æ­§ä¹‰ï¼Œè€Œ LoRA å¾®è°ƒè§£ç å™¨åˆ™è¿›è¡Œå¯¹è¯å¹¶å‘å‡ºæ¥åœ°ä½ç½®æ ‡è®°ã€‚æˆ‘ä»¬åœ¨ IVG çš„çœŸå®äº¤äº’æ•°æ®é›†å’Œæ£€æµ‹å™¨çš„æ··åˆæ¨¡ç³Šåº¦é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚é€šè¿‡ä»… InViG çš„ç›‘ç£ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒæ—¶ä½¿ç”¨äº†å‚æ•°é«˜æ•ˆçš„å¾®è°ƒã€‚åŒæ ·ï¼Œæ­§ä¹‰æ£€æµ‹å™¨çš„æ€§èƒ½ä¼˜äºå…ˆå‰çš„åŸºçº¿ã€‚æ€»ä½“è€Œè¨€ï¼ŒCLUE å°† VLM çš„å†…éƒ¨è·¨æ¨¡æ€æ³¨æ„åŠ›è½¬åŒ–ä¸ºæ˜ç¡®çš„ã€åŸºäºç©ºé—´çš„ä¿¡å·ï¼Œç”¨äºå†³å®šä½•æ—¶æé—®ã€‚æ•°æ®å’Œä»£ç å¯å…¬å¼€è·å–ï¼šmouadabrini.github.io/clue

</details>

---

## 264. Legs Over Arms: On the Predictive Value of Lower-Body Pose for Human Trajectory Prediction from Egocentric Robot Perception

**ä¸­æ–‡æ ‡é¢˜**: åŒè…¿æ”¾åœ¨æ‰‹è‡‚ä¸Šï¼šè®ºä¸‹åŠèº«å§¿åŠ¿å¯¹åŸºäºè‡ªæˆ‘ä¸­å¿ƒæœºå™¨äººæ„ŸçŸ¥çš„äººä½“è½¨è¿¹é¢„æµ‹çš„é¢„æµ‹ä»·å€¼

**Date**: 2026-02-09 | **arXiv**: [2602.09076v1](http://arxiv.org/abs/2602.09076v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09076v1)

<details><summary><b>Abstract</b></summary>

Predicting human trajectory is crucial for social robot navigation in crowded environments. While most existing approaches treat human as point mass, we present a study on multi-agent trajectory prediction that leverages different human skeletal features for improved forecast accuracy. In particular, we systematically evaluate the predictive utility of 2D and 3D skeletal keypoints and derived biomechanical cues as additional inputs. Through a comprehensive study on the JRDB dataset and another new dataset for social navigation with 360-degree panoramic videos, we find that focusing on lower-body 3D keypoints yields a 13% reduction in Average Displacement Error and augmenting 3D keypoint inputs with corresponding biomechanical cues provides a further 1-4% improvement. Notably, the performance gain persists when using 2D keypoint inputs extracted from equirectangular panoramic images, indicating that monocular surround vision can capture informative cues for motion forecasting. Our finding that robots can forecast human movement efficiently by watching their legs provides actionable insights for designing sensing capabilities for social robot navigation.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

é¢„æµ‹äººç±»è½¨è¿¹å¯¹äºç¤¾äº¤æœºå™¨äººåœ¨æ‹¥æŒ¤ç¯å¢ƒä¸­çš„å¯¼èˆªè‡³å…³é‡è¦ã€‚è™½ç„¶å¤§å¤šæ•°ç°æœ‰æ–¹æ³•å°†äººç±»è§†ä¸ºè´¨ç‚¹ï¼Œä½†æˆ‘ä»¬æå‡ºäº†ä¸€é¡¹å…³äºå¤šæ™ºèƒ½ä½“è½¨è¿¹é¢„æµ‹çš„ç ”ç©¶ï¼Œè¯¥ç ”ç©¶åˆ©ç”¨ä¸åŒçš„äººç±»éª¨éª¼ç‰¹å¾æ¥æé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼° 2D å’Œ 3D éª¨éª¼å…³é”®ç‚¹çš„é¢„æµ‹æ•ˆç”¨ä»¥åŠæ´¾ç”Ÿçš„ç”Ÿç‰©åŠ›å­¦çº¿ç´¢ä½œä¸ºé¢å¤–è¾“å…¥ã€‚é€šè¿‡å¯¹ JRDB æ•°æ®é›†å’Œå¦ä¸€ä¸ªç”¨äº 360 åº¦å…¨æ™¯è§†é¢‘ç¤¾äº¤å¯¼èˆªçš„æ–°æ•°æ®é›†çš„ç»¼åˆç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°ï¼Œå…³æ³¨ä¸‹åŠèº« 3D å…³é”®ç‚¹å¯ä½¿å¹³å‡ä½ç§»è¯¯å·®å‡å°‘ 13%ï¼Œè€Œé€šè¿‡ç›¸åº”çš„ç”Ÿç‰©åŠ›å­¦çº¿ç´¢å¢å¼º 3D å…³é”®ç‚¹è¾“å…¥å¯è¿›ä¸€æ­¥æé«˜ 1-4%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå½“ä½¿ç”¨ä»ç­‰è·æŸ±çŠ¶å…¨æ™¯å›¾åƒæå–çš„ 2D å…³é”®ç‚¹è¾“å…¥æ—¶ï¼Œæ€§èƒ½å¢ç›Šä»ç„¶å­˜åœ¨ï¼Œè¿™è¡¨æ˜å•ç›®ç¯ç»•è§†è§‰å¯ä»¥æ•è·ç”¨äºè¿åŠ¨é¢„æµ‹çš„ä¿¡æ¯çº¿ç´¢ã€‚æˆ‘ä»¬å‘ç°æœºå™¨äººå¯ä»¥é€šè¿‡è§‚å¯Ÿäººç±»çš„è…¿éƒ¨æ¥æœ‰æ•ˆé¢„æµ‹äººç±»çš„è¿åŠ¨ï¼Œè¿™ä¸ºè®¾è®¡ç¤¾äº¤æœºå™¨äººå¯¼èˆªçš„ä¼ æ„ŸåŠŸèƒ½æä¾›äº†å¯è¡Œçš„è§è§£ã€‚

</details>

---

## 265. Multi-Staged Framework for Safety Analysis of Offloaded Services in Distributed Intelligent Transportation Systems

**ä¸­æ–‡æ ‡é¢˜**: åˆ†å¸ƒå¼æ™ºèƒ½äº¤é€šç³»ç»Ÿå¸è½½æœåŠ¡å®‰å…¨åˆ†æçš„å¤šé˜¶æ®µæ¡†æ¶

**Date**: 2026-02-09 | **arXiv**: [2602.08821v1](http://arxiv.org/abs/2602.08821v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08821v1)

<details><summary><b>Abstract</b></summary>

The integration of service-oriented architectures (SOA) with function offloading for distributed, intelligent transportation systems (ITS) offers the opportunity for connected autonomous vehicles (CAVs) to extend their locally available services. One major goal of offloading a subset of functions in the processing chain of a CAV to remote devices is to reduce the overall computational complexity on the CAV. The extension of using remote services, however, requires careful safety analysis, since the remotely created data are corrupted more easily, e.g., through an attacker on the remote device or by intercepting the wireless transmission. To tackle this problem, we first analyze the concept of SOA for distributed environments. From this, we derive a safety framework that validates the reliability of remote services and the data received locally. Since it is possible for the autonomous driving task to offload multiple different services, we propose a specific multi-staged framework for safety analysis dependent on the service composition of local and remote services. For efficiency reasons, we directly include the multi-staged framework for safety analysis in our service-oriented function offloading framework (SOFOF) that we have proposed in earlier work. The evaluation compares the performance of the extended framework considering computational complexity, with energy savings being a major motivation for function offloading, and its capability to detect data from corrupted remote services.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

é¢å‘æœåŠ¡çš„æ¶æ„ (SOA) ä¸åˆ†å¸ƒå¼æ™ºèƒ½äº¤é€šç³»ç»Ÿ (ITS) åŠŸèƒ½å¸è½½çš„é›†æˆä¸ºè”ç½‘è‡ªåŠ¨é©¾é©¶è½¦è¾† (CAV) æä¾›äº†æ‰©å±•å…¶æœ¬åœ°å¯ç”¨æœåŠ¡çš„æœºä¼šã€‚å°† CAV å¤„ç†é“¾ä¸­çš„åŠŸèƒ½å­é›†å¸è½½åˆ°è¿œç¨‹è®¾å¤‡çš„ä¸€ä¸ªä¸»è¦ç›®æ ‡æ˜¯é™ä½ CAV çš„æ•´ä½“è®¡ç®—å¤æ‚æ€§ã€‚ç„¶è€Œï¼Œä½¿ç”¨è¿œç¨‹æœåŠ¡çš„æ‰©å±•éœ€è¦ä»”ç»†çš„å®‰å…¨åˆ†æï¼Œå› ä¸ºè¿œç¨‹åˆ›å»ºçš„æ•°æ®æ›´å®¹æ˜“è¢«ç ´åï¼Œä¾‹å¦‚ï¼Œé€šè¿‡è¿œç¨‹è®¾å¤‡ä¸Šçš„æ”»å‡»è€…æˆ–é€šè¿‡æ‹¦æˆªæ— çº¿ä¼ è¾“ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ†æåˆ†å¸ƒå¼ç¯å¢ƒçš„ SOA æ¦‚å¿µã€‚ç”±æ­¤ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºä¸€ä¸ªå®‰å…¨æ¡†æ¶ï¼Œç”¨äºéªŒè¯è¿œç¨‹æœåŠ¡å’Œæœ¬åœ°æ¥æ”¶çš„æ•°æ®çš„å¯é æ€§ã€‚ç”±äºè‡ªåŠ¨é©¾é©¶ä»»åŠ¡å¯ä»¥å¸è½½å¤šä¸ªä¸åŒçš„æœåŠ¡ï¼Œå› æ­¤æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…·ä½“çš„å¤šé˜¶æ®µæ¡†æ¶ï¼Œç”¨äºä¾èµ–äºæœ¬åœ°å’Œè¿œç¨‹æœåŠ¡çš„æœåŠ¡ç»„åˆè¿›è¡Œå®‰å…¨åˆ†æã€‚å‡ºäºæ•ˆç‡åŸå› ï¼Œæˆ‘ä»¬ç›´æ¥å°†å¤šé˜¶æ®µå®‰å…¨åˆ†ææ¡†æ¶åŒ…å«åœ¨æˆ‘ä»¬åœ¨æ—©æœŸå·¥ä½œä¸­æå‡ºçš„é¢å‘æœåŠ¡çš„åŠŸèƒ½å¸è½½æ¡†æ¶ï¼ˆSOFOFï¼‰ä¸­ã€‚è¯¥è¯„ä¼°æ¯”è¾ƒäº†æ‰©å±•æ¡†æ¶çš„æ€§èƒ½ï¼Œè€ƒè™‘åˆ°è®¡ç®—å¤æ‚æ€§ï¼ŒèŠ‚èƒ½æ˜¯åŠŸèƒ½å¸è½½çš„ä¸»è¦åŠ¨æœºï¼Œä»¥åŠä»æŸåçš„è¿œç¨‹æœåŠ¡ä¸­æ£€æµ‹æ•°æ®çš„èƒ½åŠ›ã€‚

</details>

---

## 266. A Generic Service-Oriented Function Offloading Framework for Connected Automated Vehicles

**ä¸­æ–‡æ ‡é¢˜**: é€‚ç”¨äºäº’è”è‡ªåŠ¨é©¾é©¶æ±½è½¦çš„é€šç”¨é¢å‘æœåŠ¡çš„åŠŸèƒ½å¸è½½æ¡†æ¶

**Date**: 2026-02-09 | **arXiv**: [2602.08799v1](http://arxiv.org/abs/2602.08799v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08799v1)

<details><summary><b>Abstract</b></summary>

Function offloading is a promising solution to address limitations concerning computational capacity and available energy of Connected Automated Vehicles~(CAVs) or other autonomous robots by distributing computational tasks between local and remote computing devices in form of distributed services. This paper presents a generic function offloading framework that can be used to offload an arbitrary set of computational tasks with a focus on autonomous driving. To provide flexibility, the function offloading framework is designed to incorporate different offloading decision making algorithms and quality of service~(QoS) requirements that can be adjusted to different scenarios or the objectives of the CAVs. With a focus on the applicability, we propose an efficient location-based approach, where the decision whether tasks are processed locally or remotely depends on the location of the CAV. We apply the proposed framework on the use case of service-oriented trajectory planning, where we offload the trajectory planning task of CAVs to a Multi-Access Edge Computing~(MEC) server. The evaluation is conducted in both simulation and real-world application. It demonstrates the potential of the function offloading framework to guarantee the QoS for trajectory planning while improving the computational efficiency of the CAVs. Moreover, the simulation results also show the adaptability of the framework to diverse scenarios involving simultaneous offloading requests from multiple CAVs.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åŠŸèƒ½å¸è½½æ˜¯ä¸€ç§å¾ˆæœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥é€šè¿‡ä»¥åˆ†å¸ƒå¼æœåŠ¡çš„å½¢å¼åœ¨æœ¬åœ°å’Œè¿œç¨‹è®¡ç®—è®¾å¤‡ä¹‹é—´åˆ†é…è®¡ç®—ä»»åŠ¡æ¥è§£å†³è”ç½‘è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼ˆCAVï¼‰æˆ–å…¶ä»–è‡ªä¸»æœºå™¨äººçš„è®¡ç®—èƒ½åŠ›å’Œå¯ç”¨èƒ½é‡çš„é™åˆ¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨åŠŸèƒ½å¸è½½æ¡†æ¶ï¼Œå¯ç”¨äºå¸è½½ä»»æ„ä¸€ç»„è®¡ç®—ä»»åŠ¡ï¼Œé‡ç‚¹å…³æ³¨è‡ªåŠ¨é©¾é©¶ã€‚ä¸ºäº†æä¾›çµæ´»æ€§ï¼ŒåŠŸèƒ½å¸è½½æ¡†æ¶è¢«è®¾è®¡ä¸ºåˆå¹¶ä¸åŒçš„å¸è½½å†³ç­–ç®—æ³•å’ŒæœåŠ¡è´¨é‡ï¼ˆQoSï¼‰è¦æ±‚ï¼Œå¯ä»¥æ ¹æ®ä¸åŒçš„åœºæ™¯æˆ– CAV çš„ç›®æ ‡è¿›è¡Œè°ƒæ•´ã€‚ç€çœ¼äºé€‚ç”¨æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„åŸºäºä½ç½®çš„æ–¹æ³•ï¼Œå…¶ä¸­ä»»åŠ¡æ˜¯åœ¨æœ¬åœ°è¿˜æ˜¯è¿œç¨‹å¤„ç†çš„å†³å®šå–å†³äº CAV çš„ä½ç½®ã€‚æˆ‘ä»¬å°†æ‰€æå‡ºçš„æ¡†æ¶åº”ç”¨äºé¢å‘æœåŠ¡çš„è½¨è¿¹è§„åˆ’ç”¨ä¾‹ï¼Œå…¶ä¸­æˆ‘ä»¬å°† CAV çš„è½¨è¿¹è§„åˆ’ä»»åŠ¡å¸è½½åˆ°å¤šè®¿é—®è¾¹ç¼˜è®¡ç®—ï¼ˆMECï¼‰æœåŠ¡å™¨ã€‚è¯„ä¼°æ˜¯åœ¨æ¨¡æ‹Ÿå’Œå®é™…åº”ç”¨ä¸­è¿›è¡Œçš„ã€‚å®ƒå±•ç¤ºäº†åŠŸèƒ½å¸è½½æ¡†æ¶åœ¨ä¿è¯è½¨è¿¹è§„åˆ’çš„ QoS çš„åŒæ—¶æé«˜ CAV çš„è®¡ç®—æ•ˆç‡çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œæ¨¡æ‹Ÿç»“æœè¿˜æ˜¾ç¤ºäº†è¯¥æ¡†æ¶å¯¹æ¶‰åŠå¤šä¸ª CAV åŒæ—¶å¸è½½è¯·æ±‚çš„ä¸åŒåœºæ™¯çš„é€‚åº”æ€§ã€‚

</details>

---

## 267. GaussianCaR: Gaussian Splatting for Efficient Camera-Radar Fusion

**ä¸­æ–‡æ ‡é¢˜**: GaussianCaRï¼šç”¨äºé«˜æ•ˆç›¸æœºé›·è¾¾èåˆçš„é«˜æ–¯æ³¼æº…

**Date**: 2026-02-09 | **arXiv**: [2602.08784v1](http://arxiv.org/abs/2602.08784v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08784v1)

<details><summary><b>Abstract</b></summary>

Robust and accurate perception of dynamic objects and map elements is crucial for autonomous vehicles performing safe navigation in complex traffic scenarios. While vision-only methods have become the de facto standard due to their technical advances, they can benefit from effective and cost-efficient fusion with radar measurements. In this work, we advance fusion methods by repurposing Gaussian Splatting as an efficient universal view transformer that bridges the view disparity gap, mapping both image pixels and radar points into a common Bird's-Eye View (BEV) representation. Our main contribution is GaussianCaR, an end-to-end network for BEV segmentation that, unlike prior BEV fusion methods, leverages Gaussian Splatting to map raw sensor information into latent features for efficient camera-radar fusion. Our architecture combines multi-scale fusion with a transformer decoder to efficiently extract BEV features. Experimental results demonstrate that our approach achieves performance on par with, or even surpassing, the state of the art on BEV segmentation tasks (57.3%, 82.9%, and 50.1% IoU for vehicles, roads, and lane dividers) on the nuScenes dataset, while maintaining a 3.2x faster inference runtime. Code and project page are available online.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¯¹åŠ¨æ€ç‰©ä½“å’Œåœ°å›¾å…ƒç´ çš„é²æ£’è€Œå‡†ç¡®çš„æ„ŸçŸ¥å¯¹äºè‡ªåŠ¨é©¾é©¶æ±½è½¦åœ¨å¤æ‚äº¤é€šåœºæ™¯ä¸­æ‰§è¡Œå®‰å…¨å¯¼èˆªè‡³å…³é‡è¦ã€‚è™½ç„¶ä»…è§†è§‰æ–¹æ³•å› å…¶æŠ€æœ¯è¿›æ­¥è€Œå·²æˆä¸ºäº‹å®ä¸Šçš„æ ‡å‡†ï¼Œä½†å®ƒä»¬å¯ä»¥å—ç›Šäºä¸é›·è¾¾æµ‹é‡çš„æœ‰æ•ˆä¸”ç»æµé«˜æ•ˆçš„èåˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å°†é«˜æ–¯æ³¼æº…é‡æ–°åˆ©ç”¨ä¸ºä¸€ç§æœ‰æ•ˆçš„é€šç”¨è§†å›¾å˜æ¢å™¨æ¥æ”¹è¿›èåˆæ–¹æ³•ï¼Œè¯¥å˜æ¢å™¨å¯ä»¥å¼¥åˆè§†å›¾è§†å·®é—´éš™ï¼Œå°†å›¾åƒåƒç´ å’Œé›·è¾¾ç‚¹æ˜ å°„åˆ°é€šç”¨é¸Ÿç°å›¾ï¼ˆBEVï¼‰è¡¨ç¤ºä¸­ã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®æ˜¯ GaussianCaRï¼Œè¿™æ˜¯ä¸€ç§ç”¨äº BEV åˆ†å‰²çš„ç«¯åˆ°ç«¯ç½‘ç»œï¼Œä¸ä¹‹å‰çš„ BEV èåˆæ–¹æ³•ä¸åŒï¼Œå®ƒåˆ©ç”¨ Gaussian Splatting å°†åŸå§‹ä¼ æ„Ÿå™¨ä¿¡æ¯æ˜ å°„åˆ°æ½œåœ¨ç‰¹å¾ä¸­ï¼Œä»¥å®ç°é«˜æ•ˆçš„ç›¸æœºé›·è¾¾èåˆã€‚æˆ‘ä»¬çš„æ¶æ„å°†å¤šå°ºåº¦èåˆä¸å˜å‹å™¨è§£ç å™¨ç›¸ç»“åˆï¼Œä»¥æœ‰æ•ˆæå– BEV ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ nuScenes æ•°æ®é›†ä¸Šçš„ BEV åˆ†å‰²ä»»åŠ¡ï¼ˆè½¦è¾†ã€é“è·¯å’Œè½¦é“åˆ†éš”çº¿çš„ IoU åˆ†åˆ«ä¸º 57.3%ã€82.9% å’Œ 50.1%ï¼‰ä¸Šå®ç°äº†ä¸ç°æœ‰æŠ€æœ¯ç›¸å½“ç”šè‡³è¶…è¶Šçš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº† 3.2 å€æ›´å¿«çš„æ¨ç†è¿è¡Œæ—¶é—´ã€‚ä»£ç å’Œé¡¹ç›®é¡µé¢å¯åœ¨çº¿è·å–ã€‚

</details>

---

## 268. Mind the Gap: Learning Implicit Impedance in Visuomotor Policies via Intent-Execution Mismatch

**ä¸­æ–‡æ ‡é¢˜**: æ³¨æ„å·®è·ï¼šé€šè¿‡æ„å›¾æ‰§è¡Œä¸åŒ¹é…å­¦ä¹ è§†è§‰è¿åŠ¨ç­–ç•¥ä¸­çš„éšå¼é˜»æŠ—

**Date**: 2026-02-09 | **arXiv**: [2602.08776v1](http://arxiv.org/abs/2602.08776v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08776v1)

**Project**: https://xucj98.github.io/mind-the-gap-page/  <details><summary><b>Abstract</b></summary>

Teleoperation inherently relies on the human operator acting as a closed-loop controller to actively compensate for hardware imperfections, including latency, mechanical friction, and lack of explicit force feedback. Standard Behavior Cloning (BC), by mimicking the robot's executed trajectory, fundamentally ignores this compensatory mechanism. In this work, we propose a Dual-State Conditioning framework that shifts the learning objective to "Intent Cloning" (master command). We posit that the Intent-Execution Mismatch, the discrepancy between master command and slave response, is not noise, but a critical signal that physically encodes implicit interaction forces and algorithmically reveals the operator's strategy for overcoming system dynamics. By predicting the master intent, our policy learns to generate a "virtual equilibrium point", effectively realizing implicit impedance control. Furthermore, by explicitly conditioning on the history of this mismatch, the model performs implicit system identification, perceiving tracking errors as external forces to close the control loop. To bridge the temporal gap caused by inference latency, we further formulate the policy as a trajectory inpainter to ensure continuous control. We validate our approach on a sensorless, low-cost bi-manual setup. Empirical results across tasks requiring contact-rich manipulation and dynamic tracking reveal a decisive gap: while standard execution-cloning fails due to the inability to overcome contact stiffness and tracking lag, our mismatch-aware approach achieves robust success. This presents a minimalist behavior cloning framework for low-cost hardware, enabling force perception and dynamic compensation without relying on explicit force sensing. Videos are available on the \href{https://xucj98.github.io/mind-the-gap-page/}{project page}.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è¿œç¨‹æ“ä½œæœ¬è´¨ä¸Šä¾èµ–äºäººç±»æ“ä½œå‘˜å……å½“é—­ç¯æ§åˆ¶å™¨æ¥ä¸»åŠ¨è¡¥å¿ç¡¬ä»¶ç¼ºé™·ï¼ŒåŒ…æ‹¬å»¶è¿Ÿã€æœºæ¢°æ‘©æ“¦å’Œç¼ºä¹æ˜ç¡®çš„åŠ›åé¦ˆã€‚æ ‡å‡†è¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰é€šè¿‡æ¨¡ä»¿æœºå™¨äººçš„æ‰§è¡Œè½¨è¿¹ï¼Œä»æ ¹æœ¬ä¸Šå¿½ç•¥äº†è¿™ç§è¡¥å¿æœºåˆ¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŒçŠ¶æ€è°ƒèŠ‚æ¡†æ¶ï¼Œå°†å­¦ä¹ ç›®æ ‡è½¬å˜ä¸ºâ€œæ„å›¾å…‹éš†â€ï¼ˆä¸»å‘½ä»¤ï¼‰ã€‚æˆ‘ä»¬å‡è®¾æ„å›¾æ‰§è¡Œä¸åŒ¹é…ï¼ˆä¸»å‘½ä»¤å’Œä»å“åº”ä¹‹é—´çš„å·®å¼‚ï¼‰ä¸æ˜¯å™ªå£°ï¼Œè€Œæ˜¯ä¸€ä¸ªå…³é”®ä¿¡å·ï¼Œå®ƒå¯¹éšå¼äº¤äº’åŠ›è¿›è¡Œç‰©ç†ç¼–ç ï¼Œå¹¶åœ¨ç®—æ³•ä¸Šæ­ç¤ºæ“ä½œå‘˜å…‹æœç³»ç»ŸåŠ¨æ€çš„ç­–ç•¥ã€‚é€šè¿‡é¢„æµ‹ä¸»æ„å›¾ï¼Œæˆ‘ä»¬çš„ç­–ç•¥å­¦ä¼šç”Ÿæˆâ€œè™šæ‹Ÿå¹³è¡¡ç‚¹â€ï¼Œæœ‰æ•ˆåœ°å®ç°éšå¼é˜»æŠ—æ§åˆ¶ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ˜ç¡®åœ°è°ƒèŠ‚è¿™ç§ä¸åŒ¹é…çš„å†å²ï¼Œè¯¥æ¨¡å‹æ‰§è¡Œéšå¼ç³»ç»Ÿè¯†åˆ«ï¼Œå°†è·Ÿè¸ªè¯¯å·®è§†ä¸ºå…³é—­æ§åˆ¶ç¯è·¯çš„å¤–åŠ›ã€‚ä¸ºäº†å¼¥è¡¥æ¨ç†å»¶è¿Ÿé€ æˆçš„æ—¶é—´å·®è·ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å°†è¯¥ç­–ç•¥åˆ¶å®šä¸ºè½¨è¿¹ä¿®å¤å™¨ï¼Œä»¥ç¡®ä¿è¿ç»­æ§åˆ¶ã€‚æˆ‘ä»¬åœ¨æ— ä¼ æ„Ÿå™¨ã€ä½æˆæœ¬çš„åŒæ‰‹åŠ¨è®¾ç½®ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚éœ€è¦å¤§é‡æ¥è§¦æ“ä½œå’ŒåŠ¨æ€è·Ÿè¸ªçš„ä»»åŠ¡çš„ç»éªŒç»“æœæ­ç¤ºäº†å†³å®šæ€§çš„å·®è·ï¼šè™½ç„¶æ ‡å‡†æ‰§è¡Œå…‹éš†ç”±äºæ— æ³•å…‹æœæ¥è§¦åˆšåº¦å’Œè·Ÿè¸ªæ»åè€Œå¤±è´¥ï¼Œä½†æˆ‘ä»¬çš„å¤±é…æ„ŸçŸ¥æ–¹æ³•å–å¾—äº†å·¨å¤§çš„æˆåŠŸã€‚è¿™ä¸ºä½æˆæœ¬ç¡¬ä»¶æä¾›äº†ä¸€ä¸ªæç®€è¡Œä¸ºå…‹éš†æ¡†æ¶ï¼Œæ— éœ€ä¾èµ–æ˜¾å¼åŠ›ä¼ æ„Ÿå³å¯å®ç°åŠ›æ„ŸçŸ¥å’ŒåŠ¨æ€è¡¥å¿ã€‚è§†é¢‘å¯åœ¨ \href{https://xucj98.github.io/mind-the-gap-page/}{é¡¹ç›®é¡µé¢} ä¸Šæ‰¾åˆ°ã€‚

</details>

---

## 269. High-Speed Vision-Based Flight in Clutter with Safety-Shielded Reinforcement Learning

**ä¸­æ–‡æ ‡é¢˜**: åˆ©ç”¨å®‰å…¨é˜²æŠ¤å¼ºåŒ–å­¦ä¹ åœ¨æ‚æ³¢ä¸­å®ç°åŸºäºè§†è§‰çš„é«˜é€Ÿé£è¡Œ

**Date**: 2026-02-09 | **arXiv**: [2602.08653v1](http://arxiv.org/abs/2602.08653v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08653v1)

<details><summary><b>Abstract</b></summary>

Quadrotor unmanned aerial vehicles (UAVs) are increasingly deployed in complex missions that demand reliable autonomous navigation and robust obstacle avoidance. However, traditional modular pipelines often incur cumulative latency, whereas purely reinforcement learning (RL) approaches typically provide limited formal safety guarantees. To bridge this gap, we propose an end-to-end RL framework augmented with model-based safety mechanisms. We incorporate physical priors in both training and deployment. During training, we design a physics-informed reward structure that provides global navigational guidance. During deployment, we integrate a real-time safety filter that projects the policy outputs onto a provably safe set to enforce strict collision-avoidance constraints. This hybrid architecture reconciles high-speed flight with robust safety assurances. Benchmark evaluations demonstrate that our method outperforms both traditional planners and recent end-to-end obstacle avoidance approaches based on differentiable physics. Extensive experiments demonstrate strong generalization, enabling reliable high-speed navigation in dense clutter and challenging outdoor forest environments at velocities up to 7.5m/s.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å››æ—‹ç¿¼æ— äººæœº (UAV) è¶Šæ¥è¶Šå¤šåœ°éƒ¨ç½²åœ¨éœ€è¦å¯é è‡ªä¸»å¯¼èˆªå’Œå¼ºå¤§é¿éšœåŠŸèƒ½çš„å¤æ‚ä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æ¨¡å—åŒ–ç®¡é“ç»å¸¸ä¼šäº§ç”Ÿç´¯ç§¯å»¶è¿Ÿï¼Œè€Œçº¯ç²¹çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•é€šå¸¸æä¾›æœ‰é™çš„å½¢å¼å®‰å…¨ä¿è¯ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå¹¶å¢å¼ºäº†åŸºäºæ¨¡å‹çš„å®‰å…¨æœºåˆ¶ã€‚æˆ‘ä»¬å°†ç‰©ç†å…ˆéªŒçº³å…¥è®­ç»ƒå’Œéƒ¨ç½²ä¸­ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŸºäºç‰©ç†çš„å¥–åŠ±ç»“æ„ï¼Œæä¾›å…¨çƒå¯¼èˆªæŒ‡å¯¼ã€‚åœ¨éƒ¨ç½²è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é›†æˆäº†ä¸€ä¸ªå®æ—¶å®‰å…¨è¿‡æ»¤å™¨ï¼Œå°†ç­–ç•¥è¾“å‡ºæŠ•å½±åˆ°å¯è¯æ˜å®‰å…¨çš„é›†åˆä¸Šï¼Œä»¥å¼ºåˆ¶æ‰§è¡Œä¸¥æ ¼çš„é˜²ç¢°æ’çº¦æŸã€‚è¿™ç§æ··åˆæ¶æ„å°†é«˜é€Ÿé£è¡Œä¸å¼ºå¤§çš„å®‰å…¨ä¿è¯èä¸ºä¸€ä½“ã€‚åŸºå‡†è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºä¼ ç»Ÿè§„åˆ’å™¨å’Œæœ€è¿‘åŸºäºå¯å¾®ç‰©ç†çš„ç«¯åˆ°ç«¯é¿éšœæ–¹æ³•ã€‚å¤§é‡å®éªŒè¯æ˜äº†å…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å¯†é›†æ‚æ³¢å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„å®¤å¤–æ£®æ—ç¯å¢ƒä¸­ä»¥é«˜è¾¾ 7.5m/s çš„é€Ÿåº¦å®ç°å¯é çš„é«˜é€Ÿå¯¼èˆªã€‚

</details>

---

## 270. Mimic Intent, Not Just Trajectories

**ä¸­æ–‡æ ‡é¢˜**: æ¨¡ä»¿æ„å›¾ï¼Œè€Œä¸ä»…ä»…æ˜¯è½¨è¿¹

**Date**: 2026-02-09 | **arXiv**: [2602.08602v1](http://arxiv.org/abs/2602.08602v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08602v1)

<details><summary><b>Abstract</b></summary>

While imitation learning (IL) has achieved impressive success in dexterous manipulation through generative modeling and pretraining, state-of-the-art approaches like Vision-Language-Action (VLA) models still struggle with adaptation to environmental changes and skill transfer. We argue this stems from mimicking raw trajectories without understanding the underlying intent. To address this, we propose explicitly disentangling behavior intent from execution details in end-2-end IL: \textit{``Mimic Intent, Not just Trajectories'' (MINT)}. We achieve this via \textit{multi-scale frequency-space tokenization}, which enforces a spectral decomposition of action chunk representation. We learn action tokens with a multi-scale coarse-to-fine structure, and force the coarsest token to capture low-frequency global structure and finer tokens to encode high-frequency details. This yields an abstract \textit{Intent token} that facilitates planning and transfer, and multi-scale \textit{Execution tokens} that enable precise adaptation to environmental dynamics. Building on this hierarchy, our policy generates trajectories through \textit{next-scale autoregression}, performing progressive \textit{intent-to-execution reasoning}, thus boosting learning efficiency and generalization. Crucially, this disentanglement enables \textit{one-shot transfer} of skills, by simply injecting the Intent token from a demonstration into the autoregressive generation process. Experiments on several manipulation benchmarks and on a real robot demonstrate state-of-the-art success rates, superior inference efficiency, robust generalization against disturbances, and effective one-shot transfer.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è™½ç„¶æ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰é€šè¿‡ç”Ÿæˆå»ºæ¨¡å’Œé¢„è®­ç»ƒåœ¨çµå·§æ“ä½œæ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æˆåŠŸï¼Œä½†è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ç­‰æœ€å…ˆè¿›çš„æ–¹æ³•ä»ç„¶éš¾ä»¥é€‚åº”ç¯å¢ƒå˜åŒ–å’ŒæŠ€èƒ½è½¬ç§»ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™æºäºæ¨¡ä»¿åŸå§‹è½¨è¿¹è€Œä¸ç†è§£æ½œåœ¨çš„æ„å›¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å»ºè®®æ˜ç¡®åœ°å°†è¡Œä¸ºæ„å›¾ä¸ end-2-end IL ä¸­çš„æ‰§è¡Œç»†èŠ‚åˆ†å¼€ï¼š\textit{``æ¨¡ä»¿æ„å›¾ï¼Œä¸ä»…ä»…æ˜¯è½¨è¿¹''ï¼ˆMINTï¼‰}ã€‚æˆ‘ä»¬é€šè¿‡ \textit{å¤šå°ºåº¦é¢‘ç‡ç©ºé—´æ ‡è®°åŒ–} æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œå®ƒå¼ºåˆ¶æ‰§è¡ŒåŠ¨ä½œå—è¡¨ç¤ºçš„é¢‘è°±åˆ†è§£ã€‚æˆ‘ä»¬å­¦ä¹ å…·æœ‰å¤šå°ºåº¦ä»ç²—åˆ°ç»†ç»“æ„çš„åŠ¨ä½œæ ‡è®°ï¼Œå¹¶å¼ºåˆ¶æœ€ç²—ç³™çš„æ ‡è®°æ•è·ä½é¢‘å…¨å±€ç»“æ„ï¼Œå¹¶å¼ºåˆ¶æ›´ç²¾ç»†çš„æ ‡è®°ç¼–ç é«˜é¢‘ç»†èŠ‚ã€‚è¿™äº§ç”Ÿäº†ä¸€ä¸ªæŠ½è±¡çš„\textit{æ„å›¾ä»¤ç‰Œ}ï¼Œæœ‰åŠ©äºè§„åˆ’å’Œä¼ è¾“ï¼Œä»¥åŠå¤šå°ºåº¦çš„\textit{æ‰§è¡Œä»¤ç‰Œ}ï¼Œå¯ä»¥ç²¾ç¡®é€‚åº”ç¯å¢ƒåŠ¨æ€ã€‚åœ¨æ­¤å±‚æ¬¡ç»“æ„çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬çš„ç­–ç•¥é€šè¿‡ \textit{next-scale autoregression} ç”Ÿæˆè½¨è¿¹ï¼Œæ‰§è¡Œæ¸è¿›çš„ \textit{æ„å›¾æ‰§è¡Œæ¨ç†}ï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼Œè¿™ç§è§£å¼€å¯ä»¥é€šè¿‡ç®€å•åœ°å°†æ¼”ç¤ºä¸­çš„æ„å›¾ä»¤ç‰Œæ³¨å…¥è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹æ¥å®ç°æŠ€èƒ½çš„ \textit{ä¸€æ¬¡æ€§è¿ç§»}ã€‚åœ¨å¤šä¸ªæ“çºµåŸºå‡†å’ŒçœŸå®æœºå™¨äººä¸Šè¿›è¡Œçš„å®éªŒè¯æ˜äº†æœ€å…ˆè¿›çš„æˆåŠŸç‡ã€å“è¶Šçš„æ¨ç†æ•ˆç‡ã€é’ˆå¯¹å¹²æ‰°çš„é²æ£’æ³›åŒ–ä»¥åŠæœ‰æ•ˆçš„ä¸€æ¬¡æ€§è¿ç§»ã€‚

</details>

---

## 271. A Precise Real-Time Force-Aware Grasping System for Robust Aerial Manipulation

**ä¸­æ–‡æ ‡é¢˜**: ç”¨äºç¨³å¥ç©ºä¸­æ“çºµçš„ç²¾ç¡®å®æ—¶åŠ›æ„ŸçŸ¥æŠ“å–ç³»ç»Ÿ

**Date**: 2026-02-09 | **arXiv**: [2602.08599v1](http://arxiv.org/abs/2602.08599v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08599v1)

<details><summary><b>Abstract</b></summary>

Aerial manipulation requires force-aware capabilities to enable safe and effective grasping and physical interaction. Previous works often rely on heavy, expensive force sensors unsuitable for typical quadrotor platforms, or perform grasping without force feedback, risking damage to fragile objects. To address these limitations, we propose a novel force-aware grasping framework incorporating six low-cost, sensitive skin-like tactile sensors. We introduce a magnetic-based tactile sensing module that provides high-precision three-dimensional force measurements. We eliminate geomagnetic interference through a reference Hall sensor and simplify the calibration process compared to previous work. The proposed framework enables precise force-aware grasping control, allowing safe manipulation of fragile objects and real-time weight measurement of grasped items. The system is validated through comprehensive real-world experiments, including balloon grasping, dynamic load variation tests, and ablation studies, demonstrating its effectiveness in various aerial manipulation scenarios. Our approach achieves fully onboard operation without external motion capture systems, significantly enhancing the practicality of force-sensitive aerial manipulation. The supplementary video is available at: https://www.youtube.com/watch?v=mbcZkrJEf1I.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç©ºä¸­æ“çºµéœ€è¦åŠ›æ„ŸçŸ¥èƒ½åŠ›ï¼Œä»¥å®ç°å®‰å…¨æœ‰æ•ˆçš„æŠ“å–å’Œç‰©ç†äº¤äº’ã€‚ä»¥å‰çš„å·¥ä½œé€šå¸¸ä¾èµ–äºç¬¨é‡ã€æ˜‚è´µçš„åŠ›ä¼ æ„Ÿå™¨ï¼Œä¸é€‚åˆå…¸å‹çš„å››æ—‹ç¿¼å¹³å°ï¼Œæˆ–è€…åœ¨æ²¡æœ‰åŠ›åé¦ˆçš„æƒ…å†µä¸‹è¿›è¡ŒæŠ“å–ï¼Œä»è€Œå­˜åœ¨æŸåæ˜“ç¢ç‰©ä½“çš„é£é™©ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŠ›æ„ŸçŸ¥æŠ“å–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…å«å…­ä¸ªä½æˆæœ¬ã€æ•æ„Ÿçš„ç±»çš®è‚¤è§¦è§‰ä¼ æ„Ÿå™¨ã€‚æˆ‘ä»¬æ¨å‡ºäº†ä¸€ç§åŸºäºç£æ€§çš„è§¦è§‰ä¼ æ„Ÿæ¨¡å—ï¼Œå¯æä¾›é«˜ç²¾åº¦çš„ä¸‰ç»´åŠ›æµ‹é‡ã€‚ä¸ä¹‹å‰çš„å·¥ä½œç›¸æ¯”ï¼Œæˆ‘ä»¬é€šè¿‡å‚è€ƒéœå°”ä¼ æ„Ÿå™¨æ¶ˆé™¤äº†åœ°ç£å¹²æ‰°å¹¶ç®€åŒ–äº†æ ¡å‡†è¿‡ç¨‹ã€‚æ‰€æå‡ºçš„æ¡†æ¶å¯ä»¥å®ç°ç²¾ç¡®çš„åŠ›æ„ŸçŸ¥æŠ“å–æ§åˆ¶ï¼Œä»è€Œå¯ä»¥å®‰å…¨åœ°æ“çºµæ˜“ç¢ç‰©ä½“å¹¶å®æ—¶æµ‹é‡æŠ“å–çš„ç‰©å“çš„é‡é‡ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å…¨é¢çš„çœŸå®å®éªŒè¿›è¡Œäº†éªŒè¯ï¼ŒåŒ…æ‹¬æ°”çƒæŠ“å–ã€åŠ¨æ€è´Ÿè½½å˜åŒ–æµ‹è¯•å’Œçƒ§èš€ç ”ç©¶ï¼Œè¯æ˜äº†å…¶åœ¨å„ç§ç©ºä¸­æ“çºµåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ— éœ€å¤–éƒ¨åŠ¨ä½œæ•æ‰ç³»ç»Ÿå³å¯å®ç°å®Œå…¨æœºè½½æ“ä½œï¼Œæ˜¾ç€å¢å¼ºäº†åŠ›æ•æ„Ÿç©ºä¸­æ“çºµçš„å®ç”¨æ€§ã€‚è¡¥å……è§†é¢‘å¯åœ¨ä»¥ä¸‹ç½‘å€è·å–ï¼šhttps://www.youtube.com/watch?v=mbcZkrJEf1Iã€‚

</details>

---

## 272. MOSAIC: Bridging the Sim-to-Real Gap in Generalist Humanoid Motion Tracking and Teleoperation with Rapid Residual Adaptation

**ä¸­æ–‡æ ‡é¢˜**: MOSAICï¼šé€šè¿‡å¿«é€Ÿæ®‹å·®é€‚åº”æ¥å¼¥åˆé€šç”¨äººå½¢è¿åŠ¨è·Ÿè¸ªå’Œè¿œç¨‹æ“ä½œä¸­çš„æ¨¡æ‹Ÿä¸çœŸå®å·®è·

**Date**: 2026-02-09 | **arXiv**: [2602.08594v1](http://arxiv.org/abs/2602.08594v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08594v1)

<details><summary><b>Abstract</b></summary>

Generalist humanoid motion trackers have recently achieved strong simulation metrics by scaling data and training, yet often remain brittle on hardware during sustained teleoperation due to interface- and dynamics-induced errors. We present MOSAIC, an open-source, full-stack system for humanoid motion tracking and whole-body teleoperation across multiple interfaces. MOSAIC first learns a teleoperation-oriented general motion tracker via RL on a multi-source motion bank with adaptive resampling and rewards that emphasize world-frame motion consistency, which is critical for mobile teleoperation. To bridge the sim-to-real interface gap without sacrificing generality, MOSAIC then performs rapid residual adaptation: an interface-specific policy is trained using minimal interface-specific data, and then distilled into the general tracker through an additive residual module, outperforming naive fine-tuning or continual learning. We validate MOSAIC with systematic ablations, out-of-distribution benchmarking, and real-robot experiments demonstrating robust offline motion replay and online long-horizon teleoperation under realistic latency and noise.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

é€šç”¨ç±»äººè¿åŠ¨è·Ÿè¸ªå™¨æœ€è¿‘é€šè¿‡æ‰©å±•æ•°æ®å’Œè®­ç»ƒå®ç°äº†å¼ºå¤§çš„æ¨¡æ‹ŸæŒ‡æ ‡ï¼Œä½†ç”±äºæ¥å£å’ŒåŠ¨åŠ›å­¦å¼•èµ·çš„é”™è¯¯ï¼Œåœ¨æŒç»­è¿œç¨‹æ“ä½œæœŸé—´ï¼Œç¡¬ä»¶é€šå¸¸ä»ç„¶è„†å¼±ã€‚æˆ‘ä»¬æ¨å‡º MOSAICï¼Œè¿™æ˜¯ä¸€ç§å¼€æºå…¨æ ˆç³»ç»Ÿï¼Œç”¨äºè·¨å¤šä¸ªç•Œé¢çš„äººå½¢è¿åŠ¨è·Ÿè¸ªå’Œå…¨èº«è¿œç¨‹æ“ä½œã€‚ MOSAIC é¦–å…ˆé€šè¿‡ RL åœ¨å¤šæºè¿åŠ¨åº“ä¸Šå­¦ä¹ é¢å‘è¿œç¨‹æ“ä½œçš„é€šç”¨è¿åŠ¨è·Ÿè¸ªå™¨ï¼Œå…·æœ‰è‡ªé€‚åº”é‡é‡‡æ ·å’Œå¥–åŠ±ï¼Œå¼ºè°ƒä¸–ç•Œæ¡†æ¶è¿åŠ¨ä¸€è‡´æ€§ï¼Œè¿™å¯¹äºç§»åŠ¨è¿œç¨‹æ“ä½œè‡³å…³é‡è¦ã€‚ä¸ºäº†åœ¨ä¸ç‰ºç‰²é€šç”¨æ€§çš„æƒ…å†µä¸‹å¼¥åˆæ¨¡æ‹Ÿä¸çœŸå®æ¥å£ä¹‹é—´çš„å·®è·ï¼ŒMOSAIC ç„¶åæ‰§è¡Œå¿«é€Ÿæ®‹å·®é€‚åº”ï¼šä½¿ç”¨æœ€å°‘çš„ç‰¹å®šäºæ¥å£çš„æ•°æ®æ¥è®­ç»ƒç‰¹å®šäºæ¥å£çš„ç­–ç•¥ï¼Œç„¶åé€šè¿‡é™„åŠ æ®‹å·®æ¨¡å—å°†å…¶æç‚¼åˆ°é€šç”¨è·Ÿè¸ªå™¨ä¸­ï¼Œä»è€Œä¼˜äºæœ´ç´ çš„å¾®è°ƒæˆ–æŒç»­å­¦ä¹ ã€‚æˆ‘ä»¬é€šè¿‡ç³»ç»Ÿæ¶ˆèã€åˆ†å¸ƒå¤–åŸºå‡†æµ‹è¯•å’ŒçœŸå®æœºå™¨äººå®éªŒæ¥éªŒè¯ MOSAICï¼Œè¿™äº›å®éªŒå±•ç¤ºäº†åœ¨çœŸå®å»¶è¿Ÿå’Œå™ªå£°ä¸‹å¼ºå¤§çš„ç¦»çº¿è¿åŠ¨é‡æ”¾å’Œåœ¨çº¿é•¿è§†è·è¿œç¨‹æ“ä½œã€‚

</details>

---

## 273. Head-to-Head autonomous racing at the limits of handling in the A2RL challenge

**ä¸­æ–‡æ ‡é¢˜**: åœ¨ A2RL æŒ‘æˆ˜èµ›ä¸­æŒ‘æˆ˜æ“æ§æé™çš„å¤´å¯¹å¤´è‡ªåŠ¨é©¾é©¶èµ›è½¦

**Date**: 2026-02-09 | **arXiv**: [2602.08571v1](http://arxiv.org/abs/2602.08571v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08571v1)

<details><summary><b>Abstract</b></summary>

Autonomous racing presents a complex challenge involving multi-agent interactions between vehicles operating at the limit of performance and dynamics. As such, it provides a valuable research and testing environment for advancing autonomous driving technology and improving road safety. This article presents the algorithms and deployment strategies developed by the TUM Autonomous Motorsport team for the inaugural Abu Dhabi Autonomous Racing League (A2RL). We showcase how our software emulates human driving behavior, pushing the limits of vehicle handling and multi-vehicle interactions to win the A2RL. Finally, we highlight the key enablers of our success and share our most significant learnings.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è‡ªåŠ¨é©¾é©¶èµ›è½¦æå‡ºäº†ä¸€ä¸ªå¤æ‚çš„æŒ‘æˆ˜ï¼Œæ¶‰åŠåœ¨æ€§èƒ½å’ŒåŠ¨æ€æé™ä¸‹è¿è¡Œçš„è½¦è¾†ä¹‹é—´çš„å¤šæ™ºèƒ½ä½“äº¤äº’ã€‚å› æ­¤ï¼Œå®ƒä¸ºæ¨è¿›è‡ªåŠ¨é©¾é©¶æŠ€æœ¯å’Œæé«˜é“è·¯å®‰å…¨æä¾›äº†å®è´µçš„ç ”ç©¶å’Œæµ‹è¯•ç¯å¢ƒã€‚æœ¬æ–‡ä»‹ç»äº†æ…•å°¼é»‘å·¥ä¸šå¤§å­¦è‡ªåŠ¨é©¾é©¶èµ›è½¦è¿åŠ¨å›¢é˜Ÿä¸ºé¦–å±Šé˜¿å¸ƒæ‰æ¯”è‡ªåŠ¨é©¾é©¶èµ›è½¦è”ç›Ÿ (A2RL) å¼€å‘çš„ç®—æ³•å’Œéƒ¨ç½²ç­–ç•¥ã€‚æˆ‘ä»¬å±•ç¤ºæˆ‘ä»¬çš„è½¯ä»¶å¦‚ä½•æ¨¡æ‹Ÿäººç±»é©¾é©¶è¡Œä¸ºï¼Œçªç ´è½¦è¾†æ“æ§å’Œå¤šè½¦è¾†äº¤äº’çš„æé™ï¼Œä»è€Œèµ¢å¾— A2RLã€‚æœ€åï¼Œæˆ‘ä»¬å¼ºè°ƒäº†æˆ‘ä»¬æˆåŠŸçš„å…³é”®æ¨åŠ¨å› ç´ ï¼Œå¹¶åˆ†äº«äº†æˆ‘ä»¬æœ€é‡è¦çš„ç»éªŒæ•™è®­ã€‚

</details>

---

## 274. Constrained Sampling to Guide Universal Manipulation RL

**ä¸­æ–‡æ ‡é¢˜**: çº¦æŸé‡‡æ ·æŒ‡å¯¼é€šç”¨æ“çºµå¼ºåŒ–å­¦ä¹ 

**Date**: 2026-02-09 | **arXiv**: [2602.08557v1](http://arxiv.org/abs/2602.08557v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08557v1)

<details><summary><b>Abstract</b></summary>

We consider how model-based solvers can be leveraged to guide training of a universal policy to control from any feasible start state to any feasible goal in a contact-rich manipulation setting. While Reinforcement Learning (RL) has demonstrated its strength in such settings, it may struggle to sufficiently explore and discover complex manipulation strategies, especially in sparse-reward settings. Our approach is based on the idea of a lower-dimensional manifold of feasible, likely-visited states during such manipulation and to guide RL with a sampler from this manifold. We propose Sample-Guided RL, which uses model-based constraint solvers to efficiently sample feasible configurations (satisfying differentiable collision, contact, and force constraints) and leverage them to guide RL for universal (goal-conditioned) manipulation policies. We study using this data directly to bias state visitation, as well as using black-box optimization of open-loop trajectories between random configurations to impose a state bias and optionally add a behavior cloning loss. In a minimalistic double sphere manipulation setting, Sample-Guided RL discovers complex manipulation strategies and achieves high success rates in reaching any statically stable state. In a more challenging panda arm setting, our approach achieves a significant success rate over a near-zero baseline, and demonstrates a breadth of complex whole-body-contact manipulation strategies.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æˆ‘ä»¬è€ƒè™‘å¦‚ä½•åˆ©ç”¨åŸºäºæ¨¡å‹çš„æ±‚è§£å™¨æ¥æŒ‡å¯¼é€šç”¨ç­–ç•¥çš„è®­ç»ƒï¼Œä»¥åœ¨æ¥è§¦ä¸°å¯Œçš„æ“çºµè®¾ç½®ä¸­æ§åˆ¶ä»ä»»ä½•å¯è¡Œçš„èµ·å§‹çŠ¶æ€åˆ°ä»»ä½•å¯è¡Œçš„ç›®æ ‡ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹  (RL) åœ¨æ­¤ç±»ç¯å¢ƒä¸­å±•ç¤ºäº†å…¶ä¼˜åŠ¿ï¼Œä½†å®ƒå¯èƒ½å¾ˆéš¾å……åˆ†æ¢ç´¢å’Œå‘ç°å¤æ‚çš„æ“çºµç­–ç•¥ï¼Œå°¤å…¶æ˜¯åœ¨ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºè¿™æ ·çš„æ“ä½œè¿‡ç¨‹ä¸­å¯è¡Œçš„ã€å¯èƒ½è®¿é—®çš„çŠ¶æ€çš„ä½ç»´æµå½¢çš„æƒ³æ³•ï¼Œå¹¶ä½¿ç”¨æ¥è‡ªè¯¥æµå½¢çš„é‡‡æ ·å™¨æ¥æŒ‡å¯¼å¼ºåŒ–å­¦ä¹ ã€‚æˆ‘ä»¬æå‡ºäº† Sample-Guided RLï¼Œå®ƒä½¿ç”¨åŸºäºæ¨¡å‹çš„çº¦æŸæ±‚è§£å™¨æ¥æœ‰æ•ˆåœ°é‡‡æ ·å¯è¡Œçš„é…ç½®ï¼ˆæ»¡è¶³å¯å¾®çš„ç¢°æ’ã€æ¥è§¦å’ŒåŠ›çº¦æŸï¼‰ï¼Œå¹¶åˆ©ç”¨å®ƒä»¬æ¥æŒ‡å¯¼ RL å®ç°é€šç”¨ï¼ˆç›®æ ‡æ¡ä»¶ï¼‰æ“çºµç­–ç•¥ã€‚æˆ‘ä»¬ç ”ç©¶ç›´æ¥ä½¿ç”¨è¿™äº›æ•°æ®æ¥åç½®çŠ¶æ€è®¿é—®ï¼Œä»¥åŠä½¿ç”¨éšæœºé…ç½®ä¹‹é—´çš„å¼€ç¯è½¨è¿¹çš„é»‘ç›’ä¼˜åŒ–æ¥æ–½åŠ çŠ¶æ€åå·®å¹¶å¯é€‰æ‹©æ·»åŠ è¡Œä¸ºå…‹éš†æŸå¤±ã€‚åœ¨ç®€çº¦çš„åŒçƒæ“çºµè®¾ç½®ä¸­ï¼Œæ ·æœ¬å¼•å¯¼å¼ºåŒ–å­¦ä¹ å‘ç°äº†å¤æ‚çš„æ“çºµç­–ç•¥ï¼Œå¹¶åœ¨è¾¾åˆ°ä»»ä½•é™æ€ç¨³å®šçŠ¶æ€æ–¹é¢å–å¾—äº†å¾ˆé«˜çš„æˆåŠŸç‡ã€‚åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„ç†ŠçŒ«æ‰‹è‡‚ç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¥è¿‘é›¶çš„åŸºçº¿ä¸Šå®ç°äº†æ˜¾ç€çš„æˆåŠŸç‡ï¼Œå¹¶å±•ç¤ºäº†å¹¿æ³›çš„å¤æ‚çš„å…¨èº«æ¥è§¦æ“çºµç­–ç•¥ã€‚

</details>

---

## 275. UniPlan: Vision-Language Task Planning for Mobile Manipulation with Unified PDDL Formulation

**ä¸­æ–‡æ ‡é¢˜**: UniPlanï¼šå…·æœ‰ç»Ÿä¸€ PDDL å…¬å¼çš„ç§»åŠ¨æ“ä½œè§†è§‰è¯­è¨€ä»»åŠ¡è§„åˆ’

**Date**: 2026-02-09 | **arXiv**: [2602.08537v1](http://arxiv.org/abs/2602.08537v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08537v1)

<details><summary><b>Abstract</b></summary>

Integration of VLM reasoning with symbolic planning has proven to be a promising approach to real-world robot task planning. Existing work like UniDomain effectively learns symbolic manipulation domains from real-world demonstrations, described in Planning Domain Definition Language (PDDL), and has successfully applied them to real-world tasks. These domains, however, are restricted to tabletop manipulation. We propose UniPlan, a vision-language task planning system for long-horizon mobile-manipulation in large-scale indoor environments, that unifies scene topology, visuals, and robot capabilities into a holistic PDDL representation. UniPlan programmatically extends learned tabletop domains from UniDomain to support navigation, door traversal, and bimanual coordination. It operates on a visual-topological map, comprising navigation landmarks anchored with scene images. Given a language instruction, UniPlan retrieves task-relevant nodes from the map and uses a VLM to ground the anchored image into task-relevant objects and their PDDL states; next, it reconnects these nodes to a compressed, densely-connected topological map, also represented in PDDL, with connectivity and costs derived from the original map; Finally, a mobile-manipulation plan is generated using off-the-shelf PDDL solvers. Evaluated on human-raised tasks in a large-scale map with real-world imagery, UniPlan significantly outperforms VLM and LLM+PDDL planning in success rate, plan quality, and computational efficiency.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

VLM æ¨ç†ä¸ç¬¦å·è§„åˆ’çš„é›†æˆå·²è¢«è¯æ˜æ˜¯ç°å®ä¸–ç•Œæœºå™¨äººä»»åŠ¡è§„åˆ’çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚åƒ UniDomain è¿™æ ·çš„ç°æœ‰å·¥ä½œå¯ä»¥æœ‰æ•ˆåœ°ä»ç°å®ä¸–ç•Œçš„æ¼”ç¤ºä¸­å­¦ä¹ ç¬¦å·æ“ä½œåŸŸï¼Œå¦‚è§„åˆ’åŸŸå®šä¹‰è¯­è¨€ (PDDL) ä¸­æ‰€è¿°ï¼Œå¹¶å·²æˆåŠŸåœ°å°†å®ƒä»¬åº”ç”¨åˆ°ç°å®ä¸–ç•Œçš„ä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œè¿™äº›åŸŸä»…é™äºæ¡Œé¢æ“ä½œã€‚æˆ‘ä»¬æå‡ºäº† UniPlanï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤§è§„æ¨¡å®¤å†…ç¯å¢ƒä¸­é•¿è§†è·ç§»åŠ¨æ“ä½œçš„è§†è§‰è¯­è¨€ä»»åŠ¡è§„åˆ’ç³»ç»Ÿï¼Œå®ƒå°†åœºæ™¯æ‹“æ‰‘ã€è§†è§‰æ•ˆæœå’Œæœºå™¨äººåŠŸèƒ½ç»Ÿä¸€åˆ°æ•´ä½“ PDDL è¡¨ç¤ºä¸­ã€‚ UniPlan ä»¥ç¼–ç¨‹æ–¹å¼æ‰©å±•ä» UniDomain å­¦ä¹ çš„æ¡Œé¢åŸŸï¼Œä»¥æ”¯æŒå¯¼èˆªã€é—¨éå†å’ŒåŒæ‰‹åè°ƒã€‚å®ƒåœ¨è§†è§‰æ‹“æ‰‘åœ°å›¾ä¸Šè¿è¡Œï¼ŒåŒ…æ‹¬é”šå®šåœºæ™¯å›¾åƒçš„å¯¼èˆªåœ°æ ‡ã€‚ç»™å®šè¯­è¨€æŒ‡ä»¤ï¼ŒUniPlan ä»åœ°å›¾ä¸­æ£€ç´¢ä¸ä»»åŠ¡ç›¸å…³çš„èŠ‚ç‚¹ï¼Œå¹¶ä½¿ç”¨ VLM å°†é”šå®šå›¾åƒè½¬åŒ–ä¸ºä¸ä»»åŠ¡ç›¸å…³çš„å¯¹è±¡åŠå…¶ PDDL çŠ¶æ€ï¼›æ¥ä¸‹æ¥ï¼Œå®ƒå°†è¿™äº›èŠ‚ç‚¹é‡æ–°è¿æ¥åˆ°å‹ç¼©çš„ã€å¯†é›†è¿æ¥çš„æ‹“æ‰‘å›¾ï¼Œä¹Ÿä»¥ PDDL è¡¨ç¤ºï¼Œè¿æ¥æ€§å’Œæˆæœ¬æ¥è‡ªåŸå§‹åœ°å›¾ï¼›æœ€åï¼Œä½¿ç”¨ç°æˆçš„ PDDL æ±‚è§£å™¨ç”Ÿæˆç§»åŠ¨æ“çºµè®¡åˆ’ã€‚åœ¨å…·æœ‰çœŸå®ä¸–ç•Œå›¾åƒçš„å¤§è§„æ¨¡åœ°å›¾ä¸­å¯¹äººå·¥ä»»åŠ¡è¿›è¡Œè¯„ä¼°åï¼ŒUniPlan åœ¨æˆåŠŸç‡ã€è®¡åˆ’è´¨é‡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢æ˜¾ç€ä¼˜äº VLM å’Œ LLM+PDDL è§„åˆ’ã€‚

</details>

---

## 276. Reliability-aware Execution Gating for Near-field and Off-axis Vision-guided Robotic Alignment

**ä¸­æ–‡æ ‡é¢˜**: ç”¨äºè¿‘åœºå’Œç¦»è½´è§†è§‰å¼•å¯¼æœºå™¨äººå¯¹å‡†çš„å¯é æ€§æ„ŸçŸ¥æ‰§è¡Œé—¨æ§

**Date**: 2026-02-09 | **arXiv**: [2602.08466v1](http://arxiv.org/abs/2602.08466v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08466v1)

<details><summary><b>Abstract</b></summary>

Vision-guided robotic systems are increasingly deployed in precision alignment tasks that require reliable execution under near-field and off-axis configurations. While recent advances in pose estimation have significantly improved numerical accuracy, practical robotic systems still suffer from frequent execution failures even when pose estimates appear accurate. This gap suggests that pose accuracy alone is insufficient to guarantee execution-level reliability. In this paper, we reveal that such failures arise from a deterministic geometric error amplification mechanism, in which small pose estimation errors are magnified through system structure and motion execution, leading to unstable or failed alignment. Rather than modifying pose estimation algorithms, we propose a Reliability-aware Execution Gating mechanism that operates at the execution level. The proposed approach evaluates geometric consistency and configuration risk before execution, and selectively rejects or scales high-risk pose updates. We validate the proposed method on a real UR5 robotic platform performing single-step visual alignment tasks under varying camera-target distances and off-axis configurations. Experimental results demonstrate that the proposed execution gating significantly improves task success rates, reduces execution variance, and suppresses tail-risk behavior, while leaving average pose accuracy largely unchanged. Importantly, the proposed mechanism is estimator-agnostic and can be readily integrated with both classical geometry-based and learning-based pose estimation pipelines. These results highlight the importance of execution-level reliability modeling and provide a practical solution for improving robustness in near-field vision-guided robotic systems.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è§†è§‰å¼•å¯¼æœºå™¨äººç³»ç»Ÿè¶Šæ¥è¶Šå¤šåœ°éƒ¨ç½²åœ¨éœ€è¦åœ¨è¿‘åœºå’Œç¦»è½´é…ç½®ä¸‹å¯é æ‰§è¡Œçš„ç²¾å¯†å¯¹å‡†ä»»åŠ¡ä¸­ã€‚å°½ç®¡å§¿æ€ä¼°è®¡æ–¹é¢çš„æœ€æ–°è¿›å±•æ˜¾ç€æé«˜äº†æ•°å€¼ç²¾åº¦ï¼Œä½†å³ä½¿å§¿æ€ä¼°è®¡çœ‹èµ·æ¥å‡†ç¡®ï¼Œå®é™…çš„æœºå™¨äººç³»ç»Ÿä»ç„¶ä¼šé¢‘ç¹å‡ºç°æ‰§è¡Œå¤±è´¥ã€‚è¿™ä¸€å·®è·è¡¨æ˜ï¼Œä»…é å§¿åŠ¿ç²¾åº¦ä¸è¶³ä»¥ä¿è¯æ‰§è¡Œçº§åˆ«çš„å¯é æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ­ç¤ºäº†æ­¤ç±»æ•…éšœæºäºç¡®å®šæ€§å‡ ä½•è¯¯å·®æ”¾å¤§æœºåˆ¶ï¼Œå…¶ä¸­å°çš„ä½å§¿ä¼°è®¡è¯¯å·®é€šè¿‡ç³»ç»Ÿç»“æ„å’Œè¿åŠ¨æ‰§è¡Œè¢«æ”¾å¤§ï¼Œå¯¼è‡´å¯¹å‡†ä¸ç¨³å®šæˆ–å¤±è´¥ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨æ‰§è¡Œçº§åˆ«è¿è¡Œçš„å¯é æ€§æ„ŸçŸ¥æ‰§è¡Œé—¨æ§æœºåˆ¶ï¼Œè€Œä¸æ˜¯ä¿®æ”¹å§¿æ€ä¼°è®¡ç®—æ³•ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨æ‰§è¡Œå‰è¯„ä¼°å‡ ä½•ä¸€è‡´æ€§å’Œé…ç½®é£é™©ï¼Œå¹¶é€‰æ‹©æ€§åœ°æ‹’ç»æˆ–ç¼©æ”¾é«˜é£é™©çš„å§¿æ€æ›´æ–°ã€‚æˆ‘ä»¬åœ¨çœŸå®çš„ UR5 æœºå™¨äººå¹³å°ä¸ŠéªŒè¯äº†æ‰€æå‡ºçš„æ–¹æ³•ï¼Œåœ¨ä¸åŒçš„ç›¸æœºç›®æ ‡è·ç¦»å’Œç¦»è½´é…ç½®ä¸‹æ‰§è¡Œå•æ­¥è§†è§‰å¯¹å‡†ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ‰§è¡Œé—¨æ§æ˜¾ç€æé«˜äº†ä»»åŠ¡æˆåŠŸç‡ï¼Œå‡å°‘äº†æ‰§è¡Œæ–¹å·®ï¼Œå¹¶æŠ‘åˆ¶äº†å°¾éƒ¨é£é™©è¡Œä¸ºï¼ŒåŒæ—¶å¹³å‡å§¿æ€ç²¾åº¦åŸºæœ¬ä¿æŒä¸å˜ã€‚é‡è¦çš„æ˜¯ï¼Œæ‰€æå‡ºçš„æœºåˆ¶ä¸ä¼°è®¡å™¨æ— å…³ï¼Œå¹¶ä¸”å¯ä»¥è½»æ¾åœ°ä¸ç»å…¸çš„åŸºäºå‡ ä½•å’ŒåŸºäºå­¦ä¹ çš„å§¿æ€ä¼°è®¡ç®¡é“é›†æˆã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†æ‰§è¡Œçº§å¯é æ€§å»ºæ¨¡çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæé«˜è¿‘åœºè§†è§‰å¼•å¯¼æœºå™¨äººç³»ç»Ÿçš„é²æ£’æ€§æä¾›äº†å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚

</details>

---

## 277. UAV-Supported Maritime Search System: Experience from Valun Bay Field Trials

**ä¸­æ–‡æ ‡é¢˜**: æ— äººæœºæ”¯æŒçš„æµ·ä¸Šæœç´¢ç³»ç»Ÿï¼šç“¦ä¼¦æ¹¾å®åœ°è¯•éªŒç»éªŒ

**Date**: 2026-02-09 | **arXiv**: [2602.08450v1](http://arxiv.org/abs/2602.08450v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08450v1)

<details><summary><b>Abstract</b></summary>

This paper presents the integration of flow field reconstruction, dynamic probabilistic modeling, search control, and machine vision detection in a system for autonomous maritime search operations. Field experiments conducted in Valun Bay (Cres Island, Croatia) involved real-time drifter data acquisition, surrogate flow model fitting based on computational fluid dynamics and numerical optimization, advanced multi-UAV search control and vision sensing, as well as deep learning-based object detection. The results demonstrate that a tightly coupled approach enables reliable detection of floating targets under realistic uncertainties and complex environmental conditions, providing concrete insights for future autonomous maritime search and rescue applications.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æœ¬æ–‡ä»‹ç»äº†è‡ªä¸»æµ·ä¸Šæœç´¢æ“ä½œç³»ç»Ÿä¸­æµåœºé‡å»ºã€åŠ¨æ€æ¦‚ç‡å»ºæ¨¡ã€æœç´¢æ§åˆ¶å’Œæœºå™¨è§†è§‰æ£€æµ‹çš„é›†æˆã€‚åœ¨ç“¦ä¼¦æ¹¾ï¼ˆå…‹ç½—åœ°äºšå…‹é›·æ–¯å²›ï¼‰è¿›è¡Œçš„ç°åœºå®éªŒæ¶‰åŠå®æ—¶æ¼‚æµå™¨æ•°æ®é‡‡é›†ã€åŸºäºè®¡ç®—æµä½“åŠ¨åŠ›å­¦å’Œæ•°å€¼ä¼˜åŒ–çš„æ›¿ä»£æµæ¨¡å‹æ‹Ÿåˆã€å…ˆè¿›çš„å¤šæ— äººæœºæœç´¢æ§åˆ¶å’Œè§†è§‰ä¼ æ„Ÿï¼Œä»¥åŠåŸºäºæ·±åº¦å­¦ä¹ çš„ç›®æ ‡æ£€æµ‹ã€‚ç»“æœè¡¨æ˜ï¼Œç´§å¯†è€¦åˆçš„æ–¹æ³•èƒ½å¤Ÿåœ¨ç°å®çš„ä¸ç¡®å®šæ€§å’Œå¤æ‚çš„ç¯å¢ƒæ¡ä»¶ä¸‹å¯é åœ°æ£€æµ‹æµ®åŠ¨ç›®æ ‡ï¼Œä¸ºæœªæ¥çš„è‡ªä¸»æµ·ä¸Šæœç´¢å’Œæ•‘æ´åº”ç”¨æä¾›å…·ä½“çš„è§è§£ã€‚

</details>

---

## 278. Post-Collision Trajectory Restoration for a Single-track Ackermann Vehicle using Heuristic Steering and Tractive Force Functions

**ä¸­æ–‡æ ‡é¢˜**: ä½¿ç”¨å¯å‘å¼è½¬å‘å’Œç‰µå¼•åŠ›å‡½æ•°æ¢å¤å•è½¨é˜¿å…‹æ›¼è½¦è¾†çš„ç¢°æ’åè½¨è¿¹

**Date**: 2026-02-09 | **arXiv**: [2602.08444v1](http://arxiv.org/abs/2602.08444v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08444v1)

<details><summary><b>Abstract</b></summary>

Post-collision trajectory restoration is a safety-critical capability for autonomous vehicles, as impact-induced lateral motion and yaw transients can rapidly drive the vehicle away from the intended path. This paper proposes a structured heuristic recovery control law that jointly commands steering and tractive force for a generalized single-track Ackermann vehicle model. The formulation explicitly accounts for time-varying longitudinal velocity in the lateral-yaw dynamics and retains nonlinear steering-coupled interaction terms that are commonly simplified in the literature. Unlike approaches that assume constant longitudinal speed, the proposed design targets the transient post-impact regime where speed variations and nonlinear coupling significantly influence recovery. The method is evaluated in simulation on the proposed generalized single-track model and a standard 3DOF single-track reference model in MATLAB, demonstrating consistent post-collision restoration behaviour across representative initial post-impact conditions.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç¢°æ’åè½¨è¿¹æ¢å¤å¯¹äºè‡ªåŠ¨é©¾é©¶è½¦è¾†æ¥è¯´æ˜¯ä¸€é¡¹å®‰å…¨å…³é”®çš„åŠŸèƒ½ï¼Œå› ä¸ºç¢°æ’å¼•èµ·çš„æ¨ªå‘è¿åŠ¨å’Œåèˆªç¬æ€å¯èƒ½ä¼šè¿…é€Ÿä½¿è½¦è¾†åç¦»é¢„æœŸè·¯å¾„ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“æ„åŒ–å¯å‘å¼æ¢å¤æ§åˆ¶å¾‹ï¼Œä¸ºå¹¿ä¹‰å•è½¨é˜¿å…‹æ›¼è½¦è¾†æ¨¡å‹è”åˆæ§åˆ¶è½¬å‘å’Œç‰µå¼•åŠ›ã€‚è¯¥å…¬å¼æ˜ç¡®åœ°è€ƒè™‘äº†æ¨ªå‘åèˆªåŠ¨åŠ›å­¦ä¸­éšæ—¶é—´å˜åŒ–çš„çºµå‘é€Ÿåº¦ï¼Œå¹¶ä¿ç•™äº†æ–‡çŒ®ä¸­é€šå¸¸ç®€åŒ–çš„éçº¿æ€§è½¬å‘è€¦åˆç›¸äº’ä½œç”¨é¡¹ã€‚ä¸å‡å®šçºµå‘é€Ÿåº¦æ’å®šçš„æ–¹æ³•ä¸åŒï¼Œæ‰€æå‡ºçš„è®¾è®¡é’ˆå¯¹ç¬æ€æ’å‡»åçŠ¶æ€ï¼Œå…¶ä¸­é€Ÿåº¦å˜åŒ–å’Œéçº¿æ€§è€¦åˆæ˜¾ç€å½±å“æ¢å¤ã€‚è¯¥æ–¹æ³•åœ¨ MATLAB ä¸­å¯¹æ‰€æå‡ºçš„å¹¿ä¹‰å•è½¨æ¨¡å‹å’Œæ ‡å‡† 3DOF å•è½¨å‚è€ƒæ¨¡å‹è¿›è¡Œäº†ä»¿çœŸè¯„ä¼°ï¼Œå±•ç¤ºäº†åœ¨ä»£è¡¨æ€§åˆå§‹ç¢°æ’åæ¡ä»¶ä¸‹ä¸€è‡´çš„ç¢°æ’åæ¢å¤è¡Œä¸ºã€‚

</details>

---

## 279. SteerVLA: Steering Vision-Language-Action Models in Long-Tail Driving Scenarios

**ä¸­æ–‡æ ‡é¢˜**: SteerVLAï¼šé•¿å°¾é©¾é©¶åœºæ™¯ä¸­çš„è½¬å‘è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹

**Date**: 2026-02-09 | **arXiv**: [2602.08440v1](http://arxiv.org/abs/2602.08440v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08440v1)

<details><summary><b>Abstract</b></summary>

A fundamental challenge in autonomous driving is the integration of high-level, semantic reasoning for long-tail events with low-level, reactive control for robust driving. While large vision-language models (VLMs) trained on web-scale data offer powerful common-sense reasoning, they lack the grounded experience necessary for safe vehicle control. We posit that an effective autonomous agent should leverage the world knowledge of VLMs to guide a steerable driving policy toward robust control in driving scenarios. To this end, we propose SteerVLA, which leverages the reasoning capabilities of VLMs to produce fine-grained language instructions that steer a vision-language-action (VLA) driving policy. Key to our method is this rich language interface between the high-level VLM and low-level VLA, which allows the high-level policy to more effectively ground its reasoning in the control outputs of the low-level policy. To provide fine-grained language supervision aligned with vehicle control, we leverage a VLM to augment existing driving data with detailed language annotations, which we find to be essential for effective reasoning and steerability. We evaluate SteerVLA on a challenging closed-loop benchmark, where it outperforms state-of-the-art methods by 4.77 points in overall driving score and by 8.04 points on a long-tail subset. The project website is available at: https://steervla.github.io/.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

è‡ªåŠ¨é©¾é©¶çš„ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜æ˜¯å°†é•¿å°¾äº‹ä»¶çš„é«˜çº§è¯­ä¹‰æ¨ç†ä¸ç¨³å¥é©¾é©¶çš„ä½çº§ååº”æ§åˆ¶ç›¸é›†æˆã€‚è™½ç„¶åŸºäºç½‘ç»œè§„æ¨¡æ•°æ®è®­ç»ƒçš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ (VLM) æä¾›äº†å¼ºå¤§çš„å¸¸è¯†æ¨ç†ï¼Œä½†å®ƒä»¬ç¼ºä¹å®‰å…¨è½¦è¾†æ§åˆ¶æ‰€éœ€çš„åŸºç¡€ç»éªŒã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œä¸€ä¸ªæœ‰æ•ˆçš„è‡ªä¸»ä»£ç†åº”è¯¥åˆ©ç”¨ VLM çš„ä¸–ç•ŒçŸ¥è¯†æ¥æŒ‡å¯¼å¯æ“çºµçš„é©¾é©¶ç­–ç•¥ï¼Œä»¥å®ç°é©¾é©¶åœºæ™¯ä¸­çš„ç¨³å¥æ§åˆ¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº† SteerVLAï¼Œå®ƒåˆ©ç”¨ VLM çš„æ¨ç†èƒ½åŠ›æ¥ç”Ÿæˆç»†ç²’åº¦çš„è¯­è¨€æŒ‡ä»¤ï¼Œä»¥å¼•å¯¼è§†è§‰-è¯­è¨€-åŠ¨ä½œ (VLA) é©±åŠ¨ç­–ç•¥ã€‚æˆ‘ä»¬æ–¹æ³•çš„å…³é”®æ˜¯é«˜çº§ VLM å’Œä½çº§ VLA ä¹‹é—´çš„ä¸°å¯Œè¯­è¨€æ¥å£ï¼Œå®ƒå…è®¸é«˜çº§ç­–ç•¥æ›´æœ‰æ•ˆåœ°å°†å…¶æ¨ç†å»ºç«‹åœ¨ä½çº§ç­–ç•¥çš„æ§åˆ¶è¾“å‡ºä¸­ã€‚ä¸ºäº†æä¾›ä¸è½¦è¾†æ§åˆ¶ç›¸ä¸€è‡´çš„ç»†ç²’åº¦è¯­è¨€ç›‘ç£ï¼Œæˆ‘ä»¬åˆ©ç”¨ VLM é€šè¿‡è¯¦ç»†çš„è¯­è¨€æ³¨é‡Šæ¥å¢å¼ºç°æœ‰çš„é©¾é©¶æ•°æ®ï¼Œæˆ‘ä»¬å‘ç°è¿™å¯¹äºæœ‰æ•ˆçš„æ¨ç†å’Œå¯æ“çºµæ€§è‡³å…³é‡è¦ã€‚æˆ‘ä»¬åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é—­ç¯åŸºå‡†ä¸Šè¯„ä¼°äº† SteerVLAï¼Œå®ƒåœ¨æ•´ä½“é©¾é©¶å¾—åˆ†ä¸Šæ¯”æœ€å…ˆè¿›çš„æ–¹æ³•é«˜å‡º 4.77 åˆ†ï¼Œåœ¨é•¿å°¾å­é›†ä¸Šé«˜å‡º 8.04 åˆ†ã€‚è¯¥é¡¹ç›®ç½‘ç«™ä½äºï¼šhttps://steervla.github.io/ã€‚

</details>

---

## 280. Bi-Adapt: Few-shot Bimanual Adaptation for Novel Categories of 3D Objects via Semantic Correspondence

**ä¸­æ–‡æ ‡é¢˜**: Bi-Adaptï¼šé€šè¿‡è¯­ä¹‰å¯¹åº”å¯¹ 3D å¯¹è±¡çš„æ–°ç±»åˆ«è¿›è¡Œå°‘é‡åŒæ‰‹é€‚åº”

**Date**: 2026-02-09 | **arXiv**: [2602.08425v2](http://arxiv.org/abs/2602.08425v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.08425v2)

**Project**: https://biadapt-project.github.io/  <details><summary><b>Abstract</b></summary>

Bimanual manipulation is imperative yet challenging for robots to execute complex tasks, requiring coordinated collaboration between two arms. However, existing methods for bimanual manipulation often rely on costly data collection and training, struggling to generalize to unseen objects in novel categories efficiently. In this paper, we present Bi-Adapt, a novel framework designed for efficient generalization for bimanual manipulation via semantic correspondence. Bi-Adapt achieves cross-category affordance mapping by leveraging the strong capability of vision foundation models. Fine-tuning with restricted data on novel categories, Bi-Adapt exhibits notable generalization to out-of-category objects in a zero-shot manner. Extensive experiments conducted in both simulation and real-world environments validate the effectiveness of our approach and demonstrate its high efficiency, achieving a high success rate on different benchmark tasks across novel categories with limited data. Project website: https://biadapt-project.github.io/

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¯¹äºæœºå™¨äººæ‰§è¡Œå¤æ‚çš„ä»»åŠ¡æ¥è¯´ï¼ŒåŒæ‰‹æ“çºµæ˜¯å¿…è¦çš„ï¼Œä½†ä¹Ÿå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéœ€è¦ä¸¤è‡‚ä¹‹é—´çš„åè°ƒåä½œã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŒæ‰‹æ“ä½œæ–¹æ³•é€šå¸¸ä¾èµ–äºæ˜‚è´µçš„æ•°æ®æ”¶é›†å’Œè®­ç»ƒï¼Œéš¾ä»¥æœ‰æ•ˆåœ°æ¨å¹¿åˆ°æ–°ç±»åˆ«ä¸­çœ‹ä¸è§çš„ç‰©ä½“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† Bi-Adaptï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è¯­ä¹‰å¯¹åº”æ¥æœ‰æ•ˆæ³›åŒ–åŒæ‰‹æ“ä½œã€‚ Bi-Adaptåˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›ï¼Œå®ç°äº†è·¨å“ç±»å¯ä¾›æ€§æ˜ å°„ã€‚ Bi-Adapt é€šè¿‡å¯¹æ–°é¢–ç±»åˆ«çš„æœ‰é™æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œä»¥é›¶æ ·æœ¬çš„æ–¹å¼å¯¹ç±»åˆ«å¤–å¯¹è±¡è¡¨ç°å‡ºæ˜¾ç€çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æ¨¡æ‹Ÿå’Œç°å®ç¯å¢ƒä¸­è¿›è¡Œçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯æ˜äº†å…¶é«˜æ•ˆç‡ï¼Œåœ¨æ•°æ®æœ‰é™çš„æ–°ç±»åˆ«çš„ä¸åŒåŸºå‡†ä»»åŠ¡ä¸Šå–å¾—äº†å¾ˆé«˜çš„æˆåŠŸç‡ã€‚é¡¹ç›®ç½‘ç«™ï¼šhttps://biadapt-project.github.io/

</details>

---

## 281. Decentralized Intent-Based Multi-Robot Task Planner with LLM Oracles on Hyperledger Fabric

**ä¸­æ–‡æ ‡é¢˜**: Hyperledger Fabric ä¸Šå…·æœ‰ LLM Oracle çš„åˆ†æ•£å¼åŸºäºæ„å›¾çš„å¤šæœºå™¨äººä»»åŠ¡è§„åˆ’å™¨

**Date**: 2026-02-09 | **arXiv**: [2602.08421v1](http://arxiv.org/abs/2602.08421v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08421v1)

<details><summary><b>Abstract</b></summary>

Large language models (LLMs) have opened new opportunities for transforming natural language user intents into executable actions. This capability enables embodied AI agents to perform complex tasks, without involvement of an expert, making human-robot interaction (HRI) more convenient. However these developments raise significant security and privacy challenges such as self-preferencing, where a single LLM service provider dominates the market and uses this power to promote their own preferences. LLM oracles have been recently proposed as a mechanism to decentralize LLMs by executing multiple LLMs from different vendors and aggregating their outputs to obtain a more reliable and trustworthy final result. However, the accuracy of these approaches highly depends on the aggregation method. The current aggregation methods mostly use semantic similarity between various LLM outputs, not suitable for robotic task planning, where the temporal order of tasks is important. To fill the gap, we propose an LLM oracle with a new aggregation method for robotic task planning. In addition, we propose a decentralized multi-robot infrastructure based on Hyperledger Fabric that can host the proposed oracle. The proposed infrastructure enables users to express their natural language intent to the system, which then can be decomposed into subtasks. These subtasks require coordinating different robots from different vendors, while enforcing fine-grained access control management on the data. To evaluate our methodology, we created the SkillChain-RTD benchmark made it publicly available. Our experimental results demonstrate the feasibility of the proposed architecture, and the proposed aggregation method outperforms other aggregation methods currently in use.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) ä¸ºå°†è‡ªç„¶è¯­è¨€ç”¨æˆ·æ„å›¾è½¬åŒ–ä¸ºå¯æ‰§è¡Œæ“ä½œæä¾›äº†æ–°çš„æœºä¼šã€‚è¿™ç§èƒ½åŠ›ä½¿å…·ä½“çš„äººå·¥æ™ºèƒ½ä»£ç†èƒ½å¤Ÿæ‰§è¡Œå¤æ‚çš„ä»»åŠ¡ï¼Œè€Œæ— éœ€ä¸“å®¶çš„å‚ä¸ï¼Œä½¿äººæœºäº¤äº’ï¼ˆHRIï¼‰æ›´åŠ æ–¹ä¾¿ã€‚ç„¶è€Œï¼Œè¿™äº›å‘å±•å¸¦æ¥äº†é‡å¤§çš„å®‰å…¨å’Œéšç§æŒ‘æˆ˜ï¼Œä¾‹å¦‚è‡ªæˆ‘åå¥½ï¼Œå•ä¸€æ³•å­¦ç¡•å£«æœåŠ¡æä¾›å•†ä¸»å¯¼å¸‚åœºå¹¶åˆ©ç”¨è¿™ç§æƒåŠ›æ¥ä¿ƒè¿›è‡ªå·±çš„åå¥½ã€‚ LLMé¢„è¨€æœºæœ€è¿‘è¢«æè®®ä½œä¸ºä¸€ç§åˆ†æ•£LLMçš„æœºåˆ¶ï¼Œé€šè¿‡æ‰§è¡Œæ¥è‡ªä¸åŒä¾›åº”å•†çš„å¤šä¸ªLLMå¹¶èšåˆå®ƒä»¬çš„è¾“å‡ºä»¥è·å¾—æ›´å¯é å’Œå€¼å¾—ä¿¡èµ–çš„æœ€ç»ˆç»“æœã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•çš„å‡†ç¡®æ€§å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºèšåˆæ–¹æ³•ã€‚å½“å‰çš„èšåˆæ–¹æ³•ä¸»è¦ä½¿ç”¨å„ç§LLMè¾“å‡ºä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œä¸é€‚åˆæœºå™¨äººä»»åŠ¡è§„åˆ’ï¼Œå…¶ä¸­ä»»åŠ¡çš„æ—¶é—´é¡ºåºå¾ˆé‡è¦ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ª LLM é¢„è¨€æœºï¼Œå®ƒå…·æœ‰ç”¨äºæœºå™¨äººä»»åŠ¡è§„åˆ’çš„æ–°èšåˆæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäº Hyperledger Fabric çš„å»ä¸­å¿ƒåŒ–å¤šæœºå™¨äººåŸºç¡€è®¾æ–½ï¼Œå¯ä»¥æ‰˜ç®¡æ‰€æå‡ºçš„é¢„è¨€æœºã€‚æ‰€æå‡ºçš„åŸºç¡€è®¾æ–½ä½¿ç”¨æˆ·èƒ½å¤Ÿå‘ç³»ç»Ÿè¡¨è¾¾ä»–ä»¬çš„è‡ªç„¶è¯­è¨€æ„å›¾ï¼Œç„¶åå¯ä»¥å°†å…¶åˆ†è§£ä¸ºå­ä»»åŠ¡ã€‚è¿™äº›å­ä»»åŠ¡éœ€è¦åè°ƒæ¥è‡ªä¸åŒä¾›åº”å•†çš„ä¸åŒæœºå™¨äººï¼ŒåŒæ—¶å¯¹æ•°æ®å®æ–½ç»†ç²’åº¦çš„è®¿é—®æ§åˆ¶ç®¡ç†ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬åˆ›å»ºäº† SkillChain-RTD åŸºå‡†å¹¶å…¬å¼€å‘å¸ƒã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¯æ˜äº†æ‰€æå‡ºçš„æ¶æ„çš„å¯è¡Œæ€§ï¼Œå¹¶ä¸”æ‰€æå‡ºçš„èšåˆæ–¹æ³•ä¼˜äºå½“å‰ä½¿ç”¨çš„å…¶ä»–èšåˆæ–¹æ³•ã€‚

</details>

---

## 282. Graph-Loc: Robust Graph-Based LiDAR Pose Tracking with Compact Structural Map Priors under Low Observability and Occlusion

**ä¸­æ–‡æ ‡é¢˜**: Graph-Locï¼šåœ¨ä½å¯è§‚æµ‹æ€§å’Œé®æŒ¡ä¸‹å…·æœ‰ç´§å‡‘ç»“æ„åœ°å›¾å…ˆéªŒçš„åŸºäºç¨³å¥å›¾çš„ LiDAR ä½å§¿è·Ÿè¸ª

**Date**: 2026-02-09 | **arXiv**: [2602.08417v1](http://arxiv.org/abs/2602.08417v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08417v1)

<details><summary><b>Abstract</b></summary>

Map-based LiDAR pose tracking is essential for long-term autonomous operation, where onboard map priors need be compact for scalable storage and fast retrieval, while online observations are often partial, repetitive, and heavily occluded. We propose Graph-Loc, a graph-based localization framework that tracks the platform pose against compact structural map priors represented as a lightweight point-line graph. Such priors can be constructed from heterogeneous sources commonly available in practice, including polygon outlines vectorized from occupancy/grid maps and CAD/model/floor-plan layouts. For each incoming LiDAR scan, Graph-Loc extracts sparse point and line primitives to form an observation graph, retrieves a pose-conditioned visible subgraph via LiDAR ray simulation, and performs scan-to-map association through unbalanced optimal transport with a local graph-context regularizer. The unbalanced formulation relaxes mass conservation, improving robustness to missing, spurious, and fragmented structures under occlusion. To enhance stability in low-observability segments, we estimate information anisotropy from the refinement normal matrix and defer updates along weakly constrained directions until sufficient constraints reappear. Experiments on public benchmarks, controlled stress tests, and real-world deployments demonstrate accurate and stable tracking with KB-level priors from heterogeneous map sources, including under geometrically degenerate and sustained occlusion and in the presence of gradual scene changes.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

åŸºäºåœ°å›¾çš„ LiDAR ä½å§¿è·Ÿè¸ªå¯¹äºé•¿æœŸè‡ªä¸»æ“ä½œè‡³å…³é‡è¦ï¼Œå…¶ä¸­æœºè½½åœ°å›¾å…ˆéªŒéœ€è¦ç´§å‡‘ï¼Œä»¥ä¾¿å¯æ‰©å±•å­˜å‚¨å’Œå¿«é€Ÿæ£€ç´¢ï¼Œè€Œåœ¨çº¿è§‚å¯Ÿé€šå¸¸æ˜¯éƒ¨åˆ†çš„ã€é‡å¤çš„å’Œä¸¥é‡é®æŒ¡çš„ã€‚æˆ‘ä»¬æå‡ºäº† Graph-Locï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå›¾çš„å®šä½æ¡†æ¶ï¼Œå¯ä»¥æ ¹æ®è¡¨ç¤ºä¸ºè½»é‡çº§ç‚¹çº¿å›¾çš„ç´§å‡‘ç»“æ„å›¾å…ˆéªŒæ¥è·Ÿè¸ªå¹³å°å§¿æ€ã€‚æ­¤ç±»å…ˆéªŒå¯ä»¥ä»å®è·µä¸­å¸¸è§çš„å¼‚æ„æºæ„å»ºï¼ŒåŒ…æ‹¬ä»å ç”¨/ç½‘æ ¼åœ°å›¾å’Œ CAD/æ¨¡å‹/å¹³é¢å›¾å¸ƒå±€çŸ¢é‡åŒ–çš„å¤šè¾¹å½¢è½®å»“ã€‚å¯¹äºæ¯æ¬¡ä¼ å…¥çš„ LiDAR æ‰«æï¼ŒGraph-Loc æå–ç¨€ç–ç‚¹å’Œçº¿åŸºå…ƒä»¥å½¢æˆè§‚å¯Ÿå›¾ï¼Œé€šè¿‡ LiDAR å°„çº¿æ¨¡æ‹Ÿæ£€ç´¢å§¿åŠ¿æ¡ä»¶å¯è§å­å›¾ï¼Œå¹¶é€šè¿‡å±€éƒ¨å›¾ä¸Šä¸‹æ–‡æ­£åˆ™åŒ–å™¨é€šè¿‡ä¸å¹³è¡¡æœ€ä½³ä¼ è¾“æ‰§è¡Œæ‰«æåˆ°åœ°å›¾å…³è”ã€‚ä¸å¹³è¡¡çš„å…¬å¼æ”¾æ¾äº†è´¨é‡å®ˆæ’ï¼Œæé«˜äº†é®æŒ¡ä¸‹ç¼ºå¤±ã€è™šå‡å’Œç¢ç‰‡ç»“æ„çš„é²æ£’æ€§ã€‚ä¸ºäº†å¢å¼ºä½å¯è§‚æµ‹æ€§æ®µçš„ç¨³å®šæ€§ï¼Œæˆ‘ä»¬ä»ç»†åŒ–æ³•çº¿çŸ©é˜µä¼°è®¡ä¿¡æ¯å„å‘å¼‚æ€§ï¼Œå¹¶æ¨è¿Ÿæ²¿å¼±çº¦æŸæ–¹å‘çš„æ›´æ–°ï¼Œç›´åˆ°é‡æ–°å‡ºç°è¶³å¤Ÿçš„çº¦æŸã€‚å…¬å…±åŸºå‡†æµ‹è¯•ã€å—æ§å‹åŠ›æµ‹è¯•å’Œç°å®ä¸–ç•Œéƒ¨ç½²çš„å®éªŒè¯æ˜äº†æ¥è‡ªå¼‚æ„åœ°å›¾æºçš„ KB çº§å…ˆéªŒçš„å‡†ç¡®å’Œç¨³å®šçš„è·Ÿè¸ªï¼ŒåŒ…æ‹¬åœ¨å‡ ä½•é€€åŒ–å’ŒæŒç»­é®æŒ¡ä»¥åŠåœºæ™¯é€æ¸å˜åŒ–çš„æƒ…å†µä¸‹ã€‚

</details>

---

## 283. BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models

**ä¸­æ–‡æ ‡é¢˜**: BiManiBenchï¼šè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åŒæ‰‹åè°ƒçš„åˆ†å±‚åŸºå‡†

**Date**: 2026-02-09 | **arXiv**: [2602.08392v1](http://arxiv.org/abs/2602.08392v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08392v1)

<details><summary><b>Abstract</b></summary>

Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI, and using them to benchmark robotic intelligence has become a pivotal trend. However, existing frameworks remain predominantly confined to single-arm manipulation, failing to capture the spatio-temporal coordination required for bimanual tasks like lifting a heavy pot. To address this, we introduce BiManiBench, a hierarchical benchmark evaluating MLLMs across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. Our framework isolates unique bimanual challenges, such as arm reachability and kinematic constraints, thereby distinguishing perceptual hallucinations from planning failures. Analysis of over 30 state-of-the-art models reveals that despite high-level reasoning proficiency, MLLMs struggle with dual-arm spatial grounding and control, frequently resulting in mutual interference and sequencing errors. These findings suggest the current paradigm lacks a deep understanding of mutual kinematic constraints, highlighting the need for future research to focus on inter-arm collision-avoidance and fine-grained temporal sequencing.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ (MLLM) æ˜¾ç€å…ˆè¿›äº†å…·ä½“äººå·¥æ™ºèƒ½ï¼Œä½¿ç”¨å®ƒä»¬æ¥è¡¡é‡æœºå™¨äººæ™ºèƒ½å·²æˆä¸ºä¸€ä¸ªå…³é”®è¶‹åŠ¿ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¡†æ¶ä»ç„¶ä¸»è¦å±€é™äºå•è‡‚æ“ä½œï¼Œæ— æ³•æ•æ‰åŒæ‰‹ä»»åŠ¡ï¼ˆä¾‹å¦‚ä¸¾é‡é”…ï¼‰æ‰€éœ€çš„æ—¶ç©ºåè°ƒã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº† BiManiBenchï¼Œè¿™æ˜¯ä¸€ä¸ªè·¨ä¸‰ä¸ªå±‚æ¬¡è¯„ä¼° MLLM çš„åˆ†å±‚åŸºå‡†ï¼šåŸºæœ¬ç©ºé—´æ¨ç†ã€é«˜çº§è¡ŒåŠ¨è§„åˆ’å’Œä½çº§æœ«ç«¯æ‰§è¡Œå™¨æ§åˆ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶éš”ç¦»äº†ç‹¬ç‰¹çš„åŒæ‰‹æŒ‘æˆ˜ï¼Œä¾‹å¦‚æ‰‹è‡‚å¯è¾¾æ€§å’Œè¿åŠ¨å­¦é™åˆ¶ï¼Œä»è€ŒåŒºåˆ†çŸ¥è§‰å¹»è§‰å’Œè®¡åˆ’å¤±è´¥ã€‚å¯¹ 30 å¤šä¸ªæœ€å…ˆè¿›æ¨¡å‹çš„åˆ†æè¡¨æ˜ï¼Œå°½ç®¡ MLLM å…·æœ‰é«˜æ°´å¹³çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒä»éš¾ä»¥åº”å¯¹åŒè‡‚ç©ºé—´æ¥åœ°å’Œæ§åˆ¶ï¼Œç»å¸¸å¯¼è‡´ç›¸äº’å¹²æ‰°å’Œæ’åºé”™è¯¯ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå½“å‰çš„èŒƒå¼ç¼ºä¹å¯¹ç›¸äº’è¿åŠ¨å­¦çº¦æŸçš„æ·±å…¥ç†è§£ï¼Œå¼ºè°ƒæœªæ¥ç ”ç©¶éœ€è¦å…³æ³¨è‡‚é—´ç¢°æ’é¿å…å’Œç»†ç²’åº¦æ—¶é—´æ’åºã€‚

</details>

---

## 284. Learning Human-Like Badminton Skills for Humanoid Robots

**ä¸­æ–‡æ ‡é¢˜**: ä»¿äººæœºå™¨äººå­¦ä¹ ç±»äººç¾½æ¯›çƒæŠ€èƒ½

**Date**: 2026-02-09 | **arXiv**: [2602.08370v1](http://arxiv.org/abs/2602.08370v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08370v1)

<details><summary><b>Abstract</b></summary>

Realizing versatile and human-like performance in high-demand sports like badminton remains a formidable challenge for humanoid robotics. Unlike standard locomotion or static manipulation, this task demands a seamless integration of explosive whole-body coordination and precise, timing-critical interception. While recent advances have achieved lifelike motion mimicry, bridging the gap between kinematic imitation and functional, physics-aware striking without compromising stylistic naturalness is non-trivial. To address this, we propose Imitation-to-Interaction, a progressive reinforcement learning framework designed to evolve a robot from a "mimic" to a capable "striker." Our approach establishes a robust motor prior from human data, distills it into a compact, model-based state representation, and stabilizes dynamics via adversarial priors. Crucially, to overcome the sparsity of expert demonstrations, we introduce a manifold expansion strategy that generalizes discrete strike points into a dense interaction volume. We validate our framework through the mastery of diverse skills, including lifts and drop shots, in simulation. Furthermore, we demonstrate the first zero-shot sim-to-real transfer of anthropomorphic badminton skills to a humanoid robot, successfully replicating the kinetic elegance and functional precision of human athletes in the physical world.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

å¯¹äºäººå½¢æœºå™¨äººæ¥è¯´ï¼Œåœ¨ç¾½æ¯›çƒç­‰é«˜è¦æ±‚è¿åŠ¨ä¸­å®ç°å¤šåŠŸèƒ½å’Œç±»äººçš„è¡¨ç°ä»ç„¶æ˜¯ä¸€ä¸ªè‰°å·¨çš„æŒ‘æˆ˜ã€‚ä¸æ ‡å‡†çš„è¿åŠ¨æˆ–é™æ€æ“çºµä¸åŒï¼Œè¿™é¡¹ä»»åŠ¡éœ€è¦çˆ†ç‚¸æ€§å…¨èº«åè°ƒå’Œç²¾ç¡®ã€æ—¶é—´å…³é”®çš„æ‹¦æˆªçš„æ— ç¼é›†æˆã€‚è™½ç„¶æœ€è¿‘çš„è¿›å±•å·²ç»å®ç°äº†é€¼çœŸçš„è¿åŠ¨æ¨¡ä»¿ï¼Œä½†åœ¨ä¸å½±å“é£æ ¼è‡ªç„¶æ€§çš„æƒ…å†µä¸‹å¼¥åˆè¿åŠ¨å­¦æ¨¡ä»¿å’ŒåŠŸèƒ½æ€§ç‰©ç†æ„ŸçŸ¥æ‰“å‡»ä¹‹é—´çš„å·®è·å¹¶éæ˜“äº‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ¨¡ä»¿åˆ°äº¤äº’ï¼Œè¿™æ˜¯ä¸€ç§æ¸è¿›å¼å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å°†æœºå™¨äººä»â€œæ¨¡ä»¿è€…â€è¿›åŒ–ä¸ºæœ‰èƒ½åŠ›çš„â€œå‰é”‹â€ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ ¹æ®äººç±»æ•°æ®å»ºç«‹äº†å¼ºå¤§çš„è¿åŠ¨å…ˆéªŒï¼Œå°†å…¶æç‚¼æˆç´§å‡‘çš„ã€åŸºäºæ¨¡å‹çš„çŠ¶æ€è¡¨ç¤ºï¼Œå¹¶é€šè¿‡å¯¹æŠ—æ€§å…ˆéªŒç¨³å®šåŠ¨æ€ã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼Œä¸ºäº†å…‹æœä¸“å®¶æ¼”ç¤ºçš„ç¨€ç–æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æµå½¢æ‰©å±•ç­–ç•¥ï¼Œå°†ç¦»æ•£çš„æ‰“å‡»ç‚¹æ¦‚æ‹¬ä¸ºå¯†é›†çš„äº¤äº’ä½“ç§¯ã€‚æˆ‘ä»¬é€šè¿‡åœ¨æ¨¡æ‹Ÿä¸­æŒæ¡å„ç§æŠ€èƒ½ï¼ˆåŒ…æ‹¬ä¸¾èµ·çƒå’ŒåŠçƒï¼‰æ¥éªŒè¯æˆ‘ä»¬çš„æ¡†æ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†é¦–æ¬¡å°†æ‹Ÿäººç¾½æ¯›çƒæŠ€èƒ½é›¶é•œå¤´æ¨¡æ‹Ÿåˆ°çœŸå®åœ°è½¬ç§»åˆ°äººå½¢æœºå™¨äººä¸Šï¼ŒæˆåŠŸåœ°å¤åˆ¶äº†äººç±»è¿åŠ¨å‘˜åœ¨ç°å®ä¸–ç•Œä¸­çš„è¿åŠ¨ä¼˜é›…å’ŒåŠŸèƒ½ç²¾ç¡®åº¦ã€‚

</details>

---

## 285. Controlled Flight of an Insect-Scale Flapping-Wing Robot via Integrated Onboard Sensing and Computation

**ä¸­æ–‡æ ‡é¢˜**: é€šè¿‡é›†æˆæœºè½½ä¼ æ„Ÿå’Œè®¡ç®—æ§åˆ¶æ˜†è™«è§„æ¨¡æ‰‘ç¿¼æœºå™¨äººçš„é£è¡Œ

**Date**: 2026-02-09 | **arXiv**: [2602.08328v1](http://arxiv.org/abs/2602.08328v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08328v1)

<details><summary><b>Abstract</b></summary>

Aerial insects can effortlessly navigate dense vegetation, whereas similarly sized aerial robots typically depend on offboard sensors and computation to maintain stable flight. This disparity restricts insect-scale robots to operation within motion capture environments, substantially limiting their applicability to tasks such as search-and-rescue and precision agriculture. In this work, we present a 1.29-gram aerial robot capable of hovering and tracking trajectories with solely onboard sensing and computation. The combination of a sensor suite, estimators, and a low-level controller achieved centimeter-scale positional flight accuracy. Additionally, we developed a hierarchical controller in which a human operator provides high-level commands to direct the robot's motion. In a 30-second flight experiment conducted outside a motion capture system, the robot avoided obstacles and ultimately landed on a sunflower. This level of sensing and computational autonomy represents a significant advancement for the aerial microrobotics community, further opening opportunities to explore onboard planning and power autonomy.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç©ºä¸­æ˜†è™«å¯ä»¥æ¯«ä¸è´¹åŠ›åœ°åœ¨èŒ‚å¯†çš„æ¤è¢«ä¸­å¯¼èˆªï¼Œè€Œç±»ä¼¼å¤§å°çš„ç©ºä¸­æœºå™¨äººé€šå¸¸ä¾é æœºå¤–ä¼ æ„Ÿå™¨å’Œè®¡ç®—æ¥ä¿æŒç¨³å®šçš„é£è¡Œã€‚è¿™ç§å·®å¼‚é™åˆ¶äº†æ˜†è™«çº§æœºå™¨äººåªèƒ½åœ¨åŠ¨ä½œæ•æ‰ç¯å¢ƒä¸­è¿è¡Œï¼Œä»è€Œæå¤§åœ°é™åˆ¶äº†å®ƒä»¬åœ¨æœç´¢æ•‘æ´å’Œç²¾å‡†å†œä¸šç­‰ä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€æ¬¾ 1.29 å…‹é‡çš„ç©ºä¸­æœºå™¨äººï¼Œèƒ½å¤Ÿä»…é€šè¿‡æœºè½½ä¼ æ„Ÿå’Œè®¡ç®—æ¥æ‚¬åœå’Œè·Ÿè¸ªè½¨è¿¹ã€‚ä¼ æ„Ÿå™¨å¥—ä»¶ã€ä¼°è®¡å™¨å’Œä½çº§æ§åˆ¶å™¨çš„ç»„åˆå®ç°äº†å˜ç±³çº§çš„ä½ç½®é£è¡Œç²¾åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åˆ†å±‚æ§åˆ¶å™¨ï¼Œå…¶ä¸­äººç±»æ“ä½œå‘˜æä¾›é«˜çº§å‘½ä»¤æ¥æŒ‡å¯¼æœºå™¨äººçš„è¿åŠ¨ã€‚åœ¨åŠ¨ä½œæ•æ‰ç³»ç»Ÿå¤–è¿›è¡Œçš„ 30 ç§’é£è¡Œå®éªŒä¸­ï¼Œæœºå™¨äººé¿å¼€äº†éšœç¢ç‰©ï¼Œæœ€ç»ˆé™è½åœ¨å‘æ—¥è‘µä¸Šã€‚è¿™ç§æ°´å¹³çš„ä¼ æ„Ÿå’Œè®¡ç®—è‡ªä¸»æ€§ä»£è¡¨äº†èˆªç©ºå¾®å‹æœºå™¨äººç¤¾åŒºçš„é‡å¤§è¿›æ­¥ï¼Œè¿›ä¸€æ­¥ä¸ºæ¢ç´¢æœºè½½è§„åˆ’å’ŒåŠ¨åŠ›è‡ªä¸»æ€§æä¾›äº†æœºä¼šã€‚

</details>

---

## 286. ReefFlex: A Generative Design Framework for Soft Robotic Grasping of Organic and Fragile objects

**ä¸­æ–‡æ ‡é¢˜**: ReefFlexï¼šç”¨äºè½¯æœºå™¨äººæŠ“å–æœ‰æœºå’Œæ˜“ç¢ç‰©ä½“çš„ç”Ÿæˆè®¾è®¡æ¡†æ¶

**Date**: 2026-02-09 | **arXiv**: [2602.08285v1](http://arxiv.org/abs/2602.08285v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08285v1)

<details><summary><b>Abstract</b></summary>

Climate change, invasive species and human activities are currently damaging the world's coral reefs at unprecedented rates, threatening their vast biodiversity and fisheries, and reducing coastal protection. Solving this vast challenge requires scalable coral regeneration technologies that can breed climate-resilient species and accelerate the natural regrowth processes; actions that are impeded by the absence of safe and robust tools to handle the fragile coral. We investigate ReefFlex, a generative soft finger design methodology that explores a diverse space of soft fingers to produce a set of candidates capable of safely grasping fragile and geometrically heterogeneous coral in a cluttered environment. Our key insight is encoding heterogeneous grasping into a reduced set of motion primitives, creating a simplified, tractable multi-objective optimisation problem. To evaluate the method, we design a soft robot for reef rehabilitation, which grows and manipulates coral in onshore aquaculture facilities for future reef out-planting. We demonstrate ReefFlex increases both grasp success and grasp quality (disturbance resistance, positioning accuracy) and reduces in adverse events encountered during coral manipulation compared to reference designs. ReefFlex, offers a generalisable method to design soft end-effectors for complex handling and paves a pathway towards automation in previously unachievable domains like coral handling for restoration.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æ°”å€™å˜åŒ–ã€å…¥ä¾µç‰©ç§å’Œäººç±»æ´»åŠ¨ç›®å‰æ­£ä»¥å‰æ‰€æœªæœ‰çš„é€Ÿåº¦ç ´åä¸–ç•ŒçŠç‘šç¤ï¼Œå¨èƒå…¶ä¸°å¯Œçš„ç”Ÿç‰©å¤šæ ·æ€§å’Œæ¸”ä¸šï¼Œå¹¶å‡å°‘æ²¿æµ·ä¿æŠ¤ã€‚è§£å†³è¿™ä¸€å·¨å¤§æŒ‘æˆ˜éœ€è¦å¯æ‰©å±•çš„çŠç‘šå†ç”ŸæŠ€æœ¯ï¼Œè¿™äº›æŠ€æœ¯å¯ä»¥åŸ¹è‚²é€‚åº”æ°”å€™å˜åŒ–çš„ç‰©ç§å¹¶åŠ é€Ÿè‡ªç„¶å†ç”Ÿè¿‡ç¨‹ï¼›ç”±äºç¼ºä¹å®‰å…¨å’Œå¼ºå¤§çš„å·¥å…·æ¥å¤„ç†è„†å¼±çš„çŠç‘šè€Œé˜»ç¢çš„è¡ŒåŠ¨ã€‚æˆ‘ä»¬ç ”ç©¶äº† ReefFlexï¼Œè¿™æ˜¯ä¸€ç§ç”Ÿæˆè½¯æ‰‹æŒ‡è®¾è®¡æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ¢ç´¢ä¸åŒçš„è½¯æ‰‹æŒ‡ç©ºé—´ï¼Œä»¥äº§ç”Ÿä¸€ç»„èƒ½å¤Ÿåœ¨æ‚ä¹±çš„ç¯å¢ƒä¸­å®‰å…¨åœ°æŠ“ä½è„†å¼±ä¸”å‡ ä½•å¼‚è´¨çŠç‘šçš„å€™é€‰çŠç‘šã€‚æˆ‘ä»¬çš„ä¸»è¦è§è§£æ˜¯å°†å¼‚æ„æŠ“å–ç¼–ç ä¸ºä¸€ç»„å‡å°‘çš„è¿åŠ¨åŸè¯­ï¼Œåˆ›å»ºä¸€ä¸ªç®€åŒ–çš„ã€æ˜“äºå¤„ç†çš„å¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ã€‚ä¸ºäº†è¯„ä¼°è¯¥æ–¹æ³•ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç”¨äºçŠç‘šç¤ä¿®å¤çš„è½¯æœºå™¨äººï¼Œå®ƒå¯ä»¥åœ¨é™†ä¸Šæ°´äº§å…»æ®–è®¾æ–½ä¸­ç”Ÿé•¿å’Œæ“çºµçŠç‘šï¼Œä»¥ä¾¿å°†æ¥è¿›è¡ŒçŠç‘šç¤å¤–æ¤ã€‚æˆ‘ä»¬è¯æ˜ï¼Œä¸å‚è€ƒè®¾è®¡ç›¸æ¯”ï¼ŒReefFlex æé«˜äº†æŠ“å–æˆåŠŸç‡å’ŒæŠ“å–è´¨é‡ï¼ˆæŠ—å¹²æ‰°æ€§ã€å®šä½ç²¾åº¦ï¼‰ï¼Œå¹¶å‡å°‘äº†çŠç‘šæ“ä½œè¿‡ç¨‹ä¸­é‡åˆ°çš„ä¸è‰¯äº‹ä»¶ã€‚ ReefFlex æä¾›äº†ä¸€ç§é€šç”¨çš„æ–¹æ³•æ¥è®¾è®¡ç”¨äºå¤æ‚å¤„ç†çš„è½¯æœ«ç«¯æ‰§è¡Œå™¨ï¼Œå¹¶ä¸ºä»¥å‰æ— æ³•â€‹â€‹å®ç°çš„é¢†åŸŸï¼ˆä¾‹å¦‚ç”¨äºä¿®å¤çš„çŠç‘šå¤„ç†ï¼‰çš„è‡ªåŠ¨åŒ–é“ºå¹³äº†é“è·¯ã€‚

</details>

---

## 287. Aerial Manipulation with Contact-Aware Onboard Perception and Hybrid Control

**ä¸­æ–‡æ ‡é¢˜**: å…·æœ‰æ¥è§¦æ„ŸçŸ¥æœºè½½æ„ŸçŸ¥å’Œæ··åˆæ§åˆ¶çš„ç©ºä¸­æ“çºµ

**Date**: 2026-02-09 | **arXiv**: [2602.08251v1](http://arxiv.org/abs/2602.08251v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08251v1)

<details><summary><b>Abstract</b></summary>

Aerial manipulation (AM) promises to move Unmanned Aerial Vehicles (UAVs) beyond passive inspection to contact-rich tasks such as grasping, assembly, and in-situ maintenance. Most prior AM demonstrations rely on external motion capture (MoCap) and emphasize position control for coarse interactions, limiting deployability. We present a fully onboard perception-control pipeline for contact-rich AM that achieves accurate motion tracking and regulated contact wrenches without MoCap. The main components are (1) an augmented visual-inertial odometry (VIO) estimator with contact-consistency factors that activate only during interaction, tightening uncertainty around the contact frame and reducing drift, and (2) image-based visual servoing (IBVS) to mitigate perception-control coupling, together with a hybrid force-motion controller that regulates contact wrenches and lateral motion for stable contact. Experiments show that our approach closes the perception-to-wrench loop using only onboard sensing, yielding an velocity estimation improvement of 66.01% at contact, reliable target approach, and stable force holding-pointing toward deployable, in-the-wild aerial manipulation.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

ç©ºä¸­æ“çºµ (AM) æœ‰æœ›ä½¿æ— äººæœº (UAV) è¶…è¶Šè¢«åŠ¨æ£€æŸ¥ï¼Œè½¬å‘æ‰§è¡ŒæŠ“å–ã€ç»„è£…å’Œç°åœºç»´æŠ¤ç­‰æ¥è§¦ä¸°å¯Œçš„ä»»åŠ¡ã€‚å¤§å¤šæ•°å…ˆå‰çš„å¢æåˆ¶é€ æ¼”ç¤ºéƒ½ä¾èµ–äºå¤–éƒ¨åŠ¨ä½œæ•æ‰ (MoCap)ï¼Œå¹¶å¼ºè°ƒç²—ç•¥äº¤äº’çš„ä½ç½®æ§åˆ¶ï¼Œä»è€Œé™åˆ¶äº†å¯éƒ¨ç½²æ€§ã€‚æˆ‘ä»¬ä¸ºæ¥è§¦ä¸°å¯Œçš„å¢æâ€‹â€‹åˆ¶é€ æä¾›äº†ä¸€ä¸ªå®Œå…¨æ¿è½½çš„æ„ŸçŸ¥æ§åˆ¶ç®¡é“ï¼Œå¯ä»¥åœ¨æ²¡æœ‰MoCapçš„æƒ…å†µä¸‹å®ç°ç²¾ç¡®çš„è¿åŠ¨è·Ÿè¸ªå’Œè°ƒèŠ‚æ¥è§¦æ‰³æ‰‹ã€‚ä¸»è¦ç»„ä»¶æ˜¯ï¼ˆ1ï¼‰å¢å¼ºè§†è§‰æƒ¯æ€§é‡Œç¨‹è®¡ï¼ˆVIOï¼‰ä¼°è®¡å™¨ï¼Œå…·æœ‰ä»…åœ¨äº¤äº’è¿‡ç¨‹ä¸­æ¿€æ´»çš„æ¥è§¦ä¸€è‡´æ€§å› ç´ ï¼Œæ”¶ç´§æ¥è§¦æ¡†æ¶å‘¨å›´çš„ä¸ç¡®å®šæ€§å¹¶å‡å°‘æ¼‚ç§»ï¼›ï¼ˆ2ï¼‰åŸºäºå›¾åƒçš„è§†è§‰ä¼ºæœï¼ˆIBVSï¼‰ä»¥å‡è½»æ„ŸçŸ¥æ§åˆ¶è€¦åˆï¼Œä»¥åŠæ··åˆåŠ›è¿åŠ¨æ§åˆ¶å™¨ï¼Œç”¨äºè°ƒèŠ‚æ¥è§¦æ‰³æ‰‹å’Œæ¨ªå‘è¿åŠ¨ä»¥å®ç°ç¨³å®šæ¥è§¦ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…ä½¿ç”¨æœºè½½ä¼ æ„Ÿæ¥é—­åˆæ„ŸçŸ¥åˆ°æ‰³æ‰‹ç¯è·¯ï¼Œæ¥è§¦æ—¶çš„é€Ÿåº¦ä¼°è®¡æé«˜äº† 66.01%ï¼Œå¯é çš„ç›®æ ‡æ¥è¿‘ï¼Œä»¥åŠç¨³å®šçš„åŠ›ä¿æŒæŒ‡å‘å¯éƒ¨ç½²çš„é‡å¤–ç©ºä¸­æ“çºµã€‚

</details>

---

## 288. STEP: Warm-Started Visuomotor Policies with Spatiotemporal Consistency Prediction

**ä¸­æ–‡æ ‡é¢˜**: æ­¥éª¤ï¼šå…·æœ‰æ—¶ç©ºä¸€è‡´æ€§é¢„æµ‹çš„çƒ­å¯åŠ¨è§†è§‰è¿åŠ¨ç­–ç•¥

**Date**: 2026-02-09 | **arXiv**: [2602.08245v1](http://arxiv.org/abs/2602.08245v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08245v1)

<details><summary><b>Abstract</b></summary>

Diffusion policies have recently emerged as a powerful paradigm for visuomotor control in robotic manipulation due to their ability to model the distribution of action sequences and capture multimodality. However, iterative denoising leads to substantial inference latency, limiting control frequency in real-time closed-loop systems. Existing acceleration methods either reduce sampling steps, bypass diffusion through direct prediction, or reuse past actions, but often struggle to jointly preserve action quality and achieve consistently low latency. In this work, we propose STEP, a lightweight spatiotemporal consistency prediction mechanism to construct high-quality warm-start actions that are both distributionally close to the target action and temporally consistent, without compromising the generative capability of the original diffusion policy. Then, we propose a velocity-aware perturbation injection mechanism that adaptively modulates actuation excitation based on temporal action variation to prevent execution stall especially for real-world tasks. We further provide a theoretical analysis showing that the proposed prediction induces a locally contractive mapping, ensuring convergence of action errors during diffusion refinement. We conduct extensive evaluations on nine simulated benchmarks and two real-world tasks. Notably, STEP with 2 steps can achieve an average 21.6% and 27.5% higher success rate than BRIDGER and DDIM on the RoboMimic benchmark and real-world tasks, respectively. These results demonstrate that STEP consistently advances the Pareto frontier of inference latency and success rate over existing methods.

</details>

<details><summary><b>ä¸­æ–‡æ‘˜è¦</b></summary>

æ‰©æ•£ç­–ç•¥æœ€è¿‘å·²æˆä¸ºæœºå™¨äººæ“ä½œä¸­è§†è§‰è¿åŠ¨æ§åˆ¶çš„å¼ºå¤§èŒƒä¾‹ï¼Œå› ä¸ºå®ƒä»¬èƒ½å¤Ÿå¯¹åŠ¨ä½œåºåˆ—çš„åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡å¹¶æ•è·å¤šæ¨¡æ€ã€‚ç„¶è€Œï¼Œè¿­ä»£å»å™ªä¼šå¯¼è‡´å¤§é‡çš„æ¨ç†å»¶è¿Ÿï¼Œä»è€Œé™åˆ¶äº†å®æ—¶é—­ç¯ç³»ç»Ÿä¸­çš„æ§åˆ¶é¢‘ç‡ã€‚ç°æœ‰çš„åŠ é€Ÿæ–¹æ³•è¦ä¹ˆå‡å°‘é‡‡æ ·æ­¥éª¤ï¼Œé€šè¿‡ç›´æ¥é¢„æµ‹ç»•è¿‡æ‰©æ•£ï¼Œè¦ä¹ˆé‡ç”¨è¿‡å»çš„åŠ¨ä½œï¼Œä½†é€šå¸¸å¾ˆéš¾å…±åŒä¿æŒåŠ¨ä½œè´¨é‡å¹¶å®ç°æŒç»­çš„ä½å»¶è¿Ÿã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†STEPï¼Œä¸€ç§è½»é‡çº§çš„æ—¶ç©ºä¸€è‡´æ€§é¢„æµ‹æœºåˆ¶ï¼Œç”¨äºæ„å»ºé«˜è´¨é‡çš„çƒ­å¯åŠ¨åŠ¨ä½œï¼Œè¯¥åŠ¨ä½œåœ¨åˆ†å¸ƒä¸Šæ¥è¿‘ç›®æ ‡åŠ¨ä½œå¹¶ä¸”åœ¨æ—¶é—´ä¸Šä¸€è‡´ï¼Œè€Œä¸æŸå®³åŸå§‹æ‰©æ•£ç­–ç•¥çš„ç”Ÿæˆèƒ½åŠ›ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€Ÿåº¦æ„ŸçŸ¥æ‰°åŠ¨æ³¨å…¥æœºåˆ¶ï¼Œè¯¥æœºåˆ¶æ ¹æ®æ—¶é—´åŠ¨ä½œå˜åŒ–è‡ªé€‚åº”åœ°è°ƒåˆ¶é©±åŠ¨æ¿€åŠ±ï¼Œä»¥é˜²æ­¢æ‰§è¡Œåœé¡¿ï¼Œç‰¹åˆ«æ˜¯å¯¹äºç°å®ä¸–ç•Œçš„ä»»åŠ¡ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æä¾›äº†ç†è®ºåˆ†æï¼Œè¡¨æ˜æ‰€æå‡ºçš„é¢„æµ‹ä¼šäº§ç”Ÿå±€éƒ¨æ”¶ç¼©æ˜ å°„ï¼Œç¡®ä¿æ‰©æ•£ç»†åŒ–æœŸé—´åŠ¨ä½œè¯¯å·®çš„æ”¶æ•›ã€‚æˆ‘ä»¬å¯¹ä¹ä¸ªæ¨¡æ‹ŸåŸºå‡†å’Œä¸¤ä¸ªå®é™…ä»»åŠ¡è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨ RoboMimic åŸºå‡†æµ‹è¯•å’Œå®é™…ä»»åŠ¡ä¸­ï¼Œ2 ä¸ªæ­¥éª¤çš„ STEP çš„æˆåŠŸç‡æ¯” BRIDGER å’Œ DDIM å¹³å‡é«˜å‡º 21.6% å’Œ 27.5%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒSTEP å§‹ç»ˆè¶…è¶Šç°æœ‰æ–¹æ³•çš„æ¨ç†å»¶è¿Ÿå’ŒæˆåŠŸç‡çš„å¸•ç´¯æ‰˜å‰æ²¿ã€‚

</details>

---



</details>

<details><summary><b>2026-02-11 (355 papers)</b></summary>

# arXiv Video Papers - 2026-02-11

**Update Time**: 2026-02-11 14:02:55

**Paper Count**: 355

---

## 1. SAGE: Scalable Agentic 3D Scene Generation for Embodied AI

**Chinese Title**: SAGE ï¼šé€‚ç”¨äºå…·ä½“AIçš„å¯æ‰©å±•Agentic 3Dåœºæ™¯ç”Ÿæˆ

**Authors**: Hongchi Xia, Xuan Li, Zhaoshuo Li, Qianli Ma, Jiashu Xu et al.

**Date**: 2026-02-10 | **arXiv**: [2602.10116v1](http://arxiv.org/abs/2602.10116v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10116v1)

**Project**: https://nvlabs.github.io/sage.  **Categories**: cs.CV, cs.RO

<details><summary><b>Abstract</b></summary>

Real-world data collection for embodied agents remains costly and unsafe, calling for scalable, realistic, and simulator-ready 3D environments. However, existing scene-generation systems often rely on rule-based or task-specific pipelines, yielding artifacts and physically invalid scenes. We present SAGE, an agentic framework that, given a user-specified embodied task (e.g., "pick up a bowl and place it on the table"), understands the intent and automatically generates simulation-ready environments at scale. The agent couples multiple generators for layout and object composition with critics that evaluate semantic plausibility, visual realism, and physical stability. Through iterative reasoning and adaptive tool selection, it self-refines the scenes until meeting user intent and physical validity. The resulting environments are realistic, diverse, and directly deployable in modern simulators for policy training. Policies trained purely on this data exhibit clear scaling trends and generalize to unseen objects and layouts, demonstrating the promise of simulation-driven scaling for embodied AI. Code, demos, and the SAGE-10k dataset can be found on the project page here: https://nvlabs.github.io/sage.

</details>

<details><summary><b>Chinese Abstract</b></summary>

å…·ä½“ä»£ç†çš„ç°å®ä¸–ç•Œæ•°æ®æ”¶é›†ä»ç„¶æ˜‚è´µä¸”ä¸å®‰å…¨ï¼Œéœ€è¦å¯æ‰©å±•ã€é€¼çœŸä¸”æ¨¡æ‹Ÿå™¨å°±ç»ªçš„3Dç¯å¢ƒã€‚ç„¶è€Œï¼Œç°æœ‰çš„åœºæ™¯ç”Ÿæˆç³»ç»Ÿé€šå¸¸ä¾èµ–äºåŸºäºè§„åˆ™æˆ–ç‰¹å®šä»»åŠ¡çš„ç®¡é“ï¼Œä»è€Œäº§ç”Ÿä¼ªå½±å’Œç‰©ç†ä¸Šæ— æ•ˆçš„åœºæ™¯ã€‚æˆ‘ä»¬å‘ˆç°SAGE ï¼Œè¿™æ˜¯ä¸€ä¸ªä»£ç†æ¡†æ¶ï¼Œç»™å®šç”¨æˆ·æŒ‡å®šçš„å…·ä½“ä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼Œ â€œæ‹¿èµ·ç¢—å¹¶å°†å…¶æ”¾åœ¨æ¡Œå­ä¸Šâ€ ï¼‰ ï¼Œäº†è§£æ„å›¾å¹¶è‡ªåŠ¨å¤§è§„æ¨¡ç”Ÿæˆæ¨¡æ‹Ÿå°±ç»ªç¯å¢ƒã€‚è¯¥ä»£ç†å°†å¤šä¸ªç”¨äºå¸ƒå±€å’Œå¯¹è±¡ç»„åˆçš„ç”Ÿæˆå™¨ä¸è¯„ä¼°è¯­ä¹‰åˆç†æ€§ã€è§†è§‰çœŸå®æ€§å’Œç‰©ç†ç¨³å®šæ€§çš„æ‰¹è¯„è€…ç›¸ç»“åˆã€‚é€šè¿‡è¿­ä»£æ¨ç†å’Œè‡ªé€‚åº”å·¥å…·é€‰æ‹©ï¼Œè‡ªæˆ‘å®Œå–„åœºæ™¯ï¼Œç›´åˆ°æ»¡è¶³ç”¨æˆ·æ„å›¾å’Œç‰©ç†æœ‰æ•ˆæ€§ã€‚ç”±æ­¤äº§ç”Ÿçš„ç¯å¢ƒé€¼çœŸã€å¤šæ ·ï¼Œå¹¶å¯ç›´æ¥éƒ¨ç½²åœ¨ç”¨äºæ”¿ç­–åŸ¹è®­çš„ç°ä»£æ¨¡æ‹Ÿå™¨ä¸­ã€‚çº¯ç²¹æ ¹æ®è¿™äº›æ•°æ®è®­ç»ƒçš„æ”¿ç­–è¡¨ç°å‡ºæ˜æ˜¾çš„æ‰©å±•è¶‹åŠ¿ï¼Œå¹¶æ¨å¹¿åˆ°çœ‹ä¸è§çš„å¯¹è±¡å’Œå¸ƒå±€ï¼Œå±•ç¤ºäº†æ¨¡æ‹Ÿé©±åŠ¨æ‰©å±•çš„å‰æ™¯ã€‚ä»£ç ã€æ¼”ç¤ºå’ŒSAGE-10kæ•°æ®é›†å¯ä»¥åœ¨é¡¹ç›®é¡µé¢ä¸Šæ‰¾åˆ°ï¼š https://nvlabs.github.io/sageã€‚

</details>

---

## 2. Quantum Multiple Rotation Averaging

**Chinese Title**: é‡å­å¤šæ—‹è½¬å¹³å‡

**Authors**: Shuteng Wang, Natacha Kuete Meli, Michael MÃ¶ller, Vladislav Golyanik

**Date**: 2026-02-10 | **arXiv**: [2602.10115v1](http://arxiv.org/abs/2602.10115v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10115v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Multiple rotation averaging (MRA) is a fundamental optimization problem in 3D vision and robotics that aims to recover globally consistent absolute rotations from noisy relative measurements. Established classical methods, such as L1-IRLS and Shonan, face limitations including local minima susceptibility and reliance on convex relaxations that fail to preserve the exact manifold geometry, leading to reduced accuracy in high-noise scenarios. We introduce IQARS (Iterative Quantum Annealing for Rotation Synchronization), the first algorithm that reformulates MRA as a sequence of local quadratic non-convex sub-problems executable on quantum annealers after binarization, to leverage inherent hardware advantages. IQARS removes convex relaxation dependence and better preserves non-Euclidean rotation manifold geometry while leveraging quantum tunneling and parallelism for efficient solution space exploration. We evaluate IQARS's performance on synthetic and real-world datasets. While current annealers remain in their nascent phase and only support solving problems of limited scale with constrained performance, we observed that IQARS on D-Wave annealers can already achieve ca. 12% higher accuracy than Shonan, i.e., the best-performing classical method evaluated empirically.

</details>

<details><summary><b>Chinese Abstract</b></summary>

å¤šæ—‹è½¬å¹³å‡ï¼ˆ MRA ï¼‰æ˜¯3Dè§†è§‰å’Œæœºå™¨äººæŠ€æœ¯ä¸­çš„ä¸€ä¸ªåŸºæœ¬ä¼˜åŒ–é—®é¢˜ï¼Œæ—¨åœ¨ä»å™ªå£°ç›¸å¯¹æµ‹é‡ä¸­æ¢å¤å…¨çƒä¸€è‡´çš„ç»å¯¹æ—‹è½¬ã€‚L1-IRLSå’Œæ¹˜å—ç­‰æ—¢å®šçš„ç»å…¸æ–¹æ³•é¢ä¸´å±€é™æ€§ï¼ŒåŒ…æ‹¬å±€éƒ¨æå°æ•æ„Ÿæ€§å’Œå¯¹å‡¸æ¾å¼›çš„ä¾èµ–ï¼Œè¿™äº›æ–¹æ³•æ— æ³•ä¿æŒç²¾ç¡®çš„æ­§ç®¡å‡ ä½•å½¢çŠ¶ï¼Œå¯¼è‡´é«˜å™ªå£°åœºæ™¯ä¸‹çš„å‡†ç¡®æ€§é™ä½ã€‚æˆ‘ä»¬å¼•å…¥äº†IQARS ï¼ˆ Iterative Quantum Annealing for Rotation Synchronization ï¼‰ ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†MRAé‡æ„ä¸ºäºŒå€¼åŒ–åé‡å­é€€ç«æœºä¸Šå¯æ‰§è¡Œçš„å±€éƒ¨äºŒæ¬¡éå‡¸å­é—®é¢˜åºåˆ—çš„ç®—æ³•ï¼Œä»¥åˆ©ç”¨å›ºæœ‰çš„ç¡¬ä»¶ä¼˜åŠ¿ã€‚IQARSæ¶ˆé™¤äº†å‡¸æ¾å¼›ä¾èµ–æ€§ï¼Œæ›´å¥½åœ°ä¿ç•™äº†éæ¬§æ°æ—‹è½¬æµå½¢å‡ ä½•ï¼ŒåŒæ—¶åˆ©ç”¨é‡å­éš§ç©¿å’Œå¹³è¡Œåº¦è¿›è¡Œæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆç©ºé—´æ¢ç´¢ã€‚æˆ‘ä»¬è¯„ä¼°IQARSåœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚è™½ç„¶ç›®å‰çš„é€€ç«æœºä»å¤„äºåˆæœŸé˜¶æ®µï¼Œä»…æ”¯æŒè§£å†³è§„æ¨¡æœ‰é™ä¸”æ€§èƒ½å—é™çš„é—®é¢˜ï¼Œä½†æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œ D-Waveé€€ç«æœºä¸Šçš„IQARSå·²ç»æ¯”æ¹˜å—ç²¾åº¦é«˜å‡ºçº¦12% ï¼Œå³é€šè¿‡ç»éªŒè¯„ä¼°çš„æœ€ä½³æ€§èƒ½ç»å…¸æ–¹æ³•ã€‚

</details>

---

## 3. ConsID-Gen: View-Consistent and Identity-Preserving Image-to-Video Generation

**Chinese Title**: ConsID-Gen ï¼šè§†å›¾ä¸€è‡´å’Œä¿ç•™èº«ä»½çš„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆ

**Authors**: Mingyang Wu, Ashirbad Mishra, Soumik Dey, Shuo Xing, Naveen Ravipati et al.

**Date**: 2026-02-10 | **arXiv**: [2602.10113v1](http://arxiv.org/abs/2602.10113v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10113v1)

**Project**: https://myangwu.github.io/ConsID-Gen.  **Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Image-to-Video generation (I2V) animates a static image into a temporally coherent video sequence following textual instructions, yet preserving fine-grained object identity under changing viewpoints remains a persistent challenge. Unlike text-to-video models, existing I2V pipelines often suffer from appearance drift and geometric distortion, artifacts we attribute to the sparsity of single-view 2D observations and weak cross-modal alignment. Here we address this problem from both data and model perspectives. First, we curate ConsIDVid, a large-scale object-centric dataset built with a scalable pipeline for high-quality, temporally aligned videos, and establish ConsIDVid-Bench, where we present a novel benchmarking and evaluation framework for multi-view consistency using metrics sensitive to subtle geometric and appearance deviations. We further propose ConsID-Gen, a view-assisted I2V generation framework that augments the first frame with unposed auxiliary views and fuses semantic and structural cues via a dual-stream visual-geometric encoder as well as a text-visual connector, yielding unified conditioning for a Diffusion Transformer backbone. Experiments across ConsIDVid-Bench demonstrate that ConsID-Gen consistently outperforms in multiple metrics, with the best overall performance surpassing leading video generation models like Wan2.1 and HunyuanVideo, delivering superior identity fidelity and temporal coherence under challenging real-world scenarios. We will release our model and dataset at https://myangwu.github.io/ConsID-Gen.

</details>

<details><summary><b>Chinese Abstract</b></summary>

å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆï¼ˆ I2V ï¼‰å°†é™æ€å›¾åƒæŒ‰ç…§æ–‡æœ¬æŒ‡ä»¤åŠ¨ç”»åŒ–ä¸ºæ—¶é—´è¿è´¯çš„è§†é¢‘åºåˆ—ï¼Œä½†åœ¨ä¸æ–­å˜åŒ–çš„è§†ç‚¹ä¸‹ä¿æŒç²¾ç»†çš„ç‰©ä½“èº«ä»½ä»ç„¶æ˜¯ä¸€ä¸ªæŒç»­çš„æŒ‘æˆ˜ã€‚ä¸æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ä¸åŒï¼Œç°æœ‰çš„I2Væµæ°´çº¿ç»å¸¸é­å—å¤–è§‚æ¼‚ç§»å’Œå‡ ä½•å¤±çœŸï¼Œæˆ‘ä»¬å°†å…¶å½’å› äºå•è§†å›¾2Dè§‚æµ‹çš„ç¨€ç–æ€§å’Œå¼±äº¤å‰æ¨¡æ€å¯¹é½ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»æ•°æ®å’Œæ¨¡å‹çš„è§’åº¦æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ç­–åˆ’äº†ConsIDVid ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„æ•°æ®é›†ï¼Œä¸ºé«˜è´¨é‡ã€æ—¶é—´å¯¹é½çš„è§†é¢‘æ„å»ºäº†å¯æ‰©å±•çš„ç®¡é“ï¼Œå¹¶å»ºç«‹äº†ConsIDVid-Bench ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨å¯¹ç»†å¾®çš„å‡ ä½•å’Œå¤–è§‚åå·®æ•æ„Ÿçš„æŒ‡æ ‡ï¼Œä¸ºå¤šè§†å›¾ä¸€è‡´æ€§æä¾›äº†ä¸€ä¸ªæ–°çš„åŸºå‡†å’Œè¯„ä¼°æ¡†æ¶ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ConsID-Gen ï¼Œè¿™æ˜¯ä¸€ç§è§†å›¾è¾…åŠ©I2Vç”Ÿæˆæ¡†æ¶ï¼Œå®ƒé€šè¿‡åŒæµè§†è§‰å‡ ä½•ç¼–ç å™¨å’Œæ–‡æœ¬-è§†è§‰è¿æ¥å™¨æ¥å¢å¼ºç¬¬ä¸€å¸§çš„è¾…åŠ©è§†å›¾ï¼Œå¹¶èåˆè¯­ä¹‰å’Œç»“æ„çº¿ç´¢ï¼Œä»è€Œä¸ºæ‰©æ•£å˜å‹å™¨éª¨å¹²ç½‘æä¾›ç»Ÿä¸€çš„æ¡ä»¶ã€‚ConsIDVid-Benchçš„å®éªŒè¡¨æ˜ï¼Œ ConsID-Genåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå§‹ç»ˆè¡¨ç°å‡ºè‰²ï¼Œæ•´ä½“æ€§èƒ½ä¼˜äºWan2.1å’ŒHunyuanVideoç­‰é¢†å…ˆçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç°å®åœºæ™¯ä¸‹æä¾›å“è¶Šçš„èº«ä»½ä¿çœŸåº¦å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å°†åœ¨https://myangwu.github.io/ConsID-Genä¸Šå‘å¸ƒæˆ‘ä»¬çš„æ¨¡å‹å’Œæ•°æ®é›†ã€‚

</details>

---

## 4. Olaf-World: Orienting Latent Actions for Video World Modeling

**Chinese Title**: Olaf-World ï¼šä¸ºè§†é¢‘ä¸–ç•Œå»ºæ¨¡å®šä½æ½œåœ¨åŠ¨ä½œ

**Authors**: Yuxin Jiang, Yuchao Gu, Ivor W. Tsang, Mike Zheng Shou

**Date**: 2026-02-10 | **arXiv**: [2602.10104v1](http://arxiv.org/abs/2602.10104v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10104v1)

**Categories**: cs.CV, cs.AI, cs.LG

<details><summary><b>Abstract</b></summary>

Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to extract control interfaces from unlabeled video, learned latents often fail to transfer across contexts: they entangle scene-specific cues and lack a shared coordinate system. This occurs because standard objectives operate only within each clip, providing no mechanism to align action semantics across contexts. Our key insight is that although actions are unobserved, their semantic effects are observable and can serve as a shared reference. We introduce Seq$Î”$-REPA, a sequence-level control-effect alignment objective that anchors integrated latent action to temporal feature differences from a frozen, self-supervised video encoder. Building on this, we present Olaf-World, a pipeline that pretrains action-conditioned video world models from large-scale passive video. Extensive experiments demonstrate that our method learns a more structured latent action space, leading to stronger zero-shot action transfer and more data-efficient adaptation to new control interfaces than state-of-the-art baselines.

</details>

<details><summary><b>Chinese Abstract</b></summary>

æ‰©å±•åŠ¨ä½œå¯æ§çš„ä¸–ç•Œæ¨¡å‹å—åˆ°åŠ¨ä½œæ ‡ç­¾ç¨€ç¼ºçš„é™åˆ¶ã€‚è™½ç„¶æ½œåœ¨åŠ¨ä½œå­¦ä¹ æœ‰æœ›ä»æœªæ ‡è®°çš„è§†é¢‘ä¸­æå–æ§åˆ¶æ¥å£ï¼Œä½†å­¦ä¹ åˆ°çš„æ½œåœ¨å†…å®¹å¾€å¾€æ— æ³•è·¨ä¸Šä¸‹æ–‡ä¼ é€’ï¼šå®ƒä»¬çº ç¼ äºåœºæ™¯ç‰¹å®šçš„çº¿ç´¢ï¼Œç¼ºä¹å…±äº«çš„åæ ‡ç³»ã€‚è¿™æ˜¯å› ä¸ºæ ‡å‡†ç›®æ ‡ä»…åœ¨æ¯ä¸ªå‰ªè¾‘ä¸­è¿è¡Œï¼Œæ²¡æœ‰æä¾›è·¨ä¸Šä¸‹æ–‡å¯¹é½åŠ¨ä½œè¯­ä¹‰çš„æœºåˆ¶ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œå°½ç®¡è¡ŒåŠ¨æœªè¢«è§‚å¯Ÿåˆ°ï¼Œä½†å…¶è¯­ä¹‰æ•ˆåº”æ˜¯å¯è§‚å¯Ÿåˆ°çš„ï¼Œå¯ä»¥ä½œä¸ºå…±äº«å‚è€ƒã€‚æˆ‘ä»¬å¼•å…¥äº†Seq $ Î” $ -REPA ï¼Œè¿™æ˜¯ä¸€ç§åºåˆ—æ°´å¹³çš„æ§åˆ¶æ•ˆåº”å¯¹é½ç›®æ ‡ï¼Œå°†æ½œåœ¨åŠ¨ä½œä¸å†»ç»“çš„è‡ªæˆ‘ç›‘ç£è§†é¢‘ç¼–ç å™¨çš„æ—¶é—´ç‰¹å¾å·®å¼‚é”šå®šåœ¨ä¸€èµ·ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å±•ç¤ºäº†Olaf-World ï¼Œè¿™æ˜¯ä¸€ä¸ªä»å¤§è§„æ¨¡è¢«åŠ¨è§†é¢‘ä¸­é¢„è®­ç»ƒåŠ¨ä½œè°ƒèŠ‚è§†é¢‘ä¸–ç•Œæ¨¡å‹çš„ç®¡é“ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å­¦ä¹ äº†æ›´ç»“æ„åŒ–çš„æ½œåœ¨åŠ¨ä½œç©ºé—´ï¼Œä»è€Œå®ç°äº†æ›´å¼ºçš„é›¶å°„åŠ¨ä½œä¼ è¾“å’Œæ›´é«˜æ•ˆçš„æ•°æ®é€‚åº”æ–°æ§åˆ¶æ¥å£ã€‚

</details>

---

## 5. VideoWorld 2: Learning Transferable Knowledge from Real-world Videos

**Chinese Title**: VideoWorld 2 ï¼šä»çœŸå®ä¸–ç•Œçš„è§†é¢‘ä¸­å­¦ä¹ å¯è½¬ç§»çš„çŸ¥è¯†

**Authors**: Zhongwei Ren, Yunchao Wei, Xiao Yu, Guixun Luo, Yao Zhao et al.

**Date**: 2026-02-10 | **arXiv**: [2602.10102v1](http://arxiv.org/abs/2602.10102v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10102v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Learning transferable knowledge from unlabeled video data and applying it in new environments is a fundamental capability of intelligent agents. This work presents VideoWorld 2, which extends VideoWorld and offers the first investigation into learning transferable knowledge directly from raw real-world videos. At its core, VideoWorld 2 introduces a dynamic-enhanced Latent Dynamics Model (dLDM) that decouples action dynamics from visual appearance: a pretrained video diffusion model handles visual appearance modeling, enabling the dLDM to learn latent codes that focus on compact and meaningful task-related dynamics. These latent codes are then modeled autoregressively to learn task policies and support long-horizon reasoning. We evaluate VideoWorld 2 on challenging real-world handcraft making tasks, where prior video generation and latent-dynamics models struggle to operate reliably. Remarkably, VideoWorld 2 achieves up to 70% improvement in task success rate and produces coherent long execution videos. In robotics, we show that VideoWorld 2 can acquire effective manipulation knowledge from the Open-X dataset, which substantially improves task performance on CALVIN. This study reveals the potential of learning transferable world knowledge directly from raw videos, with all code, data, and models to be open-sourced for further research.

</details>

<details><summary><b>Chinese Abstract</b></summary>

ä»æœªæ ‡è®°çš„è§†é¢‘æ•°æ®ä¸­å­¦ä¹ å¯è½¬ç§»çš„çŸ¥è¯†å¹¶å°†å…¶åº”ç”¨äºæ–°ç¯å¢ƒä¸­æ˜¯æ™ºèƒ½ä»£ç†çš„åŸºæœ¬èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†VideoWorld 2 ï¼Œå®ƒæ‰©å±•äº†VideoWorld ï¼Œå¹¶æä¾›äº†ç›´æ¥ä»åŸå§‹çœŸå®ä¸–ç•Œè§†é¢‘ä¸­å­¦ä¹ å¯è½¬ç§»çŸ¥è¯†çš„é¦–æ¬¡è°ƒæŸ¥ã€‚VideoWorld 2çš„æ ¸å¿ƒå¼•å…¥äº†åŠ¨æ€å¢å¼ºæ½œä¼åŠ¨åŠ›å­¦æ¨¡å‹ï¼ˆ dLDM ï¼‰ ï¼Œå°†åŠ¨ä½œåŠ¨åŠ›å­¦ä¸è§†è§‰å¤–è§‚åˆ†ç¦»ï¼šé¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹å¤„ç†è§†è§‰å¤–è§‚å»ºæ¨¡ï¼Œä½¿dLDMèƒ½å¤Ÿå­¦ä¹ ä¸“æ³¨äºç´§å‡‘å’Œæœ‰æ„ä¹‰çš„ä»»åŠ¡ç›¸å…³åŠ¨æ€çš„æ½œä¼ä»£ç ã€‚ç„¶åå¯¹è¿™äº›æ½œåœ¨ä»£ç è¿›è¡Œè‡ªå›å½’å»ºæ¨¡ï¼Œä»¥å­¦ä¹ ä»»åŠ¡ç­–ç•¥å¹¶æ”¯æŒé•¿è·ç¦»æ¨ç†ã€‚æˆ‘ä»¬é’ˆå¯¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ç°å®ä¸–ç•Œæ‰‹å·¥åˆ¶ä½œä»»åŠ¡å¯¹VideoWorld 2è¿›è¡Œè¯„ä¼°ï¼Œä¹‹å‰çš„è§†é¢‘ç”Ÿæˆå’Œæ½œåœ¨åŠ¨æ€æ¨¡å‹éš¾ä»¥å¯é è¿è¡Œã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ VideoWorld 2å°†ä»»åŠ¡æˆåŠŸç‡æé«˜äº†70% ï¼Œå¹¶åˆ¶ä½œäº†è¿è´¯çš„é•¿æ‰§è¡Œè§†é¢‘ã€‚åœ¨æœºå™¨äººå­¦ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†VideoWorld 2å¯ä»¥ä»Open-Xæ•°æ®é›†ä¸­è·å¾—æœ‰æ•ˆçš„æ“ä½œçŸ¥è¯†ï¼Œè¿™å¤§å¤§æé«˜äº†CALVINçš„ä»»åŠ¡æ€§èƒ½ã€‚è¿™é¡¹ç ”ç©¶æ­ç¤ºäº†ç›´æ¥ä»åŸå§‹è§†é¢‘ä¸­å­¦ä¹ å¯è½¬ç§»ä¸–ç•ŒçŸ¥è¯†çš„æ½œåŠ›ï¼Œæ‰€æœ‰ä»£ç ã€æ•°æ®å’Œæ¨¡å‹éƒ½å°†å¼€æºä»¥ä¾›è¿›ä¸€æ­¥ç ”ç©¶ã€‚

</details>

---

## 6. VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model

**Chinese Title**: VLA-JEPA ï¼šåˆ©ç”¨æ½œåœ¨ä¸–ç•Œæ¨¡å‹å¢å¼ºè§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹

**Authors**: Jingwen Sun, Wenyao Zhang, Zekun Qi, Shaojie Ren, Zezhi Liu et al.

**Date**: 2026-02-10 | **arXiv**: [2602.10098v1](http://arxiv.org/abs/2602.10098v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10098v1)

**Categories**: cs.RO, cs.CV

<details><summary><b>Abstract</b></summary>

Pretraining Vision-Language-Action (VLA) policies on internet-scale video is appealing, yet current latent-action objectives often learn the wrong thing: they remain anchored to pixel variation rather than action-relevant state transitions, making them vulnerable to appearance bias, nuisance motion, and information leakage. We introduce VLA-JEPA, a JEPA-style pretraining framework that sidesteps these pitfalls by design. The key idea is \emph{leakage-free state prediction}: a target encoder produces latent representations from future frames, while the student pathway sees only the current observation -- future information is used solely as supervision targets, never as input. By predicting in latent space rather than pixel space, VLA-JEPA learns dynamics abstractions that are robust to camera motion and irrelevant background changes. This yields a simple two-stage recipe -- JEPA pretraining followed by action-head fine-tuning -- without the multi-stage complexity of prior latent-action pipelines. Experiments on LIBERO, LIBERO-Plus, SimplerEnv and real-world manipulation tasks show that VLA-JEPA achieves consistent gains in generalization and robustness over existing methods.

</details>

<details><summary><b>Chinese Abstract</b></summary>

é’ˆå¯¹äº’è”ç½‘è§„æ¨¡è§†é¢‘çš„é¢„è®­ç»ƒè§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆ VLA ï¼‰æ”¿ç­–å¾ˆæœ‰å¸å¼•åŠ›ï¼Œä½†ç›®å‰çš„æ½œåœ¨è¡ŒåŠ¨ç›®æ ‡å¾€å¾€ä¼šå­¦åˆ°é”™è¯¯çš„ä¸œè¥¿ï¼šå®ƒä»¬ä»ç„¶å›ºå®šåœ¨åƒç´ å˜åŒ–ä¸Šï¼Œè€Œä¸æ˜¯ä¸è¡ŒåŠ¨ç›¸å…³çš„çŠ¶æ€è½¬æ¢ä¸Šï¼Œè¿™ä½¿å¾—å®ƒä»¬å®¹æ˜“å—åˆ°å¤–è§‚åè§ã€æ»‹æ‰°åŠ¨ä½œå’Œä¿¡æ¯æ³„éœ²çš„å½±å“ã€‚æˆ‘ä»¬æ¨å‡ºäº†VLA-JEPA ï¼Œè¿™æ˜¯ä¸€ç§JEPAé£æ ¼çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡è®¾è®¡é¿å¼€äº†è¿™äº›é™·é˜±ã€‚å…³é”®æ€æƒ³æ˜¯\ emph {æ— æ³„æ¼çŠ¶æ€é¢„æµ‹} ï¼šç›®æ ‡ç¼–ç å™¨ä»æœªæ¥çš„å¸§ä¸­äº§ç”Ÿæ½œåœ¨çš„è¡¨ç¤ºï¼Œè€Œå­¦ç”Ÿé€”å¾„åªçœ‹åˆ°å½“å‰çš„è§‚å¯Ÿç»“æœ-æœªæ¥çš„ä¿¡æ¯ä»…ç”¨ä½œç›‘ç£ç›®æ ‡ï¼Œä»ä¸ä½œä¸ºè¾“å…¥ã€‚é€šè¿‡åœ¨æ½œåœ¨ç©ºé—´è€Œä¸æ˜¯åƒç´ ç©ºé—´ä¸­è¿›è¡Œé¢„æµ‹ï¼Œ VLA-JEPAå¯ä»¥å­¦ä¹ å¯¹æ‘„åƒæœºè¿åŠ¨å’Œä¸ç›¸å…³çš„èƒŒæ™¯å˜åŒ–å…·æœ‰é²æ£’æ€§çš„åŠ¨æ€æŠ½è±¡ã€‚è¿™äº§ç”Ÿäº†ä¸€ä¸ªç®€å•çš„ä¸¤é˜¶æ®µé…æ–¹--JEPAé¢„è®­ç»ƒï¼Œç„¶åæ˜¯åŠ¨ä½œå¤´å¾®è°ƒ--æ²¡æœ‰å…ˆå‰æ½œä¼ä½œç”¨æµæ°´çº¿çš„å¤šé˜¶æ®µå¤æ‚æ€§ã€‚åœ¨LIBEROã€LIBERO-Plusã€SimplerEnvå’Œå®é™…æ“ä½œä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œ VLA-JEPAåœ¨æ³›åŒ–å’Œé²æ£’æ€§æ–¹é¢å–å¾—äº†ä¸€è‡´çš„å¢ç›Šã€‚

</details>

---

## 7. Causality in Video Diffusers is Separable from Denoising

**Chinese Title**: è§†é¢‘æ‰©æ•£å™¨çš„å› æœå…³ç³»ä¸é™å™ªæ˜¯åˆ†å¼€çš„

**Authors**: Xingjian Bai, Guande He, Zhengqi Li, Eli Shechtman, Xun Huang et al.

**Date**: 2026-02-10 | **arXiv**: [2602.10095v1](http://arxiv.org/abs/2602.10095v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10095v1)

**Categories**: cs.CV, cs.AI, cs.LG

<details><summary><b>Abstract</b></summary>

Causality -- referring to temporal, uni-directional cause-effect relationships between components -- underlies many complex generative processes, including videos, language, and robot trajectories. Current causal diffusion models entangle temporal reasoning with iterative denoising, applying causal attention across all layers, at every denoising step, and over the entire context. In this paper, we show that the causal reasoning in these models is separable from the multi-step denoising process. Through systematic probing of autoregressive video diffusers, we uncover two key regularities: (1) early layers produce highly similar features across denoising steps, indicating redundant computation along the diffusion trajectory; and (2) deeper layers exhibit sparse cross-frame attention and primarily perform intra-frame rendering. Motivated by these findings, we introduce Separable Causal Diffusion (SCD), a new architecture that explicitly decouples once-per-frame temporal reasoning, via a causal transformer encoder, from multi-step frame-wise rendering, via a lightweight diffusion decoder. Extensive experiments on both pretraining and post-training tasks across synthetic and real benchmarks show that SCD significantly improves throughput and per-frame latency while matching or surpassing the generation quality of strong causal diffusion baselines.

</details>

<details><summary><b>Chinese Abstract</b></summary>

å› æœå…³ç³»æ˜¯æŒ‡ç»„ä»¶ä¹‹é—´çš„æ—¶é—´ã€å•å‘å› æœå…³ç³»ï¼Œæ˜¯è®¸å¤šå¤æ‚ç”Ÿæˆè¿‡ç¨‹çš„åŸºç¡€ï¼ŒåŒ…æ‹¬è§†é¢‘ã€è¯­è¨€å’Œæœºå™¨äººè½¨è¿¹ã€‚å½“å‰çš„å› æœæ‰©æ•£æ¨¡å‹å°†æ—¶é—´æ¨ç†ä¸è¿­ä»£å»å™ªçº ç¼ åœ¨ä¸€èµ·ï¼Œå°†å› æœæ³¨æ„åŠ›åº”ç”¨äºæ‰€æœ‰å±‚ã€æ¯ä¸ªå»å™ªæ­¥éª¤å’Œæ•´ä¸ªä¸Šä¸‹æ–‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™äº›æ¨¡å‹ä¸­çš„å› æœæ¨ç†ä¸å¤šæ­¥å»å™ªè¿‡ç¨‹æ˜¯å¯åˆ†ç¦»çš„ã€‚é€šè¿‡å¯¹è‡ªå›å½’è§†é¢‘æ‰©æ•£å™¨çš„ç³»ç»Ÿæ¢æµ‹ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸¤ä¸ªå…³é”®è§„å¾‹ï¼š ï¼ˆ 1 ï¼‰æ—©æœŸå±‚åœ¨å»å™ªæ­¥éª¤ä¸­äº§ç”Ÿé«˜åº¦ç›¸ä¼¼çš„ç‰¹å¾ï¼Œè¡¨æ˜æ²¿æ‰©æ•£è½¨è¿¹çš„å†—ä½™è®¡ç®—ï¼› ï¼ˆ 2 ï¼‰æ›´æ·±å±‚è¡¨ç°å‡ºç¨€ç–çš„è·¨å¸§æ³¨æ„åŠ›ï¼Œä¸»è¦æ‰§è¡Œå¸§å†…æ¸²æŸ“ã€‚åœ¨è¿™äº›å‘ç°çš„æ¿€åŠ±ä¸‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯åˆ†ç¦»å› æœæ‰©æ•£ï¼ˆ SCD ï¼‰ ï¼Œè¿™æ˜¯ä¸€ç§æ–°æ¶æ„ï¼Œé€šè¿‡å› æœå˜å‹å™¨ç¼–ç å™¨é€šè¿‡è½»é‡çº§æ‰©æ•£è§£ç å™¨å°†æ¯å¸§ä¸€æ¬¡çš„æ—¶é—´æ¨ç†ä¸å¤šæ­¥é€å¸§æ¸²æŸ“æ˜¾å¼åˆ†ç¦»ã€‚é’ˆå¯¹åˆæˆåŸºå‡†å’ŒçœŸå®åŸºå‡†çš„é¢„è®­ç»ƒå’Œè®­ç»ƒåä»»åŠ¡çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œ SCDæ˜¾è‘—æé«˜äº†ååé‡å’Œæ¯å¸§å»¶è¿Ÿï¼ŒåŒæ—¶åŒ¹é…æˆ–è¶…è¿‡äº†å¼ºå› æœæ‰©æ•£åŸºå‡†çš„ç”Ÿæˆè´¨é‡ã€‚

</details>

---

## 8. 4RC: 4D Reconstruction via Conditional Querying Anytime and Anywhere

**Chinese Title**: 4RC ï¼šéšæ—¶éšåœ°é€šè¿‡æ¡ä»¶æŸ¥è¯¢è¿›è¡Œ4Dé‡å»º

**Authors**: Yihang Luo, Shangchen Zhou, Yushi Lan, Xingang Pan, Chen Change Loy

**Date**: 2026-02-10 | **arXiv**: [2602.10094v1](http://arxiv.org/abs/2602.10094v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10094v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

We present 4RC, a unified feed-forward framework for 4D reconstruction from monocular videos. Unlike existing approaches that typically decouple motion from geometry or produce limited 4D attributes such as sparse trajectories or two-view scene flow, 4RC learns a holistic 4D representation that jointly captures dense scene geometry and motion dynamics. At its core, 4RC introduces a novel encode-once, query-anywhere and anytime paradigm: a transformer backbone encodes the entire video into a compact spatio-temporal latent space, from which a conditional decoder can efficiently query 3D geometry and motion for any query frame at any target timestamp. To facilitate learning, we represent per-view 4D attributes in a minimally factorized form by decomposing them into base geometry and time-dependent relative motion. Extensive experiments demonstrate that 4RC outperforms prior and concurrent methods across a wide range of 4D reconstruction tasks.

</details>

<details><summary><b>Chinese Abstract</b></summary>

æˆ‘ä»¬å±•ç¤ºäº†4RC ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºä»å•ç›®è§†é¢‘è¿›è¡Œ4Dé‡å»ºçš„ç»Ÿä¸€å‰é¦ˆæ¡†æ¶ã€‚ä¸é€šå¸¸å°†è¿åŠ¨ä¸å‡ ä½•ä½“è§£è€¦æˆ–äº§ç”Ÿæœ‰é™çš„4Då±æ€§ï¼ˆå¦‚ç¨€ç–è½¨è¿¹æˆ–åŒè§†å›¾åœºæ™¯æµï¼‰çš„ç°æœ‰æ–¹æ³•ä¸åŒï¼Œ 4RCå­¦ä¹ è”åˆæ•è·å¯†é›†åœºæ™¯å‡ ä½•ä½“å’Œè¿åŠ¨åŠ¨æ€çš„æ•´ä½“4Dè¡¨ç¤ºã€‚ä»æœ¬è´¨ä¸Šè®²ï¼Œ 4RCå¼•å…¥äº†ä¸€ç§æ–°çš„ä¸€æ¬¡ç¼–ç ã€éšæ—¶éšåœ°æŸ¥è¯¢èŒƒå¼ï¼šå˜å‹å™¨ä¸»å¹²å°†æ•´ä¸ªè§†é¢‘ç¼–ç æˆä¸€ä¸ªç´§å‡‘çš„æ—¶ç©ºæ½œä¼ç©ºé—´ï¼Œæ¡ä»¶è§£ç å™¨å¯ä»¥ä»ä¸­é«˜æ•ˆåœ°æŸ¥è¯¢3Då‡ ä½•å’Œè¿åŠ¨ï¼Œä»¥åœ¨ä»»ä½•ç›®æ ‡æ—¶é—´æˆ³ä¸‹æŸ¥è¯¢ä»»ä½•æŸ¥è¯¢å¸§ã€‚ä¸ºäº†ä¾¿äºå­¦ä¹ ï¼Œæˆ‘ä»¬é€šè¿‡å°†æ¯ä¸ªè§†å›¾çš„4Då±æ€§åˆ†è§£ä¸ºåŸºæœ¬å‡ ä½•å½¢çŠ¶å’Œæ—¶é—´ç›¸å…³çš„ç›¸å¯¹è¿åŠ¨ï¼Œä»¥æœ€å°åˆ†è§£çš„å½¢å¼è¡¨ç¤ºè¿™äº›å±æ€§ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œ 4RCåœ¨å¹¿æ³›çš„4Dé‡å»ºä»»åŠ¡ä¸­ä¼˜äºå…ˆå‰å’Œå¹¶å‘æ–¹æ³•ã€‚

</details>

---

## 9. Can Image Splicing and Copy-Move Forgery Be Detected by the Same Model? Forensim: An Attention-Based State-Space Approach

**Chinese Title**: åŒä¸€æ¨¡å‹æ˜¯å¦å¯ä»¥æ£€æµ‹åˆ°å›¾åƒæ‹¼æ¥å’Œå¤åˆ¶-ç§»åŠ¨ä¼ªé€ ï¼Ÿ Forensim ï¼šåŸºäºæ³¨æ„åŠ›çš„çŠ¶æ€ç©ºé—´æ–¹æ³•

**Authors**: Soumyaroop Nandi, Prem Natarajan

**Date**: 2026-02-10 | **arXiv**: [2602.10079v1](http://arxiv.org/abs/2602.10079v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10079v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

We introduce Forensim, an attention-based state-space framework for image forgery detection that jointly localizes both manipulated (target) and source regions. Unlike traditional approaches that rely solely on artifact cues to detect spliced or forged areas, Forensim is designed to capture duplication patterns crucial for understanding context. In scenarios such as protest imagery, detecting only the forged region, for example a duplicated act of violence inserted into a peaceful crowd, can mislead interpretation, highlighting the need for joint source-target localization. Forensim outputs three-class masks (pristine, source, target) and supports detection of both splicing and copy-move forgeries within a unified architecture. We propose a visual state-space model that leverages normalized attention maps to identify internal similarities, paired with a region-based block attention module to distinguish manipulated regions. This design enables end-to-end training and precise localization. Forensim achieves state-of-the-art performance on standard benchmarks. We also release CMFD-Anything, a new dataset addressing limitations of existing copy-move forgery datasets.

</details>

<details><summary><b>Chinese Abstract</b></summary>

æˆ‘ä»¬ä»‹ç»äº†Forensim ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå›¾åƒä¼ªé€ æ£€æµ‹çš„åŸºäºæ³¨æ„åŠ›çš„çŠ¶æ€ç©ºé—´æ¡†æ¶ï¼Œå¯è”åˆå®šä½è¢«æ“çºµï¼ˆç›®æ ‡ï¼‰å’ŒæºåŒºåŸŸã€‚ä¸ä»…ä¾é ä¼ªåƒæç¤ºæ¥æ£€æµ‹æ‹¼æ¥æˆ–ä¼ªé€ åŒºåŸŸçš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œ Forensimæ—¨åœ¨æ•è·å¯¹ç†è§£ä¸Šä¸‹æ–‡è‡³å…³é‡è¦çš„é‡å¤æ¨¡å¼ã€‚åœ¨æŠ—è®®å›¾åƒç­‰åœºæ™¯ä¸­ï¼Œä»…æ£€æµ‹ä¼ªé€ çš„åŒºåŸŸï¼Œä¾‹å¦‚æ’å…¥å’Œå¹³äººç¾¤ä¸­çš„é‡å¤æš´åŠ›è¡Œä¸ºï¼Œå¯èƒ½ä¼šè¯¯å¯¼è§£é‡Šï¼Œçªå‡ºäº†è”åˆæº-ç›®æ ‡æœ¬åœ°åŒ–çš„å¿…è¦æ€§ã€‚Forensimè¾“å‡ºä¸‰ç±»æ©ç ï¼ˆåŸå§‹ã€æºã€ç›®æ ‡ï¼‰ ï¼Œå¹¶æ”¯æŒåœ¨ç»Ÿä¸€æ¶æ„å†…æ£€æµ‹æ‹¼æ¥å’Œå¤åˆ¶ç§»åŠ¨ä¼ªé€ ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è§†è§‰çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨å½’ä¸€åŒ–çš„æ³¨æ„åŠ›å›¾æ¥è¯†åˆ«å†…éƒ¨ç›¸ä¼¼æ€§ï¼Œå¹¶ä¸åŸºäºåŒºåŸŸçš„å—æ³¨æ„åŠ›æ¨¡å—é…å¯¹ï¼Œä»¥åŒºåˆ†è¢«æ“çºµçš„åŒºåŸŸã€‚è¿™ç§è®¾è®¡å¯ä»¥å®ç°ç«¯åˆ°ç«¯çš„åŸ¹è®­å’Œç²¾ç¡®çš„æœ¬åœ°åŒ–ã€‚Forensimåœ¨æ ‡å‡†åŸºå‡†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†CMFD-Anything ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œè§£å†³äº†ç°æœ‰å¤åˆ¶ç§»åŠ¨ä¼ªé€ æ•°æ®é›†çš„å±€é™æ€§ã€‚

</details>

---

## 10. Spatio-Temporal Attention for Consistent Video Semantic Segmentation in Automated Driving

**Chinese Title**: è‡ªåŠ¨é©¾é©¶ä¸­ä¸€è‡´è§†é¢‘è¯­ä¹‰åˆ†å‰²çš„æ—¶ç©ºæ³¨æ„åŠ›

**Authors**: Serin Varghese, Kevin Ross, Fabian Hueger, Kira Maag

**Date**: 2026-02-10 | **arXiv**: [2602.10052v1](http://arxiv.org/abs/2602.10052v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10052v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Deep neural networks, especially transformer-based architectures, have achieved remarkable success in semantic segmentation for environmental perception. However, existing models process video frames independently, thus failing to leverage temporal consistency, which could significantly improve both accuracy and stability in dynamic scenes. In this work, we propose a Spatio-Temporal Attention (STA) mechanism that extends transformer attention blocks to incorporate multi-frame context, enabling robust temporal feature representations for video semantic segmentation. Our approach modifies standard self-attention to process spatio-temporal feature sequences while maintaining computational efficiency and requiring minimal changes to existing architectures. STA demonstrates broad applicability across diverse transformer architectures and remains effective across both lightweight and larger-scale models. A comprehensive evaluation on the Cityscapes and BDD100k datasets shows substantial improvements of 9.20 percentage points in temporal consistency metrics and up to 1.76 percentage points in mean intersection over union compared to single-frame baselines. These results demonstrate STA as an effective architectural enhancement for video-based semantic segmentation applications.

</details>

<details><summary><b>Chinese Abstract</b></summary>

æ·±åº¦ç¥ç»ç½‘ç»œï¼Œç‰¹åˆ«æ˜¯åŸºäºå˜å‹å™¨çš„æ¶æ„ï¼Œåœ¨ç¯å¢ƒæ„ŸçŸ¥çš„è¯­ä¹‰åˆ†å‰²æ–¹é¢å–å¾—äº†æ˜¾ç€çš„æˆåŠŸã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹ç‹¬ç«‹å¤„ç†è§†é¢‘å¸§ï¼Œå› æ­¤æ— æ³•åˆ©ç”¨æ—¶é—´ä¸€è‡´æ€§ï¼Œè¿™å¯ä»¥æ˜¾ç€æé«˜åŠ¨æ€åœºæ™¯ä¸­çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ—¶ç©ºæ³¨æ„ï¼ˆ STA ï¼‰æœºåˆ¶ï¼Œè¯¥æœºåˆ¶æ‰©å±•äº†å˜å‹å™¨æ³¨æ„å—ä»¥ç»“åˆå¤šå¸§ä¸Šä¸‹æ–‡ï¼Œä»è€Œä¸ºè§†é¢‘è¯­ä¹‰åˆ†å‰²æä¾›é²æ£’çš„æ—¶é—´ç‰¹å¾è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¿®æ”¹äº†å¤„ç†æ—¶ç©ºç‰¹å¾åºåˆ—çš„æ ‡å‡†è‡ªæˆ‘æ³¨æ„åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ï¼Œå¹¶éœ€è¦å¯¹ç°æœ‰æ¶æ„è¿›è¡Œæœ€å°çš„æ›´æ”¹ã€‚STAåœ¨å„ç§å˜å‹å™¨æ¶æ„ä¸­å…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§ï¼Œå¹¶ä¸”åœ¨è½»é‡çº§å’Œå¤§å‹æ¨¡å‹ä¸­ä»ç„¶æœ‰æ•ˆã€‚å¯¹åŸå¸‚æ™¯è§‚å’ŒBDD100kæ•°æ®é›†çš„ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼Œä¸å•å¸§åŸºçº¿ç›¸æ¯”ï¼Œæ—¶é—´ä¸€è‡´æ€§æŒ‡æ ‡å¤§å¹…æé«˜äº†9.20ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹³å‡äº¤é›†æé«˜äº†1.76ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œ STAæ˜¯åŸºäºè§†é¢‘çš„è¯­ä¹‰åˆ†å‰²åº”ç”¨çš„æœ‰æ•ˆæ¶æ„å¢å¼ºã€‚

</details>

---

## 11. Fake-HR1: Rethinking reasoning of vision language model for synthetic image detection

**Chinese Title**: Fake-HR1 ï¼šé‡æ–°æ€è€ƒåˆæˆå›¾åƒæ£€æµ‹è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†

**Authors**: Changjiang Jiang, Xinkuan Sha, Fengchang Yu, Jingjing Liu, Jian Liu et al.

**Date**: 2026-02-10 | **arXiv**: [2602.10042v1](http://arxiv.org/abs/2602.10042v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10042v1)

**Categories**: cs.CV, cs.AI

<details><summary><b>Abstract</b></summary>

Recent studies have demonstrated that incorporating Chain-of-Thought (CoT) reasoning into the detection process can enhance a model's ability to detect synthetic images. However, excessively lengthy reasoning incurs substantial resource overhead, including token consumption and latency, which is particularly redundant when handling obviously generated forgeries. To address this issue, we propose Fake-HR1, a large-scale hybrid-reasoning model that, to the best of our knowledge, is the first to adaptively determine whether reasoning is necessary based on the characteristics of the generative detection task. To achieve this, we design a two-stage training framework: we first perform Hybrid Fine-Tuning (HFT) for cold-start initialization, followed by online reinforcement learning with Hybrid-Reasoning Grouped Policy Optimization (HGRPO) to implicitly learn when to select an appropriate reasoning mode. Experimental results show that Fake-HR1 adaptively performs reasoning across different types of queries, surpassing existing LLMs in both reasoning ability and generative detection performance, while significantly improving response efficiency.

</details>

<details><summary><b>Chinese Abstract</b></summary>

æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå°†æ€ç»´é“¾ï¼ˆ CoT ï¼‰æ¨ç†çº³å…¥æ£€æµ‹è¿‡ç¨‹å¯ä»¥å¢å¼ºæ¨¡å‹æ£€æµ‹åˆæˆå›¾åƒçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿‡é•¿çš„æ¨ç†ä¼šäº§ç”Ÿå¤§é‡çš„èµ„æºå¼€é”€ï¼ŒåŒ…æ‹¬ä»¤ç‰Œæ¶ˆè€—å’Œå»¶è¿Ÿï¼Œè¿™åœ¨å¤„ç†æ˜æ˜¾ç”Ÿæˆçš„ä¼ªé€ æ—¶ç‰¹åˆ«å¤šä½™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤§è§„æ¨¡æ··åˆæ¨ç†æ¨¡å‹Fake-HR1 ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªæ ¹æ®ç”Ÿæˆæ£€æµ‹ä»»åŠ¡çš„ç‰¹å¾è‡ªé€‚åº”ç¡®å®šæ¨ç†æ˜¯å¦å¿…è¦çš„æ¨¡å‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è®­ç»ƒæ¡†æ¶ï¼šé¦–å…ˆæ‰§è¡Œæ··åˆå¾®è°ƒï¼ˆ HFT ï¼‰è¿›è¡Œå†·å¯åŠ¨åˆå§‹åŒ–ï¼Œç„¶åä½¿ç”¨æ··åˆæ¨ç†åˆ†ç»„ç­–ç•¥ä¼˜åŒ–ï¼ˆ HGRPO ï¼‰è¿›è¡Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œä»¥éšå¼å­¦ä¹ ä½•æ—¶é€‰æ‹©åˆé€‚çš„æ¨ç†æ¨¡å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ Fake-HR1åœ¨ä¸åŒç±»å‹çš„æŸ¥è¯¢ä¸­è‡ªé€‚åº”åœ°æ‰§è¡Œæ¨ç†ï¼Œåœ¨æ¨ç†èƒ½åŠ›å’Œç”Ÿæˆæ£€æµ‹æ€§èƒ½ä¸Šéƒ½è¶…è¿‡äº†ç°æœ‰çš„LLM ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†å“åº”æ•ˆç‡ã€‚

</details>

---

## 12. Faster-GS: Analyzing and Improving Gaussian Splatting Optimization

**Chinese Title**: æ›´å¿«çš„GS ï¼šåˆ†æå’Œæ”¹è¿›é«˜æ–¯æ–‘ç‚¹ä¼˜åŒ–

**Authors**: Florian Hahlbohm, Linus Franke, Martin Eisemann, Marcus Magnor

**Date**: 2026-02-10 | **arXiv**: [2602.09999v1](http://arxiv.org/abs/2602.09999v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09999v1)

**Categories**: cs.CV, cs.GR

<details><summary><b>Abstract</b></summary>

Recent advances in 3D Gaussian Splatting (3DGS) have focused on accelerating optimization while preserving reconstruction quality. However, many proposed methods entangle implementation-level improvements with fundamental algorithmic modifications or trade performance for fidelity, leading to a fragmented research landscape that complicates fair comparison. In this work, we consolidate and evaluate the most effective and broadly applicable strategies from prior 3DGS research and augment them with several novel optimizations. We further investigate underexplored aspects of the framework, including numerical stability, Gaussian truncation, and gradient approximation. The resulting system, Faster-GS, provides a rigorously optimized algorithm that we evaluate across a comprehensive suite of benchmarks. Our experiments demonstrate that Faster-GS achieves up to 5$\times$ faster training while maintaining visual quality, establishing a new cost-effective and resource efficient baseline for 3DGS optimization. Furthermore, we demonstrate that optimizations can be applied to 4D Gaussian reconstruction, leading to efficient non-rigid scene optimization.

</details>

<details><summary><b>Chinese Abstract</b></summary>

3Dé«˜æ–¯æ‹¼æ¥ï¼ˆ 3DGS ï¼‰çš„æœ€æ–°è¿›å±•ä¾§é‡äºåŠ é€Ÿä¼˜åŒ–ï¼ŒåŒæ—¶ä¿æŒé‡å»ºè´¨é‡ã€‚ç„¶è€Œï¼Œè®¸å¤šæå‡ºçš„æ–¹æ³•å°†å®ç°å±‚é¢çš„æ”¹è¿›ä¸åŸºæœ¬çš„ç®—æ³•ä¿®æ”¹æˆ–äº¤æ˜“æ€§èƒ½çº ç¼ åœ¨ä¸€èµ·ï¼Œå¯¼è‡´ç ”ç©¶ç¯å¢ƒåˆ†æ•£ï¼Œä½¿å…¬å¹³æ¯”è¾ƒå˜å¾—å¤æ‚ã€‚In this work, we consolidate and evaluate the most effective and broadly applicable strategies from prior 3DGS research and augment them with several novel optimizations. We further investigate underexplored aspects of the framework, including numerical stability, Gaussian truncation, and gradient approximation. The resulting system, Faster-GS, provides a rigorously optimized algorithm that we evaluate across a comprehensive suite of benchmarks.Our experiments demonstrate that Faster-GS achieves up to 5$\times$ faster training while maintaining visual quality, establishing a new cost-effective and resource efficient baseline for 3DGS optimization. Furthermore, we demonstrate that optimizations can be applied to 4D Gaussian reconstruction, leading to efficient non-rigid scene optimization.

</details>

---

## 13. Efficient Special Stain Classification

**Chinese Title**: Efficient Special Stain Classification

**Authors**: Oskar Thaeter, Christian Grashei, Anette Haas, Elisa Schmoeckel, Han Li et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09989v1](http://arxiv.org/abs/2602.09989v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09989v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Stains are essential in histopathology to visualize specific tissue characteristics, with Haematoxylin and Eosin (H&E) serving as the clinical standard. However, pathologists frequently   utilize a variety of special stains for the diagnosis of specific morphologies. Maintaining accurate metadata for these slides is critical for quality control in clinical archives and for   the integrity of computational pathology datasets. In this work, we compare two approaches for automated classification of stains using whole slide images, covering the 14 most commonly   used special stains in our institute alongside standard and frozen-section H&E. We evaluate a Multi-Instance Learning (MIL) pipeline and a proposed lightweight thumbnail-based approach.   On internal test data, MIL achieved the highest performance (macro F1: 0.941 for 16 classes; 0.969 for 14 merged classes), while the thumbnail approach remained competitive (0.897 and   0.953, respectively). On external TCGA data, the thumbnail model generalized best (weighted F1: 0.843 vs. 0.807 for MIL). The thumbnail approach also increased throughput by two orders of   magnitude (5.635 vs. 0.018 slides/s for MIL with all patches). We conclude that thumbnail-based classification provides a scalable and robust solution for routine visual quality control   in digital pathology workflows.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Stains are essential in histopathology to visualize specific tissue characteristics, with Haematoxylin and Eosin (H&E) serving as the clinical standard. However, pathologists frequently   utilize a variety of special stains for the diagnosis of specific morphologies. Maintaining accurate metadata for these slides is critical for quality control in clinical archives and for   the integrity of computational pathology datasets.In this work, we compare two approaches for automated classification of stains using whole slide images, covering the 14 most commonly   used special stains in our institute alongside standard and frozen-section H&E. We evaluate a Multi-Instance Learning (MIL) pipeline and a proposed lightweight thumbnail-based approach.On internal test data, MIL achieved the highest performance (macro F1: 0.941 for 16 classes; 0.969 for 14 merged classes), while the thumbnail approach remained competitive (0.897 and   0.953, respectively). On external TCGA data, the thumbnail model generalized best (weighted F1: 0.843 vs. 0.807 for MIL). The thumbnail approach also increased throughput by two orders of   magnitude (5.635 vs. 0.018 slides/s for MIL with all patches).We conclude that thumbnail-based classification provides a scalable and robust solution for routine visual quality control   in digital pathology workflows.

</details>

---

## 14. Online Monitoring Framework for Automotive Time Series Data using JEPA Embeddings

**Chinese Title**: Online Monitoring Framework for Automotive Time Series Data using JEPA Embeddings

**Authors**: Alexander Fertig, Karthikeyan Chandra Sekaran, Lakshman Balasubramanian, Michael Botsch

**Date**: 2026-02-10 | **arXiv**: [2602.09985v1](http://arxiv.org/abs/2602.09985v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09985v1)

**Categories**: cs.LG, cs.AI, cs.CV

<details><summary><b>Abstract</b></summary>

As autonomous vehicles are rolled out, measures must be taken to ensure their safe operation. In order to supervise a system that is already in operation, monitoring frameworks are frequently employed. These run continuously online in the background, supervising the system status and recording anomalies. This work proposes an online monitoring framework to detect anomalies in object state representations. Thereby, a key challenge is creating a framework for anomaly detection without anomaly labels, which are usually unavailable for unknown anomalies. To address this issue, this work applies a self-supervised embedding method to translate object data into a latent representation space. For this, a JEPA-based self-supervised prediction task is constructed, allowing training without anomaly labels and the creation of rich object embeddings. The resulting expressive JEPA embeddings serve as input for established anomaly detection methods, in order to identify anomalies within object state representations. This framework is particularly useful for applications in real-world environments, where new or unknown anomalies may occur during operation for which there are no labels available. Experiments performed on the publicly available, real-world nuScenes dataset illustrate the framework's capabilities.

</details>

<details><summary><b>Chinese Abstract</b></summary>

As autonomous vehicles are rolled out, measures must be taken to ensure their safe operation. In order to supervise a system that is already in operation, monitoring frameworks are frequently employed. These run continuously online in the background, supervising the system status and recording anomalies. This work proposes an online monitoring framework to detect anomalies in object state representations.Thereby, a key challenge is creating a framework for anomaly detection without anomaly labels, which are usually unavailable for unknown anomalies. To address this issue, this work applies a self-supervised embedding method to translate object data into a latent representation space. For this, a JEPA-based self-supervised prediction task is constructed, allowing training without anomaly labels and the creation of rich object embeddings.The resulting expressive JEPA embeddings serve as input for established anomaly detection methods, in order to identify anomalies within object state representations. This framework is particularly useful for applications in real-world environments, where new or unknown anomalies may occur during operation for which there are no labels available.Experiments performed on the publicly available, real-world nuScenes dataset illustrate the framework's capabilities.

</details>

---

## 15. Coupled Inference in Diffusion Models for Semantic Decomposition

**Chinese Title**: Coupled Inference in Diffusion Models for Semantic Decomposition

**Authors**: Calvin Yeung, Ali Zakeri, Zhuowen Zou, Mohsen Imani

**Date**: 2026-02-10 | **arXiv**: [2602.09983v1](http://arxiv.org/abs/2602.09983v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09983v1)

**Categories**: cs.CV, cs.AI, cs.LG

<details><summary><b>Abstract</b></summary>

Many visual scenes can be described as compositions of latent factors. Effective recognition, reasoning, and editing often require not only forming such compositional representations, but also solving the decomposition problem. One popular choice for constructing these representations is through the binding operation. Resonator networks, which can be understood as coupled Hopfield networks, were proposed as a way to perform decomposition on such bound representations. Recent works have shown notable similarities between Hopfield networks and diffusion models. Motivated by these observations, we introduce a framework for semantic decomposition using coupled inference in diffusion models. Our method frames semantic decomposition as an inverse problem and couples the diffusion processes using a reconstruction-driven guidance term that encourages the composition of factor estimates to match the bound vector. We also introduce a novel iterative sampling scheme that improves the performance of our model. Finally, we show that attention-based resonator networks are a special case of our framework. Empirically, we demonstrate that our coupled inference framework outperforms resonator networks across a range of synthetic semantic decomposition tasks.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Many visual scenes can be described as compositions of latent factors. Effective recognition, reasoning, and editing often require not only forming such compositional representations, but also solving the decomposition problem. One popular choice for constructing these representations is through the binding operation.Resonator networks, which can be understood as coupled Hopfield networks, were proposed as a way to perform decomposition on such bound representations. Recent works have shown notable similarities between Hopfield networks and diffusion models. Motivated by these observations, we introduce a framework for semantic decomposition using coupled inference in diffusion models.Our method frames semantic decomposition as an inverse problem and couples the diffusion processes using a reconstruction-driven guidance term that encourages the composition of factor estimates to match the bound vector. We also introduce a novel iterative sampling scheme that improves the performance of our model. Finally, we show that attention-based resonator networks are a special case of our framework.Empirically, we demonstrate that our coupled inference framework outperforms resonator networks across a range of synthetic semantic decomposition tasks.

</details>

---

## 16. Learning to Detect Baked Goods with Limited Supervision

**Chinese Title**: Learning to Detect Baked Goods with Limited Supervision

**Authors**: Thomas H. Schmitt, Maximilian Bundscherer, Tobias Bocklet

**Date**: 2026-02-10 | **arXiv**: [2602.09979v1](http://arxiv.org/abs/2602.09979v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09979v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Monitoring leftover products provides valuable insights that can be used to optimize future production. This is especially important for German bakeries because freshly baked goods have a very short shelf life. Automating this process can reduce labor costs, improve accuracy, and streamline operations. We propose automating this process using an object detection model to identify baked goods from images. However, the large diversity of German baked goods makes fully supervised training prohibitively expensive and limits scalability. Although open-vocabulary detectors (e.g., OWLv2, Grounding DINO) offer lexibility, we demonstrate that they are insufficient for our task. While motivated by bakeries, our work addresses the broader challenges of deploying computer vision in industries, where tasks are specialized and annotated datasets are scarce. We compile dataset splits with varying supervision levels, covering 19 classes of baked goods. We propose two training workflows to train an object detection model with limited supervision. First, we combine OWLv2 and Grounding DINO localization with image-level supervision to train the model in a weakly supervised manner. Second, we improve viewpoint robustness by fine-tuning on video frames annotated using Segment Anything 2 as a pseudo-label propagation model. Using these workflows, we train YOLOv11 for our detection task due to its favorable speed accuracy tradeoff. Relying solely on image-level supervision, the model achieves a mean Average Precision (mAP) of 0.91. Finetuning with pseudo-labels raises model performance by 19.3% under non-ideal deployment conditions. Combining these workflows trains a model that surpasses our fully-supervised baseline model under non-ideal deployment conditions, despite relying only on image-level supervision.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Monitoring leftover products provides valuable insights that can be used to optimize future production. This is especially important for German bakeries because freshly baked goods have a very short shelf life. Automating this process can reduce labor costs, improve accuracy, and streamline operations. We propose automating this process using an object detection model to identify baked goods from images.However, the large diversity of German baked goods makes fully supervised training prohibitively expensive and limits scalability. Although open-vocabulary detectors (e.g., OWLv2, Grounding DINO) offer lexibility, we demonstrate that they are insufficient for our task. While motivated by bakeries, our work addresses the broader challenges of deploying computer vision in industries, where tasks are specialized and annotated datasets are scarce.We compile dataset splits with varying supervision levels, covering 19 classes of baked goods. We propose two training workflows to train an object detection model with limited supervision. First, we combine OWLv2 and Grounding DINO localization with image-level supervision to train the model in a weakly supervised manner.Second, we improve viewpoint robustness by fine-tuning on video frames annotated using Segment Anything 2 as a pseudo-label propagation model. Using these workflows, we train YOLOv11 for our detection task due to its favorable speed accuracy tradeoff. Relying solely on image-level supervision, the model achieves a mean Average Precision (mAP) of 0.91.Finetuning with pseudo-labels raises model performance by 19.3% under non-ideal deployment conditions. Combining these workflows trains a model that surpasses our fully-supervised baseline model under non-ideal deployment conditions, despite relying only on image-level supervision.

</details>

---

## 17. Bladder Vessel Segmentation using a Hybrid Attention-Convolution Framework

**Chinese Title**: Bladder Vessel Segmentation using a Hybrid Attention-Convolution Framework

**Authors**: Franziska KrauÃŸ, Matthias Ege, Zoltan Lovasz, Albrecht Bartz-Schmidt, Igor Tsaur et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09949v1](http://arxiv.org/abs/2602.09949v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09949v1)

**Categories**: cs.CV, cs.AI

<details><summary><b>Abstract</b></summary>

Urinary bladder cancer surveillance requires tracking tumor sites across repeated interventions, yet the deformable and hollow bladder lacks stable landmarks for orientation. While blood vessels visible during endoscopy offer a patient-specific "vascular fingerprint" for navigation, automated segmentation is challenged by imperfect endoscopic data, including sparse labels, artifacts like bubbles or variable lighting, continuous deformation, and mucosal folds that mimic vessels. State-of-the-art vessel segmentation methods often fail to address these domain-specific complexities. We introduce a Hybrid Attention-Convolution (HAC) architecture that combines Transformers to capture global vessel topology prior with a CNN that learns a residual refinement map to precisely recover thin-vessel details. To prioritize structural connectivity, the Transformer is trained on optimized ground truth data that exclude short and terminal branches. Furthermore, to address data scarcity, we employ a physics-aware pretraining, that is a self-supervised strategy using clinically grounded augmentations on unlabeled data. Evaluated on the BlaVeS dataset, consisting of endoscopic video frames, our approach achieves high accuracy (0.94) and superior precision (0.61) and clDice (0.66) compared to state-of-the-art medical segmentation models. Crucially, our method successfully suppresses false positives from mucosal folds that dynamically appear and vanish as the bladder fills and empties during surgery. Hence, HAC provides the reliable structural stability required for clinical navigation.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Urinary bladder cancer surveillance requires tracking tumor sites across repeated interventions, yet the deformable and hollow bladder lacks stable landmarks for orientation.While blood vessels visible during endoscopy offer a patient-specific "vascular fingerprint" for navigation, automated segmentation is challenged by imperfect endoscopic data, including sparse labels, artifacts like bubbles or variable lighting, continuous deformation, and mucosal folds that mimic vessels. State-of-the-art vessel segmentation methods often fail to address these domain-specific complexities.We introduce a Hybrid Attention-Convolution (HAC) architecture that combines Transformers to capture global vessel topology prior with a CNN that learns a residual refinement map to precisely recover thin-vessel details. To prioritize structural connectivity, the Transformer is trained on optimized ground truth data that exclude short and terminal branches.Furthermore, to address data scarcity, we employ a physics-aware pretraining, that is a self-supervised strategy using clinically grounded augmentations on unlabeled data. Evaluated on the BlaVeS dataset, consisting of endoscopic video frames, our approach achieves high accuracy (0.94) and superior precision (0.61) and clDice (0.66) compared to state-of-the-art medical segmentation models.Crucially, our method successfully suppresses false positives from mucosal folds that dynamically appear and vanish as the bladder fills and empties during surgery. Hence, HAC provides the reliable structural stability required for clinical navigation.

</details>

---

## 18. VersaViT: Enhancing MLLM Vision Backbones via Task-Guided Optimization

**Chinese Title**: VersaViT: Enhancing MLLM Vision Backbones via Task-Guided Optimization

**Authors**: Yikun Liu, Yuan Liu, Shangzhe Di, Haicheng Wang, Zhongyin Zhao et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09934v1](http://arxiv.org/abs/2602.09934v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09934v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Multimodal Large Language Models (MLLMs) have recently achieved remarkable success in visual-language understanding, demonstrating superior high-level semantic alignment within their vision encoders. An important question thus arises: Can these encoders serve as versatile vision backbones, capable of reliably performing classic vision-centric tasks as well? To address the question, we make the following contributions: (i) we identify that the vision encoders within MLLMs exhibit deficiencies in their dense feature representations, as evidenced by their suboptimal performance on dense prediction tasks (e.g., semantic segmentation, depth estimation); (ii) we propose VersaViT, a well-rounded vision transformer that instantiates a novel multi-task framework for collaborative post-training. This framework facilitates the optimization of the vision backbone via lightweight task heads with multi-granularity supervision; (iii) extensive experiments across various downstream tasks demonstrate the effectiveness of our method, yielding a versatile vision backbone suited for both language-mediated reasoning and pixel-level understanding.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Multimodal Large Language Models (MLLMs) have recently achieved remarkable success in visual-language understanding, demonstrating superior high-level semantic alignment within their vision encoders. An important question thus arises: Can these encoders serve as versatile vision backbones, capable of reliably performing classic vision-centric tasks as well?To address the question, we make the following contributions: (i) we identify that the vision encoders within MLLMs exhibit deficiencies in their dense feature representations, as evidenced by their suboptimal performance on dense prediction tasks (e.g., semantic segmentation, depth estimation); (ii) we propose VersaViT, a well-rounded vision transformer that instantiates a novel multi-task framework for collaborative post-training.This framework facilitates the optimization of the vision backbone via lightweight task heads with multi-granularity supervision; (iii) extensive experiments across various downstream tasks demonstrate the effectiveness of our method, yielding a versatile vision backbone suited for both language-mediated reasoning and pixel-level understanding.

</details>

---

## 19. Monocular Normal Estimation via Shading Sequence Estimation

**Chinese Title**: Monocular Normal Estimation via Shading Sequence Estimation

**Authors**: Zongrui Li, Xinhua Ma, Minghui Hu, Yunqing Zhao, Yingchen Yu et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09929v1](http://arxiv.org/abs/2602.09929v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09929v1)

**Categories**: cs.CV, cs.AI

<details><summary><b>Abstract</b></summary>

Monocular normal estimation aims to estimate the normal map from a single RGB image of an object under arbitrary lights. Existing methods rely on deep models to directly predict normal maps. However, they often suffer from 3D misalignment: while the estimated normal maps may appear to have a correct appearance, the reconstructed surfaces often fail to align with the geometric details. We argue that this misalignment stems from the current paradigm: the model struggles to distinguish and reconstruct varying geometry represented in normal maps, as the differences in underlying geometry are reflected only through relatively subtle color variations. To address this issue, we propose a new paradigm that reformulates normal estimation as shading sequence estimation, where shading sequences are more sensitive to various geometric information. Building on this paradigm, we present RoSE, a method that leverages image-to-video generative models to predict shading sequences. The predicted shading sequences are then converted into normal maps by solving a simple ordinary least-squares problem. To enhance robustness and better handle complex objects, RoSE is trained on a synthetic dataset, MultiShade, with diverse shapes, materials, and light conditions. Experiments demonstrate that RoSE achieves state-of-the-art performance on real-world benchmark datasets for object-based monocular normal estimation.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Monocular normal estimation aims to estimate the normal map from a single RGB image of an object under arbitrary lights. Existing methods rely on deep models to directly predict normal maps. However, they often suffer from 3D misalignment: while the estimated normal maps may appear to have a correct appearance, the reconstructed surfaces often fail to align with the geometric details.We argue that this misalignment stems from the current paradigm: the model struggles to distinguish and reconstruct varying geometry represented in normal maps, as the differences in underlying geometry are reflected only through relatively subtle color variations.To address this issue, we propose a new paradigm that reformulates normal estimation as shading sequence estimation, where shading sequences are more sensitive to various geometric information. Building on this paradigm, we present RoSE, a method that leverages image-to-video generative models to predict shading sequences. The predicted shading sequences are then converted into normal maps by solving a simple ordinary least-squares problem.To enhance robustness and better handle complex objects, RoSE is trained on a synthetic dataset, MultiShade, with diverse shapes, materials, and light conditions. Experiments demonstrate that RoSE achieves state-of-the-art performance on real-world benchmark datasets for object-based monocular normal estimation.

</details>

---

## 20. AdaTSQ: Pushing the Pareto Frontier of Diffusion Transformers via Temporal-Sensitivity Quantization

**Chinese Title**: AdaTSQ: Pushing the Pareto Frontier of Diffusion Transformers via Temporal-Sensitivity Quantization

**Authors**: Shaoqiu Zhang, Zizhong Ding, Kaicheng Yang, Junyi Wu, Xianglong Yan et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09883v1](http://arxiv.org/abs/2602.09883v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09883v1)

**Code**: https://github.com/Qiushao-E/AdaTSQ.

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Diffusion Transformers (DiTs) have emerged as the state-of-the-art backbone for high-fidelity image and video generation. However, their massive computational cost and memory footprint hinder deployment on edge devices. While post-training quantization (PTQ) has proven effective for large language models (LLMs), directly applying existing methods to DiTs yields suboptimal results due to the neglect of the unique temporal dynamics inherent in diffusion processes. In this paper, we propose AdaTSQ, a novel PTQ framework that pushes the Pareto frontier of efficiency and quality by exploiting the temporal sensitivity of DiTs. First, we propose a Pareto-aware timestep-dynamic bit-width allocation strategy. We model the quantization policy search as a constrained pathfinding problem. We utilize a beam search algorithm guided by end-to-end reconstruction error to dynamically assign layer-wise bit-widths across different timesteps. Second, we propose a Fisher-guided temporal calibration mechanism. It leverages temporal Fisher information to prioritize calibration data from highly sensitive timesteps, seamlessly integrating with Hessian-based weight optimization. Extensive experiments on four advanced DiTs (e.g., Flux-Dev, Flux-Schnell, Z-Image, and Wan2.1) demonstrate that AdaTSQ significantly outperforms state-of-the-art methods like SVDQuant and ViDiT-Q. Our code will be released at https://github.com/Qiushao-E/AdaTSQ.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Diffusion Transformers (DiTs) have emerged as the state-of-the-art backbone for high-fidelity image and video generation. However, their massive computational cost and memory footprint hinder deployment on edge devices.While post-training quantization (PTQ) has proven effective for large language models (LLMs), directly applying existing methods to DiTs yields suboptimal results due to the neglect of the unique temporal dynamics inherent in diffusion processes. In this paper, we propose AdaTSQ, a novel PTQ framework that pushes the Pareto frontier of efficiency and quality by exploiting the temporal sensitivity of DiTs.First, we propose a Pareto-aware timestep-dynamic bit-width allocation strategy. We model the quantization policy search as a constrained pathfinding problem. We utilize a beam search algorithm guided by end-to-end reconstruction error to dynamically assign layer-wise bit-widths across different timesteps. Second, we propose a Fisher-guided temporal calibration mechanism.It leverages temporal Fisher information to prioritize calibration data from highly sensitive timesteps, seamlessly integrating with Hessian-based weight optimization. Extensive experiments on four advanced DiTs (e.g., Flux-Dev, Flux-Schnell, Z-Image, and Wan2.1) demonstrate that AdaTSQ significantly outperforms state-of-the-art methods like SVDQuant and ViDiT-Q. Our code will be released at https://github.com/Qiushao-E/AdaTSQ.

</details>

---

## 21. MVISTA-4D: View-Consistent 4D World Model with Test-Time Action Inference for Robotic Manipulation

**Chinese Title**: MVISTA-4D: View-Consistent 4D World Model with Test-Time Action Inference for Robotic Manipulation

**Authors**: Jiaxu Wang, Yicheng Jiang, Tianlun He, Jingkai Sun, Qiang Zhang et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09878v1](http://arxiv.org/abs/2602.09878v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09878v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

World-model-based imagine-then-act becomes a promising paradigm for robotic manipulation, yet existing approaches typically support either purely image-based forecasting or reasoning over partial 3D geometry, limiting their ability to predict complete 4D scene dynamics. This work proposes a novel embodied 4D world model that enables geometrically consistent, arbitrary-view RGBD generation: given only a single-view RGBD observation as input, the model imagines the remaining viewpoints, which can then be back-projected and fused to assemble a more complete 3D structure across time. To efficiently learn the multi-view, cross-modality generation, we explicitly design cross-view and cross-modality feature fusion that jointly encourage consistency between RGB and depth and enforce geometric alignment across views. Beyond prediction, converting generated futures into actions is often handled by inverse dynamics, which is ill-posed because multiple actions can explain the same transition. We address this with a test-time action optimization strategy that backpropagates through the generative model to infer a trajectory-level latent best matching the predicted future, and a residual inverse dynamics model that turns this trajectory prior into accurate executable actions. Experiments on three datasets demonstrate strong performance on both 4D scene generation and downstream manipulation, and ablations provide practical insights into the key design choices.

</details>

<details><summary><b>Chinese Abstract</b></summary>

World-model-based imagine-then-act becomes a promising paradigm for robotic manipulation, yet existing approaches typically support either purely image-based forecasting or reasoning over partial 3D geometry, limiting their ability to predict complete 4D scene dynamics.This work proposes a novel embodied 4D world model that enables geometrically consistent, arbitrary-view RGBD generation: given only a single-view RGBD observation as input, the model imagines the remaining viewpoints, which can then be back-projected and fused to assemble a more complete 3D structure across time.To efficiently learn the multi-view, cross-modality generation, we explicitly design cross-view and cross-modality feature fusion that jointly encourage consistency between RGB and depth and enforce geometric alignment across views. Beyond prediction, converting generated futures into actions is often handled by inverse dynamics, which is ill-posed because multiple actions can explain the same transition.We address this with a test-time action optimization strategy that backpropagates through the generative model to infer a trajectory-level latent best matching the predicted future, and a residual inverse dynamics model that turns this trajectory prior into accurate executable actions.Experiments on three datasets demonstrate strong performance on both 4D scene generation and downstream manipulation, and ablations provide practical insights into the key design choices.

</details>

---

## 22. Free-GVC: Towards Training-Free Extreme Generative Video Compression with Temporal Coherence

**Chinese Title**: Free-GVC: Towards Training-Free Extreme Generative Video Compression with Temporal Coherence

**Authors**: Xiaoyue Ling, Chuqin Zhou, Chunyi Li, Yunuo Chen, Yuan Tian et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09868v1](http://arxiv.org/abs/2602.09868v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09868v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Building on recent advances in video generation, generative video compression has emerged as a new paradigm for achieving visually pleasing reconstructions. However, existing methods exhibit limited exploitation of temporal correlations, causing noticeable flicker and degraded temporal coherence at ultra-low bitrates. In this paper, we propose Free-GVC, a training-free generative video compression framework that reformulates video coding as latent trajectory compression guided by a video diffusion prior. Our method operates at the group-of-pictures (GOP) level, encoding video segments into a compact latent space and progressively compressing them along the diffusion trajectory. To ensure perceptually consistent reconstruction across GOPs, we introduce an Adaptive Quality Control module that dynamically constructs an online rate-perception surrogate model to predict the optimal diffusion step for each GOP. In addition, an Inter-GOP Alignment module establishes frame overlap and performs latent fusion between adjacent groups, thereby mitigating flicker and enhancing temporal coherence. Experiments show that Free-GVC achieves an average of 93.29% BD-Rate reduction in DISTS over the latest neural codec DCVC-RT, and a user study further confirms its superior perceptual quality and temporal coherence at ultra-low bitrates.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Building on recent advances in video generation, generative video compression has emerged as a new paradigm for achieving visually pleasing reconstructions. However, existing methods exhibit limited exploitation of temporal correlations, causing noticeable flicker and degraded temporal coherence at ultra-low bitrates.In this paper, we propose Free-GVC, a training-free generative video compression framework that reformulates video coding as latent trajectory compression guided by a video diffusion prior. Our method operates at the group-of-pictures (GOP) level, encoding video segments into a compact latent space and progressively compressing them along the diffusion trajectory.To ensure perceptually consistent reconstruction across GOPs, we introduce an Adaptive Quality Control module that dynamically constructs an online rate-perception surrogate model to predict the optimal diffusion step for each GOP. In addition, an Inter-GOP Alignment module establishes frame overlap and performs latent fusion between adjacent groups, thereby mitigating flicker and enhancing temporal coherence.Experiments show that Free-GVC achieves an average of 93.29% BD-Rate reduction in DISTS over the latest neural codec DCVC-RT, and a user study further confirms its superior perceptual quality and temporal coherence at ultra-low bitrates.

</details>

---

## 23. Code2World: A GUI World Model via Renderable Code Generation

**Chinese Title**: Code2World: A GUI World Model via Renderable Code Generation

**Authors**: Yuhao Zheng, Li'an Zhong, Yi Wang, Rui Dai, Kaikui Liu et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09856v1](http://arxiv.org/abs/2602.09856v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09856v1)

**Code**: https://github.com/AMAP-ML/Code2World.

**Categories**: cs.CV, cs.AI, cs.CL, cs.HC

<details><summary><b>Abstract</b></summary>

Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability.To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs.To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image.Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.

</details>

---

## 24. Reason-IAD: Knowledge-Guided Dynamic Latent Reasoning for Explainable Industrial Anomaly Detection

**Chinese Title**: Reason-IAD: Knowledge-Guided Dynamic Latent Reasoning for Explainable Industrial Anomaly Detection

**Authors**: Peng Chen, Chao Huang, Yunkang Cao, Chengliang Liu, Wenqiang Wang et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09850v1](http://arxiv.org/abs/2602.09850v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09850v1)

**Code**: https://github.com/chenpeng052/Reason-IAD.

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Industrial anomaly detection demands precise reasoning over fine-grained defect patterns. However, existing multimodal large language models (MLLMs), pretrained on general-domain data, often struggle to capture category-specific anomalies, thereby limiting both detection accuracy and interpretability. To address these limitations, we propose Reason-IAD, a knowledge-guided dynamic latent reasoning framework for explainable industrial anomaly detection. Reason-IAD comprises two core components. First, a retrieval-augmented knowledge module incorporates category-specific textual descriptions into the model input, enabling context-aware reasoning over domain-specific defects. Second, an entropy-driven latent reasoning mechanism conducts iterative exploration within a compact latent space using optimizable latent think tokens, guided by an entropy-based reward that encourages confident and stable predictions. Furthermore, a dynamic visual injection strategy selectively incorporates the most informative image patches into the latent sequence, directing the reasoning process toward regions critical for anomaly detection. Extensive experimental results demonstrate that Reason-IAD consistently outperforms state-of-the-art methods. The code will be publicly available at https://github.com/chenpeng052/Reason-IAD.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Industrial anomaly detection demands precise reasoning over fine-grained defect patterns. However, existing multimodal large language models (MLLMs), pretrained on general-domain data, often struggle to capture category-specific anomalies, thereby limiting both detection accuracy and interpretability.To address these limitations, we propose Reason-IAD, a knowledge-guided dynamic latent reasoning framework for explainable industrial anomaly detection. Reason-IAD comprises two core components. First, a retrieval-augmented knowledge module incorporates category-specific textual descriptions into the model input, enabling context-aware reasoning over domain-specific defects.Second, an entropy-driven latent reasoning mechanism conducts iterative exploration within a compact latent space using optimizable latent think tokens, guided by an entropy-based reward that encourages confident and stable predictions. Furthermore, a dynamic visual injection strategy selectively incorporates the most informative image patches into the latent sequence, directing the reasoning process toward regions critical for anomaly detection.Extensive experimental results demonstrate that Reason-IAD consistently outperforms state-of-the-art methods. The code will be publicly available at https://github.com/chenpeng052/Reason-IAD.

</details>

---

## 25. Kelix Technique Report

**Chinese Title**: Kelix Technique Report

**Authors**: Boyang Ding, Chenglong Chu, Dunju Zang, Han Li, Jiangxia Cao et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09843v1](http://arxiv.org/abs/2602.09843v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09843v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Autoregressive large language models (LLMs) scale well by expressing diverse tasks as sequences of discrete natural-language tokens and training with next-token prediction, which unifies comprehension and generation under self-supervision. Extending this paradigm to multimodal data requires a shared, discrete representation across modalities. However, most vision-language models (VLMs) still rely on a hybrid interface: discrete text tokens paired with continuous Vision Transformer (ViT) features. Because supervision is largely text-driven, these models are often biased toward understanding and cannot fully leverage large-scale self-supervised learning on non-text data. Recent work has explored discrete visual tokenization to enable fully autoregressive multimodal modeling, showing promising progress toward unified understanding and generation. Yet existing discrete vision tokens frequently lose information due to limited code capacity, resulting in noticeably weaker understanding than continuous-feature VLMs. We present Kelix, a fully discrete autoregressive unified model that closes the understanding gap between discrete and continuous visual representations.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Autoregressive large language models (LLMs) scale well by expressing diverse tasks as sequences of discrete natural-language tokens and training with next-token prediction, which unifies comprehension and generation under self-supervision. Extending this paradigm to multimodal data requires a shared, discrete representation across modalities.However, most vision-language models (VLMs) still rely on a hybrid interface: discrete text tokens paired with continuous Vision Transformer (ViT) features. Because supervision is largely text-driven, these models are often biased toward understanding and cannot fully leverage large-scale self-supervised learning on non-text data.Recent work has explored discrete visual tokenization to enable fully autoregressive multimodal modeling, showing promising progress toward unified understanding and generation. Yet existing discrete vision tokens frequently lose information due to limited code capacity, resulting in noticeably weaker understanding than continuous-feature VLMs.We present Kelix, a fully discrete autoregressive unified model that closes the understanding gap between discrete and continuous visual representations.

</details>

---

## 26. ARK: A Dual-Axis Multimodal Retrieval Benchmark along Reasoning and Knowledge

**Chinese Title**: ARK: A Dual-Axis Multimodal Retrieval Benchmark along Reasoning and Knowledge

**Authors**: Yijie Lin, Guofeng Ding, Haochen Zhou, Haobin Li, Mouxing Yang et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09839v1](http://arxiv.org/abs/2602.09839v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09839v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Existing multimodal retrieval benchmarks largely emphasize semantic matching on daily-life images and offer limited diagnostics of professional knowledge and complex reasoning. To address this gap, we introduce ARK, a benchmark designed to analyze multimodal retrieval from two complementary perspectives: (i) knowledge domains (five domains with 17 subtypes), which characterize the content and expertise retrieval relies on, and (ii) reasoning skills (six categories), which characterize the type of inference over multimodal evidence required to identify the correct candidate. Specifically, ARK evaluates retrieval with both unimodal and multimodal queries and candidates, covering 16 heterogeneous visual data types. To avoid shortcut matching during evaluation, most queries are paired with targeted hard negatives that require multi-step reasoning. We evaluate 23 representative text-based and multimodal retrievers on ARK and observe a pronounced gap between knowledge-intensive and reasoning-intensive retrieval, with fine-grained visual and spatial reasoning emerging as persistent bottlenecks. We further show that simple enhancements such as re-ranking and rewriting yield consistent improvements, but substantial headroom remains.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Existing multimodal retrieval benchmarks largely emphasize semantic matching on daily-life images and offer limited diagnostics of professional knowledge and complex reasoning.To address this gap, we introduce ARK, a benchmark designed to analyze multimodal retrieval from two complementary perspectives: (i) knowledge domains (five domains with 17 subtypes), which characterize the content and expertise retrieval relies on, and (ii) reasoning skills (six categories), which characterize the type of inference over multimodal evidence required to identify the correct candidate.Specifically, ARK evaluates retrieval with both unimodal and multimodal queries and candidates, covering 16 heterogeneous visual data types. To avoid shortcut matching during evaluation, most queries are paired with targeted hard negatives that require multi-step reasoning.We evaluate 23 representative text-based and multimodal retrievers on ARK and observe a pronounced gap between knowledge-intensive and reasoning-intensive retrieval, with fine-grained visual and spatial reasoning emerging as persistent bottlenecks. We further show that simple enhancements such as re-ranking and rewriting yield consistent improvements, but substantial headroom remains.

</details>

---

## 27. SAKED: Mitigating Hallucination in Large Vision-Language Models via Stability-Aware Knowledge Enhanced Decoding

**Chinese Title**: SAKED: Mitigating Hallucination in Large Vision-Language Models via Stability-Aware Knowledge Enhanced Decoding

**Authors**: Zhaoxu Li, Chenqi Kong, Peijun Bao, Song Xia, Yi Tu et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09825v1](http://arxiv.org/abs/2602.09825v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09825v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Hallucinations in Large Vision-Language Models (LVLMs) pose significant security and reliability risks in real-world applications. Inspired by the observation that humans are more error-prone when uncertain or hesitant, we investigate how instability in a model 's internal knowledge contributes to LVLM hallucinations. We conduct extensive empirical analyses from three perspectives, namely attention heads, model layers, and decoding tokens, and identify three key hallucination patterns: (i) visual activation drift across attention heads, (ii) pronounced knowledge fluctuations across layers, and (iii) visual focus distraction between neighboring output tokens. Building on these findings, we propose Stability-Aware Knowledge-Enhanced Decoding (SAKED), which introduces a layer-wise Knowledge Stability Score (KSS) to quantify knowledge stability throughout the model. By contrasting the most stability-aware and stability-agnostic layers, SAKED suppresses decoding noise and dynamically leverages the most reliable internal knowledge for faithful token generation. Moreover, SAKED is training-free and can be seamlessly integrated into different architectures. Extensive experiments demonstrate that SAKED achieves state-of-the-art performance for hallucination mitigation on various models, tasks, and benchmarks.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Hallucinations in Large Vision-Language Models (LVLMs) pose significant security and reliability risks in real-world applications. Inspired by the observation that humans are more error-prone when uncertain or hesitant, we investigate how instability in a model 's internal knowledge contributes to LVLM hallucinations.We conduct extensive empirical analyses from three perspectives, namely attention heads, model layers, and decoding tokens, and identify three key hallucination patterns: (i) visual activation drift across attention heads, (ii) pronounced knowledge fluctuations across layers, and (iii) visual focus distraction between neighboring output tokens.Building on these findings, we propose Stability-Aware Knowledge-Enhanced Decoding (SAKED), which introduces a layer-wise Knowledge Stability Score (KSS) to quantify knowledge stability throughout the model. By contrasting the most stability-aware and stability-agnostic layers, SAKED suppresses decoding noise and dynamically leverages the most reliable internal knowledge for faithful token generation.Moreover, SAKED is training-free and can be seamlessly integrated into different architectures. Extensive experiments demonstrate that SAKED achieves state-of-the-art performance for hallucination mitigation on various models, tasks, and benchmarks.

</details>

---

## 28. SciFlow-Bench: Evaluating Structure-Aware Scientific Diagram Generation via Inverse Parsing

**Chinese Title**: SciFlow-Bench: Evaluating Structure-Aware Scientific Diagram Generation via Inverse Parsing

**Authors**: Tong Zhang, Honglin Lin, Zhou Liu, Chong Chen, Wentao Zhang

**Date**: 2026-02-10 | **arXiv**: [2602.09809v1](http://arxiv.org/abs/2602.09809v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09809v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Scientific diagrams convey explicit structural information, yet modern text-to-image models often produce visually plausible but structurally incorrect results. Existing benchmarks either rely on image-centric or subjective metrics insensitive to structure, or evaluate intermediate symbolic representations rather than final rendered images, leaving pixel-based diagram generation underexplored. We introduce SciFlow-Bench, a structure-first benchmark for evaluating scientific diagram generation directly from pixel-level outputs. Built from real scientific PDFs, SciFlow-Bench pairs each source framework figure with a canonical ground-truth graph and evaluates models as black-box image generators under a closed-loop, round-trip protocol that inverse-parses generated diagram images back into structured graphs for comparison. This design enforces evaluation by structural recoverability rather than visual similarity alone, and is enabled by a hierarchical multi-agent system that coordinates planning, perception, and structural reasoning. Experiments show that preserving structural correctness remains a fundamental challenge, particularly for diagrams with complex topology, underscoring the need for structure-aware evaluation.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Scientific diagrams convey explicit structural information, yet modern text-to-image models often produce visually plausible but structurally incorrect results. Existing benchmarks either rely on image-centric or subjective metrics insensitive to structure, or evaluate intermediate symbolic representations rather than final rendered images, leaving pixel-based diagram generation underexplored.We introduce SciFlow-Bench, a structure-first benchmark for evaluating scientific diagram generation directly from pixel-level outputs. Built from real scientific PDFs, SciFlow-Bench pairs each source framework figure with a canonical ground-truth graph and evaluates models as black-box image generators under a closed-loop, round-trip protocol that inverse-parses generated diagram images back into structured graphs for comparison.This design enforces evaluation by structural recoverability rather than visual similarity alone, and is enabled by a hierarchical multi-agent system that coordinates planning, perception, and structural reasoning. Experiments show that preserving structural correctness remains a fundamental challenge, particularly for diagrams with complex topology, underscoring the need for structure-aware evaluation.

</details>

---

## 29. Where Do Images Come From? Analyzing Captions to Geographically Profile Datasets

**Chinese Title**: Where Do Images Come From? Analyzing Captions to Geographically Profile Datasets

**Authors**: Abhipsa Basu, Yugam Bahl, Kirti Bhagat, Preethi Seshadri, R. Venkatesh Babu et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09775v1](http://arxiv.org/abs/2602.09775v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09775v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Recent studies show that text-to-image models often fail to generate geographically representative images, raising concerns about the representativeness of their training data and motivating the question: which parts of the world do these training examples come from? We geographically profile large-scale multimodal datasets by mapping image-caption pairs to countries based on location information extracted from captions using LLMs. Studying English captions from three widely used datasets (Re-LAION, DataComp1B, and Conceptual Captions) across $20$ common entities (e.g., house, flag), we find that the United States, the United Kingdom, and Canada account for $48.0\%$ of samples, while South American and African countries are severely under-represented with only $1.8\%$ and $3.8\%$ of images, respectively. We observe a strong correlation between a country's GDP and its representation in the data ($Ï= 0.82$). Examining non-English subsets for $4$ languages from the Re-LAION dataset, we find that representation skews heavily toward countries where these languages are predominantly spoken. Additionally, we find that higher representation does not necessarily translate to greater visual or semantic diversity. Finally, analyzing country-specific images generated by Stable Diffusion v1.3 trained on Re-LAION, we show that while generations appear realistic, they are severely limited in their coverage compared to real-world images.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Recent studies show that text-to-image models often fail to generate geographically representative images, raising concerns about the representativeness of their training data and motivating the question: which parts of the world do these training examples come from? We geographically profile large-scale multimodal datasets by mapping image-caption pairs to countries based on location information extracted from captions using LLMs.Studying English captions from three widely used datasets (Re-LAION, DataComp1B, and Conceptual Captions) across $20$ common entities (e.g., house, flag), we find that the United States, the United Kingdom, and Canada account for $48.0\%$ of samples, while South American and African countries are severely under-represented with only $1.8\%$ and $3.8\%$ of images, respectively.We observe a strong correlation between a country's GDP and its representation in the data ($Ï= 0.82$). Examining non-English subsets for $4$ languages from the Re-LAION dataset, we find that representation skews heavily toward countries where these languages are predominantly spoken. Additionally, we find that higher representation does not necessarily translate to greater visual or semantic diversity.Finally, analyzing country-specific images generated by Stable Diffusion v1.3 trained on Re-LAION, we show that while generations appear realistic, they are severely limited in their coverage compared to real-world images.

</details>

---

## 30. Self-Supervised Learning as Discrete Communication

**Chinese Title**: Self-Supervised Learning as Discrete Communication

**Authors**: Kawtar Zaher, Ilyass Moummad, Olivier Buisson, Alexis Joly

**Date**: 2026-02-10 | **arXiv**: [2602.09764v1](http://arxiv.org/abs/2602.09764v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09764v1)

**Categories**: cs.CV, cs.IR, cs.LG

<details><summary><b>Abstract</b></summary>

Most self-supervised learning (SSL) methods learn continuous visual representations by aligning different views of the same input, offering limited control over how information is structured across representation dimensions. In this work, we frame visual self-supervised learning as a discrete communication process between a teacher and a student network, where semantic information is transmitted through a fixed-capacity binary channel. Rather than aligning continuous features, the student predicts multi-label binary messages produced by the teacher. Discrete agreement is enforced through an element-wise binary cross-entropy objective, while a coding-rate regularization term encourages effective utilization of the constrained channel, promoting structured representations. We further show that periodically reinitializing the projection head strengthens this effect by encouraging embeddings that remain predictive across multiple discrete encodings. Extensive experiments demonstrate consistent improvements over continuous agreement baselines on image classification, retrieval, and dense visual prediction tasks, as well as under domain shift through self-supervised adaptation. Beyond backbone representations, we analyze the learned binary codes and show that they form a compact and informative discrete language, capturing semantic factors reusable across classes.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Most self-supervised learning (SSL) methods learn continuous visual representations by aligning different views of the same input, offering limited control over how information is structured across representation dimensions. In this work, we frame visual self-supervised learning as a discrete communication process between a teacher and a student network, where semantic information is transmitted through a fixed-capacity binary channel.Rather than aligning continuous features, the student predicts multi-label binary messages produced by the teacher. Discrete agreement is enforced through an element-wise binary cross-entropy objective, while a coding-rate regularization term encourages effective utilization of the constrained channel, promoting structured representations.We further show that periodically reinitializing the projection head strengthens this effect by encouraging embeddings that remain predictive across multiple discrete encodings. Extensive experiments demonstrate consistent improvements over continuous agreement baselines on image classification, retrieval, and dense visual prediction tasks, as well as under domain shift through self-supervised adaptation.Beyond backbone representations, we analyze the learned binary codes and show that they form a compact and informative discrete language, capturing semantic factors reusable across classes.

</details>

---

## 31. Robust Vision Systems for Connected and Autonomous Vehicles: Security Challenges and Attack Vectors

**Chinese Title**: Robust Vision Systems for Connected and Autonomous Vehicles: Security Challenges and Attack Vectors

**Authors**: Sandeep Gupta, Roberto Passerone

**Date**: 2026-02-10 | **arXiv**: [2602.09740v1](http://arxiv.org/abs/2602.09740v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09740v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

This article investigates the robustness of vision systems in Connected and Autonomous Vehicles (CAVs), which is critical for developing Level-5 autonomous driving capabilities. Safe and reliable CAV navigation undeniably depends on robust vision systems that enable accurate detection of objects, lane markings, and traffic signage. We analyze the key sensors and vision components essential for CAV navigation to derive a reference architecture for CAV vision system (CAVVS). This reference architecture provides a basis for identifying potential attack surfaces of CAVVS. Subsequently, we elaborate on identified attack vectors targeting each attack surface, rigorously evaluating their implications for confidentiality, integrity, and availability (CIA). Our study provides a comprehensive understanding of attack vector dynamics in vision systems, which is crucial for formulating robust security measures that can uphold the principles of the CIA triad.

</details>

<details><summary><b>Chinese Abstract</b></summary>

This article investigates the robustness of vision systems in Connected and Autonomous Vehicles (CAVs), which is critical for developing Level-5 autonomous driving capabilities. Safe and reliable CAV navigation undeniably depends on robust vision systems that enable accurate detection of objects, lane markings, and traffic signage.We analyze the key sensors and vision components essential for CAV navigation to derive a reference architecture for CAV vision system (CAVVS). This reference architecture provides a basis for identifying potential attack surfaces of CAVVS. Subsequently, we elaborate on identified attack vectors targeting each attack surface, rigorously evaluating their implications for confidentiality, integrity, and availability (CIA).Our study provides a comprehensive understanding of attack vector dynamics in vision systems, which is crucial for formulating robust security measures that can uphold the principles of the CIA triad.

</details>

---

## 32. Toward Fine-Grained Facial Control in 3D Talking Head Generation

**Chinese Title**: Toward Fine-Grained Facial Control in 3D Talking Head Generation

**Authors**: Shaoyang Xie, Xiaofeng Cong, Baosheng Yu, Zhipeng Gui, Jie Gui et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09736v1](http://arxiv.org/abs/2602.09736v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09736v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Audio-driven talking head generation is a core component of digital avatars, and 3D Gaussian Splatting has shown strong performance in real-time rendering of high-fidelity talking heads. However, achieving precise control over fine-grained facial movements remains a significant challenge, particularly due to lip-synchronization inaccuracies and facial jitter, both of which can contribute to the uncanny valley effect. To address these challenges, we propose Fine-Grained 3D Gaussian Splatting (FG-3DGS), a novel framework that enables temporally consistent and high-fidelity talking head generation. Our method introduces a frequency-aware disentanglement strategy to explicitly model facial regions based on their motion characteristics. Low-frequency regions, such as the cheeks, nose, and forehead, are jointly modeled using a standard MLP, while high-frequency regions, including the eyes and mouth, are captured separately using a dedicated network guided by facial area masks. The predicted motion dynamics, represented as Gaussian deltas, are applied to the static Gaussians to generate the final head frames, which are rendered via a rasterizer using frame-specific camera parameters. Additionally, a high-frequency-refined post-rendering alignment mechanism, learned from large-scale audio-video pairs by a pretrained model, is incorporated to enhance per-frame generation and achieve more accurate lip synchronization. Extensive experiments on widely used datasets for talking head generation demonstrate that our method outperforms recent state-of-the-art approaches in producing high-fidelity, lip-synced talking head videos.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Audio-driven talking head generation is a core component of digital avatars, and 3D Gaussian Splatting has shown strong performance in real-time rendering of high-fidelity talking heads. However, achieving precise control over fine-grained facial movements remains a significant challenge, particularly due to lip-synchronization inaccuracies and facial jitter, both of which can contribute to the uncanny valley effect.To address these challenges, we propose Fine-Grained 3D Gaussian Splatting (FG-3DGS), a novel framework that enables temporally consistent and high-fidelity talking head generation. Our method introduces a frequency-aware disentanglement strategy to explicitly model facial regions based on their motion characteristics.Low-frequency regions, such as the cheeks, nose, and forehead, are jointly modeled using a standard MLP, while high-frequency regions, including the eyes and mouth, are captured separately using a dedicated network guided by facial area masks.The predicted motion dynamics, represented as Gaussian deltas, are applied to the static Gaussians to generate the final head frames, which are rendered via a rasterizer using frame-specific camera parameters. Additionally, a high-frequency-refined post-rendering alignment mechanism, learned from large-scale audio-video pairs by a pretrained model, is incorporated to enhance per-frame generation and achieve more accurate lip synchronization.Extensive experiments on widely used datasets for talking head generation demonstrate that our method outperforms recent state-of-the-art approaches in producing high-fidelity, lip-synced talking head videos.

</details>

---

## 33. Allure of Craquelure: A Variational-Generative Approach to Crack Detection in Paintings

**Chinese Title**: Allure of Craquelure: A Variational-Generative Approach to Crack Detection in Paintings

**Authors**: Laura Paul, Holger Rauhut, Martin Burger, Samira Kabri, Tim Roith

**Date**: 2026-02-10 | **arXiv**: [2602.09730v1](http://arxiv.org/abs/2602.09730v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09730v1)

**Categories**: cs.CV, cs.LG, math.NA

<details><summary><b>Abstract</b></summary>

Recent advances in imaging technologies, deep learning and numerical performance have enabled non-invasive detailed analysis of artworks, supporting their documentation and conservation. In particular, automated detection of craquelure in digitized paintings is crucial for assessing degradation and guiding restoration, yet remains challenging due to the possibly complex scenery and the visual similarity between cracks and crack-like artistic features such as brush strokes or hair. We propose a hybrid approach that models crack detection as an inverse problem, decomposing an observed image into a crack-free painting and a crack component. A deep generative model is employed as powerful prior for the underlying artwork, while crack structures are captured using a Mumford--Shah-type variational functional together with a crack prior. Joint optimization yields a pixel-level map of crack localizations in the painting.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Recent advances in imaging technologies, deep learning and numerical performance have enabled non-invasive detailed analysis of artworks, supporting their documentation and conservation.In particular, automated detection of craquelure in digitized paintings is crucial for assessing degradation and guiding restoration, yet remains challenging due to the possibly complex scenery and the visual similarity between cracks and crack-like artistic features such as brush strokes or hair.We propose a hybrid approach that models crack detection as an inverse problem, decomposing an observed image into a crack-free painting and a crack component. A deep generative model is employed as powerful prior for the underlying artwork, while crack structures are captured using a Mumford--Shah-type variational functional together with a crack prior. Joint optimization yields a pixel-level map of crack localizations in the painting.

</details>

---

## 34. GenSeg-R1: RL-Driven Vision-Language Grounding for Fine-Grained Referring Segmentation

**Chinese Title**: GenSeg-R1: RL-Driven Vision-Language Grounding for Fine-Grained Referring Segmentation

**Authors**: Sandesh Hegde, Jaison Saji Chacko, Debarshi Banerjee, Uma Mahesh

**Date**: 2026-02-10 | **arXiv**: [2602.09701v1](http://arxiv.org/abs/2602.09701v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09701v1)

**Categories**: cs.CV, cs.AI

<details><summary><b>Abstract</b></summary>

We study fine-grained referring image segmentation via a decoupled reason-then-segment pipeline. A vision-language model (VLM) receives an image and a natural-language query, reasons about the scene, and emits structured spatial prompts: a bounding box plus two interior keypoints for every referred instance. A frozen promptable segmenter (SAM 2) converts these prompts into high-quality masks.   Within our GenSeg-R1 framework we finetune Qwen3-VL models (4B and 8B parameters) using Group Relative Policy Optimization (GRPO), requiring no supervised reasoning-chain annotations. On RefCOCOg validation our best model (GenSeg-R1-8B) achieves 0.7127 cIoU and 0.7382 mIoU, substantially outperforming the corresponding Qwen3-VL Instruct baselines (+15.3 and +21.9 points, respectively) and surpassing Seg-Zero-7B [3] by +3.3 cIoU under identical evaluation.   We further introduce GenSeg-R1-G, a variant trained on GRefCOCO [9] with a SAM 2 in-the-loop reward that directly optimizes mask quality. On GRefCOCO validation GenSeg-R1-G achieves 76.69% target mIoU with 82.40% accuracy on negative (no-target) prompts, substantially outperforming Seg-R1-7B and Seg-Zero-7B, which lack no-target detection capability. On ReasonSeg test, GenSeg-R1-4B reaches 68.40% mIoU, surpassing Seg-Zero-7B by +7.0 and Seg-R1-7B by +10.7 points.

</details>

<details><summary><b>Chinese Abstract</b></summary>

We study fine-grained referring image segmentation via a decoupled reason-then-segment pipeline. A vision-language model (VLM) receives an image and a natural-language query, reasons about the scene, and emits structured spatial prompts: a bounding box plus two interior keypoints for every referred instance. A frozen promptable segmenter (SAM 2) converts these prompts into high-quality masks.Within our GenSeg-R1 framework we finetune Qwen3-VL models (4B and 8B parameters) using Group Relative Policy Optimization (GRPO), requiring no supervised reasoning-chain annotations.On RefCOCOg validation our best model (GenSeg-R1-8B) achieves 0.7127 cIoU and 0.7382 mIoU, substantially outperforming the corresponding Qwen3-VL Instruct baselines (+15.3 and +21.9 points, respectively) and surpassing Seg-Zero-7B [3] by +3.3 cIoU under identical evaluation. We further introduce GenSeg-R1-G, a variant trained on GRefCOCO [9] with a SAM 2 in-the-loop reward that directly optimizes mask quality.On GRefCOCO validation GenSeg-R1-G achieves 76.69% target mIoU with 82.40% accuracy on negative (no-target) prompts, substantially outperforming Seg-R1-7B and Seg-Zero-7B, which lack no-target detection capability. On ReasonSeg test, GenSeg-R1-4B reaches 68.40% mIoU, surpassing Seg-Zero-7B by +7.0 and Seg-R1-7B by +10.7 points.

</details>

---

## 35. Semi-supervised Liver Segmentation and Patch-based Fibrosis Staging with Registration-aided Multi-parametric MRI

**Chinese Title**: Semi-supervised Liver Segmentation and Patch-based Fibrosis Staging with Registration-aided Multi-parametric MRI

**Authors**: Boya Wang, Ruizhe Li, Chao Chen, Xin Chen

**Date**: 2026-02-10 | **arXiv**: [2602.09686v1](http://arxiv.org/abs/2602.09686v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09686v1)

**Code**: https://github.com/mileywang3061/Care-Liver

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Liver fibrosis poses a substantial challenge in clinical practice, emphasizing the necessity for precise liver segmentation and accurate disease staging. Based on the CARE Liver 2025 Track 4 Challenge, this study introduces a multi-task deep learning framework developed for liver segmentation (LiSeg) and liver fibrosis staging (LiFS) using multiparametric MRI. The LiSeg phase addresses the challenge of limited annotated images and the complexities of multi-parametric MRI data by employing a semi-supervised learning model that integrates image segmentation and registration. By leveraging both labeled and unlabeled data, the model overcomes the difficulties introduced by domain shifts and variations across modalities. In the LiFS phase, we employed a patchbased method which allows the visualization of liver fibrosis stages based on the classification outputs. Our approach effectively handles multimodality imaging data, limited labels, and domain shifts. The proposed method has been tested by the challenge organizer on an independent test set that includes in-distribution (ID) and out-of-distribution (OOD) cases using three-channel MRIs (T1, T2, DWI) and seven-channel MRIs (T1, T2, DWI, GED1-GED4). The code is freely available. Github link: https://github.com/mileywang3061/Care-Liver

</details>

<details><summary><b>Chinese Abstract</b></summary>

Liver fibrosis poses a substantial challenge in clinical practice, emphasizing the necessity for precise liver segmentation and accurate disease staging. Based on the CARE Liver 2025 Track 4 Challenge, this study introduces a multi-task deep learning framework developed for liver segmentation (LiSeg) and liver fibrosis staging (LiFS) using multiparametric MRI.The LiSeg phase addresses the challenge of limited annotated images and the complexities of multi-parametric MRI data by employing a semi-supervised learning model that integrates image segmentation and registration. By leveraging both labeled and unlabeled data, the model overcomes the difficulties introduced by domain shifts and variations across modalities.In the LiFS phase, we employed a patchbased method which allows the visualization of liver fibrosis stages based on the classification outputs. Our approach effectively handles multimodality imaging data, limited labels, and domain shifts.The proposed method has been tested by the challenge organizer on an independent test set that includes in-distribution (ID) and out-of-distribution (OOD) cases using three-channel MRIs (T1, T2, DWI) and seven-channel MRIs (T1, T2, DWI, GED1-GED4). The code is freely available. Github link: https://github.com/mileywang3061/Care-Liver

</details>

---

## 36. TreeCUA: Efficiently Scaling GUI Automation with Tree-Structured Verifiable Evolution

**Chinese Title**: TreeCUA: Efficiently Scaling GUI Automation with Tree-Structured Verifiable Evolution

**Authors**: Deyang Jiang, Jing Huang, Xuanle Zhao, Lei Chen, Liming Zheng et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09662v1](http://arxiv.org/abs/2602.09662v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09662v1)

**Code**: https://github.com/UITron-hub/TreeCUA.

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Effectively scaling GUI automation is essential for computer-use agents (CUAs); however, existing work primarily focuses on scaling GUI grounding rather than the more crucial GUI planning, which requires more sophisticated data collection. In reality, the exploration process of a CUA across apps/desktops/web pages typically follows a tree structure, with earlier functional entry points often being explored more frequently. Thus, organizing large-scale trajectories into tree structures can reduce data cost and streamline the data scaling of GUI planning. In this work, we propose TreeCUA to efficiently scale GUI automation with tree-structured verifiable evolution. We propose a multi-agent collaborative framework to explore the environment, verify actions, summarize trajectories, and evaluate quality to generate high-quality and scalable GUI trajectories. To improve efficiency, we devise a novel tree-based topology to store and replay duplicate exploration nodes, and design an adaptive exploration algorithm to balance the depth (\emph{i.e.}, trajectory difficulty) and breadth (\emph{i.e.}, trajectory diversity). Moreover, we develop world knowledge guidance and global memory backtracking to avoid low-quality generation. Finally, we naturally extend and propose the TreeCUA-DPO method from abundant tree node information, improving GUI planning capability by referring to the branch information of adjacent trajectories. Experimental results show that TreeCUA and TreeCUA-DPO offer significant improvements, and out-of-domain (OOD) studies further demonstrate strong generalization. All trajectory node information and code will be available at https://github.com/UITron-hub/TreeCUA.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Effectively scaling GUI automation is essential for computer-use agents (CUAs); however, existing work primarily focuses on scaling GUI grounding rather than the more crucial GUI planning, which requires more sophisticated data collection. In reality, the exploration process of a CUA across apps/desktops/web pages typically follows a tree structure, with earlier functional entry points often being explored more frequently.Thus, organizing large-scale trajectories into tree structures can reduce data cost and streamline the data scaling of GUI planning. In this work, we propose TreeCUA to efficiently scale GUI automation with tree-structured verifiable evolution. We propose a multi-agent collaborative framework to explore the environment, verify actions, summarize trajectories, and evaluate quality to generate high-quality and scalable GUI trajectories.To improve efficiency, we devise a novel tree-based topology to store and replay duplicate exploration nodes, and design an adaptive exploration algorithm to balance the depth (\emph{i.e.}, trajectory difficulty) and breadth (\emph{i.e.}, trajectory diversity). Moreover, we develop world knowledge guidance and global memory backtracking to avoid low-quality generation.Finally, we naturally extend and propose the TreeCUA-DPO method from abundant tree node information, improving GUI planning capability by referring to the branch information of adjacent trajectories. Experimental results show that TreeCUA and TreeCUA-DPO offer significant improvements, and out-of-domain (OOD) studies further demonstrate strong generalization.All trajectory node information and code will be available at https://github.com/UITron-hub/TreeCUA.

</details>

---

## 37. Time2General: Learning Spatiotemporal Invariant Representations for Domain-Generalization Video Semantic Segmentation

**Chinese Title**: Time2General: Learning Spatiotemporal Invariant Representations for Domain-Generalization Video Semantic Segmentation

**Authors**: Siyu Chen, Ting Han, Haoling Huang, Chaolei Wang, Chengzheng Fu et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09648v1](http://arxiv.org/abs/2602.09648v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09648v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Domain Generalized Video Semantic Segmentation (DGVSS) is trained on a single labeled driving domain and is directly deployed on unseen domains without target labels and test-time adaptation while maintaining temporally consistent predictions over video streams. In practice, both domain shift and temporal-sampling shift break correspondence-based propagation and fixed-stride temporal aggregation, causing severe frame-to-frame flicker even in label-stable regions. We propose Time2General, a DGVSS framework built on Stability Queries. Time2General introduces a Spatio-Temporal Memory Decoder that aggregates multi-frame context into a clip-level spatio-temporal memory and decodes temporally consistent per-frame masks without explicit correspondence propagation. To further suppress flicker and improve robustness to varying sampling rates, the Masked Temporal Consistency Loss is proposed to regularize temporal prediction discrepancies across different strides, and randomize training strides to expose the model to diverse temporal gaps. Extensive experiments on multiple driving benchmarks show that Time2General achieves a substantial improvement in cross-domain accuracy and temporal stability over prior DGSS and VSS baselines while running at up to 18 FPS. Code will be released after the review process.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Domain Generalized Video Semantic Segmentation (DGVSS) is trained on a single labeled driving domain and is directly deployed on unseen domains without target labels and test-time adaptation while maintaining temporally consistent predictions over video streams.In practice, both domain shift and temporal-sampling shift break correspondence-based propagation and fixed-stride temporal aggregation, causing severe frame-to-frame flicker even in label-stable regions. We propose Time2General, a DGVSS framework built on Stability Queries.Time2General introduces a Spatio-Temporal Memory Decoder that aggregates multi-frame context into a clip-level spatio-temporal memory and decodes temporally consistent per-frame masks without explicit correspondence propagation.To further suppress flicker and improve robustness to varying sampling rates, the Masked Temporal Consistency Loss is proposed to regularize temporal prediction discrepancies across different strides, and randomize training strides to expose the model to diverse temporal gaps.Extensive experiments on multiple driving benchmarks show that Time2General achieves a substantial improvement in cross-domain accuracy and temporal stability over prior DGSS and VSS baselines while running at up to 18 FPS. Code will be released after the review process.

</details>

---

## 38. Towards Training-free Multimodal Hate Localisation with Large Language Models

**Chinese Title**: Towards Training-free Multimodal Hate Localisation with Large Language Models

**Authors**: Yueming Sun, Long Yang, Jianbo Jiao, Zeyu Fu

**Date**: 2026-02-10 | **arXiv**: [2602.09637v1](http://arxiv.org/abs/2602.09637v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09637v1)

**Categories**: cs.CV, cs.MM

<details><summary><b>Abstract</b></summary>

The proliferation of hateful content in online videos poses severe threats to individual well-being and societal harmony. However, existing solutions for video hate detection either rely heavily on large-scale human annotations or lack fine-grained temporal precision. In this work, we propose LELA, the first training-free Large Language Model (LLM) based framework for hate video localization. Distinct from state-of-the-art models that depend on supervised pipelines, LELA leverages LLMs and modality-specific captioning to detect and temporally localize hateful content in a training-free manner. Our method decomposes a video into five modalities, including image, speech, OCR, music, and video context, and uses a multi-stage prompting scheme to compute fine-grained hateful scores for each frame. We further introduce a composition matching mechanism to enhance cross-modal reasoning. Experiments on two challenging benchmarks, HateMM and MultiHateClip, demonstrate that LELA outperforms all existing training-free baselines by a large margin. We also provide extensive ablations and qualitative visualizations, establishing LELA as a strong foundation for scalable and interpretable hate video localization.

</details>

<details><summary><b>Chinese Abstract</b></summary>

The proliferation of hateful content in online videos poses severe threats to individual well-being and societal harmony. However, existing solutions for video hate detection either rely heavily on large-scale human annotations or lack fine-grained temporal precision. In this work, we propose LELA, the first training-free Large Language Model (LLM) based framework for hate video localization.Distinct from state-of-the-art models that depend on supervised pipelines, LELA leverages LLMs and modality-specific captioning to detect and temporally localize hateful content in a training-free manner. Our method decomposes a video into five modalities, including image, speech, OCR, music, and video context, and uses a multi-stage prompting scheme to compute fine-grained hateful scores for each frame.We further introduce a composition matching mechanism to enhance cross-modal reasoning. Experiments on two challenging benchmarks, HateMM and MultiHateClip, demonstrate that LELA outperforms all existing training-free baselines by a large margin. We also provide extensive ablations and qualitative visualizations, establishing LELA as a strong foundation for scalable and interpretable hate video localization.

</details>

---

## 39. AGMark: Attention-Guided Dynamic Watermarking for Large Vision-Language Models

**Chinese Title**: AGMark: Attention-Guided Dynamic Watermarking for Large Vision-Language Models

**Authors**: Yue Li, Xin Yi, Dongsheng Shi, Yongyi Cui, Gerard de Melo et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09611v1](http://arxiv.org/abs/2602.09611v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09611v1)

**Categories**: cs.CV, cs.AI, cs.CR

<details><summary><b>Abstract</b></summary>

Watermarking has emerged as a pivotal solution for content traceability and intellectual property protection in Large Vision-Language Models (LVLMs). However, vision-agnostic watermarks may introduce visually irrelevant tokens and disrupt visual grounding by enforcing indiscriminate pseudo-random biases. Additionally, current vision-specific watermarks rely on a static, one-time estimation of vision critical weights and ignore the weight distribution density when determining the proportion of protected tokens. This design fails to account for dynamic changes in visual dependence during generation and may introduce low-quality tokens in the long tail. To address these challenges, we propose Attention-Guided Dynamic Watermarking (AGMark), a novel framework that embeds detectable signals while strictly preserving visual fidelity. At each decoding step, AGMark first dynamically identifies semantic-critical evidence based on attention weights for visual relevance, together with context-aware coherence cues, resulting in a more adaptive and well-calibrated evidence-weight distribution. It then determines the proportion of semantic-critical tokens by jointly considering uncertainty awareness (token entropy) and evidence calibration (weight density), thereby enabling adaptive vocabulary partitioning to avoid irrelevant tokens. Empirical results confirm that AGMark outperforms conventional methods, observably improving generation quality and yielding particularly strong gains in visual semantic fidelity in the later stages of generation. The framework maintains highly competitive detection accuracy (at least 99.36\% AUC) and robust attack resilience (at least 88.61\% AUC) without sacrificing inference efficiency, effectively establishing a new standard for reliability-preserving multi-modal watermarking.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Watermarking has emerged as a pivotal solution for content traceability and intellectual property protection in Large Vision-Language Models (LVLMs). However, vision-agnostic watermarks may introduce visually irrelevant tokens and disrupt visual grounding by enforcing indiscriminate pseudo-random biases.Additionally, current vision-specific watermarks rely on a static, one-time estimation of vision critical weights and ignore the weight distribution density when determining the proportion of protected tokens. This design fails to account for dynamic changes in visual dependence during generation and may introduce low-quality tokens in the long tail.To address these challenges, we propose Attention-Guided Dynamic Watermarking (AGMark), a novel framework that embeds detectable signals while strictly preserving visual fidelity. At each decoding step, AGMark first dynamically identifies semantic-critical evidence based on attention weights for visual relevance, together with context-aware coherence cues, resulting in a more adaptive and well-calibrated evidence-weight distribution.It then determines the proportion of semantic-critical tokens by jointly considering uncertainty awareness (token entropy) and evidence calibration (weight density), thereby enabling adaptive vocabulary partitioning to avoid irrelevant tokens.Empirical results confirm that AGMark outperforms conventional methods, observably improving generation quality and yielding particularly strong gains in visual semantic fidelity in the later stages of generation.The framework maintains highly competitive detection accuracy (at least 99.36\% AUC) and robust attack resilience (at least 88.61\% AUC) without sacrificing inference efficiency, effectively establishing a new standard for reliability-preserving multi-modal watermarking.

</details>

---

## 40. Tele-Omni: a Unified Multimodal Framework for Video Generation and Editing

**Chinese Title**: Tele-Omni: a Unified Multimodal Framework for Video Generation and Editing

**Authors**: Jialun Liu, Yukuo Ma, Xiao Cao, Tian Li, Gonghu Shang et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09609v1](http://arxiv.org/abs/2602.09609v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09609v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Recent advances in diffusion-based video generation have substantially improved visual fidelity and temporal coherence. However, most existing approaches remain task-specific and rely primarily on textual instructions, limiting their ability to handle multimodal inputs, contextual references, and diverse video generation and editing scenarios within a unified framework. Moreover, many video editing methods depend on carefully engineered pipelines tailored to individual operations, which hinders scalability and composability. In this paper, we propose Tele-Omni, a unified multimodal framework for video generation and editing that follows multimodal instructions, including text, images, and reference videos, within a single model. Tele-Omni leverages pretrained multimodal large language models to parse heterogeneous instructions and infer structured generation or editing intents, while diffusion-based generators perform high-quality video synthesis conditioned on these structured signals. To enable joint training across heterogeneous video tasks, we introduce a task-aware data processing pipeline that unifies multimodal inputs into a structured instruction format while preserving task-specific constraints. Tele-Omni supports a wide range of video-centric tasks, including text-to-video generation, image-to-video generation, first-last-frame video generation, in-context video generation, and in-context video editing. By decoupling instruction parsing from video synthesis and combining it with task-aware data design, Tele-Omni achieves flexible multimodal control while maintaining strong temporal coherence and visual consistency. Experimental results demonstrate that Tele-Omni achieves competitive performance across multiple tasks.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Recent advances in diffusion-based video generation have substantially improved visual fidelity and temporal coherence. However, most existing approaches remain task-specific and rely primarily on textual instructions, limiting their ability to handle multimodal inputs, contextual references, and diverse video generation and editing scenarios within a unified framework.Moreover, many video editing methods depend on carefully engineered pipelines tailored to individual operations, which hinders scalability and composability. In this paper, we propose Tele-Omni, a unified multimodal framework for video generation and editing that follows multimodal instructions, including text, images, and reference videos, within a single model.Tele-Omni leverages pretrained multimodal large language models to parse heterogeneous instructions and infer structured generation or editing intents, while diffusion-based generators perform high-quality video synthesis conditioned on these structured signals.To enable joint training across heterogeneous video tasks, we introduce a task-aware data processing pipeline that unifies multimodal inputs into a structured instruction format while preserving task-specific constraints. Tele-Omni supports a wide range of video-centric tasks, including text-to-video generation, image-to-video generation, first-last-frame video generation, in-context video generation, and in-context video editing.By decoupling instruction parsing from video synthesis and combining it with task-aware data design, Tele-Omni achieves flexible multimodal control while maintaining strong temporal coherence and visual consistency. Experimental results demonstrate that Tele-Omni achieves competitive performance across multiple tasks.

</details>

---

## 41. Hand2World: Autoregressive Egocentric Interaction Generation via Free-Space Hand Gestures

**Chinese Title**: Hand2World: Autoregressive Egocentric Interaction Generation via Free-Space Hand Gestures

**Authors**: Yuxi Wang, Wenqi Ouyang, Tianyi Wei, Yi Dong, Zhiqi Shen et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09600v1](http://arxiv.org/abs/2602.09600v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09600v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Egocentric interactive world models are essential for augmented reality and embodied AI, where visual generation must respond to user input with low latency, geometric consistency, and long-term stability. We study egocentric interaction generation from a single scene image under free-space hand gestures, aiming to synthesize photorealistic videos in which hands enter the scene, interact with objects, and induce plausible world dynamics under head motion. This setting introduces fundamental challenges, including distribution shift between free-space gestures and contact-heavy training data, ambiguity between hand motion and camera motion in monocular views, and the need for arbitrary-length video generation. We present Hand2World, a unified autoregressive framework that addresses these challenges through occlusion-invariant hand conditioning based on projected 3D hand meshes, allowing visibility and occlusion to be inferred from scene context rather than encoded in the control signal. To stabilize egocentric viewpoint changes, we inject explicit camera geometry via per-pixel PlÃ¼cker-ray embeddings, disentangling camera motion from hand motion and preventing background drift. We further develop a fully automated monocular annotation pipeline and distill a bidirectional diffusion model into a causal generator, enabling arbitrary-length synthesis. Experiments on three egocentric interaction benchmarks show substantial improvements in perceptual quality and 3D consistency while supporting camera control and long-horizon interactive generation.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Egocentric interactive world models are essential for augmented reality and embodied AI, where visual generation must respond to user input with low latency, geometric consistency, and long-term stability.We study egocentric interaction generation from a single scene image under free-space hand gestures, aiming to synthesize photorealistic videos in which hands enter the scene, interact with objects, and induce plausible world dynamics under head motion.This setting introduces fundamental challenges, including distribution shift between free-space gestures and contact-heavy training data, ambiguity between hand motion and camera motion in monocular views, and the need for arbitrary-length video generation.We present Hand2World, a unified autoregressive framework that addresses these challenges through occlusion-invariant hand conditioning based on projected 3D hand meshes, allowing visibility and occlusion to be inferred from scene context rather than encoded in the control signal.To stabilize egocentric viewpoint changes, we inject explicit camera geometry via per-pixel PlÃ¼cker-ray embeddings, disentangling camera motion from hand motion and preventing background drift. We further develop a fully automated monocular annotation pipeline and distill a bidirectional diffusion model into a causal generator, enabling arbitrary-length synthesis.Experiments on three egocentric interaction benchmarks show substantial improvements in perceptual quality and 3D consistency while supporting camera control and long-horizon interactive generation.

</details>

---

## 42. Delving into Spectral Clustering with Vision-Language Representations

**Chinese Title**: Delving into Spectral Clustering with Vision-Language Representations

**Authors**: Bo Peng, Yuanwei Hu, Bo Liu, Ling Chen, Jie Lu et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09586v1](http://arxiv.org/abs/2602.09586v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09586v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Spectral clustering is known as a powerful technique in unsupervised data analysis. The vast majority of approaches to spectral clustering are driven by a single modality, leaving the rich information in multi-modal representations untapped. Inspired by the recent success of vision-language pre-training, this paper enriches the landscape of spectral clustering from a single-modal to a multi-modal regime. Particularly, we propose Neural Tangent Kernel Spectral Clustering that leverages cross-modal alignment in pre-trained vision-language models. By anchoring the neural tangent kernel with positive nouns, i.e., those semantically close to the images of interest, we arrive at formulating the affinity between images as a coupling of their visual proximity and semantic overlap. We show that this formulation amplifies within-cluster connections while suppressing spurious ones across clusters, hence encouraging block-diagonal structures. In addition, we present a regularized affinity diffusion mechanism that adaptively ensembles affinity matrices induced by different prompts. Extensive experiments on \textbf{16} benchmarks -- including classical, large-scale, fine-grained and domain-shifted datasets -- manifest that our method consistently outperforms the state-of-the-art by a large margin.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Spectral clustering is known as a powerful technique in unsupervised data analysis. The vast majority of approaches to spectral clustering are driven by a single modality, leaving the rich information in multi-modal representations untapped. Inspired by the recent success of vision-language pre-training, this paper enriches the landscape of spectral clustering from a single-modal to a multi-modal regime.Particularly, we propose Neural Tangent Kernel Spectral Clustering that leverages cross-modal alignment in pre-trained vision-language models. By anchoring the neural tangent kernel with positive nouns, i.e., those semantically close to the images of interest, we arrive at formulating the affinity between images as a coupling of their visual proximity and semantic overlap.We show that this formulation amplifies within-cluster connections while suppressing spurious ones across clusters, hence encouraging block-diagonal structures. In addition, we present a regularized affinity diffusion mechanism that adaptively ensembles affinity matrices induced by different prompts.Extensive experiments on \textbf{16} benchmarks -- including classical, large-scale, fine-grained and domain-shifted datasets -- manifest that our method consistently outperforms the state-of-the-art by a large margin.

</details>

---

## 43. Scalpel: Fine-Grained Alignment of Attention Activation Manifolds via Mixture Gaussian Bridges to Mitigate Multimodal Hallucination

**Chinese Title**: Scalpel: Fine-Grained Alignment of Attention Activation Manifolds via Mixture Gaussian Bridges to Mitigate Multimodal Hallucination

**Authors**: Ziqiang Shi, Rujie Liu, Shanshan Yu, Satoshi Munakata, Koichi Shirahata

**Date**: 2026-02-10 | **arXiv**: [2602.09541v1](http://arxiv.org/abs/2602.09541v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09541v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Rapid progress in large vision-language models (LVLMs) has achieved unprecedented performance in vision-language tasks. However, due to the strong prior of large language models (LLMs) and misaligned attention across modalities, LVLMs often generate outputs inconsistent with visual content - termed hallucination. To address this, we propose \textbf{Scalpel}, a method that reduces hallucination by refining attention activation distributions toward more credible regions. Scalpel predicts trusted attention directions for each head in Transformer layers during inference and adjusts activations accordingly. It employs a Gaussian mixture model to capture multi-peak distributions of attention in trust and hallucination manifolds, and uses entropic optimal transport (equivalent to SchrÃ¶dinger bridge problem) to map Gaussian components precisely. During mitigation, Scalpel dynamically adjusts intervention strength and direction based on component membership and mapping relationships between hallucination and trust activations. Extensive experiments across multiple datasets and benchmarks demonstrate that Scalpel effectively mitigates hallucinations, outperforming previous methods and achieving state-of-the-art performance. Moreover, Scalpel is model- and data-agnostic, requiring no additional computation, only a single decoding step.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Rapid progress in large vision-language models (LVLMs) has achieved unprecedented performance in vision-language tasks. However, due to the strong prior of large language models (LLMs) and misaligned attention across modalities, LVLMs often generate outputs inconsistent with visual content - termed hallucination.To address this, we propose \textbf{Scalpel}, a method that reduces hallucination by refining attention activation distributions toward more credible regions. Scalpel predicts trusted attention directions for each head in Transformer layers during inference and adjusts activations accordingly.It employs a Gaussian mixture model to capture multi-peak distributions of attention in trust and hallucination manifolds, and uses entropic optimal transport (equivalent to SchrÃ¶dinger bridge problem) to map Gaussian components precisely. During mitigation, Scalpel dynamically adjusts intervention strength and direction based on component membership and mapping relationships between hallucination and trust activations.Extensive experiments across multiple datasets and benchmarks demonstrate that Scalpel effectively mitigates hallucinations, outperforming previous methods and achieving state-of-the-art performance. Moreover, Scalpel is model- and data-agnostic, requiring no additional computation, only a single decoding step.

</details>

---

## 44. AUHead: Realistic Emotional Talking Head Generation via Action Units Control

**Chinese Title**: AUHead: Realistic Emotional Talking Head Generation via Action Units Control

**Authors**: Jiayi Lyu, Leigang Qu, Wenjing Zhang, Hanyu Jiang, Kai Liu et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09534v1](http://arxiv.org/abs/2602.09534v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09534v1)

**Code**: https://github.com/laura990501/AUHead_ICLR

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Realistic talking-head video generation is critical for virtual avatars, film production, and interactive systems. Current methods struggle with nuanced emotional expressions due to the lack of fine-grained emotion control. To address this issue, we introduce a novel two-stage method (AUHead) to disentangle fine-grained emotion control, i.e. , Action Units (AUs), from audio and achieve controllable generation. In the first stage, we explore the AU generation abilities of large audio-language models (ALMs), by spatial-temporal AU tokenization and an "emotion-then-AU" chain-of-thought mechanism. It aims to disentangle AUs from raw speech, effectively capturing subtle emotional cues. In the second stage, we propose an AU-driven controllable diffusion model that synthesizes realistic talking-head videos conditioned on AU sequences. Specifically, we first map the AU sequences into the structured 2D facial representation to enhance spatial fidelity, and then model the AU-vision interaction within cross-attention modules. To achieve flexible AU-quality trade-off control, we introduce an AU disentanglement guidance strategy during inference, further refining the emotional expressiveness and identity consistency of the generated videos. Results on benchmark datasets demonstrate that our approach achieves competitive performance in emotional realism, accurate lip synchronization, and visual coherence, significantly surpassing existing techniques. Our implementation is available at https://github.com/laura990501/AUHead_ICLR

</details>

<details><summary><b>Chinese Abstract</b></summary>

Realistic talking-head video generation is critical for virtual avatars, film production, and interactive systems. Current methods struggle with nuanced emotional expressions due to the lack of fine-grained emotion control. To address this issue, we introduce a novel two-stage method (AUHead) to disentangle fine-grained emotion control, i.e. , Action Units (AUs), from audio and achieve controllable generation.In the first stage, we explore the AU generation abilities of large audio-language models (ALMs), by spatial-temporal AU tokenization and an "emotion-then-AU" chain-of-thought mechanism. It aims to disentangle AUs from raw speech, effectively capturing subtle emotional cues. In the second stage, we propose an AU-driven controllable diffusion model that synthesizes realistic talking-head videos conditioned on AU sequences.Specifically, we first map the AU sequences into the structured 2D facial representation to enhance spatial fidelity, and then model the AU-vision interaction within cross-attention modules. To achieve flexible AU-quality trade-off control, we introduce an AU disentanglement guidance strategy during inference, further refining the emotional expressiveness and identity consistency of the generated videos.Results on benchmark datasets demonstrate that our approach achieves competitive performance in emotional realism, accurate lip synchronization, and visual coherence, significantly surpassing existing techniques. Our implementation is available at https://github.com/laura990501/AUHead_ICLR

</details>

---

## 45. DR.Experts: Differential Refinement of Distortion-Aware Experts for Blind Image Quality Assessment

**Chinese Title**: DR.Experts: Differential Refinement of Distortion-Aware Experts for Blind Image Quality Assessment

**Authors**: Bohan Fu, Guanyi Qin, Fazhan Zhang, Zihao Huang, Mingxuan Li et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09531v1](http://arxiv.org/abs/2602.09531v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09531v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Blind Image Quality Assessment, aiming to replicate human perception of visual quality without reference, plays a key role in vision tasks, yet existing models often fail to effectively capture subtle distortion cues, leading to a misalignment with human subjective judgments. We identify that the root cause of this limitation lies in the lack of reliable distortion priors, as methods typically learn shallow relationships between unified image features and quality scores, resulting in their insensitive nature to distortions and thus limiting their performance. To address this, we introduce DR.Experts, a novel prior-driven BIQA framework designed to explicitly incorporate distortion priors, enabling a reliable quality assessment. DR.Experts begins by leveraging a degradation-aware vision-language model to obtain distortion-specific priors, which are further refined and enhanced by the proposed Distortion-Saliency Differential Module through distinguishing them from semantic attentions, thereby ensuring the genuine representations of distortions. The refined priors, along with semantics and bridging representation, are then fused by a proposed mixture-of-experts style module named the Dynamic Distortion Weighting Module. This mechanism weights each distortion-specific feature as per its perceptual impact, ensuring that the final quality prediction aligns with human perception. Extensive experiments conducted on five challenging BIQA benchmarks demonstrate the superiority of DR.Experts over current methods and showcase its excellence in terms of generalization and data efficiency.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Blind Image Quality Assessment, aiming to replicate human perception of visual quality without reference, plays a key role in vision tasks, yet existing models often fail to effectively capture subtle distortion cues, leading to a misalignment with human subjective judgments.We identify that the root cause of this limitation lies in the lack of reliable distortion priors, as methods typically learn shallow relationships between unified image features and quality scores, resulting in their insensitive nature to distortions and thus limiting their performance.To address this, we introduce DR.Experts, a novel prior-driven BIQA framework designed to explicitly incorporate distortion priors, enabling a reliable quality assessment.DR.Experts begins by leveraging a degradation-aware vision-language model to obtain distortion-specific priors, which are further refined and enhanced by the proposed Distortion-Saliency Differential Module through distinguishing them from semantic attentions, thereby ensuring the genuine representations of distortions.The refined priors, along with semantics and bridging representation, are then fused by a proposed mixture-of-experts style module named the Dynamic Distortion Weighting Module. This mechanism weights each distortion-specific feature as per its perceptual impact, ensuring that the final quality prediction aligns with human perception.Extensive experiments conducted on five challenging BIQA benchmarks demonstrate the superiority of DR.Experts over current methods and showcase its excellence in terms of generalization and data efficiency.

</details>

---

## 46. SCA-Net: Spatial-Contextual Aggregation Network for Enhanced Small Building and Road Change Detection

**Chinese Title**: SCA-Net: Spatial-Contextual Aggregation Network for Enhanced Small Building and Road Change Detection

**Authors**: Emad Gholibeigi, Abbas Koochari, Azadeh ZamaniFar

**Date**: 2026-02-10 | **arXiv**: [2602.09529v1](http://arxiv.org/abs/2602.09529v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09529v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Automated change detection in remote sensing imagery is critical for urban management, environmental monitoring, and disaster assessment. While deep learning models have advanced this field, they often struggle with challenges like low sensitivity to small objects and high computational costs. This paper presents SCA-Net, an enhanced architecture built upon the Change-Agent framework for precise building and road change detection in bi-temporal images. Our model incorporates several key innovations: a novel Difference Pyramid Block for multi-scale change analysis, an Adaptive Multi-scale Processing module combining shape-aware and high-resolution enhancement blocks, and multi-level attention mechanisms (PPM and CSAGate) for joint contextual and detail processing. Furthermore, a dynamic composite loss function and a four-phase training strategy are introduced to stabilize training and accelerate convergence. Comprehensive evaluations on the LEVIR-CD and LEVIR-MCI datasets demonstrate SCA-Net's superior performance over Change-Agent and other state-of-the-art methods. Our approach achieves a significant 2.64% improvement in mean Intersection over Union (mIoU) on LEVIR-MCI and a remarkable 57.9% increase in IoU for small buildings, while reducing the training time by 61%. This work provides an efficient, accurate, and robust solution for practical change detection applications.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Automated change detection in remote sensing imagery is critical for urban management, environmental monitoring, and disaster assessment. While deep learning models have advanced this field, they often struggle with challenges like low sensitivity to small objects and high computational costs.This paper presents SCA-Net, an enhanced architecture built upon the Change-Agent framework for precise building and road change detection in bi-temporal images.Our model incorporates several key innovations: a novel Difference Pyramid Block for multi-scale change analysis, an Adaptive Multi-scale Processing module combining shape-aware and high-resolution enhancement blocks, and multi-level attention mechanisms (PPM and CSAGate) for joint contextual and detail processing.Furthermore, a dynamic composite loss function and a four-phase training strategy are introduced to stabilize training and accelerate convergence. Comprehensive evaluations on the LEVIR-CD and LEVIR-MCI datasets demonstrate SCA-Net's superior performance over Change-Agent and other state-of-the-art methods.Our approach achieves a significant 2.64% improvement in mean Intersection over Union (mIoU) on LEVIR-MCI and a remarkable 57.9% increase in IoU for small buildings, while reducing the training time by 61%. This work provides an efficient, accurate, and robust solution for practical change detection applications.

</details>

---

## 47. SchrÃ¶Mind: Mitigating Hallucinations in Multimodal Large Language Models via Solving the SchrÃ¶dinger Bridge Problem

**Chinese Title**: SchrÃ¶Mind: Mitigating Hallucinations in Multimodal Large Language Models via Solving the SchrÃ¶dinger Bridge Problem

**Authors**: Ziqiang Shi, Rujie Liu, Shanshan Yu, Satoshi Munakata, Koichi Shirahata

**Date**: 2026-02-10 | **arXiv**: [2602.09528v1](http://arxiv.org/abs/2602.09528v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09528v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Recent advancements in Multimodal Large Language Models (MLLMs) have achieved significant success across various domains. However, their use in high-stakes fields like healthcare remains limited due to persistent hallucinations, where generated text contradicts or ignores visual input. We contend that MLLMs can comprehend images but struggle to produce accurate token sequences. Minor perturbations can shift attention from truthful to untruthful states, and the autoregressive nature of text generation often prevents error correction. To address this, we propose SchrÃ¶Mind-a novel framework reducing hallucinations via solving the SchrÃ¶dinger bridge problem. It establishes a token-level mapping between hallucinatory and truthful activations with minimal transport cost through lightweight training, while preserving the model's original capabilities. Extensive experiments on the POPE and MME benchmarks demonstrate the superiority of SchrÃ¶dinger, which achieves state-of-the-art performance while introducing only minimal computational overhead.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Recent advancements in Multimodal Large Language Models (MLLMs) have achieved significant success across various domains. However, their use in high-stakes fields like healthcare remains limited due to persistent hallucinations, where generated text contradicts or ignores visual input. We contend that MLLMs can comprehend images but struggle to produce accurate token sequences.Minor perturbations can shift attention from truthful to untruthful states, and the autoregressive nature of text generation often prevents error correction. To address this, we propose SchrÃ¶Mind-a novel framework reducing hallucinations via solving the SchrÃ¶dinger bridge problem.It establishes a token-level mapping between hallucinatory and truthful activations with minimal transport cost through lightweight training, while preserving the model's original capabilities. Extensive experiments on the POPE and MME benchmarks demonstrate the superiority of SchrÃ¶dinger, which achieves state-of-the-art performance while introducing only minimal computational overhead.

</details>

---

## 48. HLGFA: High-Low Resolution Guided Feature Alignment for Unsupervised Anomaly Detection

**Chinese Title**: HLGFA: High-Low Resolution Guided Feature Alignment for Unsupervised Anomaly Detection

**Authors**: Han Zhou, Yuxuan Gao, Yinchao Du, Xuezhe Zheng

**Date**: 2026-02-10 | **arXiv**: [2602.09524v1](http://arxiv.org/abs/2602.09524v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09524v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Unsupervised industrial anomaly detection (UAD) is essential for modern manufacturing inspection, where defect samples are scarce and reliable detection is required. In this paper, we propose HLGFA, a high-low resolution guided feature alignment framework that learns normality by modeling cross-resolution feature consistency between high-resolution and low-resolution representations of normal samples, instead of relying on pixel-level reconstruction. Dual-resolution inputs are processed by a shared frozen backbone to extract multi-level features, and high-resolution representations are decomposed into structure and detail priors to guide the refinement of low-resolution features through conditional modulation and gated residual correction. During inference, anomalies are naturally identified as regions where cross-resolution alignment breaks down. In addition, a noise-aware data augmentation strategy is introduced to suppress nuisance-induced responses commonly observed in industrial environments. Extensive experiments on standard benchmarks demonstrate the effectiveness of HLGFA, achieving 97.9% pixel-level AUROC and 97.5% image-level AUROC on the MVTec AD dataset, outperforming representative reconstruction-based and feature-based methods.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Unsupervised industrial anomaly detection (UAD) is essential for modern manufacturing inspection, where defect samples are scarce and reliable detection is required.In this paper, we propose HLGFA, a high-low resolution guided feature alignment framework that learns normality by modeling cross-resolution feature consistency between high-resolution and low-resolution representations of normal samples, instead of relying on pixel-level reconstruction.Dual-resolution inputs are processed by a shared frozen backbone to extract multi-level features, and high-resolution representations are decomposed into structure and detail priors to guide the refinement of low-resolution features through conditional modulation and gated residual correction. During inference, anomalies are naturally identified as regions where cross-resolution alignment breaks down.In addition, a noise-aware data augmentation strategy is introduced to suppress nuisance-induced responses commonly observed in industrial environments. Extensive experiments on standard benchmarks demonstrate the effectiveness of HLGFA, achieving 97.9% pixel-level AUROC and 97.5% image-level AUROC on the MVTec AD dataset, outperforming representative reconstruction-based and feature-based methods.

</details>

---

## 49. Singpath-VL Technical Report

**Chinese Title**: Singpath-VL Technical Report

**Authors**: Zhen Qiu, Kaiwen Xiao, Zhengwei Lu, Xiangyu Liu, Lei Zhao et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09523v1](http://arxiv.org/abs/2602.09523v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09523v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

We present Singpath-VL, a vision-language large model, to fill the vacancy of AI assistant in cervical cytology. Recent advances in multi-modal large language models (MLLMs) have significantly propelled the field of computational pathology. However, their application in cytopathology, particularly cervical cytology, remains underexplored, primarily due to the scarcity of large-scale, high-quality annotated datasets. To bridge this gap, we first develop a novel three-stage pipeline to synthesize a million-scale image-description dataset. The pipeline leverages multiple general-purpose MLLMs as weak annotators, refines their outputs through consensus fusion and expert knowledge injection, and produces high-fidelity descriptions of cell morphology. Using this dataset, we then fine-tune the Qwen3-VL-4B model via a multi-stage strategy to create a specialized cytopathology MLLM. The resulting model, named Singpath-VL, demonstrates superior performance in fine-grained morphological perception and cell-level diagnostic classification. To advance the field, we will open-source a portion of the synthetic dataset and benchmark.

</details>

<details><summary><b>Chinese Abstract</b></summary>

We present Singpath-VL, a vision-language large model, to fill the vacancy of AI assistant in cervical cytology. Recent advances in multi-modal large language models (MLLMs) have significantly propelled the field of computational pathology. However, their application in cytopathology, particularly cervical cytology, remains underexplored, primarily due to the scarcity of large-scale, high-quality annotated datasets.To bridge this gap, we first develop a novel three-stage pipeline to synthesize a million-scale image-description dataset. The pipeline leverages multiple general-purpose MLLMs as weak annotators, refines their outputs through consensus fusion and expert knowledge injection, and produces high-fidelity descriptions of cell morphology.Using this dataset, we then fine-tune the Qwen3-VL-4B model via a multi-stage strategy to create a specialized cytopathology MLLM. The resulting model, named Singpath-VL, demonstrates superior performance in fine-grained morphological perception and cell-level diagnostic classification. To advance the field, we will open-source a portion of the synthetic dataset and benchmark.

</details>

---

## 50. Attention to details, logits to truth: visual-aware attention and logits enhancement to mitigate hallucinations in LVLMs

**Chinese Title**: Attention to details, logits to truth: visual-aware attention and logits enhancement to mitigate hallucinations in LVLMs

**Authors**: Jingyi Wang, Fei Li, Rujie Liu

**Date**: 2026-02-10 | **arXiv**: [2602.09521v1](http://arxiv.org/abs/2602.09521v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09521v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Existing Large Vision-Language Models (LVLMs) exhibit insufficient visual attention, leading to hallucinations. To alleviate this problem, some previous studies adjust and amplify visual attention. These methods present a limitation that boosting attention for all visual tokens inevitably increases attention to task irrelevant tokens. To tackle this challenge, we propose a training free attentional intervention algorithm to enhance the attention of task-relevant tokens based on the argument that task-relevant tokens generally demonstrate high visual-textual similarities. Specifically, the vision-text cross-attention submatrices, which represent visual-textual correlations, are extracted to construct the reweighting matrices to reallocate attention. Besides, to enhance the contribution of visual tokens, we inject visual attention values into the beam search decoding to identify solutions with higher visual attention. Extensive experiments demonstrate that this method significantly reduces hallucinations across mainstream LVLMs, while preserving the accuracy and coherence of generated content.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Existing Large Vision-Language Models (LVLMs) exhibit insufficient visual attention, leading to hallucinations. To alleviate this problem, some previous studies adjust and amplify visual attention. These methods present a limitation that boosting attention for all visual tokens inevitably increases attention to task irrelevant tokens.To tackle this challenge, we propose a training free attentional intervention algorithm to enhance the attention of task-relevant tokens based on the argument that task-relevant tokens generally demonstrate high visual-textual similarities. Specifically, the vision-text cross-attention submatrices, which represent visual-textual correlations, are extracted to construct the reweighting matrices to reallocate attention.Besides, to enhance the contribution of visual tokens, we inject visual attention values into the beam search decoding to identify solutions with higher visual attention. Extensive experiments demonstrate that this method significantly reduces hallucinations across mainstream LVLMs, while preserving the accuracy and coherence of generated content.

</details>

---

## 51. A Universal Action Space for General Behavior Analysis

**Chinese Title**: A Universal Action Space for General Behavior Analysis

**Authors**: Hung-Shuo Chang, Yue-Cheng Yang, Yu-Hsi Chen, Wei-Hsin Chen, Chien-Yao Wang et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09518v1](http://arxiv.org/abs/2602.09518v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09518v1)

**Code**: https://github.com/franktpmvu/Universal-Action-Space.

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Analyzing animal and human behavior has long been a challenging task in computer vision. Early approaches from the 1970s to the 1990s relied on hand-crafted edge detection, segmentation, and low-level features such as color, shape, and texture to locate objects and infer their identities-an inherently ill-posed problem. Behavior analysis in this era typically proceeded by tracking identified objects over time and modeling their trajectories using sparse feature points, which further limited robustness and generalization. A major shift occurred with the introduction of ImageNet by Deng and Li in 2010, which enabled large-scale visual recognition through deep neural networks and effectively served as a comprehensive visual dictionary. This development allowed object recognition to move beyond complex low-level processing toward learned high-level representations. In this work, we follow this paradigm to build a large-scale Universal Action Space (UAS) using existing labeled human-action datasets. We then use this UAS as the foundation for analyzing and categorizing mammalian and chimpanzee behavior datasets. The source code is released on GitHub at https://github.com/franktpmvu/Universal-Action-Space.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Analyzing animal and human behavior has long been a challenging task in computer vision. Early approaches from the 1970s to the 1990s relied on hand-crafted edge detection, segmentation, and low-level features such as color, shape, and texture to locate objects and infer their identities-an inherently ill-posed problem.Behavior analysis in this era typically proceeded by tracking identified objects over time and modeling their trajectories using sparse feature points, which further limited robustness and generalization. A major shift occurred with the introduction of ImageNet by Deng and Li in 2010, which enabled large-scale visual recognition through deep neural networks and effectively served as a comprehensive visual dictionary.This development allowed object recognition to move beyond complex low-level processing toward learned high-level representations. In this work, we follow this paradigm to build a large-scale Universal Action Space (UAS) using existing labeled human-action datasets. We then use this UAS as the foundation for analyzing and categorizing mammalian and chimpanzee behavior datasets.The source code is released on GitHub at https://github.com/franktpmvu/Universal-Action-Space.

</details>

---

## 52. Equilibrium contrastive learning for imbalanced image classification

**Chinese Title**: Equilibrium contrastive learning for imbalanced image classification

**Authors**: Sumin Roh, Harim Kim, Ho Yun Lee, Il Yong Chun

**Date**: 2026-02-10 | **arXiv**: [2602.09506v1](http://arxiv.org/abs/2602.09506v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09506v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Contrastive learning (CL) is a predominant technique in image classification, but they showed limited performance with an imbalanced dataset. Recently, several supervised CL methods have been proposed to promote an ideal regular simplex geometric configuration in the representation space-characterized by intra-class feature collapse and uniform inter-class mean spacing, especially for imbalanced datasets. In particular, existing prototype-based methods include class prototypes, as additional samples to consider all classes. However, the existing CL methods suffer from two limitations. First, they do not consider the alignment between the class means/prototypes and classifiers, which could lead to poor generalization. Second, existing prototype-based methods treat prototypes as only one additional sample per class, making their influence depend on the number of class instances in a batch and causing unbalanced contributions across classes. To address these limitations, we propose Equilibrium Contrastive Learning (ECL), a supervised CL framework designed to promote geometric equilibrium, where class features, means, and classifiers are harmoniously balanced under data imbalance. The proposed ECL framework uses two main components. First, ECL promotes the representation geometric equilibrium (i.e., a regular simplex geometry characterized by collapsed class samples and uniformly distributed class means), while balancing the contributions of class-average features and class prototypes. Second, ECL establishes a classifier-class center geometric equilibrium by aligning classifier weights and class prototypes. We ran experiments with three long-tailed datasets, the CIFAR-10(0)-LT, ImageNet-LT, and the two imbalanced medical datasets, the ISIC 2019 and our constructed LCCT dataset. Results show that ECL outperforms existing SOTA supervised CL methods designed for imbalanced classification.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Contrastive learning (CL) is a predominant technique in image classification, but they showed limited performance with an imbalanced dataset. Recently, several supervised CL methods have been proposed to promote an ideal regular simplex geometric configuration in the representation space-characterized by intra-class feature collapse and uniform inter-class mean spacing, especially for imbalanced datasets.In particular, existing prototype-based methods include class prototypes, as additional samples to consider all classes. However, the existing CL methods suffer from two limitations. First, they do not consider the alignment between the class means/prototypes and classifiers, which could lead to poor generalization.Second, existing prototype-based methods treat prototypes as only one additional sample per class, making their influence depend on the number of class instances in a batch and causing unbalanced contributions across classes.To address these limitations, we propose Equilibrium Contrastive Learning (ECL), a supervised CL framework designed to promote geometric equilibrium, where class features, means, and classifiers are harmoniously balanced under data imbalance. The proposed ECL framework uses two main components.First, ECL promotes the representation geometric equilibrium (i.e., a regular simplex geometry characterized by collapsed class samples and uniformly distributed class means), while balancing the contributions of class-average features and class prototypes. Second, ECL establishes a classifier-class center geometric equilibrium by aligning classifier weights and class prototypes.We ran experiments with three long-tailed datasets, the CIFAR-10(0)-LT, ImageNet-LT, and the two imbalanced medical datasets, the ISIC 2019 and our constructed LCCT dataset. Results show that ECL outperforms existing SOTA supervised CL methods designed for imbalanced classification.

</details>

---

## 53. Beyond Next-Token Alignment: Distilling Multimodal Large Language Models via Token Interactions

**Chinese Title**: Beyond Next-Token Alignment: Distilling Multimodal Large Language Models via Token Interactions

**Authors**: Lin Chen, Xiaoke Zhao, Kun Ding, Weiwei Feng, Changtao Miao et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09483v1](http://arxiv.org/abs/2602.09483v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09483v1)

**Code**: https://github.com/lchen1019/Align-TI.

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Multimodal Large Language Models (MLLMs) demonstrate impressive cross-modal capabilities, yet their substantial size poses significant deployment challenges. Knowledge distillation (KD) is a promising solution for compressing these models, but existing methods primarily rely on static next-token alignment, neglecting the dynamic token interactions, which embed essential capabilities for multimodal understanding and generation. To this end, we introduce Align-TI, a novel KD framework designed from the perspective of Token Interactions. Our approach is motivated by the insight that MLLMs rely on two primary interactions: vision-instruction token interactions to extract relevant visual information, and intra-response token interactions for coherent generation. Accordingly, Align-TI introduces two components: IVA enables the student model to imitate the teacher's instruction-relevant visual information extract capability by aligning on salient visual regions. TPA captures the teacher's dynamic generative logic by aligning the sequential token-to-token transition probabilities. Extensive experiments demonstrate Align-TI's superiority. Notably, our approach achieves $2.6\%$ relative improvement over Vanilla KD, and our distilled Align-TI-2B even outperforms LLaVA-1.5-7B (a much larger MLLM) by $7.0\%$, establishing a new state-of-the-art distillation framework for training parameter-efficient MLLMs. Code is available at https://github.com/lchen1019/Align-TI.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Multimodal Large Language Models (MLLMs) demonstrate impressive cross-modal capabilities, yet their substantial size poses significant deployment challenges. Knowledge distillation (KD) is a promising solution for compressing these models, but existing methods primarily rely on static next-token alignment, neglecting the dynamic token interactions, which embed essential capabilities for multimodal understanding and generation.To this end, we introduce Align-TI, a novel KD framework designed from the perspective of Token Interactions. Our approach is motivated by the insight that MLLMs rely on two primary interactions: vision-instruction token interactions to extract relevant visual information, and intra-response token interactions for coherent generation.Accordingly, Align-TI introduces two components: IVA enables the student model to imitate the teacher's instruction-relevant visual information extract capability by aligning on salient visual regions. TPA captures the teacher's dynamic generative logic by aligning the sequential token-to-token transition probabilities. Extensive experiments demonstrate Align-TI's superiority.Notably, our approach achieves $2.6\%$ relative improvement over Vanilla KD, and our distilled Align-TI-2B even outperforms LLaVA-1.5-7B (a much larger MLLM) by $7.0\%$, establishing a new state-of-the-art distillation framework for training parameter-efficient MLLMs. Code is available at https://github.com/lchen1019/Align-TI.

</details>

---

## 54. Weakly Supervised Contrastive Learning for Histopathology Patch Embeddings

**Chinese Title**: Weakly Supervised Contrastive Learning for Histopathology Patch Embeddings

**Authors**: Bodong Zhang, Xiwen Li, Hamid Manoochehri, Xiaoya Tang, Deepika Sirohi et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09477v1](http://arxiv.org/abs/2602.09477v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09477v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Digital histopathology whole slide images (WSIs) provide gigapixel-scale high-resolution images that are highly useful for disease diagnosis. However, digital histopathology image analysis faces significant challenges due to the limited training labels, since manually annotating specific regions or small patches cropped from large WSIs requires substantial time and effort. Weakly supervised multiple instance learning (MIL) offers a practical and efficient solution by requiring only bag-level (slide-level) labels, while each bag typically contains multiple instances (patches). Most MIL methods directly use frozen image patch features generated by various image encoders as inputs and primarily focus on feature aggregation. However, feature representation learning for encoder pretraining in MIL settings has largely been neglected.   In our work, we propose a novel feature representation learning framework called weakly supervised contrastive learning (WeakSupCon) that incorporates bag-level label information during training. Our method does not rely on instance-level pseudo-labeling, yet it effectively separates patches with different labels in the feature space. Experimental results demonstrate that the image features generated by our WeakSupCon method lead to improved downstream MIL performance compared to self-supervised contrastive learning approaches in three datasets. Our related code is available at github.com/BzhangURU/Paper_WeakSupCon_for_MIL

</details>

<details><summary><b>Chinese Abstract</b></summary>

Digital histopathology whole slide images (WSIs) provide gigapixel-scale high-resolution images that are highly useful for disease diagnosis. However, digital histopathology image analysis faces significant challenges due to the limited training labels, since manually annotating specific regions or small patches cropped from large WSIs requires substantial time and effort.Weakly supervised multiple instance learning (MIL) offers a practical and efficient solution by requiring only bag-level (slide-level) labels, while each bag typically contains multiple instances (patches). Most MIL methods directly use frozen image patch features generated by various image encoders as inputs and primarily focus on feature aggregation.However, feature representation learning for encoder pretraining in MIL settings has largely been neglected. In our work, we propose a novel feature representation learning framework called weakly supervised contrastive learning (WeakSupCon) that incorporates bag-level label information during training. Our method does not rely on instance-level pseudo-labeling, yet it effectively separates patches with different labels in the feature space.Experimental results demonstrate that the image features generated by our WeakSupCon method lead to improved downstream MIL performance compared to self-supervised contrastive learning approaches in three datasets. Our related code is available at github.com/BzhangURU/Paper_WeakSupCon_for_MIL

</details>

---

## 55. FD-DB: Frequency-Decoupled Dual-Branch Network for Unpaired Synthetic-to-Real Domain Translation

**Chinese Title**: FD-DB: Frequency-Decoupled Dual-Branch Network for Unpaired Synthetic-to-Real Domain Translation

**Authors**: Chuanhai Zang, Jiabao Hu, XW Song

**Date**: 2026-02-10 | **arXiv**: [2602.09476v1](http://arxiv.org/abs/2602.09476v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09476v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Synthetic data provide low-cost, accurately annotated samples for geometry-sensitive vision tasks, but appearance and imaging differences between synthetic and real domains cause severe domain shift and degrade downstream performance. Unpaired synthetic-to-real translation can reduce this gap without paired supervision, yet existing methods often face a trade-off between photorealism and structural stability: unconstrained generation may introduce deformation or spurious textures, while overly rigid constraints limit adaptation to real-domain statistics. We propose FD-DB, a frequency-decoupled dual-branch model that separates appearance transfer into low-frequency interpretable editing and high-frequency residual compensation. The interpretable branch predicts physically meaningful editing parameters (white balance, exposure, contrast, saturation, blur, and grain) to build a stable low-frequency appearance base with strong content preservation. The free branch complements fine details through residual generation, and a gated fusion mechanism combines the two branches under explicit frequency constraints to limit low-frequency drift. We further adopt a two-stage training schedule that first stabilizes the editing branch and then releases the residual branch to improve optimization stability. Experiments on the YCB-V dataset show that FD-DB improves real-domain appearance consistency and significantly boosts downstream semantic segmentation performance while preserving geometric and semantic structures.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Synthetic data provide low-cost, accurately annotated samples for geometry-sensitive vision tasks, but appearance and imaging differences between synthetic and real domains cause severe domain shift and degrade downstream performance.Unpaired synthetic-to-real translation can reduce this gap without paired supervision, yet existing methods often face a trade-off between photorealism and structural stability: unconstrained generation may introduce deformation or spurious textures, while overly rigid constraints limit adaptation to real-domain statistics.We propose FD-DB, a frequency-decoupled dual-branch model that separates appearance transfer into low-frequency interpretable editing and high-frequency residual compensation. The interpretable branch predicts physically meaningful editing parameters (white balance, exposure, contrast, saturation, blur, and grain) to build a stable low-frequency appearance base with strong content preservation.The free branch complements fine details through residual generation, and a gated fusion mechanism combines the two branches under explicit frequency constraints to limit low-frequency drift. We further adopt a two-stage training schedule that first stabilizes the editing branch and then releases the residual branch to improve optimization stability.Experiments on the YCB-V dataset show that FD-DB improves real-domain appearance consistency and significantly boosts downstream semantic segmentation performance while preserving geometric and semantic structures.

</details>

---

## 56. A Scoping Review of Deep Learning for Urban Visual Pollution and Proposal of a Real-Time Monitoring Framework with a Visual Pollution Index

**Chinese Title**: A Scoping Review of Deep Learning for Urban Visual Pollution and Proposal of a Real-Time Monitoring Framework with a Visual Pollution Index

**Authors**: Mohammad Masudur Rahman, Md. Rashedur Rahman, Ashraful Islam, Saadia B Alam, M Ashraful Amin

**Date**: 2026-02-10 | **arXiv**: [2602.09446v1](http://arxiv.org/abs/2602.09446v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09446v1)

**Categories**: cs.CV, cs.LG

<details><summary><b>Abstract</b></summary>

Urban Visual Pollution (UVP) has emerged as a critical concern, yet research on automatic detection and application remains fragmented. This scoping review maps the existing deep learning-based approaches for detecting, classifying, and designing a comprehensive application framework for visual pollution management. Following the PRISMA-ScR guidelines, seven academic databases (Scopus, Web of Science, IEEE Xplore, ACM DL, ScienceDirect, SpringerNatureLink, and Wiley) were systematically searched and reviewed, and 26 articles were found. Most research focuses on specific pollutant categories and employs variations of YOLO, Faster R-CNN, and EfficientDet architectures. Although several datasets exist, they are limited to specific areas and lack standardized taxonomies. Few studies integrate detection into real-time application systems, yet they tend to be geographically skewed. We proposed a framework for monitoring visual pollution that integrates a visual pollution index to assess the severity of visual pollution for a certain area. This review highlights the need for a unified UVP management system that incorporates pollutant taxonomy, a cross-city benchmark dataset, a generalized deep learning model, and an assessment index that supports sustainable urban aesthetics and enhances the well-being of urban dwellers.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Urban Visual Pollution (UVP) has emerged as a critical concern, yet research on automatic detection and application remains fragmented. This scoping review maps the existing deep learning-based approaches for detecting, classifying, and designing a comprehensive application framework for visual pollution management.Following the PRISMA-ScR guidelines, seven academic databases (Scopus, Web of Science, IEEE Xplore, ACM DL, ScienceDirect, SpringerNatureLink, and Wiley) were systematically searched and reviewed, and 26 articles were found. Most research focuses on specific pollutant categories and employs variations of YOLO, Faster R-CNN, and EfficientDet architectures.Although several datasets exist, they are limited to specific areas and lack standardized taxonomies. Few studies integrate detection into real-time application systems, yet they tend to be geographically skewed. We proposed a framework for monitoring visual pollution that integrates a visual pollution index to assess the severity of visual pollution for a certain area.This review highlights the need for a unified UVP management system that incorporates pollutant taxonomy, a cross-city benchmark dataset, a generalized deep learning model, and an assessment index that supports sustainable urban aesthetics and enhances the well-being of urban dwellers.

</details>

---

## 57. Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning

**Chinese Title**: Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning

**Authors**: Xu Ma, Yitian Zhang, Qihua Dong, Yun Fu

**Date**: 2026-02-10 | **arXiv**: [2602.09439v1](http://arxiv.org/abs/2602.09439v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09439v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

High-quality and open datasets remain a major bottleneck for text-to-image (T2I) fine-tuning. Despite rapid progress in model architectures and training pipelines, most publicly available fine-tuning datasets suffer from low resolution, poor text-image alignment, or limited diversity, resulting in a clear performance gap between open research models and enterprise-grade models. In this work, we present Fine-T2I, a large-scale, high-quality, and fully open dataset for T2I fine-tuning. Fine-T2I spans 10 task combinations, 32 prompt categories, 11 visual styles, and 5 prompt templates, and combines synthetic images generated by strong modern models with carefully curated real images from professional photographers. All samples are rigorously filtered for text-image alignment, visual fidelity, and prompt quality, with over 95% of initial candidates removed. The final dataset contains over 6 million text-image pairs, around 2 TB on disk, approaching the scale of pretraining datasets while maintaining fine-tuning-level quality. Across a diverse set of pretrained diffusion and autoregressive models, fine-tuning on Fine-T2I consistently improves both generation quality and instruction adherence, as validated by human evaluation, visual comparison, and automatic metrics. We release Fine-T2I under an open license to help close the data gap in T2I fine-tuning in the open community.

</details>

<details><summary><b>Chinese Abstract</b></summary>

High-quality and open datasets remain a major bottleneck for text-to-image (T2I) fine-tuning. Despite rapid progress in model architectures and training pipelines, most publicly available fine-tuning datasets suffer from low resolution, poor text-image alignment, or limited diversity, resulting in a clear performance gap between open research models and enterprise-grade models.In this work, we present Fine-T2I, a large-scale, high-quality, and fully open dataset for T2I fine-tuning. Fine-T2I spans 10 task combinations, 32 prompt categories, 11 visual styles, and 5 prompt templates, and combines synthetic images generated by strong modern models with carefully curated real images from professional photographers.All samples are rigorously filtered for text-image alignment, visual fidelity, and prompt quality, with over 95% of initial candidates removed. The final dataset contains over 6 million text-image pairs, around 2 TB on disk, approaching the scale of pretraining datasets while maintaining fine-tuning-level quality.Across a diverse set of pretrained diffusion and autoregressive models, fine-tuning on Fine-T2I consistently improves both generation quality and instruction adherence, as validated by human evaluation, visual comparison, and automatic metrics. We release Fine-T2I under an open license to help close the data gap in T2I fine-tuning in the open community.

</details>

---

## 58. SceneReVis: A Self-Reflective Vision-Grounded Framework for 3D Indoor Scene Synthesis via Multi-turn RL

**Chinese Title**: SceneReVis: A Self-Reflective Vision-Grounded Framework for 3D Indoor Scene Synthesis via Multi-turn RL

**Authors**: Yang Zhao, Shizhao Sun, Meisheng Zhang, Yingdong Shi, Xubo Yang et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09432v1](http://arxiv.org/abs/2602.09432v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09432v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Current one-pass 3D scene synthesis methods often suffer from spatial hallucinations, such as collisions, due to a lack of deliberative reasoning. To bridge this gap, we introduce SceneReVis, a vision-grounded self-reflection framework that employs an iterative ``diagnose-and-act'' loop to explicitly intercept and resolve spatial conflicts using multi-modal feedback. To support this step-wise paradigm, we construct SceneChain-12k, a large-scale dataset of causal construction trajectories derived through a novel reverse engineering pipeline. We further propose a two-stage training recipe that transitions from Supervised Fine-Tuning to Agentic Reinforcement Learning, evolving the model into an active spatial planner. Extensive experiments demonstrate that SceneReVis achieves state-of-the-art performance in high-fidelity generation and goal-oriented optimization, with robust generalization to long-tail domains.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Current one-pass 3D scene synthesis methods often suffer from spatial hallucinations, such as collisions, due to a lack of deliberative reasoning. To bridge this gap, we introduce SceneReVis, a vision-grounded self-reflection framework that employs an iterative ``diagnose-and-act'' loop to explicitly intercept and resolve spatial conflicts using multi-modal feedback.To support this step-wise paradigm, we construct SceneChain-12k, a large-scale dataset of causal construction trajectories derived through a novel reverse engineering pipeline. We further propose a two-stage training recipe that transitions from Supervised Fine-Tuning to Agentic Reinforcement Learning, evolving the model into an active spatial planner.Extensive experiments demonstrate that SceneReVis achieves state-of-the-art performance in high-fidelity generation and goal-oriented optimization, with robust generalization to long-tail domains.

</details>

---

## 59. Understanding and Enhancing Encoder-based Adversarial Transferability against Large Vision-Language Models

**Chinese Title**: Understanding and Enhancing Encoder-based Adversarial Transferability against Large Vision-Language Models

**Authors**: Xinwei Zhang, Li Bai, Tianwei Zhang, Youqian Zhang, Qingqing Ye et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09431v1](http://arxiv.org/abs/2602.09431v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09431v1)

**Categories**: cs.CR, cs.CV

<details><summary><b>Abstract</b></summary>

Large vision-language models (LVLMs) have achieved impressive success across multimodal tasks, but their reliance on visual inputs exposes them to significant adversarial threats. Existing encoder-based attacks perturb the input image by optimizing solely on the vision encoder, rather than the entire LVLM, offering a computationally efficient alternative to end-to-end optimization. However, their transferability across different LVLM architectures in realistic black-box scenarios remains poorly understood. To address this gap, we present the first systematic study towards encoder-based adversarial transferability in LVLMs. Our contributions are threefold. First, through large-scale benchmarking over eight diverse LVLMs, we reveal that existing attacks exhibit severely limited transferability. Second, we perform in-depth analysis, disclosing two root causes that hinder the transferability: (1) inconsistent visual grounding across models, where different models focus their attention on distinct regions; (2) redundant semantic alignment within models, where a single object is dispersed across multiple overlapping token representations. Third, we propose Semantic-Guided Multimodal Attack (SGMA), a novel framework to enhance the transferability. Inspired by the discovered causes in our analysis, SGMA directs perturbations toward semantically critical regions and disrupts cross-modal grounding at both global and local levels. Extensive experiments across different victim models and tasks show that SGMA achieves higher transferability than existing attacks. These results expose critical security risks in LVLM deployment and underscore the urgent need for robust multimodal defenses.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Large vision-language models (LVLMs) have achieved impressive success across multimodal tasks, but their reliance on visual inputs exposes them to significant adversarial threats. Existing encoder-based attacks perturb the input image by optimizing solely on the vision encoder, rather than the entire LVLM, offering a computationally efficient alternative to end-to-end optimization.However, their transferability across different LVLM architectures in realistic black-box scenarios remains poorly understood. To address this gap, we present the first systematic study towards encoder-based adversarial transferability in LVLMs. Our contributions are threefold. First, through large-scale benchmarking over eight diverse LVLMs, we reveal that existing attacks exhibit severely limited transferability.Second, we perform in-depth analysis, disclosing two root causes that hinder the transferability: (1) inconsistent visual grounding across models, where different models focus their attention on distinct regions; (2) redundant semantic alignment within models, where a single object is dispersed across multiple overlapping token representations.Third, we propose Semantic-Guided Multimodal Attack (SGMA), a novel framework to enhance the transferability. Inspired by the discovered causes in our analysis, SGMA directs perturbations toward semantically critical regions and disrupts cross-modal grounding at both global and local levels. Extensive experiments across different victim models and tasks show that SGMA achieves higher transferability than existing attacks.These results expose critical security risks in LVLM deployment and underscore the urgent need for robust multimodal defenses.

</details>

---

## 60. Bridging the Modality Gap in Roadside LiDAR: A Training-Free Vision-Language Model Framework for Vehicle Classification

**Chinese Title**: Bridging the Modality Gap in Roadside LiDAR: A Training-Free Vision-Language Model Framework for Vehicle Classification

**Authors**: Yiqiao Li, Bo Shang, Jie Wei

**Date**: 2026-02-10 | **arXiv**: [2602.09425v1](http://arxiv.org/abs/2602.09425v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09425v1)

**Categories**: cs.CV, cs.LG

<details><summary><b>Abstract</b></summary>

Fine-grained truck classification is critical for intelligent transportation systems (ITS), yet current LiDAR-based methods face scalability challenges due to their reliance on supervised deep learning and labor-intensive manual annotation. Vision-Language Models (VLMs) offer promising few-shot generalization, but their application to roadside LiDAR is limited by a modality gap between sparse 3D point clouds and dense 2D imagery. We propose a framework that bridges this gap by adapting off-the-shelf VLMs for fine-grained truck classification without parameter fine-tuning. Our new depth-aware image generation pipeline applies noise removal, spatial and temporal registration, orientation rectification, morphological operations, and anisotropic smoothing to transform sparse, occluded LiDAR scans into depth-encoded 2D visual proxies. Validated on a real-world dataset of 20 vehicle classes, our approach achieves competitive classification accuracy with as few as 16-30 examples per class, offering a scalable alternative to data-intensive supervised baselines. We further observe a "Semantic Anchor" effect: text-based guidance regularizes performance in ultra-low-shot regimes $k < 4$, but degrades accuracy in more-shot settings due to semantic mismatch. Furthermore, we demonstrate the efficacy of this framework as a Cold Start strategy, using VLM-generated labels to bootstrap lightweight supervised models. Notably, the few-shot VLM-based model achieves over correct classification rate of 75 percent for specific drayage categories (20ft, 40ft, and 53ft containers) entirely without the costly training or fine-tuning, significantly reducing the intensive demands of initial manual labeling, thus achieving a method of practical use in ITS applications.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Fine-grained truck classification is critical for intelligent transportation systems (ITS), yet current LiDAR-based methods face scalability challenges due to their reliance on supervised deep learning and labor-intensive manual annotation. Vision-Language Models (VLMs) offer promising few-shot generalization, but their application to roadside LiDAR is limited by a modality gap between sparse 3D point clouds and dense 2D imagery.We propose a framework that bridges this gap by adapting off-the-shelf VLMs for fine-grained truck classification without parameter fine-tuning. Our new depth-aware image generation pipeline applies noise removal, spatial and temporal registration, orientation rectification, morphological operations, and anisotropic smoothing to transform sparse, occluded LiDAR scans into depth-encoded 2D visual proxies.Validated on a real-world dataset of 20 vehicle classes, our approach achieves competitive classification accuracy with as few as 16-30 examples per class, offering a scalable alternative to data-intensive supervised baselines. We further observe a "Semantic Anchor" effect: text-based guidance regularizes performance in ultra-low-shot regimes $k < 4$, but degrades accuracy in more-shot settings due to semantic mismatch.Furthermore, we demonstrate the efficacy of this framework as a Cold Start strategy, using VLM-generated labels to bootstrap lightweight supervised models.Notably, the few-shot VLM-based model achieves over correct classification rate of 75 percent for specific drayage categories (20ft, 40ft, and 53ft containers) entirely without the costly training or fine-tuning, significantly reducing the intensive demands of initial manual labeling, thus achieving a method of practical use in ITS applications.

</details>

---

## 61. LARV: Data-Free Layer-wise Adaptive Rescaling Veneer for Model Merging

**Chinese Title**: LARV: Data-Free Layer-wise Adaptive Rescaling Veneer for Model Merging

**Authors**: Xinyu Wang, Ke Deng, Fei Dou, Jinbo Bi, Jin Lu

**Date**: 2026-02-10 | **arXiv**: [2602.09413v1](http://arxiv.org/abs/2602.09413v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09413v1)

**Categories**: cs.CV, cs.AI, cs.LG

<details><summary><b>Abstract</b></summary>

Model merging aims to combine multiple fine-tuned models into a single multi-task model without access to training data. Existing task-vector merging methods such as TIES, TSV-M, and Iso-C/CTS differ in their aggregation rules but treat all layers nearly uniformly. This assumption overlooks the strong layer-wise heterogeneity in large vision transformers, where shallow layers are sensitive to interference while deeper layers encode stable task-specific features. We introduce LARV, a training-free, data-free, merger-agnostic Layer-wise Adaptive Rescaling Veneer that plugs into any task-vector merger and assigns a per-layer scale to each task vector before aggregation, and show it consistently boosts diverse merging rules. LARV adaptively suppresses shallow-layer interference and amplifies deeper-layer alignment using a simple deterministic schedule, requiring no retraining or modification to existing mergers. To our knowledge, this is the first work to perform layer-aware scaling for task-vector merging. LARV computes simple data-free layer proxies and turns them into scales through a lightweight rule; we study several instantiations within one framework (e.g., tiered two/three-level scaling with fixed values, or continuous mappings) and show that tiered choices offer the best robustness, while continuous mappings remain an ablation. LARV is orthogonal to the base merger and adds negligible cost. On FusionBench with Vision Transformers, LARV consistently improves all task-vector baselines across 8/14/20-task settings; for example, Iso-C + LARV reaches 85.9% on ViT-B/32, 89.2% on ViT-B/16, and 92.6% on ViT-L/14. Layerwise analysis and corruption tests further indicate that LARV suppresses shallow-layer interference while modestly amplifying deeper, task-stable features, turning model merging into a robust, layer-aware procedure rather than a uniform one.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Model merging aims to combine multiple fine-tuned models into a single multi-task model without access to training data. Existing task-vector merging methods such as TIES, TSV-M, and Iso-C/CTS differ in their aggregation rules but treat all layers nearly uniformly.This assumption overlooks the strong layer-wise heterogeneity in large vision transformers, where shallow layers are sensitive to interference while deeper layers encode stable task-specific features.We introduce LARV, a training-free, data-free, merger-agnostic Layer-wise Adaptive Rescaling Veneer that plugs into any task-vector merger and assigns a per-layer scale to each task vector before aggregation, and show it consistently boosts diverse merging rules.LARV adaptively suppresses shallow-layer interference and amplifies deeper-layer alignment using a simple deterministic schedule, requiring no retraining or modification to existing mergers. To our knowledge, this is the first work to perform layer-aware scaling for task-vector merging.LARV computes simple data-free layer proxies and turns them into scales through a lightweight rule; we study several instantiations within one framework (e.g., tiered two/three-level scaling with fixed values, or continuous mappings) and show that tiered choices offer the best robustness, while continuous mappings remain an ablation. LARV is orthogonal to the base merger and adds negligible cost.On FusionBench with Vision Transformers, LARV consistently improves all task-vector baselines across 8/14/20-task settings; for example, Iso-C + LARV reaches 85.9% on ViT-B/32, 89.2% on ViT-B/16, and 92.6% on ViT-L/14.Layerwise analysis and corruption tests further indicate that LARV suppresses shallow-layer interference while modestly amplifying deeper, task-stable features, turning model merging into a robust, layer-aware procedure rather than a uniform one.

</details>

---

## 62. K-Sort Eval: Efficient Preference Evaluation for Visual Generation via Corrected VLM-as-a-Judge

**Chinese Title**: K-Sort Eval: Efficient Preference Evaluation for Visual Generation via Corrected VLM-as-a-Judge

**Authors**: Zhikai Li, Jiatong Li, Xuewen Liu, Wangbo Zhao, Pan Du et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09411v1](http://arxiv.org/abs/2602.09411v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09411v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

The rapid development of visual generative models raises the need for more scalable and human-aligned evaluation methods. While the crowdsourced Arena platforms offer human preference assessments by collecting human votes, they are costly and time-consuming, inherently limiting their scalability. Leveraging vision-language model (VLMs) as substitutes for manual judgments presents a promising solution. However, the inherent hallucinations and biases of VLMs hinder alignment with human preferences, thus compromising evaluation reliability. Additionally, the static evaluation approach lead to low efficiency. In this paper, we propose K-Sort Eval, a reliable and efficient VLM-based evaluation framework that integrates posterior correction and dynamic matching. Specifically, we curate a high-quality dataset from thousands of human votes in K-Sort Arena, with each instance containing the outputs and rankings of K models. When evaluating a new model, it undergoes (K+1)-wise free-for-all comparisons with existing models, and the VLM provide the rankings. To enhance alignment and reliability, we propose a posterior correction method, which adaptively corrects the posterior probability in Bayesian updating based on the consistency between the VLM prediction and human supervision. Moreover, we propose a dynamic matching strategy, which balances uncertainty and diversity to maximize the expected benefit of each comparison, thus ensuring more efficient evaluation. Extensive experiments show that K-Sort Eval delivers evaluation results consistent with K-Sort Arena, typically requiring fewer than 90 model runs, demonstrating both its efficiency and reliability.

</details>

<details><summary><b>Chinese Abstract</b></summary>

The rapid development of visual generative models raises the need for more scalable and human-aligned evaluation methods. While the crowdsourced Arena platforms offer human preference assessments by collecting human votes, they are costly and time-consuming, inherently limiting their scalability. Leveraging vision-language model (VLMs) as substitutes for manual judgments presents a promising solution.However, the inherent hallucinations and biases of VLMs hinder alignment with human preferences, thus compromising evaluation reliability. Additionally, the static evaluation approach lead to low efficiency. In this paper, we propose K-Sort Eval, a reliable and efficient VLM-based evaluation framework that integrates posterior correction and dynamic matching.Specifically, we curate a high-quality dataset from thousands of human votes in K-Sort Arena, with each instance containing the outputs and rankings of K models. When evaluating a new model, it undergoes (K+1)-wise free-for-all comparisons with existing models, and the VLM provide the rankings.To enhance alignment and reliability, we propose a posterior correction method, which adaptively corrects the posterior probability in Bayesian updating based on the consistency between the VLM prediction and human supervision. Moreover, we propose a dynamic matching strategy, which balances uncertainty and diversity to maximize the expected benefit of each comparison, thus ensuring more efficient evaluation.Extensive experiments show that K-Sort Eval delivers evaluation results consistent with K-Sort Arena, typically requiring fewer than 90 model runs, demonstrating both its efficiency and reliability.

</details>

---

## 63. Single-Slice-to-3D Reconstruction in Medical Imaging and Natural Objects: A Comparative Benchmark with SAM 3D

**Chinese Title**: Single-Slice-to-3D Reconstruction in Medical Imaging and Natural Objects: A Comparative Benchmark with SAM 3D

**Authors**: Yan Luo, Advaith Ravishankar, Serena Liu, Yutong Yang, Mengyu Wang

**Date**: 2026-02-10 | **arXiv**: [2602.09407v1](http://arxiv.org/abs/2602.09407v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09407v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

A 3D understanding of anatomy is central to diagnosis and treatment planning, yet volumetric imaging remains costly with long wait times. Image-to-3D foundations models can solve this issue by reconstructing 3D data from 2D modalites. Current foundation models are trained on natural image distributions to reconstruct naturalistic objects from a single image by leveraging geometric priors across pixels. However, it is unclear whether these learned geometric priors transfer to medical data. In this study, we present a controlled zero-shot benchmark of single slice medical image-to-3D reconstruction across five state-of-the-art image-to-3D models: SAM3D, Hunyuan3D-2.1, Direct3D, Hi3DGen, and TripoSG. These are evaluated across six medical datasets spanning anatomical and pathological structures and two natrual datasets, using voxel based metrics and point cloud distance metrics. Across medical datasets, voxel based overlap remains moderate for all models, consistent with a depth reconstruction failure mode when inferring volume from a single slice. In contrast, global distance metrics show more separation between methods: SAM3D achieves the strongest overall topological similarity to ground truth medical 3D data, while alternative models are more prone to over-simplication of reconstruction. Our results quantify the limits of single-slice medical reconstruction and highlight depth ambiguity caused by the planar nature of 2D medical data, motivating multi-view image-to-3D reconstruction to enable reliable medical 3D inference.

</details>

<details><summary><b>Chinese Abstract</b></summary>

A 3D understanding of anatomy is central to diagnosis and treatment planning, yet volumetric imaging remains costly with long wait times. Image-to-3D foundations models can solve this issue by reconstructing 3D data from 2D modalites. Current foundation models are trained on natural image distributions to reconstruct naturalistic objects from a single image by leveraging geometric priors across pixels.However, it is unclear whether these learned geometric priors transfer to medical data. In this study, we present a controlled zero-shot benchmark of single slice medical image-to-3D reconstruction across five state-of-the-art image-to-3D models: SAM3D, Hunyuan3D-2.1, Direct3D, Hi3DGen, and TripoSG.These are evaluated across six medical datasets spanning anatomical and pathological structures and two natrual datasets, using voxel based metrics and point cloud distance metrics. Across medical datasets, voxel based overlap remains moderate for all models, consistent with a depth reconstruction failure mode when inferring volume from a single slice.In contrast, global distance metrics show more separation between methods: SAM3D achieves the strongest overall topological similarity to ground truth medical 3D data, while alternative models are more prone to over-simplication of reconstruction.Our results quantify the limits of single-slice medical reconstruction and highlight depth ambiguity caused by the planar nature of 2D medical data, motivating multi-view image-to-3D reconstruction to enable reliable medical 3D inference.

</details>

---

## 64. Fully Differentiable Bidirectional Dual-Task Synergistic Learning for Semi-Supervised 3D Medical Image Segmentation

**Chinese Title**: Fully Differentiable Bidirectional Dual-Task Synergistic Learning for Semi-Supervised 3D Medical Image Segmentation

**Authors**: Jun Li

**Date**: 2026-02-10 | **arXiv**: [2602.09378v1](http://arxiv.org/abs/2602.09378v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09378v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Semi-supervised learning relaxes the need of large pixel-wise labeled datasets for image segmentation by leveraging unlabeled data. The scarcity of high-quality labeled data remains a major challenge in medical image analysis due to the high annotation costs and the need for specialized clinical expertise. Semi-supervised learning has demonstrated significant potential in addressing this bottleneck, with pseudo-labeling and consistency regularization emerging as two predominant paradigms. Dual-task collaborative learning, an emerging consistency-aware paradigm, seeks to derive supplementary supervision by establishing prediction consistency between related tasks. However, current methodologies are limited to unidirectional interaction mechanisms (typically regression-to-segmentation), as segmentation results can only be transformed into regression outputs in an offline manner, thereby failing to fully exploit the potential benefits of online bidirectional cross-task collaboration. Thus, we propose a fully Differentiable Bidirectional Synergistic Learning (DBiSL) framework, which seamlessly integrates and enhances four critical SSL components: supervised learning, consistency regularization, pseudo-supervised learning, and uncertainty estimation. Experiments on two benchmark datasets demonstrate our method's state-of-the-art performance. Beyond technical contributions, this work provides new insights into unified SSL framework design and establishes a new architectural foundation for dual-task-driven SSL, while offering a generic multitask learning framework applicable to broader computer vision applications. The code will be released on github upon acceptance.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Semi-supervised learning relaxes the need of large pixel-wise labeled datasets for image segmentation by leveraging unlabeled data. The scarcity of high-quality labeled data remains a major challenge in medical image analysis due to the high annotation costs and the need for specialized clinical expertise.Semi-supervised learning has demonstrated significant potential in addressing this bottleneck, with pseudo-labeling and consistency regularization emerging as two predominant paradigms. Dual-task collaborative learning, an emerging consistency-aware paradigm, seeks to derive supplementary supervision by establishing prediction consistency between related tasks.However, current methodologies are limited to unidirectional interaction mechanisms (typically regression-to-segmentation), as segmentation results can only be transformed into regression outputs in an offline manner, thereby failing to fully exploit the potential benefits of online bidirectional cross-task collaboration.Thus, we propose a fully Differentiable Bidirectional Synergistic Learning (DBiSL) framework, which seamlessly integrates and enhances four critical SSL components: supervised learning, consistency regularization, pseudo-supervised learning, and uncertainty estimation. Experiments on two benchmark datasets demonstrate our method's state-of-the-art performance.Beyond technical contributions, this work provides new insights into unified SSL framework design and establishes a new architectural foundation for dual-task-driven SSL, while offering a generic multitask learning framework applicable to broader computer vision applications. The code will be released on github upon acceptance.

</details>

---

## 65. Deep Modeling and Interpretation for Bladder Cancer Classification

**Chinese Title**: Deep Modeling and Interpretation for Bladder Cancer Classification

**Authors**: Ahmad Chaddad, Yihang Wu, Xianrui Chen

**Date**: 2026-02-10 | **arXiv**: [2602.09324v1](http://arxiv.org/abs/2602.09324v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09324v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Deep models based on vision transformer (ViT) and convolutional neural network (CNN) have demonstrated remarkable performance on natural datasets. However, these models may not be similar in medical imaging, where abnormal regions cover only a small portion of the image. This challenge motivates this study to investigate the latest deep models for bladder cancer classification tasks. We propose the following to evaluate these deep models: 1) standard classification using 13 models (four CNNs and eight transormer-based models), 2) calibration analysis to examine if these models are well calibrated for bladder cancer classification, and 3) we use GradCAM++ to evaluate the interpretability of these models for clinical diagnosis. We simulate $\sim 300$ experiments on a publicly multicenter bladder cancer dataset, and the experimental results demonstrate that the ConvNext series indicate limited generalization ability to classify bladder cancer images (e.g., $\sim 60\%$ accuracy). In addition, ViTs show better calibration effects compared to ConvNext and swin transformer series. We also involve test time augmentation to improve the models interpretability. Finally, no model provides a one-size-fits-all solution for a feasible interpretable model. ConvNext series are suitable for in-distribution samples, while ViT and its variants are suitable for interpreting out-of-distribution samples.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Deep models based on vision transformer (ViT) and convolutional neural network (CNN) have demonstrated remarkable performance on natural datasets. However, these models may not be similar in medical imaging, where abnormal regions cover only a small portion of the image. This challenge motivates this study to investigate the latest deep models for bladder cancer classification tasks.We propose the following to evaluate these deep models: 1) standard classification using 13 models (four CNNs and eight transormer-based models), 2) calibration analysis to examine if these models are well calibrated for bladder cancer classification, and 3) we use GradCAM++ to evaluate the interpretability of these models for clinical diagnosis.We simulate $\sim 300$ experiments on a publicly multicenter bladder cancer dataset, and the experimental results demonstrate that the ConvNext series indicate limited generalization ability to classify bladder cancer images (e.g., $\sim 60\%$ accuracy). In addition, ViTs show better calibration effects compared to ConvNext and swin transformer series. We also involve test time augmentation to improve the models interpretability.Finally, no model provides a one-size-fits-all solution for a feasible interpretable model. ConvNext series are suitable for in-distribution samples, while ViT and its variants are suitable for interpreting out-of-distribution samples.

</details>

---

## 66. GAFR-Net: A Graph Attention and Fuzzy-Rule Network for Interpretable Breast Cancer Image Classification

**Chinese Title**: GAFR-Net: A Graph Attention and Fuzzy-Rule Network for Interpretable Breast Cancer Image Classification

**Authors**: Lin-Guo Gao, Suxing Liu

**Date**: 2026-02-10 | **arXiv**: [2602.09318v1](http://arxiv.org/abs/2602.09318v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09318v1)

**Categories**: cs.CV, cs.AI

<details><summary><b>Abstract</b></summary>

Accurate classification of breast cancer histopathology images is pivotal for early oncological diagnosis and therapeutic intervention.However, conventional deep learning architectures often encounter performance degradation under limited annotations and suffer from a "blackbox" nature, hindering their clinical integration. To mitigate these limitations, we propose GAFRNet, a robust and interpretable Graph Attention and FuzzyRule Network specifically engineered for histopathology image classification with scarce supervision. GAFRNet constructs a similarity-driven graph representation to model intersample relationships and employs a multihead graph attention mechanism to capture complex relational features across heterogeneous tissue structures.Concurrently, a differentiable fuzzy-rule module encodes intrinsic topological descriptorsincluding node degree, clustering coefficient, and label consistencyinto explicit, human-understandable diagnostic logic. This design establishes transparent "IF-THEN" mappings that mimic the heuristic deduction process of medical experts, providing clear reasoning behind each prediction without relying on post-hoc attribution methods. Extensive evaluations on three benchmark datasets (BreakHis, Mini-DDSM, and ICIAR2018) demonstrate that GAFR-Net consistently outperforms various state-of-the-art methods across multiple magnifications and classification tasks. These results validate the superior generalization and practical utility of GAFR-Net as a reliable decision-support tool for weakly supervised medical image analysis.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Accurate classification of breast cancer histopathology images is pivotal for early oncological diagnosis and therapeutic intervention.However, conventional deep learning architectures often encounter performance degradation under limited annotations and suffer from a "blackbox" nature, hindering their clinical integration.To mitigate these limitations, we propose GAFRNet, a robust and interpretable Graph Attention and FuzzyRule Network specifically engineered for histopathology image classification with scarce supervision.GAFRNet constructs a similarity-driven graph representation to model intersample relationships and employs a multihead graph attention mechanism to capture complex relational features across heterogeneous tissue structures.Concurrently, a differentiable fuzzy-rule module encodes intrinsic topological descriptorsincluding node degree, clustering coefficient, and label consistencyinto explicit, human-understandable diagnostic logic.This design establishes transparent "IF-THEN" mappings that mimic the heuristic deduction process of medical experts, providing clear reasoning behind each prediction without relying on post-hoc attribution methods. Extensive evaluations on three benchmark datasets (BreakHis, Mini-DDSM, and ICIAR2018) demonstrate that GAFR-Net consistently outperforms various state-of-the-art methods across multiple magnifications and classification tasks.These results validate the superior generalization and practical utility of GAFR-Net as a reliable decision-support tool for weakly supervised medical image analysis.

</details>

---

## 67. X-Mark: Saliency-Guided Robust Dataset Ownership Verification for Medical Imaging

**Chinese Title**: X-Mark: Saliency-Guided Robust Dataset Ownership Verification for Medical Imaging

**Authors**: Pranav Kulkarni, Junfeng Guo, Heng Huang

**Date**: 2026-02-10 | **arXiv**: [2602.09284v1](http://arxiv.org/abs/2602.09284v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09284v1)

**Categories**: cs.CV, cs.AI, cs.LG

<details><summary><b>Abstract</b></summary>

High-quality medical imaging datasets are essential for training deep learning models, but their unauthorized use raises serious copyright and ethical concerns. Medical imaging presents a unique challenge for existing dataset ownership verification methods designed for natural images, as static watermark patterns generated in fixed-scale images scale poorly dynamic and high-resolution scans with limited visual diversity and subtle anatomical structures, while preserving diagnostic quality. In this paper, we propose X-Mark, a sample-specific clean-label watermarking method for chest x-ray copyright protection. Specifically, X-Mark uses a conditional U-Net to generate unique perturbations within salient regions of each sample. We design a multi-component training objective to ensure watermark efficacy, robustness against dynamic scaling processes while preserving diagnostic quality and visual-distinguishability. We incorporate Laplacian regularization into our training objective to penalize high-frequency perturbations and achieve watermark scale-invariance. Ownership verification is performed in a black-box setting to detect characteristic behaviors in suspicious models. Extensive experiments on CheXpert verify the effectiveness of X-Mark, achieving WSR of 100% and reducing probability of false positives in Ind-M scenario by 12%, while demonstrating resistance to potential adaptive attacks.

</details>

<details><summary><b>Chinese Abstract</b></summary>

High-quality medical imaging datasets are essential for training deep learning models, but their unauthorized use raises serious copyright and ethical concerns.Medical imaging presents a unique challenge for existing dataset ownership verification methods designed for natural images, as static watermark patterns generated in fixed-scale images scale poorly dynamic and high-resolution scans with limited visual diversity and subtle anatomical structures, while preserving diagnostic quality.In this paper, we propose X-Mark, a sample-specific clean-label watermarking method for chest x-ray copyright protection. Specifically, X-Mark uses a conditional U-Net to generate unique perturbations within salient regions of each sample. We design a multi-component training objective to ensure watermark efficacy, robustness against dynamic scaling processes while preserving diagnostic quality and visual-distinguishability.We incorporate Laplacian regularization into our training objective to penalize high-frequency perturbations and achieve watermark scale-invariance. Ownership verification is performed in a black-box setting to detect characteristic behaviors in suspicious models.Extensive experiments on CheXpert verify the effectiveness of X-Mark, achieving WSR of 100% and reducing probability of false positives in Ind-M scenario by 12%, while demonstrating resistance to potential adaptive attacks.

</details>

---

## 68. Anagent For Enhancing Scientific Table & Figure Analysis

**Chinese Title**: Anagent For Enhancing Scientific Table & Figure Analysis

**Authors**: Xuehang Guo, Zhiyong Lu, Tom Hope, Qingyun Wang

**Date**: 2026-02-10 | **arXiv**: [2602.10081v1](http://arxiv.org/abs/2602.10081v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10081v1)

**Project**: https://xhguo7.github.io/Anagent/.  **Categories**: cs.CL, cs.AI

<details><summary><b>Abstract</b></summary>

In scientific research, analysis requires accurately interpreting complex multimodal knowledge, integrating evidence from different sources, and drawing inferences grounded in domain-specific knowledge. However, current artificial intelligence (AI) systems struggle to consistently demonstrate such capabilities. The complexity and variability of scientific tables and figures, combined with heterogeneous structures and long-context requirements, pose fundamental obstacles to scientific table \& figure analysis. To quantify these challenges, we introduce AnaBench, a large-scale benchmark featuring $63,178$ instances from nine scientific domains, systematically categorized along seven complexity dimensions. To tackle these challenges, we propose Anagent, a multi-agent framework for enhanced scientific table \& figure analysis through four specialized agents: Planner decomposes tasks into actionable subtasks, Expert retrieves task-specific information through targeted tool execution, Solver synthesizes information to generate coherent analysis, and Critic performs iterative refinement through five-dimensional quality assessment. We further develop modular training strategies that leverage supervised finetuning and specialized reinforcement learning to optimize individual capabilities while maintaining effective collaboration. Comprehensive evaluation across 170 subdomains demonstrates that Anagent achieves substantial improvements, up to $\uparrow 13.43\%$ in training-free settings and $\uparrow 42.12\%$ with finetuning, while revealing that task-oriented reasoning and context-aware problem-solving are essential for high-quality scientific table \& figure analysis. Our project page: https://xhguo7.github.io/Anagent/.

</details>

<details><summary><b>Chinese Abstract</b></summary>

In scientific research, analysis requires accurately interpreting complex multimodal knowledge, integrating evidence from different sources, and drawing inferences grounded in domain-specific knowledge. However, current artificial intelligence (AI) systems struggle to consistently demonstrate such capabilities.The complexity and variability of scientific tables and figures, combined with heterogeneous structures and long-context requirements, pose fundamental obstacles to scientific table \& figure analysis. To quantify these challenges, we introduce AnaBench, a large-scale benchmark featuring $63,178$ instances from nine scientific domains, systematically categorized along seven complexity dimensions.To tackle these challenges, we propose Anagent, a multi-agent framework for enhanced scientific table \& figure analysis through four specialized agents: Planner decomposes tasks into actionable subtasks, Expert retrieves task-specific information through targeted tool execution, Solver synthesizes information to generate coherent analysis, and Critic performs iterative refinement through five-dimensional quality assessment.We further develop modular training strategies that leverage supervised finetuning and specialized reinforcement learning to optimize individual capabilities while maintaining effective collaboration.Comprehensive evaluation across 170 subdomains demonstrates that Anagent achieves substantial improvements, up to $\uparrow 13.43\%$ in training-free settings and $\uparrow 42.12\%$ with finetuning, while revealing that task-oriented reasoning and context-aware problem-solving are essential for high-quality scientific table \& figure analysis. Our project page: https://xhguo7.github.io/Anagent/.

</details>

---

## 69. RoboSubtaskNet: Temporal Sub-task Segmentation for Human-to-Robot Skill Transfer in Real-World Environments

**Chinese Title**: RoboSubtaskNet: Temporal Sub-task Segmentation for Human-to-Robot Skill Transfer in Real-World Environments

**Authors**: Dharmendra Sharma, Archit Sharma, John Reberio, Vaibhav Kesharwani, Peeyush Thakur et al.

**Date**: 2026-02-10 | **arXiv**: [2602.10015v1](http://arxiv.org/abs/2602.10015v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10015v1)

**Categories**: cs.RO, cs.AI

<details><summary><b>Abstract</b></summary>

Temporally locating and classifying fine-grained sub-task segments in long, untrimmed videos is crucial to safe human-robot collaboration. Unlike generic activity recognition, collaborative manipulation requires sub-task labels that are directly robot-executable. We present RoboSubtaskNet, a multi-stage human-to-robot sub-task segmentation framework that couples attention-enhanced I3D features (RGB plus optical flow) with a modified MS-TCN employing a Fibonacci dilation schedule to capture better short-horizon transitions such as reach-pick-place. The network is trained with a composite objective comprising cross-entropy and temporal regularizers (truncated MSE and a transition-aware term) to reduce over-segmentation and to encourage valid sub-task progressions. To close the gap between vision benchmarks and control, we introduce RoboSubtask, a dataset of healthcare and industrial demonstrations annotated at the sub-task level and designed for deterministic mapping to manipulator primitives. Empirically, RoboSubtaskNet outperforms MS-TCN and MS-TCN++ on GTEA and our RoboSubtask benchmark (boundary-sensitive and sequence metrics), while remaining competitive on the long-horizon Breakfast benchmark. Specifically, RoboSubtaskNet attains F1 @ 50 = 79.5%, Edit = 88.6%, Acc = 78.9% on GTEA; F1 @ 50 = 30.4%, Edit = 52.0%, Acc = 53.5% on Breakfast; and F1 @ 50 = 94.2%, Edit = 95.6%, Acc = 92.2% on RoboSubtask. We further validate the full perception-to-execution pipeline on a 7-DoF Kinova Gen3 manipulator, achieving reliable end-to-end behavior in physical trials (overall task success approx 91.25%). These results demonstrate a practical path from sub-task level video understanding to deployed robotic manipulation in real-world settings.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Temporally locating and classifying fine-grained sub-task segments in long, untrimmed videos is crucial to safe human-robot collaboration. Unlike generic activity recognition, collaborative manipulation requires sub-task labels that are directly robot-executable.We present RoboSubtaskNet, a multi-stage human-to-robot sub-task segmentation framework that couples attention-enhanced I3D features (RGB plus optical flow) with a modified MS-TCN employing a Fibonacci dilation schedule to capture better short-horizon transitions such as reach-pick-place.The network is trained with a composite objective comprising cross-entropy and temporal regularizers (truncated MSE and a transition-aware term) to reduce over-segmentation and to encourage valid sub-task progressions.To close the gap between vision benchmarks and control, we introduce RoboSubtask, a dataset of healthcare and industrial demonstrations annotated at the sub-task level and designed for deterministic mapping to manipulator primitives. Empirically, RoboSubtaskNet outperforms MS-TCN and MS-TCN++ on GTEA and our RoboSubtask benchmark (boundary-sensitive and sequence metrics), while remaining competitive on the long-horizon Breakfast benchmark.Specifically, RoboSubtaskNet attains F1 @ 50 = 79.5%, Edit = 88.6%, Acc = 78.9% on GTEA; F1 @ 50 = 30.4%, Edit = 52.0%, Acc = 53.5% on Breakfast; and F1 @ 50 = 94.2%, Edit = 95.6%, Acc = 92.2% on RoboSubtask. We further validate the full perception-to-execution pipeline on a 7-DoF Kinova Gen3 manipulator, achieving reliable end-to-end behavior in physical trials (overall task success approx 91.25%).These results demonstrate a practical path from sub-task level video understanding to deployed robotic manipulation in real-world settings.

</details>

---

## 70. Discovering High Level Patterns from Simulation Traces

**Chinese Title**: Discovering High Level Patterns from Simulation Traces

**Authors**: Sean Memery, Kartic Subr

**Date**: 2026-02-10 | **arXiv**: [2602.10009v1](http://arxiv.org/abs/2602.10009v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10009v1)

**Categories**: cs.AI, cs.HC

<details><summary><b>Abstract</b></summary>

Artificial intelligence (AI) agents embedded in environments with physics-based interaction face many challenges including reasoning, planning, summarization, and question answering. This problem is exacerbated when a human user wishes to either guide or interact with the agent in natural language. Although the use of Language Models (LMs) is the default choice, as an AI tool, they struggle with tasks involving physics. The LM's capability for physical reasoning is learned from observational data, rather than being grounded in simulation. A common approach is to include simulation traces as context, but this suffers from poor scalability as simulation traces contain larger volumes of fine-grained numerical and semantic data. In this paper, we propose a natural language guided method to discover coarse-grained patterns (e.g., 'rigid-body collision', 'stable support', etc.) from detailed simulation logs. Specifically, we synthesize programs that operate on simulation logs and map them to a series of high level activated patterns. We show, through two physics benchmarks, that this annotated representation of the simulation log is more amenable to natural language reasoning about physical systems. We demonstrate how this method enables LMs to generate effective reward programs from goals specified in natural language, which may be used within the context of planning or supervised learning.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Artificial intelligence (AI) agents embedded in environments with physics-based interaction face many challenges including reasoning, planning, summarization, and question answering. This problem is exacerbated when a human user wishes to either guide or interact with the agent in natural language. Although the use of Language Models (LMs) is the default choice, as an AI tool, they struggle with tasks involving physics.The LM's capability for physical reasoning is learned from observational data, rather than being grounded in simulation. A common approach is to include simulation traces as context, but this suffers from poor scalability as simulation traces contain larger volumes of fine-grained numerical and semantic data.In this paper, we propose a natural language guided method to discover coarse-grained patterns (e.g., 'rigid-body collision', 'stable support', etc.) from detailed simulation logs. Specifically, we synthesize programs that operate on simulation logs and map them to a series of high level activated patterns.We show, through two physics benchmarks, that this annotated representation of the simulation log is more amenable to natural language reasoning about physical systems. We demonstrate how this method enables LMs to generate effective reward programs from goals specified in natural language, which may be used within the context of planning or supervised learning.

</details>

---

## 71. ESTAR: Early-Stopping Token-Aware Reasoning For Efficient Inference

**Chinese Title**: ESTAR: Early-Stopping Token-Aware Reasoning For Efficient Inference

**Authors**: Junda Wang, Zhichao Yang, Dongxu Zhang, Sanjit Singh Batra, Robert E. Tillman

**Date**: 2026-02-10 | **arXiv**: [2602.10004v1](http://arxiv.org/abs/2602.10004v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10004v1)

**Categories**: cs.AI

<details><summary><b>Abstract</b></summary>

Large reasoning models (LRMs) achieve state-of-the-art performance by generating long chains-of-thought, but often waste computation on redundant reasoning after the correct answer has already been reached. We introduce Early-Stopping for Token-Aware Reasoning (ESTAR), which detects and reduces such reasoning redundancy to improve efficiency without sacrificing accuracy. Our method combines (i) a trajectory-based classifier that identifies when reasoning can be safely stopped, (ii) supervised fine-tuning to teach LRMs to propose self-generated <stop> signals, and (iii) <stop>-aware reinforcement learning that truncates rollouts at self-generated stop points with compute-aware rewards. Experiments on four reasoning datasets show that ESTAR reduces reasoning length by about 3.7x (from 4,799 to 1,290) while preserving accuracy (74.9% vs. 74.2%), with strong cross-domain generalization. These results highlight early stopping as a simple yet powerful mechanism for improving reasoning efficiency in LRMs.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Large reasoning models (LRMs) achieve state-of-the-art performance by generating long chains-of-thought, but often waste computation on redundant reasoning after the correct answer has already been reached. We introduce Early-Stopping for Token-Aware Reasoning (ESTAR), which detects and reduces such reasoning redundancy to improve efficiency without sacrificing accuracy.Our method combines (i) a trajectory-based classifier that identifies when reasoning can be safely stopped, (ii) supervised fine-tuning to teach LRMs to propose self-generated <stop> signals, and (iii) <stop>-aware reinforcement learning that truncates rollouts at self-generated stop points with compute-aware rewards.Experiments on four reasoning datasets show that ESTAR reduces reasoning length by about 3.7x (from 4,799 to 1,290) while preserving accuracy (74.9% vs. 74.2%), with strong cross-domain generalization. These results highlight early stopping as a simple yet powerful mechanism for improving reasoning efficiency in LRMs.

</details>

---

## 72. A Unified Assessment of the Poverty of the Stimulus Argument for Neural Language Models

**Chinese Title**: A Unified Assessment of the Poverty of the Stimulus Argument for Neural Language Models

**Authors**: Xiulin Yang, Arianna Bisazza, Nathan Schneider, Ethan Gotlieb Wilcox

**Date**: 2026-02-10 | **arXiv**: [2602.09992v1](http://arxiv.org/abs/2602.09992v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09992v1)

**Categories**: cs.CL, cs.AI

<details><summary><b>Abstract</b></summary>

How can children acquire native-level syntax from limited input? According to the Poverty of the Stimulus Hypothesis (PoSH), the linguistic input children receive is insufficient to explain certain generalizations that are robustly learned; innate linguistic constraints, many have argued, are thus necessary to explain language learning. Neural language models, which lack such language-specific constraints in their design, offer a computational test of this longstanding (but controversial) claim. We introduce \poshbench, a training-and-evaluation suite targeting question formation, islands to movement, and other English phenomena at the center of the PoSH arguments. Training Transformer models on 10--50M words of developmentally plausible text, we find indications of generalization on all phenomena even without direct positive evidence -- yet neural models remain less data-efficient and their generalizations are weaker than those of children. We further enhance our models with three recently proposed cognitively motivated inductive biases. We find these biases improve general syntactic competence but not \poshbench performance. Our findings challenge the claim that innate syntax is the only possible route to generalization, while suggesting that human-like data efficiency requires inductive biases beyond those tested here.

</details>

<details><summary><b>Chinese Abstract</b></summary>

How can children acquire native-level syntax from limited input? According to the Poverty of the Stimulus Hypothesis (PoSH), the linguistic input children receive is insufficient to explain certain generalizations that are robustly learned; innate linguistic constraints, many have argued, are thus necessary to explain language learning.Neural language models, which lack such language-specific constraints in their design, offer a computational test of this longstanding (but controversial) claim. We introduce \poshbench, a training-and-evaluation suite targeting question formation, islands to movement, and other English phenomena at the center of the PoSH arguments.Training Transformer models on 10--50M words of developmentally plausible text, we find indications of generalization on all phenomena even without direct positive evidence -- yet neural models remain less data-efficient and their generalizations are weaker than those of children. We further enhance our models with three recently proposed cognitively motivated inductive biases.We find these biases improve general syntactic competence but not \poshbench performance. Our findings challenge the claim that innate syntax is the only possible route to generalization, while suggesting that human-like data efficiency requires inductive biases beyond those tested here.

</details>

---

## 73. Empirical Stability Analysis of Kolmogorov-Arnold Networks in Hard-Constrained Recurrent Physics-Informed Discovery

**Chinese Title**: Empirical Stability Analysis of Kolmogorov-Arnold Networks in Hard-Constrained Recurrent Physics-Informed Discovery

**Authors**: Enzo Nicolas Spotorno, Josafat Leal Filho, Antonio Augusto Medeiros Frohlich

**Date**: 2026-02-10 | **arXiv**: [2602.09988v1](http://arxiv.org/abs/2602.09988v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09988v1)

**Categories**: cs.LG, cs.AI, physics.comp-ph

<details><summary><b>Abstract</b></summary>

We investigate the integration of Kolmogorov-Arnold Networks (KANs) into hard-constrained recurrent physics-informed architectures (HRPINN) to evaluate the fidelity of learned residual manifolds in oscillatory systems. Motivated by the Kolmogorov-Arnold representation theorem and preliminary gray-box results, we hypothesized that KANs would enable efficient recovery of unknown terms compared to MLPs. Through initial sensitivity analysis on configuration sensitivity, parameter scale, and training paradigm, we found that while small KANs are competitive on univariate polynomial residuals (Duffing), they exhibit severe hyperparameter fragility, instability in deeper configurations, and consistent failure on multiplicative terms (Van der Pol), generally outperformed by standard MLPs. These empirical challenges highlight limitations of the additive inductive bias in the original KAN formulation for state coupling and provide preliminary empirical evidence of inductive bias limitations for future hybrid modeling.

</details>

<details><summary><b>Chinese Abstract</b></summary>

We investigate the integration of Kolmogorov-Arnold Networks (KANs) into hard-constrained recurrent physics-informed architectures (HRPINN) to evaluate the fidelity of learned residual manifolds in oscillatory systems. Motivated by the Kolmogorov-Arnold representation theorem and preliminary gray-box results, we hypothesized that KANs would enable efficient recovery of unknown terms compared to MLPs.Through initial sensitivity analysis on configuration sensitivity, parameter scale, and training paradigm, we found that while small KANs are competitive on univariate polynomial residuals (Duffing), they exhibit severe hyperparameter fragility, instability in deeper configurations, and consistent failure on multiplicative terms (Van der Pol), generally outperformed by standard MLPs.These empirical challenges highlight limitations of the additive inductive bias in the original KAN formulation for state coupling and provide preliminary empirical evidence of inductive bias limitations for future hybrid modeling.

</details>

---

## 74. Infusion: Shaping Model Behavior by Editing Training Data via Influence Functions

**Chinese Title**: Infusion: Shaping Model Behavior by Editing Training Data via Influence Functions

**Authors**: J Rosser, Robert Kirk, Edward Grefenstette, Jakob Foerster, Laura Ruis

**Date**: 2026-02-10 | **arXiv**: [2602.09987v1](http://arxiv.org/abs/2602.09987v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09987v1)

**Code**: https://github.com/jrosseruk/infusion.

**Categories**: cs.LG, cs.AI, cs.CY

<details><summary><b>Abstract</b></summary>

Influence functions are commonly used to attribute model behavior to training documents. We explore the reverse: crafting training data that induces model behavior. Our framework, Infusion, uses scalable influence-function approximations to compute small perturbations to training documents that induce targeted changes in model behavior through parameter shifts. We evaluate Infusion on data poisoning tasks across vision and language domains. On CIFAR-10, we show that making subtle edits via Infusion to just 0.2% (100/45,000) of the training documents can be competitive with the baseline of inserting a small number of explicit behavior examples. We also find that Infusion transfers across architectures (ResNet $\leftrightarrow$ CNN), suggesting a single poisoned corpus can affect multiple independently trained models. In preliminary language experiments, we characterize when our approach increases the probability of target behaviors and when it fails, finding it most effective at amplifying behaviors the model has already learned. Taken together, these results show that small, subtle edits to training data can systematically shape model behavior, underscoring the importance of training data interpretability for adversaries and defenders alike. We provide the code here: https://github.com/jrosseruk/infusion.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Influence functions are commonly used to attribute model behavior to training documents. We explore the reverse: crafting training data that induces model behavior. Our framework, Infusion, uses scalable influence-function approximations to compute small perturbations to training documents that induce targeted changes in model behavior through parameter shifts. We evaluate Infusion on data poisoning tasks across vision and language domains.On CIFAR-10, we show that making subtle edits via Infusion to just 0.2% (100/45,000) of the training documents can be competitive with the baseline of inserting a small number of explicit behavior examples. We also find that Infusion transfers across architectures (ResNet $\leftrightarrow$ CNN), suggesting a single poisoned corpus can affect multiple independently trained models.In preliminary language experiments, we characterize when our approach increases the probability of target behaviors and when it fails, finding it most effective at amplifying behaviors the model has already learned. Taken together, these results show that small, subtle edits to training data can systematically shape model behavior, underscoring the importance of training data interpretability for adversaries and defenders alike.We provide the code here: https://github.com/jrosseruk/infusion.

</details>

---

## 75. Supervised Metric Regularization Through Alternating Optimization for Multi-Regime Physics-Informed Neural Networks

**Chinese Title**: Supervised Metric Regularization Through Alternating Optimization for Multi-Regime Physics-Informed Neural Networks

**Authors**: Enzo Nicolas Spotorno, Josafat Ribeiro Leal, Antonio Augusto Frohlich

**Date**: 2026-02-10 | **arXiv**: [2602.09980v1](http://arxiv.org/abs/2602.09980v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09980v1)

**Categories**: cs.LG, cs.AI, physics.comp-ph

<details><summary><b>Abstract</b></summary>

Standard Physics-Informed Neural Networks (PINNs) often face challenges when modeling parameterized dynamical systems with sharp regime transitions, such as bifurcations. In these scenarios, the continuous mapping from parameters to solutions can result in spectral bias or "mode collapse", where the network averages distinct physical behaviors. We propose a Topology-Aware PINN (TAPINN) that aims to mitigate this challenge by structuring the latent space via Supervised Metric Regularization. Unlike standard parametric PINNs that map physical parameters directly to solutions, our method conditions the solver on a latent state optimized to reflect the metric-based separation between regimes, showing ~49% lower physics residual (0.082 vs. 0.160). We train this architecture using a phase-based Alternating Optimization (AO) schedule to manage gradient conflicts between the metric and physics objectives. Preliminary experiments on the Duffing Oscillator demonstrate that while standard baselines suffer from spectral bias and high-capacity Hypernetworks overfit (memorizing data while violating physics), our approach achieves stable convergence with 2.18x lower gradient variance than a multi-output Sobolev Error baseline, and 5x fewer parameters than a hypernetwork-based alternative.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Standard Physics-Informed Neural Networks (PINNs) often face challenges when modeling parameterized dynamical systems with sharp regime transitions, such as bifurcations. In these scenarios, the continuous mapping from parameters to solutions can result in spectral bias or "mode collapse", where the network averages distinct physical behaviors.We propose a Topology-Aware PINN (TAPINN) that aims to mitigate this challenge by structuring the latent space via Supervised Metric Regularization. Unlike standard parametric PINNs that map physical parameters directly to solutions, our method conditions the solver on a latent state optimized to reflect the metric-based separation between regimes, showing ~49% lower physics residual (0.082 vs. 0.160).We train this architecture using a phase-based Alternating Optimization (AO) schedule to manage gradient conflicts between the metric and physics objectives.Preliminary experiments on the Duffing Oscillator demonstrate that while standard baselines suffer from spectral bias and high-capacity Hypernetworks overfit (memorizing data while violating physics), our approach achieves stable convergence with 2.18x lower gradient variance than a multi-output Sobolev Error baseline, and 5x fewer parameters than a hypernetwork-based alternative.

</details>

---

## 76. Closing Reasoning Gaps in Clinical Agents with Differential Reasoning Learning

**Chinese Title**: Closing Reasoning Gaps in Clinical Agents with Differential Reasoning Learning

**Authors**: Jinsong Liu, Yuhang Jiang, Ramayya Krishnan, Rema Padman, Yiye Zhang et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09945v1](http://arxiv.org/abs/2602.09945v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09945v1)

**Categories**: cs.AI

<details><summary><b>Abstract</b></summary>

Clinical decision support requires not only correct answers but also clinically valid reasoning. We propose Differential Reasoning Learning (DRL), a framework that improves clinical agents by learning from reasoning discrepancies. From reference reasoning rationales (e.g., physician-authored clinical rationale, clinical guidelines, or outputs from more capable models) and the agent's free-form chain-of-thought (CoT), DRL extracts reasoning graphs as directed acyclic graphs (DAGs) and performs a clinically weighted graph edit distance (GED)-based discrepancy analysis. An LLM-as-a-judge aligns semantically equivalent nodes and diagnoses discrepancies between graphs. These graph-level discrepancy diagnostics are converted into natural-language instructions and stored in a Differential Reasoning Knowledge Base (DR-KB). At inference, we retrieve top-$k$ instructions via Retrieval-Augmented Generation (RAG) to augment the agent prompt and patch likely logic gaps. Evaluation on open medical question answering (QA) benchmarks and a Return Visit Admissions (RVA) prediction task from internal clinical data demonstrates gains over baselines, improving both final-answer accuracy and reasoning fidelity. Ablation studies confirm gains from infusing reference reasoning rationales and the top-$k$ retrieval strategy. Clinicians' review of the output provides further assurance of the approach. Together, results suggest that DRL supports more reliable clinical decision-making in complex reasoning scenarios and offers a practical mechanism for deployment under limited token budgets.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Clinical decision support requires not only correct answers but also clinically valid reasoning. We propose Differential Reasoning Learning (DRL), a framework that improves clinical agents by learning from reasoning discrepancies.From reference reasoning rationales (e.g., physician-authored clinical rationale, clinical guidelines, or outputs from more capable models) and the agent's free-form chain-of-thought (CoT), DRL extracts reasoning graphs as directed acyclic graphs (DAGs) and performs a clinically weighted graph edit distance (GED)-based discrepancy analysis. An LLM-as-a-judge aligns semantically equivalent nodes and diagnoses discrepancies between graphs.These graph-level discrepancy diagnostics are converted into natural-language instructions and stored in a Differential Reasoning Knowledge Base (DR-KB). At inference, we retrieve top-$k$ instructions via Retrieval-Augmented Generation (RAG) to augment the agent prompt and patch likely logic gaps.Evaluation on open medical question answering (QA) benchmarks and a Return Visit Admissions (RVA) prediction task from internal clinical data demonstrates gains over baselines, improving both final-answer accuracy and reasoning fidelity. Ablation studies confirm gains from infusing reference reasoning rationales and the top-$k$ retrieval strategy. Clinicians' review of the output provides further assurance of the approach.Together, results suggest that DRL supports more reliable clinical decision-making in complex reasoning scenarios and offers a practical mechanism for deployment under limited token budgets.

</details>

---

## 77. Instruct2Act: From Human Instruction to Actions Sequencing and Execution via Robot Action Network for Robotic Manipulation

**Chinese Title**: Instruct2Act: From Human Instruction to Actions Sequencing and Execution via Robot Action Network for Robotic Manipulation

**Authors**: Archit Sharma, Dharmendra Sharma, John Rebeiro, Peeyush Thakur, Narendra Dhar et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09940v1](http://arxiv.org/abs/2602.09940v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09940v1)

**Categories**: cs.RO, cs.AI

<details><summary><b>Abstract</b></summary>

Robots often struggle to follow free-form human instructions in real-world settings due to computational and sensing limitations. We address this gap with a lightweight, fully on-device pipeline that converts natural-language commands into reliable manipulation. Our approach has two stages: (i) the instruction to actions module (Instruct2Act), a compact BiLSTM with a multi-head-attention autoencoder that parses an instruction into an ordered sequence of atomic actions (e.g., reach, grasp, move, place); and (ii) the robot action network (RAN), which uses the dynamic adaptive trajectory radial network (DATRN) together with a vision-based environment analyzer (YOLOv8) to generate precise control trajectories for each sub-action. The entire system runs on a modest system with no cloud services. On our custom proprietary dataset, Instruct2Act attains 91.5% sub-actions prediction accuracy while retaining a small footprint. Real-robot evaluations across four tasks (pick-place, pick-pour, wipe, and pick-give) yield an overall 90% success; sub-action inference completes in < 3.8s, with end-to-end executions in 30-60s depending on task complexity. These results demonstrate that fine-grained instruction-to-action parsing, coupled with DATRN-based trajectory generation and vision-guided grounding, provides a practical path to deterministic, real-time manipulation in resource-constrained, single-camera settings.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Robots often struggle to follow free-form human instructions in real-world settings due to computational and sensing limitations. We address this gap with a lightweight, fully on-device pipeline that converts natural-language commands into reliable manipulation.Our approach has two stages: (i) the instruction to actions module (Instruct2Act), a compact BiLSTM with a multi-head-attention autoencoder that parses an instruction into an ordered sequence of atomic actions (e.g., reach, grasp, move, place); and (ii) the robot action network (RAN), which uses the dynamic adaptive trajectory radial network (DATRN) together with a vision-based environment analyzer (YOLOv8) to generate precise control trajectories for each sub-action.The entire system runs on a modest system with no cloud services. On our custom proprietary dataset, Instruct2Act attains 91.5% sub-actions prediction accuracy while retaining a small footprint. Real-robot evaluations across four tasks (pick-place, pick-pour, wipe, and pick-give) yield an overall 90% success; sub-action inference completes in < 3.8s, with end-to-end executions in 30-60s depending on task complexity.These results demonstrate that fine-grained instruction-to-action parsing, coupled with DATRN-based trajectory generation and vision-guided grounding, provides a practical path to deterministic, real-time manipulation in resource-constrained, single-camera settings.

</details>

---

## 78. TaCo: A Benchmark for Lossless and Lossy Codecs of Heterogeneous Tactile Data

**Chinese Title**: TaCo: A Benchmark for Lossless and Lossy Codecs of Heterogeneous Tactile Data

**Authors**: Zhengxue Cheng, Yan Zhao, Keyu Wang, Hengdi Zhang, Li Song

**Date**: 2026-02-10 | **arXiv**: [2602.09893v1](http://arxiv.org/abs/2602.09893v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09893v1)

**Categories**: cs.RO, cs.AI

<details><summary><b>Abstract</b></summary>

Tactile sensing is crucial for embodied intelligence, providing fine-grained perception and control in complex environments. However, efficient tactile data compression, which is essential for real-time robotic applications under strict bandwidth constraints, remains underexplored. The inherent heterogeneity and spatiotemporal complexity of tactile data further complicate this challenge. To bridge this gap, we introduce TaCo, the first comprehensive benchmark for Tactile data Codecs. TaCo evaluates 30 compression methods, including off-the-shelf compression algorithms and neural codecs, across five diverse datasets from various sensor types. We systematically assess both lossless and lossy compression schemes on four key tasks: lossless storage, human visualization, material and object classification, and dexterous robotic grasping. Notably, we pioneer the development of data-driven codecs explicitly trained on tactile data, TaCo-LL (lossless) and TaCo-L (lossy). Results have validated the superior performance of our TaCo-LL and TaCo-L. This benchmark provides a foundational framework for understanding the critical trade-offs between compression efficiency and task performance, paving the way for future advances in tactile perception.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Tactile sensing is crucial for embodied intelligence, providing fine-grained perception and control in complex environments. However, efficient tactile data compression, which is essential for real-time robotic applications under strict bandwidth constraints, remains underexplored. The inherent heterogeneity and spatiotemporal complexity of tactile data further complicate this challenge.To bridge this gap, we introduce TaCo, the first comprehensive benchmark for Tactile data Codecs. TaCo evaluates 30 compression methods, including off-the-shelf compression algorithms and neural codecs, across five diverse datasets from various sensor types.We systematically assess both lossless and lossy compression schemes on four key tasks: lossless storage, human visualization, material and object classification, and dexterous robotic grasping. Notably, we pioneer the development of data-driven codecs explicitly trained on tactile data, TaCo-LL (lossless) and TaCo-L (lossy). Results have validated the superior performance of our TaCo-LL and TaCo-L.This benchmark provides a foundational framework for understanding the critical trade-offs between compression efficiency and task performance, paving the way for future advances in tactile perception.

</details>

---

## 79. Efficient Unsupervised Environment Design through Hierarchical Policy Representation Learning

**Chinese Title**: Efficient Unsupervised Environment Design through Hierarchical Policy Representation Learning

**Authors**: Dexun Li, Sidney Tio, Pradeep Varakantham

**Date**: 2026-02-10 | **arXiv**: [2602.09813v1](http://arxiv.org/abs/2602.09813v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09813v1)

**Categories**: cs.AI

<details><summary><b>Abstract</b></summary>

Unsupervised Environment Design (UED) has emerged as a promising approach to developing general-purpose agents through automated curriculum generation. Popular UED methods focus on Open-Endedness, where teacher algorithms rely on stochastic processes for infinite generation of useful environments. This assumption becomes impractical in resource-constrained scenarios where teacher-student interaction opportunities are limited. To address this challenge, we introduce a hierarchical Markov Decision Process (MDP) framework for environment design. Our framework features a teacher agent that leverages student policy representations derived from discovered evaluation environments, enabling it to generate training environments based on the student's capabilities. To improve efficiency, we incorporate a generative model that augments the teacher's training dataset with synthetic data, reducing the need for teacher-student interactions. In experiments across several domains, we show that our method outperforms baseline approaches while requiring fewer teacher-student interactions in a single episode. The results suggest the applicability of our approach in settings where training opportunities are limited.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Unsupervised Environment Design (UED) has emerged as a promising approach to developing general-purpose agents through automated curriculum generation. Popular UED methods focus on Open-Endedness, where teacher algorithms rely on stochastic processes for infinite generation of useful environments. This assumption becomes impractical in resource-constrained scenarios where teacher-student interaction opportunities are limited.To address this challenge, we introduce a hierarchical Markov Decision Process (MDP) framework for environment design. Our framework features a teacher agent that leverages student policy representations derived from discovered evaluation environments, enabling it to generate training environments based on the student's capabilities.To improve efficiency, we incorporate a generative model that augments the teacher's training dataset with synthetic data, reducing the need for teacher-student interactions. In experiments across several domains, we show that our method outperforms baseline approaches while requiring fewer teacher-student interactions in a single episode. The results suggest the applicability of our approach in settings where training opportunities are limited.

</details>

---

## 80. A Controlled Study of Double DQN and Dueling DQN Under Cross-Environment Transfer

**Chinese Title**: A Controlled Study of Double DQN and Dueling DQN Under Cross-Environment Transfer

**Authors**: Azka Nasir, Fatima Dossa, Muhammad Ahmed Atif, Mohammad Ahmed Atif

**Date**: 2026-02-10 | **arXiv**: [2602.09810v1](http://arxiv.org/abs/2602.09810v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09810v1)

**Categories**: cs.LG, cs.AI

<details><summary><b>Abstract</b></summary>

Transfer learning in deep reinforcement learning is often motivated by improved stability and reduced training cost, but it can also fail under substantial domain shift. This paper presents a controlled empirical study examining how architectural differences between Double Deep Q-Networks (DDQN) and Dueling DQN influence transfer behavior across environments. Using CartPole as a source task and LunarLander as a structurally distinct target task, we evaluate a fixed layer-wise representation transfer protocol under identical hyperparameters and training conditions, with baseline agents trained from scratch used to contextualize transfer effects. Empirical results show that DDQN consistently avoids negative transfer under the examined setup and maintains learning dynamics comparable to baseline performance in the target environment. In contrast, Dueling DQN consistently exhibits negative transfer under identical conditions, characterized by degraded rewards and unstable optimization behavior. Statistical analysis across multiple random seeds confirms a significant performance gap under transfer. These findings suggest that architectural inductive bias is strongly associated with robustness to cross-environment transfer in value-based deep reinforcement learning under the examined transfer protocol.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Transfer learning in deep reinforcement learning is often motivated by improved stability and reduced training cost, but it can also fail under substantial domain shift. This paper presents a controlled empirical study examining how architectural differences between Double Deep Q-Networks (DDQN) and Dueling DQN influence transfer behavior across environments.Using CartPole as a source task and LunarLander as a structurally distinct target task, we evaluate a fixed layer-wise representation transfer protocol under identical hyperparameters and training conditions, with baseline agents trained from scratch used to contextualize transfer effects.Empirical results show that DDQN consistently avoids negative transfer under the examined setup and maintains learning dynamics comparable to baseline performance in the target environment. In contrast, Dueling DQN consistently exhibits negative transfer under identical conditions, characterized by degraded rewards and unstable optimization behavior.Statistical analysis across multiple random seeds confirms a significant performance gap under transfer. These findings suggest that architectural inductive bias is strongly associated with robustness to cross-environment transfer in value-based deep reinforcement learning under the examined transfer protocol.

</details>

---

## 81. Would a Large Language Model Pay Extra for a View? Inferring Willingness to Pay from Subjective Choices

**Chinese Title**: Would a Large Language Model Pay Extra for a View? Inferring Willingness to Pay from Subjective Choices

**Authors**: Manon Reusens, Sofie Goethals, Toon Calders, David Martens

**Date**: 2026-02-10 | **arXiv**: [2602.09802v1](http://arxiv.org/abs/2602.09802v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09802v1)

**Categories**: cs.AI, cs.CL

<details><summary><b>Abstract</b></summary>

As Large Language Models (LLMs) are increasingly deployed in applications such as travel assistance and purchasing support, they are often required to make subjective choices on behalf of users in settings where no objectively correct answer exists. We study LLM decision-making in a travel-assistant context by presenting models with choice dilemmas and analyzing their responses using multinomial logit models to derive implied willingness to pay (WTP) estimates. These WTP values are subsequently compared to human benchmark values from the economics literature. In addition to a baseline setting, we examine how model behavior changes under more realistic conditions, including the provision of information about users' past choices and persona-based prompting. Our results show that while meaningful WTP values can be derived for larger LLMs, they also display systematic deviations at the attribute level. Additionally, they tend to overestimate human WTP overall, particularly when expensive options or business-oriented personas are introduced. Conditioning models on prior preferences for cheaper options yields valuations that are closer to human benchmarks. Overall, our findings highlight both the potential and the limitations of using LLMs for subjective decision support and underscore the importance of careful model selection, prompt design, and user representation when deploying such systems in practice.

</details>

<details><summary><b>Chinese Abstract</b></summary>

As Large Language Models (LLMs) are increasingly deployed in applications such as travel assistance and purchasing support, they are often required to make subjective choices on behalf of users in settings where no objectively correct answer exists.We study LLM decision-making in a travel-assistant context by presenting models with choice dilemmas and analyzing their responses using multinomial logit models to derive implied willingness to pay (WTP) estimates. These WTP values are subsequently compared to human benchmark values from the economics literature.In addition to a baseline setting, we examine how model behavior changes under more realistic conditions, including the provision of information about users' past choices and persona-based prompting. Our results show that while meaningful WTP values can be derived for larger LLMs, they also display systematic deviations at the attribute level.Additionally, they tend to overestimate human WTP overall, particularly when expensive options or business-oriented personas are introduced. Conditioning models on prior preferences for cheaper options yields valuations that are closer to human benchmarks.Overall, our findings highlight both the potential and the limitations of using LLMs for subjective decision support and underscore the importance of careful model selection, prompt design, and user representation when deploying such systems in practice.

</details>

---

## 82. GHS-TDA: A Synergistic Reasoning Framework Integrating Global Hypothesis Space with Topological Data Analysis

**Chinese Title**: GHS-TDA: A Synergistic Reasoning Framework Integrating Global Hypothesis Space with Topological Data Analysis

**Authors**: Jiaquan Zhang, Chaoning Zhang, Shuxu Chen, Xudong Wang, Zhenzhen Huang et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09794v1](http://arxiv.org/abs/2602.09794v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09794v1)

**Categories**: cs.AI

<details><summary><b>Abstract</b></summary>

Chain-of-Thought (CoT) has been shown to significantly improve the reasoning accuracy of large language models (LLMs) on complex tasks. However, due to the autoregressive, step-by-step generation paradigm, existing CoT methods suffer from two fundamental limitations. First, the reasoning process is highly sensitive to early decisions: once an initial error is introduced, it tends to propagate and amplify through subsequent steps, while the lack of a global coordination and revision mechanism makes such errors difficult to correct, ultimately leading to distorted reasoning chains. Second, current CoT approaches lack structured analysis techniques for filtering redundant reasoning and extracting key reasoning features, resulting in unstable reasoning processes and limited interpretability. To address these issues, we propose GHS-TDA. GHS-TDA first constructs a semantically enriched global hypothesis graph to aggregate, align, and coordinate multiple candidate reasoning paths, thereby providing alternative global correction routes when local reasoning fails. It then applies topological data analysis based on persistent homology to capture stable multi-scale structures, remove redundancy and inconsistencies, and extract a more reliable reasoning skeleton. By jointly leveraging reasoning diversity and topological stability, GHS-TDA achieves self-adaptive convergence, produces high-confidence and interpretable reasoning paths, and consistently outperforms strong baselines in terms of both accuracy and robustness across multiple reasoning benchmarks.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Chain-of-Thought (CoT) has been shown to significantly improve the reasoning accuracy of large language models (LLMs) on complex tasks. However, due to the autoregressive, step-by-step generation paradigm, existing CoT methods suffer from two fundamental limitations.First, the reasoning process is highly sensitive to early decisions: once an initial error is introduced, it tends to propagate and amplify through subsequent steps, while the lack of a global coordination and revision mechanism makes such errors difficult to correct, ultimately leading to distorted reasoning chains.Second, current CoT approaches lack structured analysis techniques for filtering redundant reasoning and extracting key reasoning features, resulting in unstable reasoning processes and limited interpretability. To address these issues, we propose GHS-TDA.GHS-TDA first constructs a semantically enriched global hypothesis graph to aggregate, align, and coordinate multiple candidate reasoning paths, thereby providing alternative global correction routes when local reasoning fails. It then applies topological data analysis based on persistent homology to capture stable multi-scale structures, remove redundancy and inconsistencies, and extract a more reliable reasoning skeleton.By jointly leveraging reasoning diversity and topological stability, GHS-TDA achieves self-adaptive convergence, produces high-confidence and interpretable reasoning paths, and consistently outperforms strong baselines in terms of both accuracy and robustness across multiple reasoning benchmarks.

</details>

---

## 83. Grounding LTL Tasks in Sub-Symbolic RL Environments for Zero-Shot Generalization

**Chinese Title**: Grounding LTL Tasks in Sub-Symbolic RL Environments for Zero-Shot Generalization

**Authors**: Matteo Pannacci, Andrea Fanti, Elena Umili, Roberto Capobianco

**Date**: 2026-02-10 | **arXiv**: [2602.09761v1](http://arxiv.org/abs/2602.09761v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09761v1)

**Categories**: cs.LG, cs.AI

<details><summary><b>Abstract</b></summary>

In this work we address the problem of training a Reinforcement Learning agent to follow multiple temporally-extended instructions expressed in Linear Temporal Logic in sub-symbolic environments. Previous multi-task work has mostly relied on knowledge of the mapping between raw observations and symbols appearing in the formulae. We drop this unrealistic assumption by jointly training a multi-task policy and a symbol grounder with the same experience. The symbol grounder is trained only from raw observations and sparse rewards via Neural Reward Machines in a semi-supervised fashion. Experiments on vision-based environments show that our method achieves performance comparable to using the true symbol grounding and significantly outperforms state-of-the-art methods for sub-symbolic environments.

</details>

<details><summary><b>Chinese Abstract</b></summary>

In this work we address the problem of training a Reinforcement Learning agent to follow multiple temporally-extended instructions expressed in Linear Temporal Logic in sub-symbolic environments. Previous multi-task work has mostly relied on knowledge of the mapping between raw observations and symbols appearing in the formulae.We drop this unrealistic assumption by jointly training a multi-task policy and a symbol grounder with the same experience. The symbol grounder is trained only from raw observations and sparse rewards via Neural Reward Machines in a semi-supervised fashion.Experiments on vision-based environments show that our method achieves performance comparable to using the true symbol grounding and significantly outperforms state-of-the-art methods for sub-symbolic environments.

</details>

---

## 84. ClinAlign: Scaling Healthcare Alignment from Clinician Preference

**Chinese Title**: ClinAlign: Scaling Healthcare Alignment from Clinician Preference

**Authors**: Shiwei Lyu, Xidong Wang, Lei Liu, Hao Zhu, Chaohe Zhang et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09653v1](http://arxiv.org/abs/2602.09653v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09653v1)

**Categories**: cs.AI

<details><summary><b>Abstract</b></summary>

Although large language models (LLMs) demonstrate expert-level medical knowledge, aligning their open-ended outputs with fine-grained clinician preferences remains challenging. Existing methods often rely on coarse objectives or unreliable automated judges that are weakly grounded in professional guidelines. We propose a two-stage framework to address this gap. First, we introduce HealthRubrics, a dataset of 7,034 physician-verified preference examples in which clinicians refine LLM-drafted rubrics to meet rigorous medical standards. Second, we distill these rubrics into HealthPrinciples: 119 broadly reusable, clinically grounded principles organized by clinical dimensions, enabling scalable supervision beyond manual annotation. We use HealthPrinciples for (1) offline alignment by synthesizing rubrics for unlabeled queries and (2) an inference-time tool for guided self-revision. A 30B parameter model that activates only 3B parameters at inference trained with our framework achieves 33.4% on HealthBench-Hard, outperforming much larger models including Deepseek-R1 and o3, establishing a resource-efficient baseline for clinical alignment.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Although large language models (LLMs) demonstrate expert-level medical knowledge, aligning their open-ended outputs with fine-grained clinician preferences remains challenging. Existing methods often rely on coarse objectives or unreliable automated judges that are weakly grounded in professional guidelines. We propose a two-stage framework to address this gap.First, we introduce HealthRubrics, a dataset of 7,034 physician-verified preference examples in which clinicians refine LLM-drafted rubrics to meet rigorous medical standards. Second, we distill these rubrics into HealthPrinciples: 119 broadly reusable, clinically grounded principles organized by clinical dimensions, enabling scalable supervision beyond manual annotation.We use HealthPrinciples for (1) offline alignment by synthesizing rubrics for unlabeled queries and (2) an inference-time tool for guided self-revision. A 30B parameter model that activates only 3B parameters at inference trained with our framework achieves 33.4% on HealthBench-Hard, outperforming much larger models including Deepseek-R1 and o3, establishing a resource-efficient baseline for clinical alignment.

</details>

---

## 85. Autoregressive Direct Preference Optimization

**Chinese Title**: Autoregressive Direct Preference Optimization

**Authors**: Masanari Oi, Mahiro Ukai, Masahiro Kaneko, Naoaki Okazaki, Nakamasa Inoue

**Date**: 2026-02-10 | **arXiv**: [2602.09533v1](http://arxiv.org/abs/2602.09533v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09533v1)

**Categories**: cs.AI

<details><summary><b>Abstract</b></summary>

Direct preference optimization (DPO) has emerged as a promising approach for aligning large language models (LLMs) with human preferences. However, the widespread reliance on the response-level Bradley-Terry (BT) model may limit its full potential, as the reference and learnable models are assumed to be autoregressive only after deriving the objective function. Motivated by this limitation, we revisit the theoretical foundations of DPO and propose a novel formulation that explicitly introduces the autoregressive assumption prior to applying the BT model. By reformulating and extending DPO, we derive a novel variant, termed Autoregressive DPO (ADPO), that explicitly integrates autoregressive modeling into the preference optimization framework. Without violating the theoretical foundations, the derived loss takes an elegant form: it shifts the summation operation in the DPO objective outside the log-sigmoid function. Furthermore, through theoretical analysis of ADPO, we show that there exist two length measures to be considered when designing DPO-based algorithms: the token length $Î¼$ and the feedback length $Î¼$'. To the best of our knowledge, we are the first to explicitly distinguish these two measures and analyze their implications for preference optimization in LLMs.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Direct preference optimization (DPO) has emerged as a promising approach for aligning large language models (LLMs) with human preferences. However, the widespread reliance on the response-level Bradley-Terry (BT) model may limit its full potential, as the reference and learnable models are assumed to be autoregressive only after deriving the objective function.Motivated by this limitation, we revisit the theoretical foundations of DPO and propose a novel formulation that explicitly introduces the autoregressive assumption prior to applying the BT model. By reformulating and extending DPO, we derive a novel variant, termed Autoregressive DPO (ADPO), that explicitly integrates autoregressive modeling into the preference optimization framework.Without violating the theoretical foundations, the derived loss takes an elegant form: it shifts the summation operation in the DPO objective outside the log-sigmoid function. Furthermore, through theoretical analysis of ADPO, we show that there exist two length measures to be considered when designing DPO-based algorithms: the token length $Î¼$ and the feedback length $Î¼$'.To the best of our knowledge, we are the first to explicitly distinguish these two measures and analyze their implications for preference optimization in LLMs.

</details>

---

## 86. Learning to Discover Iterative Spectral Algorithms

**Chinese Title**: Learning to Discover Iterative Spectral Algorithms

**Authors**: Zihang Liu, Oleg Balabanov, Yaoqing Yang, Michael W. Mahoney

**Date**: 2026-02-10 | **arXiv**: [2602.09530v1](http://arxiv.org/abs/2602.09530v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09530v1)

**Categories**: cs.LG, cs.AI, math.NA

<details><summary><b>Abstract</b></summary>

We introduce AutoSpec, a neural network framework for discovering iterative spectral algorithms for large-scale numerical linear algebra and numerical optimization. Our self-supervised models adapt to input operators using coarse spectral information (e.g., eigenvalue estimates and residual norms), and they predict recurrence coefficients for computing or applying a matrix polynomial tailored to a downstream task. The effectiveness of AutoSpec relies on three ingredients: an architecture whose inference pass implements short, executable numerical linear algebra recurrences; efficient training on small synthetic problems with transfer to large-scale real-world operators; and task-defined objectives that enforce the desired approximation or preconditioning behavior across the range of spectral profiles represented in the training set. We apply AutoSpec to discovering algorithms for representative numerical linear algebra tasks: accelerating matrix-function approximation; accelerating sparse linear solvers; and spectral filtering/preconditioning for eigenvalue computations. On real-world matrices, the learned procedures deliver orders-of-magnitude improvements in accuracy and/or reductions in iteration count, relative to basic baselines. We also find clear connections to classical theory: the induced polynomials often exhibit near-equiripple, near-minimax behavior characteristic of Chebyshev polynomials.

</details>

<details><summary><b>Chinese Abstract</b></summary>

We introduce AutoSpec, a neural network framework for discovering iterative spectral algorithms for large-scale numerical linear algebra and numerical optimization. Our self-supervised models adapt to input operators using coarse spectral information (e.g., eigenvalue estimates and residual norms), and they predict recurrence coefficients for computing or applying a matrix polynomial tailored to a downstream task.The effectiveness of AutoSpec relies on three ingredients: an architecture whose inference pass implements short, executable numerical linear algebra recurrences; efficient training on small synthetic problems with transfer to large-scale real-world operators; and task-defined objectives that enforce the desired approximation or preconditioning behavior across the range of spectral profiles represented in the training set.We apply AutoSpec to discovering algorithms for representative numerical linear algebra tasks: accelerating matrix-function approximation; accelerating sparse linear solvers; and spectral filtering/preconditioning for eigenvalue computations. On real-world matrices, the learned procedures deliver orders-of-magnitude improvements in accuracy and/or reductions in iteration count, relative to basic baselines.We also find clear connections to classical theory: the induced polynomials often exhibit near-equiripple, near-minimax behavior characteristic of Chebyshev polynomials.

</details>

---

## 87. Bridging Efficiency and Transparency: Explainable CoT Compression in Multimodal Large Reasoning Models

**Chinese Title**: Bridging Efficiency and Transparency: Explainable CoT Compression in Multimodal Large Reasoning Models

**Authors**: Yizhi Wang, Linan Yue, Min-Ling Zhang

**Date**: 2026-02-10 | **arXiv**: [2602.09485v1](http://arxiv.org/abs/2602.09485v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09485v1)

**Categories**: cs.AI

<details><summary><b>Abstract</b></summary>

Long chains of thought (Long CoTs) are widely employed in multimodal reasoning models to tackle complex tasks by capturing detailed visual information. However, these Long CoTs are often excessively lengthy and contain redundant reasoning steps, which can hinder inference efficiency. Compressing these long CoTs is a natural solution, yet existing approaches face two major challenges: (1) they may compromise the integrity of visual-textual reasoning by removing essential alignment cues, and (2) the compression process lacks explainability, making it difficult to discern which information is critical. To address these problems, we propose XMCC, an eXplainable Multimodal CoT Compressor that formulates compression as a sequential decision-making process optimized via reinforcement learning. XMCC can effectively shorten reasoning trajectories while preserving key reasoning steps and answer correctness, and simultaneously generates natural-language explanations for its compression decisions. Extensive experiments on representative multimodal reasoning benchmarks demonstrate that XMCC not only reduces reasoning length but also provides explainable explanations, validating its effectiveness.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Long chains of thought (Long CoTs) are widely employed in multimodal reasoning models to tackle complex tasks by capturing detailed visual information. However, these Long CoTs are often excessively lengthy and contain redundant reasoning steps, which can hinder inference efficiency.Compressing these long CoTs is a natural solution, yet existing approaches face two major challenges: (1) they may compromise the integrity of visual-textual reasoning by removing essential alignment cues, and (2) the compression process lacks explainability, making it difficult to discern which information is critical.To address these problems, we propose XMCC, an eXplainable Multimodal CoT Compressor that formulates compression as a sequential decision-making process optimized via reinforcement learning. XMCC can effectively shorten reasoning trajectories while preserving key reasoning steps and answer correctness, and simultaneously generates natural-language explanations for its compression decisions.Extensive experiments on representative multimodal reasoning benchmarks demonstrate that XMCC not only reduces reasoning length but also provides explainable explanations, validating its effectiveness.

</details>

---

## 88. SpotAgent: Grounding Visual Geo-localization in Large Vision-Language Models through Agentic Reasoning

**Chinese Title**: SpotAgent: Grounding Visual Geo-localization in Large Vision-Language Models through Agentic Reasoning

**Authors**: Furong Jia, Ling Dai, Wenjin Deng, Fan Zhang, Chen Hu et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09463v1](http://arxiv.org/abs/2602.09463v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09463v1)

**Categories**: cs.AI

<details><summary><b>Abstract</b></summary>

Large Vision-Language Models (LVLMs) have demonstrated strong reasoning capabilities in geo-localization, yet they often struggle in real-world scenarios where visual cues are sparse, long-tailed, and highly ambiguous. Previous approaches, bound by internal knowledge, often fail to provide verifiable results, yielding confident but ungrounded predictions when faced with confounded evidence. To address these challenges, we propose SpotAgent, a framework that formalizes geo-localization into an agentic reasoning process that leverages expert-level reasoning to synergize visual interpretation with tool-assisted verification. SpotAgent actively explores and verifies visual cues by leveraging external tools (e.g., web search, maps) through a ReAct diagram. We introduce a 3-stage post-training pipeline starting with a Supervised Fine-Tuning (SFT) stage for basic alignment, followed by an Agentic Cold Start phase utilizing high-quality trajectories synthesized via a Multi-Agent framework, aiming to instill tool-calling expertise. Subsequently, the model's reasoning capabilities are refined through Reinforcement Learning. We propose a Spatially-Aware Dynamic Filtering strategy to enhance the efficiency of the RL stage by prioritizing learnable samples based on spatial difficulty. Extensive experiments on standard benchmarks demonstrate that SpotAgent achieves state-of-the-art performance, effectively mitigating hallucinations while delivering precise and verifiable geo-localization.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Large Vision-Language Models (LVLMs) have demonstrated strong reasoning capabilities in geo-localization, yet they often struggle in real-world scenarios where visual cues are sparse, long-tailed, and highly ambiguous. Previous approaches, bound by internal knowledge, often fail to provide verifiable results, yielding confident but ungrounded predictions when faced with confounded evidence.To address these challenges, we propose SpotAgent, a framework that formalizes geo-localization into an agentic reasoning process that leverages expert-level reasoning to synergize visual interpretation with tool-assisted verification. SpotAgent actively explores and verifies visual cues by leveraging external tools (e.g., web search, maps) through a ReAct diagram.We introduce a 3-stage post-training pipeline starting with a Supervised Fine-Tuning (SFT) stage for basic alignment, followed by an Agentic Cold Start phase utilizing high-quality trajectories synthesized via a Multi-Agent framework, aiming to instill tool-calling expertise. Subsequently, the model's reasoning capabilities are refined through Reinforcement Learning.We propose a Spatially-Aware Dynamic Filtering strategy to enhance the efficiency of the RL stage by prioritizing learnable samples based on spatial difficulty. Extensive experiments on standard benchmarks demonstrate that SpotAgent achieves state-of-the-art performance, effectively mitigating hallucinations while delivering precise and verifiable geo-localization.

</details>

---

## 89. P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads

**Chinese Title**: P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads

**Authors**: Yun Luo, Futing Wang, Qianjia Cheng, Fangchen Yu, Haodi Lei et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09443v1](http://arxiv.org/abs/2602.09443v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09443v1)

**Categories**: cs.AI

<details><summary><b>Abstract</b></summary>

The transition from symbolic manipulation to science-grade reasoning represents a pivotal frontier for Large Language Models (LLMs), with physics serving as the critical test anchor for binding abstract logic to physical reality. Physics demands that a model maintain physical consistency with the laws governing the universe, a task that fundamentally requires multimodal perception to ground abstract logic in reality. At the Olympiad level, diagrams are often constitutive rather than illustrative, containing essential constraints, such as boundary conditions and spatial symmetries, that are absent from the text. To bridge this visual-logical gap, we introduce P1-VL, a family of open-source vision-language models engineered for advanced scientific reasoning. Our method harmonizes Curriculum Reinforcement Learning, which employs progressive difficulty expansion to stabilize post-training, with Agentic Augmentation, enabling iterative self-verification at inference. Evaluated on HiPhO, a rigorous benchmark of 13 exams from 2024-2025, our flagship P1-VL-235B-A22B becomes the first open-source Vision-Language Model (VLM) to secure 12 gold medals and achieves the state-of-the-art performance in the open-source models. Our agent-augmented system achieves the No.2 overall rank globally, trailing only Gemini-3-Pro. Beyond physics, P1-VL demonstrates remarkable scientific reasoning capacity and generalizability, establishing significant leads over base models in STEM benchmarks. By open-sourcing P1-VL, we provide a foundational step toward general-purpose physical intelligence to better align visual perceptions with abstract physical laws for machine scientific discovery.

</details>

<details><summary><b>Chinese Abstract</b></summary>

The transition from symbolic manipulation to science-grade reasoning represents a pivotal frontier for Large Language Models (LLMs), with physics serving as the critical test anchor for binding abstract logic to physical reality. Physics demands that a model maintain physical consistency with the laws governing the universe, a task that fundamentally requires multimodal perception to ground abstract logic in reality.At the Olympiad level, diagrams are often constitutive rather than illustrative, containing essential constraints, such as boundary conditions and spatial symmetries, that are absent from the text. To bridge this visual-logical gap, we introduce P1-VL, a family of open-source vision-language models engineered for advanced scientific reasoning.Our method harmonizes Curriculum Reinforcement Learning, which employs progressive difficulty expansion to stabilize post-training, with Agentic Augmentation, enabling iterative self-verification at inference.Evaluated on HiPhO, a rigorous benchmark of 13 exams from 2024-2025, our flagship P1-VL-235B-A22B becomes the first open-source Vision-Language Model (VLM) to secure 12 gold medals and achieves the state-of-the-art performance in the open-source models. Our agent-augmented system achieves the No.2 overall rank globally, trailing only Gemini-3-Pro.Beyond physics, P1-VL demonstrates remarkable scientific reasoning capacity and generalizability, establishing significant leads over base models in STEM benchmarks. By open-sourcing P1-VL, we provide a foundational step toward general-purpose physical intelligence to better align visual perceptions with abstract physical laws for machine scientific discovery.

</details>

---

## 90. Sci-VLA: Agentic VLA Inference Plugin for Long-Horizon Tasks in Scientific Experiments

**Chinese Title**: Sci-VLA: Agentic VLA Inference Plugin for Long-Horizon Tasks in Scientific Experiments

**Authors**: Yiwen Pang, Bo Zhou, Changjin Li, Xuanhao Wang, Shengxiang Xu et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09430v1](http://arxiv.org/abs/2602.09430v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09430v1)

**Categories**: cs.RO, cs.AI

<details><summary><b>Abstract</b></summary>

Robotic laboratories play a critical role in autonomous scientific discovery by enabling scalable, continuous experimental execution. Recent vision-language-action (VLA) models offer a promising foundation for robotic laboratories. However, scientific experiments typically involve long-horizon tasks composed of multiple atomic tasks, posing a fundamental challenge to existing VLA models. While VLA models fine-tuned for scientific tasks can reliably execute atomic experimental actions seen during training, they often fail to perform composite tasks formed by reordering and composing these known atomic actions. This limitation arises from a distributional mismatch between training-time atomic tasks and inference-time composite tasks, which prevents VLA models from executing necessary transitional operations between atomic tasks. To address this challenge, we propose an Agentic VLA Inference Plugin for Long-Horizon Tasks in Scientific Experiments. It introduces an LLM-based agentic inference mechanism that intervenes when executing sequential manipulation tasks. By performing explicit transition inference and generating transitional robotic action code, the proposed plugin guides VLA models through missing transitional steps, enabling reliable execution of composite scientific workflows without any additional training. This inference-only intervention makes our method computationally efficient, data-efficient, and well-suited for open-ended and long-horizon robotic laboratory tasks. We build 3D assets of scientific instruments and common scientific operating scenes within an existing simulation environment. In these scenes, we have verified that our method increases the average success rate per atomic task by 42\% during inference. Furthermore, we show that our method can be easily transferred from the simulation to real scientific laboratories.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Robotic laboratories play a critical role in autonomous scientific discovery by enabling scalable, continuous experimental execution. Recent vision-language-action (VLA) models offer a promising foundation for robotic laboratories. However, scientific experiments typically involve long-horizon tasks composed of multiple atomic tasks, posing a fundamental challenge to existing VLA models.While VLA models fine-tuned for scientific tasks can reliably execute atomic experimental actions seen during training, they often fail to perform composite tasks formed by reordering and composing these known atomic actions. This limitation arises from a distributional mismatch between training-time atomic tasks and inference-time composite tasks, which prevents VLA models from executing necessary transitional operations between atomic tasks.To address this challenge, we propose an Agentic VLA Inference Plugin for Long-Horizon Tasks in Scientific Experiments. It introduces an LLM-based agentic inference mechanism that intervenes when executing sequential manipulation tasks.By performing explicit transition inference and generating transitional robotic action code, the proposed plugin guides VLA models through missing transitional steps, enabling reliable execution of composite scientific workflows without any additional training. This inference-only intervention makes our method computationally efficient, data-efficient, and well-suited for open-ended and long-horizon robotic laboratory tasks.We build 3D assets of scientific instruments and common scientific operating scenes within an existing simulation environment. In these scenes, we have verified that our method increases the average success rate per atomic task by 42\% during inference. Furthermore, we show that our method can be easily transferred from the simulation to real scientific laboratories.

</details>

---

## 91. Beyond Input-Output: Rethinking Creativity through Design-by-Analogy in Human-AI Collaboration

**Chinese Title**: Beyond Input-Output: Rethinking Creativity through Design-by-Analogy in Human-AI Collaboration

**Authors**: Xuechen Li, Shuai Zhang, Nan Cao, Qing Chen

**Date**: 2026-02-10 | **arXiv**: [2602.09423v1](http://arxiv.org/abs/2602.09423v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09423v1)

**Categories**: cs.HC, cs.AI

<details><summary><b>Abstract</b></summary>

While the proliferation of foundation models has significantly boosted individual productivity, it also introduces a potential challenge: the homogenization of creative content. In response, we revisit Design-by-Analogy (DbA), a cognitively grounded approach that fosters novel solutions by mapping inspiration across domains. However, prevailing perspectives often restrict DbA to early ideation or specific data modalities, while reducing AI-driven design to simplified input-output pipelines. Such conceptual limitations inadvertently foster widespread design fixation. To address this, we expand the understanding of DbA by embedding it into the entire creative process, thereby demonstrating its capacity to mitigate such fixation. Through a systematic review of 85 studies, we identify six forms of representation and classify techniques across seven stages of the creative process. We further discuss three major application domains: creative industries, intelligent manufacturing, and education and services, demonstrating DbA's practical relevance. Building on this synthesis, we frame DbA as a mediating technology for human-AI collaboration and outline the potential opportunities and inherent risks for advancing creativity support in HCI and design research.

</details>

<details><summary><b>Chinese Abstract</b></summary>

While the proliferation of foundation models has significantly boosted individual productivity, it also introduces a potential challenge: the homogenization of creative content. In response, we revisit Design-by-Analogy (DbA), a cognitively grounded approach that fosters novel solutions by mapping inspiration across domains.However, prevailing perspectives often restrict DbA to early ideation or specific data modalities, while reducing AI-driven design to simplified input-output pipelines. Such conceptual limitations inadvertently foster widespread design fixation. To address this, we expand the understanding of DbA by embedding it into the entire creative process, thereby demonstrating its capacity to mitigate such fixation.Through a systematic review of 85 studies, we identify six forms of representation and classify techniques across seven stages of the creative process. We further discuss three major application domains: creative industries, intelligent manufacturing, and education and services, demonstrating DbA's practical relevance.Building on this synthesis, we frame DbA as a mediating technology for human-AI collaboration and outline the potential opportunities and inherent risks for advancing creativity support in HCI and design research.

</details>

---

## 92. Squeezing More from the Stream : Learning Representation Online for Streaming Reinforcement Learning

**Chinese Title**: Squeezing More from the Stream : Learning Representation Online for Streaming Reinforcement Learning

**Authors**:  Nilaksh, Antoine Clavaud, Mathieu Reymond, FranÃ§ois Rivest, Sarath Chandar

**Date**: 2026-02-10 | **arXiv**: [2602.09396v1](http://arxiv.org/abs/2602.09396v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09396v1)

**Categories**: cs.LG, cs.AI

<details><summary><b>Abstract</b></summary>

In streaming Reinforcement Learning (RL), transitions are observed and discarded immediately after a single update. While this minimizes resource usage for on-device applications, it makes agents notoriously sample-inefficient, since value-based losses alone struggle to extract meaningful representations from transient data. We propose extending Self-Predictive Representations (SPR) to the streaming pipeline to maximize the utility of every observed frame. However, due to the highly correlated samples induced by the streaming regime, naively applying this auxiliary loss results in training instabilities. Thus, we introduce orthogonal gradient updates relative to the momentum target and resolve gradient conflicts arising from streaming-specific optimizers. Validated across the Atari, MinAtar, and Octax suites, our approach systematically outperforms existing streaming baselines. Latent-space analysis, including t-SNE visualizations and effective-rank measurements, confirms that our method learns significantly richer representations, bridging the performance gap caused by the absence of a replay buffer, while remaining efficient enough to train on just a few CPU cores.

</details>

<details><summary><b>Chinese Abstract</b></summary>

In streaming Reinforcement Learning (RL), transitions are observed and discarded immediately after a single update. While this minimizes resource usage for on-device applications, it makes agents notoriously sample-inefficient, since value-based losses alone struggle to extract meaningful representations from transient data.We propose extending Self-Predictive Representations (SPR) to the streaming pipeline to maximize the utility of every observed frame. However, due to the highly correlated samples induced by the streaming regime, naively applying this auxiliary loss results in training instabilities. Thus, we introduce orthogonal gradient updates relative to the momentum target and resolve gradient conflicts arising from streaming-specific optimizers.Validated across the Atari, MinAtar, and Octax suites, our approach systematically outperforms existing streaming baselines. Latent-space analysis, including t-SNE visualizations and effective-rank measurements, confirms that our method learns significantly richer representations, bridging the performance gap caused by the absence of a replay buffer, while remaining efficient enough to train on just a few CPU cores.

</details>

---

## 93. The Critical Horizon: Inspection Design Principles for Multi-Stage Operations and Deep Reasoning

**Chinese Title**: The Critical Horizon: Inspection Design Principles for Multi-Stage Operations and Deep Reasoning

**Authors**: Seyed Morteza Emadi

**Date**: 2026-02-10 | **arXiv**: [2602.09394v1](http://arxiv.org/abs/2602.09394v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09394v1)

**Categories**: stat.ML, cs.AI, cs.IT, cs.LG

<details><summary><b>Abstract</b></summary>

Manufacturing lines, service journeys, supply chains, and AI reasoning chains share a common challenge: attributing a terminal outcome to the intermediate stage that caused it. We establish an information-theoretic barrier to this credit assignment problem: the signal connecting early steps to final outcomes decays exponentially with depth, creating a critical horizon beyond which no algorithm can learn from endpoint data alone. We prove four results. First, a Signal Decay Bound: sample complexity for attributing outcomes to early stages grows exponentially in the number of intervening steps. Second, Width Limits: parallel rollouts provide only logarithmic relief, with correlation capping the effective number of independent samples. Third, an Objective Mismatch: additive reward aggregation optimizes the wrong quantity when sequential validity requires all steps to be correct. Fourth, Optimal Inspection Design: uniform checkpoint spacing is minimax-optimal under homogeneous signal attenuation, while a greedy algorithm yields optimal non-uniform schedules under heterogeneous attenuation. Together, these results provide a common analytical foundation for inspection design in operations and supervision design in AI.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Manufacturing lines, service journeys, supply chains, and AI reasoning chains share a common challenge: attributing a terminal outcome to the intermediate stage that caused it. We establish an information-theoretic barrier to this credit assignment problem: the signal connecting early steps to final outcomes decays exponentially with depth, creating a critical horizon beyond which no algorithm can learn from endpoint data alone.We prove four results. First, a Signal Decay Bound: sample complexity for attributing outcomes to early stages grows exponentially in the number of intervening steps. Second, Width Limits: parallel rollouts provide only logarithmic relief, with correlation capping the effective number of independent samples.Third, an Objective Mismatch: additive reward aggregation optimizes the wrong quantity when sequential validity requires all steps to be correct. Fourth, Optimal Inspection Design: uniform checkpoint spacing is minimax-optimal under homogeneous signal attenuation, while a greedy algorithm yields optimal non-uniform schedules under heterogeneous attenuation.Together, these results provide a common analytical foundation for inspection design in operations and supervision design in AI.

</details>

---

## 94. Contractual Deepfakes: Can Large Language Models Generate Contracts?

**Chinese Title**: Contractual Deepfakes: Can Large Language Models Generate Contracts?

**Authors**: Eliza Mik

**Date**: 2026-02-10 | **arXiv**: [2602.09384v1](http://arxiv.org/abs/2602.09384v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09384v1)

**Categories**: cs.CL, cs.AI

<details><summary><b>Abstract</b></summary>

Notwithstanding their unprecedented ability to generate text, LLMs do not understand the meaning of words, have no sense of context and cannot reason. Their output constitutes an approximation of statistically dominant word patterns. And yet, the drafting of contracts is often presented as a typical legal task that could be facilitated by this technology. This paper seeks to put an end to such unreasonable ideas. Predicting words differs from using language in the circumstances of specific transactions and reconstituting common contractual phrases differs from reasoning about the law. LLMs seem to be able to generate generic and superficially plausible contractual documents. In the cold light of day, such documents may turn out to be useless assemblages of inconsistent provisions or contracts that are enforceable but unsuitable for a given transaction. This paper casts a shadow on the simplistic assumption that LLMs threaten the continued viability of the legal industry.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Notwithstanding their unprecedented ability to generate text, LLMs do not understand the meaning of words, have no sense of context and cannot reason. Their output constitutes an approximation of statistically dominant word patterns. And yet, the drafting of contracts is often presented as a typical legal task that could be facilitated by this technology. This paper seeks to put an end to such unreasonable ideas.Predicting words differs from using language in the circumstances of specific transactions and reconstituting common contractual phrases differs from reasoning about the law. LLMs seem to be able to generate generic and superficially plausible contractual documents. In the cold light of day, such documents may turn out to be useless assemblages of inconsistent provisions or contracts that are enforceable but unsuitable for a given transaction.This paper casts a shadow on the simplistic assumption that LLMs threaten the continued viability of the legal industry.

</details>

---

## 95. Decoupled MPPI-Based Multi-Arm Motion Planning

**Chinese Title**: Decoupled MPPI-Based Multi-Arm Motion Planning

**Authors**: Dan Evron, Elias Goldsztejn, Ronen I. Brafman

**Date**: 2026-02-10 | **arXiv**: [2602.10114v1](http://arxiv.org/abs/2602.10114v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10114v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Recent advances in sampling-based motion planning algorithms for high DOF arms leverage GPUs to provide SOTA performance. These algorithms can be used to control multiple arms jointly, but this approach scales poorly. To address this, we extend STORM, a sampling-based model-predictive-control (MPC) motion planning algorithm, to handle multiple robots in a distributed fashion. First, we modify STORM to handle dynamic obstacles. Then, we let each arm compute its own motion plan prefix, which it shares with the other arms, which treat it as a dynamic obstacle. Finally, we add a dynamic priority scheme. The new algorithm, MR-STORM, demonstrates clear empirical advantages over SOTA algorithms when operating with both static and dynamic obstacles.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Recent advances in sampling-based motion planning algorithms for high DOF arms leverage GPUs to provide SOTA performance. These algorithms can be used to control multiple arms jointly, but this approach scales poorly. To address this, we extend STORM, a sampling-based model-predictive-control (MPC) motion planning algorithm, to handle multiple robots in a distributed fashion. First, we modify STORM to handle dynamic obstacles.Then, we let each arm compute its own motion plan prefix, which it shares with the other arms, which treat it as a dynamic obstacle. Finally, we add a dynamic priority scheme. The new algorithm, MR-STORM, demonstrates clear empirical advantages over SOTA algorithms when operating with both static and dynamic obstacles.

</details>

---

## 96. ST4VLA: Spatially Guided Training for Vision-Language-Action Models

**Chinese Title**: ST4VLA: Spatially Guided Training for Vision-Language-Action Models

**Authors**: Jinhui Ye, Fangjing Wang, Ning Gao, Junqiu Yu, Yangkun Zhu et al.

**Date**: 2026-02-10 | **arXiv**: [2602.10109v1](http://arxiv.org/abs/2602.10109v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10109v1)

**Project**: https://internrobotics.github.io/internvla-m1.github.io/  **Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Large vision-language models (VLMs) excel at multimodal understanding but fall short when extended to embodied tasks, where instructions must be transformed into low-level motor actions. We introduce ST4VLA, a dual-system Vision-Language-Action framework that leverages Spatial Guided Training to align action learning with spatial priors in VLMs. ST4VLA includes two stages: (i) spatial grounding pre-training, which equips the VLM with transferable priors via scalable point, box, and trajectory prediction from both web-scale and robot-specific data, and (ii) spatially guided action post-training, which encourages the model to produce richer spatial priors to guide action generation via spatial prompting. This design preserves spatial grounding during policy learning and promotes consistent optimization across spatial and action objectives. Empirically, ST4VLA achieves substantial improvements over vanilla VLA, with performance increasing from 66.1 -> 84.6 on Google Robot and from 54.7 -> 73.2 on WidowX Robot, establishing new state-of-the-art results on SimplerEnv. It also demonstrates stronger generalization to unseen objects and paraphrased instructions, as well as robustness to long-horizon perturbations in real-world settings. These results highlight scalable spatially guided training as a promising direction for robust, generalizable robot learning. Source code, data and models are released at https://internrobotics.github.io/internvla-m1.github.io/

</details>

<details><summary><b>Chinese Abstract</b></summary>

Large vision-language models (VLMs) excel at multimodal understanding but fall short when extended to embodied tasks, where instructions must be transformed into low-level motor actions. We introduce ST4VLA, a dual-system Vision-Language-Action framework that leverages Spatial Guided Training to align action learning with spatial priors in VLMs.ST4VLA includes two stages: (i) spatial grounding pre-training, which equips the VLM with transferable priors via scalable point, box, and trajectory prediction from both web-scale and robot-specific data, and (ii) spatially guided action post-training, which encourages the model to produce richer spatial priors to guide action generation via spatial prompting.This design preserves spatial grounding during policy learning and promotes consistent optimization across spatial and action objectives. Empirically, ST4VLA achieves substantial improvements over vanilla VLA, with performance increasing from 66.1 -> 84.6 on Google Robot and from 54.7 -> 73.2 on WidowX Robot, establishing new state-of-the-art results on SimplerEnv.It also demonstrates stronger generalization to unseen objects and paraphrased instructions, as well as robustness to long-horizon perturbations in real-world settings. These results highlight scalable spatially guided training as a promising direction for robust, generalizable robot learning. Source code, data and models are released at https://internrobotics.github.io/internvla-m1.github.io/

</details>

---

## 97. EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration

**Chinese Title**: EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration

**Authors**: Modi Shi, Shijia Peng, Jin Chen, Haoran Jiang, Yinghui Li et al.

**Date**: 2026-02-10 | **arXiv**: [2602.10106v1](http://arxiv.org/abs/2602.10106v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10106v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Human demonstrations offer rich environmental diversity and scale naturally, making them an appealing alternative to robot teleoperation. While this paradigm has advanced robot-arm manipulation, its potential for the more challenging, data-hungry problem of humanoid loco-manipulation remains largely unexplored. We present EgoHumanoid, the first framework to co-train a vision-language-action policy using abundant egocentric human demonstrations together with a limited amount of robot data, enabling humanoids to perform loco-manipulation across diverse real-world environments. To bridge the embodiment gap between humans and robots, including discrepancies in physical morphology and viewpoint, we introduce a systematic alignment pipeline spanning from hardware design to data processing. A portable system for scalable human data collection is developed, and we establish practical collection protocols to improve transferability. At the core of our human-to-humanoid alignment pipeline lies two key components. The view alignment reduces visual domain discrepancies caused by camera height and perspective variation. The action alignment maps human motions into a unified, kinematically feasible action space for humanoid control. Extensive real-world experiments demonstrate that incorporating robot-free egocentric data significantly outperforms robot-only baselines by 51\%, particularly in unseen environments. Our analysis further reveals which behaviors transfer effectively and the potential for scaling human data.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Human demonstrations offer rich environmental diversity and scale naturally, making them an appealing alternative to robot teleoperation. While this paradigm has advanced robot-arm manipulation, its potential for the more challenging, data-hungry problem of humanoid loco-manipulation remains largely unexplored.We present EgoHumanoid, the first framework to co-train a vision-language-action policy using abundant egocentric human demonstrations together with a limited amount of robot data, enabling humanoids to perform loco-manipulation across diverse real-world environments.To bridge the embodiment gap between humans and robots, including discrepancies in physical morphology and viewpoint, we introduce a systematic alignment pipeline spanning from hardware design to data processing. A portable system for scalable human data collection is developed, and we establish practical collection protocols to improve transferability. At the core of our human-to-humanoid alignment pipeline lies two key components.The view alignment reduces visual domain discrepancies caused by camera height and perspective variation. The action alignment maps human motions into a unified, kinematically feasible action space for humanoid control. Extensive real-world experiments demonstrate that incorporating robot-free egocentric data significantly outperforms robot-only baselines by 51\%, particularly in unseen environments.Our analysis further reveals which behaviors transfer effectively and the potential for scaling human data.

</details>

---

## 98. DexImit: Learning Bimanual Dexterous Manipulation from Monocular Human Videos

**Chinese Title**: DexImit: Learning Bimanual Dexterous Manipulation from Monocular Human Videos

**Authors**: Juncheng Mu, Sizhe Yang, Yiming Bao, Hojin Bae, Tianming Wei et al.

**Date**: 2026-02-10 | **arXiv**: [2602.10105v1](http://arxiv.org/abs/2602.10105v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10105v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Data scarcity fundamentally limits the generalization of bimanual dexterous manipulation, as real-world data collection for dexterous hands is expensive and labor-intensive. Human manipulation videos, as a direct carrier of manipulation knowledge, offer significant potential for scaling up robot learning. However, the substantial embodiment gap between human hands and robotic dexterous hands makes direct pretraining from human videos extremely challenging. To bridge this gap and unleash the potential of large-scale human manipulation video data, we propose DexImit, an automated framework that converts monocular human manipulation videos into physically plausible robot data, without any additional information. DexImit employs a four-stage generation pipeline: (1) reconstructing hand-object interactions from arbitrary viewpoints with near-metric scale; (2) performing subtask decomposition and bimanual scheduling; (3) synthesizing robot trajectories consistent with the demonstrated interactions; (4) comprehensive data augmentation for zero-shot real-world deployment. Building on these designs, DexImit can generate large-scale robot data based on human videos, either from the Internet or video generation models. DexImit is capable of handling diverse manipulation tasks, including tool use (e.g., cutting an apple), long-horizon tasks (e.g., making a beverage), and fine-grained manipulations (e.g., stacking cups).

</details>

<details><summary><b>Chinese Abstract</b></summary>

Data scarcity fundamentally limits the generalization of bimanual dexterous manipulation, as real-world data collection for dexterous hands is expensive and labor-intensive. Human manipulation videos, as a direct carrier of manipulation knowledge, offer significant potential for scaling up robot learning.However, the substantial embodiment gap between human hands and robotic dexterous hands makes direct pretraining from human videos extremely challenging. To bridge this gap and unleash the potential of large-scale human manipulation video data, we propose DexImit, an automated framework that converts monocular human manipulation videos into physically plausible robot data, without any additional information.DexImit employs a four-stage generation pipeline: (1) reconstructing hand-object interactions from arbitrary viewpoints with near-metric scale; (2) performing subtask decomposition and bimanual scheduling; (3) synthesizing robot trajectories consistent with the demonstrated interactions; (4) comprehensive data augmentation for zero-shot real-world deployment.Building on these designs, DexImit can generate large-scale robot data based on human videos, either from the Internet or video generation models. DexImit is capable of handling diverse manipulation tasks, including tool use (e.g., cutting an apple), long-horizon tasks (e.g., making a beverage), and fine-grained manipulations (e.g., stacking cups).

</details>

---

## 99. Robo3R: Enhancing Robotic Manipulation with Accurate Feed-Forward 3D Reconstruction

**Chinese Title**: Robo3R: Enhancing Robotic Manipulation with Accurate Feed-Forward 3D Reconstruction

**Authors**: Sizhe Yang, Linning Xu, Hao Li, Juncheng Mu, Jia Zeng et al.

**Date**: 2026-02-10 | **arXiv**: [2602.10101v1](http://arxiv.org/abs/2602.10101v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10101v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

3D spatial perception is fundamental to generalizable robotic manipulation, yet obtaining reliable, high-quality 3D geometry remains challenging. Depth sensors suffer from noise and material sensitivity, while existing reconstruction models lack the precision and metric consistency required for physical interaction. We introduce Robo3R, a feed-forward, manipulation-ready 3D reconstruction model that predicts accurate, metric-scale scene geometry directly from RGB images and robot states in real time. Robo3R jointly infers scale-invariant local geometry and relative camera poses, which are unified into the scene representation in the canonical robot frame via a learned global similarity transformation. To meet the precision demands of manipulation, Robo3R employs a masked point head for sharp, fine-grained point clouds, and a keypoint-based Perspective-n-Point (PnP) formulation to refine camera extrinsics and global alignment. Trained on Robo3R-4M, a curated large-scale synthetic dataset with four million high-fidelity annotated frames, Robo3R consistently outperforms state-of-the-art reconstruction methods and depth sensors. Across downstream tasks including imitation learning, sim-to-real transfer, grasp synthesis, and collision-free motion planning, we observe consistent gains in performance, suggesting the promise of this alternative 3D sensing module for robotic manipulation.

</details>

<details><summary><b>Chinese Abstract</b></summary>

3D spatial perception is fundamental to generalizable robotic manipulation, yet obtaining reliable, high-quality 3D geometry remains challenging. Depth sensors suffer from noise and material sensitivity, while existing reconstruction models lack the precision and metric consistency required for physical interaction.We introduce Robo3R, a feed-forward, manipulation-ready 3D reconstruction model that predicts accurate, metric-scale scene geometry directly from RGB images and robot states in real time. Robo3R jointly infers scale-invariant local geometry and relative camera poses, which are unified into the scene representation in the canonical robot frame via a learned global similarity transformation.To meet the precision demands of manipulation, Robo3R employs a masked point head for sharp, fine-grained point clouds, and a keypoint-based Perspective-n-Point (PnP) formulation to refine camera extrinsics and global alignment. Trained on Robo3R-4M, a curated large-scale synthetic dataset with four million high-fidelity annotated frames, Robo3R consistently outperforms state-of-the-art reconstruction methods and depth sensors.Across downstream tasks including imitation learning, sim-to-real transfer, grasp synthesis, and collision-free motion planning, we observe consistent gains in performance, suggesting the promise of this alternative 3D sensing module for robotic manipulation.

</details>

---

## 100. UniVTAC: A Unified Simulation Platform for Visuo-Tactile Manipulation Data Generation, Learning, and Benchmarking

**Chinese Title**: UniVTAC: A Unified Simulation Platform for Visuo-Tactile Manipulation Data Generation, Learning, and Benchmarking

**Authors**: Baijun Chen, Weijie Wan, Tianxing Chen, Xianda Guo, Congsheng Xu et al.

**Date**: 2026-02-10 | **arXiv**: [2602.10093v1](http://arxiv.org/abs/2602.10093v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10093v1)

**Project**: https://univtac.github.io/.  **Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Robotic manipulation has seen rapid progress with vision-language-action (VLA) policies. However, visuo-tactile perception is critical for contact-rich manipulation, as tasks such as insertion are difficult to complete robustly using vision alone. At the same time, acquiring large-scale and reliable tactile data in the physical world remains costly and challenging, and the lack of a unified evaluation platform further limits policy learning and systematic analysis. To address these challenges, we propose UniVTAC, a simulation-based visuo-tactile data synthesis platform that supports three commonly used visuo-tactile sensors and enables scalable and controllable generation of informative contact interactions. Based on this platform, we introduce the UniVTAC Encoder, a visuo-tactile encoder trained on large-scale simulation-synthesized data with designed supervisory signals, providing tactile-centric visuo-tactile representations for downstream manipulation tasks. In addition, we present the UniVTAC Benchmark, which consists of eight representative visuo-tactile manipulation tasks for evaluating tactile-driven policies. Experimental results show that integrating the UniVTAC Encoder improves average success rates by 17.1% on the UniVTAC Benchmark, while real-world robotic experiments further demonstrate a 25% improvement in task success. Our webpage is available at https://univtac.github.io/.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Robotic manipulation has seen rapid progress with vision-language-action (VLA) policies. However, visuo-tactile perception is critical for contact-rich manipulation, as tasks such as insertion are difficult to complete robustly using vision alone.At the same time, acquiring large-scale and reliable tactile data in the physical world remains costly and challenging, and the lack of a unified evaluation platform further limits policy learning and systematic analysis.To address these challenges, we propose UniVTAC, a simulation-based visuo-tactile data synthesis platform that supports three commonly used visuo-tactile sensors and enables scalable and controllable generation of informative contact interactions.Based on this platform, we introduce the UniVTAC Encoder, a visuo-tactile encoder trained on large-scale simulation-synthesized data with designed supervisory signals, providing tactile-centric visuo-tactile representations for downstream manipulation tasks. In addition, we present the UniVTAC Benchmark, which consists of eight representative visuo-tactile manipulation tasks for evaluating tactile-driven policies.Experimental results show that integrating the UniVTAC Encoder improves average success rates by 17.1% on the UniVTAC Benchmark, while real-world robotic experiments further demonstrate a 25% improvement in task success. Our webpage is available at https://univtac.github.io/.

</details>

---

## 101. RoboInter: A Holistic Intermediate Representation Suite Towards Robotic Manipulation

**Chinese Title**: RoboInter: A Holistic Intermediate Representation Suite Towards Robotic Manipulation

**Authors**: Hao Li, Ziqin Wang, Zi-han Ding, Shuai Yang, Yilun Chen et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09973v1](http://arxiv.org/abs/2602.09973v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09973v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Advances in large vision-language models (VLMs) have stimulated growing interest in vision-language-action (VLA) systems for robot manipulation. However, existing manipulation datasets remain costly to curate, highly embodiment-specific, and insufficient in coverage and diversity, thereby hindering the generalization of VLA models. Recent approaches attempt to mitigate these limitations via a plan-then-execute paradigm, where high-level plans (e.g., subtasks, trace) are first generated and subsequently translated into low-level actions, but they critically rely on extra intermediate supervision, which is largely absent from existing datasets. To bridge this gap, we introduce the RoboInter Manipulation Suite, a unified resource including data, benchmarks, and models of intermediate representations for manipulation. It comprises RoboInter-Tool, a lightweight GUI that enables semi-automatic annotation of diverse representations, and RoboInter-Data, a large-scale dataset containing over 230k episodes across 571 diverse scenes, which provides dense per-frame annotations over more than 10 categories of intermediate representations, substantially exceeding prior work in scale and annotation quality. Building upon this foundation, RoboInter-VQA introduces 9 spatial and 20 temporal embodied VQA categories to systematically benchmark and enhance the embodied reasoning capabilities of VLMs. Meanwhile, RoboInter-VLA offers an integrated plan-then-execute framework, supporting modular and end-to-end VLA variants that bridge high-level planning with low-level execution via intermediate supervision. In total, RoboInter establishes a practical foundation for advancing robust and generalizable robotic learning via fine-grained and diverse intermediate representations.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Advances in large vision-language models (VLMs) have stimulated growing interest in vision-language-action (VLA) systems for robot manipulation. However, existing manipulation datasets remain costly to curate, highly embodiment-specific, and insufficient in coverage and diversity, thereby hindering the generalization of VLA models.Recent approaches attempt to mitigate these limitations via a plan-then-execute paradigm, where high-level plans (e.g., subtasks, trace) are first generated and subsequently translated into low-level actions, but they critically rely on extra intermediate supervision, which is largely absent from existing datasets.To bridge this gap, we introduce the RoboInter Manipulation Suite, a unified resource including data, benchmarks, and models of intermediate representations for manipulation.It comprises RoboInter-Tool, a lightweight GUI that enables semi-automatic annotation of diverse representations, and RoboInter-Data, a large-scale dataset containing over 230k episodes across 571 diverse scenes, which provides dense per-frame annotations over more than 10 categories of intermediate representations, substantially exceeding prior work in scale and annotation quality.Building upon this foundation, RoboInter-VQA introduces 9 spatial and 20 temporal embodied VQA categories to systematically benchmark and enhance the embodied reasoning capabilities of VLMs. Meanwhile, RoboInter-VLA offers an integrated plan-then-execute framework, supporting modular and end-to-end VLA variants that bridge high-level planning with low-level execution via intermediate supervision.In total, RoboInter establishes a practical foundation for advancing robust and generalizable robotic learning via fine-grained and diverse intermediate representations.

</details>

---

## 102. Hydra-Nav: Object Navigation via Adaptive Dual-Process Reasoning

**Chinese Title**: Hydra-Nav: Object Navigation via Adaptive Dual-Process Reasoning

**Authors**: Zixuan Wang, Huang Fang, Shaoan Wang, Yuanfei Luo, Heng Dong et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09972v1](http://arxiv.org/abs/2602.09972v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09972v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

While large vision-language models (VLMs) show promise for object goal navigation, current methods still struggle with low success rates and inefficient localization of unseen objects--failures primarily attributed to weak temporal-spatial reasoning. Meanwhile, recent attempts to inject reasoning into VLM-based agents improve success rates but incur substantial computational overhead. To address both the ineffectiveness and inefficiency of existing approaches, we introduce Hydra-Nav, a unified VLM architecture that adaptively switches between a deliberative slow system for analyzing exploration history and formulating high-level plans, and a reactive fast system for efficient execution. We train Hydra-Nav through a three-stage curriculum: (i) spatial-action alignment to strengthen trajectory planning, (ii) memory-reasoning integration to enhance temporal-spatial reasoning over long-horizon exploration, and (iii) iterative rejection fine-tuning to enable selective reasoning at critical decision points. Extensive experiments demonstrate that Hydra-Nav achieves state-of-the-art performance on the HM3D, MP3D, and OVON benchmarks, outperforming the second-best methods by 11.1%, 17.4%, and 21.2%, respectively. Furthermore, we introduce SOT (Success weighted by Operation Time), a new metric to measure search efficiency across VLMs with varying reasoning intensity. Results show that adaptive reasoning significantly enhances search efficiency over fixed-frequency baselines.

</details>

<details><summary><b>Chinese Abstract</b></summary>

While large vision-language models (VLMs) show promise for object goal navigation, current methods still struggle with low success rates and inefficient localization of unseen objects--failures primarily attributed to weak temporal-spatial reasoning. Meanwhile, recent attempts to inject reasoning into VLM-based agents improve success rates but incur substantial computational overhead.To address both the ineffectiveness and inefficiency of existing approaches, we introduce Hydra-Nav, a unified VLM architecture that adaptively switches between a deliberative slow system for analyzing exploration history and formulating high-level plans, and a reactive fast system for efficient execution.We train Hydra-Nav through a three-stage curriculum: (i) spatial-action alignment to strengthen trajectory planning, (ii) memory-reasoning integration to enhance temporal-spatial reasoning over long-horizon exploration, and (iii) iterative rejection fine-tuning to enable selective reasoning at critical decision points.Extensive experiments demonstrate that Hydra-Nav achieves state-of-the-art performance on the HM3D, MP3D, and OVON benchmarks, outperforming the second-best methods by 11.1%, 17.4%, and 21.2%, respectively. Furthermore, we introduce SOT (Success weighted by Operation Time), a new metric to measure search efficiency across VLMs with varying reasoning intensity.Results show that adaptive reasoning significantly enhances search efficiency over fixed-frequency baselines.

</details>

---

## 103. TriPilot-FF: Coordinated Whole-Body Teleoperation with Force Feedback

**Chinese Title**: TriPilot-FF: Coordinated Whole-Body Teleoperation with Force Feedback

**Authors**: Zihao Li, Yanan Zhou, Ranpeng Qiu, Hangyu Wu, Guoqiang Ren et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09888v1](http://arxiv.org/abs/2602.09888v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09888v1)

**Project**: http://bit.ly/46H3ZJT.  **Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Mobile manipulators broaden the operational envelope for robot manipulation. However, the whole-body teleoperation of such robots remains a problem: operators must coordinate a wheeled base and two arms while reasoning about obstacles and contact. Existing interfaces are predominantly hand-centric (e.g., VR controllers and joysticks), leaving foot-operated channels underexplored for continuous base control. We present TriPilot-FF, an open-source whole-body teleoperation system for a custom bimanual mobile manipulator that introduces a foot-operated pedal with lidar-driven pedal haptics, coupled with upper-body bimanual leader-follower teleoperation. Using only a low-cost base-mounted lidar, TriPilot-FF renders a resistive pedal cue from proximity-to-obstacle signals in the commanded direction, shaping operator commands toward collision-averse behaviour without an explicit collision-avoidance controller. The system also supports arm-side force reflection for contact awareness and provides real-time force and visual guidance of bimanual manipulability to prompt mobile base repositioning, thereby improving reach. We demonstrate the capability of TriPilot-FF to effectively ``co-pilot'' the human operator over long time-horizons and tasks requiring precise mobile base movement and coordination. Finally, we incorporate teleoperation feedback signals into an Action Chunking with Transformers (ACT) policy and demonstrate improved performance when the additional information is available. We release the pedal device design, full software stack, and conduct extensive real-world evaluations on a bimanual wheeled platform. The project page of TriPilot-FF is http://bit.ly/46H3ZJT.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Mobile manipulators broaden the operational envelope for robot manipulation. However, the whole-body teleoperation of such robots remains a problem: operators must coordinate a wheeled base and two arms while reasoning about obstacles and contact. Existing interfaces are predominantly hand-centric (e.g., VR controllers and joysticks), leaving foot-operated channels underexplored for continuous base control.We present TriPilot-FF, an open-source whole-body teleoperation system for a custom bimanual mobile manipulator that introduces a foot-operated pedal with lidar-driven pedal haptics, coupled with upper-body bimanual leader-follower teleoperation.Using only a low-cost base-mounted lidar, TriPilot-FF renders a resistive pedal cue from proximity-to-obstacle signals in the commanded direction, shaping operator commands toward collision-averse behaviour without an explicit collision-avoidance controller.The system also supports arm-side force reflection for contact awareness and provides real-time force and visual guidance of bimanual manipulability to prompt mobile base repositioning, thereby improving reach. We demonstrate the capability of TriPilot-FF to effectively ``co-pilot'' the human operator over long time-horizons and tasks requiring precise mobile base movement and coordination.Finally, we incorporate teleoperation feedback signals into an Action Chunking with Transformers (ACT) policy and demonstrate improved performance when the additional information is available. We release the pedal device design, full software stack, and conduct extensive real-world evaluations on a bimanual wheeled platform. The project page of TriPilot-FF is http://bit.ly/46H3ZJT.

</details>

---

## 104. BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation

**Chinese Title**: BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation

**Authors**: Yucheng Hu, Jianke Zhang, Yuanfei Luo, Yanjiang Guo, Xiaoyu Chen et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09849v1](http://arxiv.org/abs/2602.09849v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09849v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Equipping embodied agents with the ability to reason about tasks, foresee physical outcomes, and generate precise actions is essential for general-purpose manipulation. While recent Vision-Language-Action (VLA) models have leveraged pre-trained foundation models, they typically focus on either linguistic planning or visual forecasting in isolation. These methods rarely integrate both capabilities simultaneously to guide action generation, leading to suboptimal performance in complex, long-horizon manipulation tasks. To bridge this gap, we propose BagelVLA, a unified model that integrates linguistic planning, visual forecasting, and action generation within a single framework. Initialized from a pretrained unified understanding and generative model, BagelVLA is trained to interleave textual reasoning and visual prediction directly into the action execution loop. To efficiently couple these modalities, we introduce Residual Flow Guidance (RFG), which initializes from current observation and leverages single-step denoising to extract predictive visual features, guiding action generation with minimal latency. Extensive experiments demonstrate that BagelVLA outperforms existing baselines by a significant margin on multiple simulated and real-world benchmarks, particularly in tasks requiring multi-stage reasoning.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Equipping embodied agents with the ability to reason about tasks, foresee physical outcomes, and generate precise actions is essential for general-purpose manipulation. While recent Vision-Language-Action (VLA) models have leveraged pre-trained foundation models, they typically focus on either linguistic planning or visual forecasting in isolation.These methods rarely integrate both capabilities simultaneously to guide action generation, leading to suboptimal performance in complex, long-horizon manipulation tasks. To bridge this gap, we propose BagelVLA, a unified model that integrates linguistic planning, visual forecasting, and action generation within a single framework.Initialized from a pretrained unified understanding and generative model, BagelVLA is trained to interleave textual reasoning and visual prediction directly into the action execution loop. To efficiently couple these modalities, we introduce Residual Flow Guidance (RFG), which initializes from current observation and leverages single-step denoising to extract predictive visual features, guiding action generation with minimal latency.Extensive experiments demonstrate that BagelVLA outperforms existing baselines by a significant margin on multiple simulated and real-world benchmarks, particularly in tasks requiring multi-stage reasoning.

</details>

---

## 105. Diverse Skill Discovery for Quadruped Robots via Unsupervised Learning

**Chinese Title**: Diverse Skill Discovery for Quadruped Robots via Unsupervised Learning

**Authors**: Ruopeng Cui, Yifei Bi, Haojie Luo, Wei Li

**Date**: 2026-02-10 | **arXiv**: [2602.09767v1](http://arxiv.org/abs/2602.09767v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09767v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Reinforcement learning necessitates meticulous reward shaping by specialists to elicit target behaviors, while imitation learning relies on costly task-specific data. In contrast, unsupervised skill discovery can potentially reduce these burdens by learning a diverse repertoire of useful skills driven by intrinsic motivation. However, existing methods exhibit two key limitations: they typically rely on a single policy to master a versatile repertoire of behaviors without modeling the shared structure or distinctions among them, which results in low learning efficiency; moreover, they are susceptible to reward hacking, where the reward signal increases and converges rapidly while the learned skills display insufficient actual diversity. In this work, we introduce an Orthogonal Mixture-of-Experts (OMoE) architecture that prevents diverse behaviors from collapsing into overlapping representations, enabling a single policy to master a wide spectrum of locomotion skills. In addition, we design a multi-discriminator framework in which different discriminators operate on distinct observation spaces, effectively mitigating reward hacking. We evaluated our method on the 12-DOF Unitree A1 quadruped robot, demonstrating a diverse set of locomotion skills. Our experiments demonstrate that the proposed framework boosts training efficiency and yields an 18.3\% expansion in state-space coverage compared to the baseline.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Reinforcement learning necessitates meticulous reward shaping by specialists to elicit target behaviors, while imitation learning relies on costly task-specific data. In contrast, unsupervised skill discovery can potentially reduce these burdens by learning a diverse repertoire of useful skills driven by intrinsic motivation.However, existing methods exhibit two key limitations: they typically rely on a single policy to master a versatile repertoire of behaviors without modeling the shared structure or distinctions among them, which results in low learning efficiency; moreover, they are susceptible to reward hacking, where the reward signal increases and converges rapidly while the learned skills display insufficient actual diversity.In this work, we introduce an Orthogonal Mixture-of-Experts (OMoE) architecture that prevents diverse behaviors from collapsing into overlapping representations, enabling a single policy to master a wide spectrum of locomotion skills. In addition, we design a multi-discriminator framework in which different discriminators operate on distinct observation spaces, effectively mitigating reward hacking.We evaluated our method on the 12-DOF Unitree A1 quadruped robot, demonstrating a diverse set of locomotion skills. Our experiments demonstrate that the proposed framework boosts training efficiency and yields an 18.3\% expansion in state-space coverage compared to the baseline.

</details>

---

## 106. NavDreamer: Video Models as Zero-Shot 3D Navigators

**Chinese Title**: NavDreamer: Video Models as Zero-Shot 3D Navigators

**Authors**: Xijie Huang, Weiqi Gai, Tianyue Wu, Congyu Wang, Zhiyang Liu et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09765v1](http://arxiv.org/abs/2602.09765v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09765v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Previous Vision-Language-Action models face critical limitations in navigation: scarce, diverse data from labor-intensive collection and static representations that fail to capture temporal dynamics and physical laws. We propose NavDreamer, a video-based framework for 3D navigation that leverages generative video models as a universal interface between language instructions and navigation trajectories. Our main hypothesis is that video's ability to encode spatiotemporal information and physical dynamics, combined with internet-scale availability, enables strong zero-shot generalization in navigation. To mitigate the stochasticity of generative predictions, we introduce a sampling-based optimization method that utilizes a VLM for trajectory scoring and selection. An inverse dynamics model is employed to decode executable waypoints from generated video plans for navigation. To systematically evaluate this paradigm in several video model backbones, we introduce a comprehensive benchmark covering object navigation, precise navigation, spatial grounding, language control, and scene reasoning. Extensive experiments demonstrate robust generalization across novel objects and unseen environments, with ablation studies revealing that navigation's high-level decision-making nature makes it particularly suited for video-based planning.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Previous Vision-Language-Action models face critical limitations in navigation: scarce, diverse data from labor-intensive collection and static representations that fail to capture temporal dynamics and physical laws. We propose NavDreamer, a video-based framework for 3D navigation that leverages generative video models as a universal interface between language instructions and navigation trajectories.Our main hypothesis is that video's ability to encode spatiotemporal information and physical dynamics, combined with internet-scale availability, enables strong zero-shot generalization in navigation. To mitigate the stochasticity of generative predictions, we introduce a sampling-based optimization method that utilizes a VLM for trajectory scoring and selection.An inverse dynamics model is employed to decode executable waypoints from generated video plans for navigation. To systematically evaluate this paradigm in several video model backbones, we introduce a comprehensive benchmark covering object navigation, precise navigation, spatial grounding, language control, and scene reasoning.Extensive experiments demonstrate robust generalization across novel objects and unseen environments, with ablation studies revealing that navigation's high-level decision-making nature makes it particularly suited for video-based planning.

</details>

---

## 107. Rethinking Visual-Language-Action Model Scaling: Alignment, Mixture, and Regularization

**Chinese Title**: Rethinking Visual-Language-Action Model Scaling: Alignment, Mixture, and Regularization

**Authors**: Ye Wang, Sipeng Zheng, Hao Luo, Wanpeng Zhang, Haoqi Yuan et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09722v1](http://arxiv.org/abs/2602.09722v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09722v1)

**Project**: https://research.beingbeyond.com/rethink_vla  **Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

While Vision-Language-Action (VLA) models show strong promise for generalist robot control, it remains unclear whether -- and under what conditions -- the standard "scale data" recipe translates to robotics, where training data is inherently heterogeneous across embodiments, sensors, and action spaces. We present a systematic, controlled study of VLA scaling that revisits core training choices for pretraining across diverse robots. Using a representative VLA framework that combines a vision-language backbone with flow-matching, we ablate key design decisions under matched conditions and evaluate in extensive simulation and real-robot experiments. To improve the reliability of real-world results, we introduce a Grouped Blind Ensemble protocol that blinds operators to model identity and separates policy execution from outcome judgment, reducing experimenter bias. Our analysis targets three dimensions of VLA scaling. (1) Physical alignment: we show that a unified end-effector (EEF)-relative action representation is critical for robust cross-embodiment transfer. (2) Embodiment mixture: we find that naively pooling heterogeneous robot datasets often induces negative transfer rather than gains, underscoring the fragility of indiscriminate data scaling. (3) Training regularization: we observe that intuitive strategies, such as sensory dropout and multi-stage fine-tuning, do not consistently improve performance at scale. Together, this study challenge some common assumptions about embodied scaling and provide practical guidance for training large-scale VLA policies from diverse robotic data. Project website: https://research.beingbeyond.com/rethink_vla

</details>

<details><summary><b>Chinese Abstract</b></summary>

While Vision-Language-Action (VLA) models show strong promise for generalist robot control, it remains unclear whether -- and under what conditions -- the standard "scale data" recipe translates to robotics, where training data is inherently heterogeneous across embodiments, sensors, and action spaces. We present a systematic, controlled study of VLA scaling that revisits core training choices for pretraining across diverse robots.Using a representative VLA framework that combines a vision-language backbone with flow-matching, we ablate key design decisions under matched conditions and evaluate in extensive simulation and real-robot experiments. To improve the reliability of real-world results, we introduce a Grouped Blind Ensemble protocol that blinds operators to model identity and separates policy execution from outcome judgment, reducing experimenter bias.Our analysis targets three dimensions of VLA scaling. (1) Physical alignment: we show that a unified end-effector (EEF)-relative action representation is critical for robust cross-embodiment transfer. (2) Embodiment mixture: we find that naively pooling heterogeneous robot datasets often induces negative transfer rather than gains, underscoring the fragility of indiscriminate data scaling.(3) Training regularization: we observe that intuitive strategies, such as sensory dropout and multi-stage fine-tuning, do not consistently improve performance at scale. Together, this study challenge some common assumptions about embodied scaling and provide practical guidance for training large-scale VLA policies from diverse robotic data. Project website: https://research.beingbeyond.com/rethink_vla

</details>

---

## 108. Fast Motion Planning for Non-Holonomic Mobile Robots via a Rectangular Corridor Representation of Structured Environments

**Chinese Title**: Fast Motion Planning for Non-Holonomic Mobile Robots via a Rectangular Corridor Representation of Structured Environments

**Authors**: Alejandro Gonzalez-Garcia, Sebastiaan Wyns, Sonia De Santis, Jan Swevers, Wilm DecrÃ©

**Date**: 2026-02-10 | **arXiv**: [2602.09714v1](http://arxiv.org/abs/2602.09714v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09714v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

We present a complete framework for fast motion planning of non-holonomic autonomous mobile robots in highly complex but structured environments. Conventional grid-based planners struggle with scalability, while many kinematically-feasible planners impose a significant computational burden due to their search space complexity. To overcome these limitations, our approach introduces a deterministic free-space decomposition that creates a compact graph of overlapping rectangular corridors. This method enables a significant reduction in the search space, without sacrificing path resolution. The framework then performs online motion planning by finding a sequence of rectangles and generating a near-time-optimal, kinematically-feasible trajectory using an analytical planner. The result is a highly efficient solution for large-scale navigation. We validate our framework through extensive simulations and on a physical robot. The implementation is publicly available as open-source software.

</details>

<details><summary><b>Chinese Abstract</b></summary>

We present a complete framework for fast motion planning of non-holonomic autonomous mobile robots in highly complex but structured environments. Conventional grid-based planners struggle with scalability, while many kinematically-feasible planners impose a significant computational burden due to their search space complexity.To overcome these limitations, our approach introduces a deterministic free-space decomposition that creates a compact graph of overlapping rectangular corridors. This method enables a significant reduction in the search space, without sacrificing path resolution. The framework then performs online motion planning by finding a sequence of rectangles and generating a near-time-optimal, kinematically-feasible trajectory using an analytical planner.The result is a highly efficient solution for large-scale navigation. We validate our framework through extensive simulations and on a physical robot. The implementation is publicly available as open-source software.

</details>

---

## 109. RANT: Ant-Inspired Multi-Robot Rainforest Exploration Using Particle Filter Localisation and Virtual Pheromone Coordination

**Chinese Title**: RANT: Ant-Inspired Multi-Robot Rainforest Exploration Using Particle Filter Localisation and Virtual Pheromone Coordination

**Authors**: Ameer Alhashemi, Layan Abdulhadi, Karam Abuodeh, Tala Baghdadi, Suryanarayana Datla

**Date**: 2026-02-10 | **arXiv**: [2602.09661v1](http://arxiv.org/abs/2602.09661v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09661v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

This paper presents RANT, an ant-inspired multi-robot exploration framework for noisy, uncertain environments. A team of differential-drive robots navigates a 10 x 10 m terrain, collects noisy probe measurements of a hidden richness field, and builds local probabilistic maps while the supervisor maintains a global evaluation. RANT combines particle-filter localisation, a behaviour-based controller with gradient-driven hotspot exploitation, and a lightweight no-revisit coordination mechanism based on virtual pheromone blocking. We experimentally analyse how team size, localisation fidelity, and coordination influence coverage, hotspot recall, and redundancy. Results show that particle filtering is essential for reliable hotspot engagement, coordination substantially reduces overlap, and increasing team size improves coverage but yields diminishing returns due to interference.

</details>

<details><summary><b>Chinese Abstract</b></summary>

This paper presents RANT, an ant-inspired multi-robot exploration framework for noisy, uncertain environments. A team of differential-drive robots navigates a 10 x 10 m terrain, collects noisy probe measurements of a hidden richness field, and builds local probabilistic maps while the supervisor maintains a global evaluation.RANT combines particle-filter localisation, a behaviour-based controller with gradient-driven hotspot exploitation, and a lightweight no-revisit coordination mechanism based on virtual pheromone blocking. We experimentally analyse how team size, localisation fidelity, and coordination influence coverage, hotspot recall, and redundancy.Results show that particle filtering is essential for reliable hotspot engagement, coordination substantially reduces overlap, and increasing team size improves coverage but yields diminishing returns due to interference.

</details>

---

## 110. AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild

**Chinese Title**: AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild

**Authors**: Xiaolou Sun, Wufei Si, Wenhui Ni, Yuntian Li, Dongming Wu et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09657v1](http://arxiv.org/abs/2602.09657v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09657v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Vision-language navigation (VLN) requires intelligent agents to navigate environments by interpreting linguistic instructions alongside visual observations, serving as a cornerstone task in Embodied AI. Current VLN research for unmanned aerial vehicles (UAVs) relies on detailed, pre-specified instructions to guide the UAV along predetermined routes. However, real-world outdoor exploration typically occurs in unknown environments where detailed navigation instructions are unavailable. Instead, only coarse-grained positional or directional guidance can be provided, requiring UAVs to autonomously navigate through continuous planning and obstacle avoidance. To bridge this gap, we propose AutoFly, an end-to-end Vision-Language-Action (VLA) model for autonomous UAV navigation. AutoFly incorporates a pseudo-depth encoder that derives depth-aware features from RGB inputs to enhance spatial reasoning, coupled with a progressive two-stage training strategy that effectively aligns visual, depth, and linguistic representations with action policies. Moreover, existing VLN datasets have fundamental limitations for real-world autonomous navigation, stemming from their heavy reliance on explicit instruction-following over autonomous decision-making and insufficient real-world data. To address these issues, we construct a novel autonomous navigation dataset that shifts the paradigm from instruction-following to autonomous behavior modeling through: (1) trajectory collection emphasizing continuous obstacle avoidance, autonomous planning, and recognition workflows; (2) comprehensive real-world data integration. Experimental results demonstrate that AutoFly achieves a 3.9% higher success rate compared to state-of-the-art VLA baselines, with consistent performance across simulated and real environments.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Vision-language navigation (VLN) requires intelligent agents to navigate environments by interpreting linguistic instructions alongside visual observations, serving as a cornerstone task in Embodied AI. Current VLN research for unmanned aerial vehicles (UAVs) relies on detailed, pre-specified instructions to guide the UAV along predetermined routes.However, real-world outdoor exploration typically occurs in unknown environments where detailed navigation instructions are unavailable. Instead, only coarse-grained positional or directional guidance can be provided, requiring UAVs to autonomously navigate through continuous planning and obstacle avoidance. To bridge this gap, we propose AutoFly, an end-to-end Vision-Language-Action (VLA) model for autonomous UAV navigation.AutoFly incorporates a pseudo-depth encoder that derives depth-aware features from RGB inputs to enhance spatial reasoning, coupled with a progressive two-stage training strategy that effectively aligns visual, depth, and linguistic representations with action policies.Moreover, existing VLN datasets have fundamental limitations for real-world autonomous navigation, stemming from their heavy reliance on explicit instruction-following over autonomous decision-making and insufficient real-world data.To address these issues, we construct a novel autonomous navigation dataset that shifts the paradigm from instruction-following to autonomous behavior modeling through: (1) trajectory collection emphasizing continuous obstacle avoidance, autonomous planning, and recognition workflows; (2) comprehensive real-world data integration.Experimental results demonstrate that AutoFly achieves a 3.9% higher success rate compared to state-of-the-art VLA baselines, with consistent performance across simulated and real environments.

</details>

---

## 111. TeleGate: Whole-Body Humanoid Teleoperation via Gated Expert Selection with Motion Prior

**Chinese Title**: TeleGate: Whole-Body Humanoid Teleoperation via Gated Expert Selection with Motion Prior

**Authors**: Jie Li, Bing Tang, Feng Wu, Rongyun Cao

**Date**: 2026-02-10 | **arXiv**: [2602.09628v1](http://arxiv.org/abs/2602.09628v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09628v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Real-time whole-body teleoperation is a critical method for humanoid robots to perform complex tasks in unstructured environments. However, developing a unified controller that robustly supports diverse human motions remains a significant challenge. Existing methods typically distill multiple expert policies into a single general policy, which often inevitably leads to performance degradation, particularly on highly dynamic motions. This paper presents TeleGate, a unified whole-body teleoperation framework for humanoid robots that achieves high-precision tracking across various motions while avoiding the performance loss inherent in knowledge distillation. Our key idea is to preserve the full capability of domain-specific expert policies by training a lightweight gating network, which dynamically activates experts in real-time based on proprioceptive states and reference trajectories. Furthermore, to compensate for the absence of future reference trajectories in real-time teleoperation, we introduce a VAE-based motion prior module that extracts implicit future motion intent from historical observations, enabling anticipatory control for motions requiring prediction such as jumping and standing up. We conducted empirical evaluations in simulation and also deployed our technique on the Unitree G1 humanoid robot. Using only 2.5 hours of motion capture data for training, our TeleGate achieves high-precision real-time teleoperation across diverse dynamic motions (e.g., running, fall recovery, and jumping), significantly outperforming the baseline methods in both tracking accuracy and success rate.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Real-time whole-body teleoperation is a critical method for humanoid robots to perform complex tasks in unstructured environments. However, developing a unified controller that robustly supports diverse human motions remains a significant challenge. Existing methods typically distill multiple expert policies into a single general policy, which often inevitably leads to performance degradation, particularly on highly dynamic motions.This paper presents TeleGate, a unified whole-body teleoperation framework for humanoid robots that achieves high-precision tracking across various motions while avoiding the performance loss inherent in knowledge distillation.Our key idea is to preserve the full capability of domain-specific expert policies by training a lightweight gating network, which dynamically activates experts in real-time based on proprioceptive states and reference trajectories.Furthermore, to compensate for the absence of future reference trajectories in real-time teleoperation, we introduce a VAE-based motion prior module that extracts implicit future motion intent from historical observations, enabling anticipatory control for motions requiring prediction such as jumping and standing up. We conducted empirical evaluations in simulation and also deployed our technique on the Unitree G1 humanoid robot.Using only 2.5 hours of motion capture data for training, our TeleGate achieves high-precision real-time teleoperation across diverse dynamic motions (e.g., running, fall recovery, and jumping), significantly outperforming the baseline methods in both tracking accuracy and success rate.

</details>

---

## 112. Preference Aligned Visuomotor Diffusion Policies for Deformable Object Manipulation

**Chinese Title**: Preference Aligned Visuomotor Diffusion Policies for Deformable Object Manipulation

**Authors**: Marco Moletta, Michael C. Welle, Danica Kragic

**Date**: 2026-02-10 | **arXiv**: [2602.09583v1](http://arxiv.org/abs/2602.09583v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09583v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Humans naturally develop preferences for how manipulation tasks should be performed, which are often subtle, personal, and difficult to articulate. Although it is important for robots to account for these preferences to increase personalization and user satisfaction, they remain largely underexplored in robotic manipulation, particularly in the context of deformable objects like garments and fabrics. In this work, we study how to adapt pretrained visuomotor diffusion policies to reflect preferred behaviors using limited demonstrations. We introduce RKO, a novel preference-alignment method that combines the benefits of two recent frameworks: RPO and KTO. We evaluate RKO against common preference learning frameworks, including these two, as well as a baseline vanilla diffusion policy, on real-world cloth-folding tasks spanning multiple garments and preference settings. We show that preference-aligned policies (particularly RKO) achieve superior performance and sample efficiency compared to standard diffusion policy fine-tuning. These results highlight the importance and feasibility of structured preference learning for scaling personalized robot behavior in complex deformable object manipulation tasks.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Humans naturally develop preferences for how manipulation tasks should be performed, which are often subtle, personal, and difficult to articulate. Although it is important for robots to account for these preferences to increase personalization and user satisfaction, they remain largely underexplored in robotic manipulation, particularly in the context of deformable objects like garments and fabrics.In this work, we study how to adapt pretrained visuomotor diffusion policies to reflect preferred behaviors using limited demonstrations. We introduce RKO, a novel preference-alignment method that combines the benefits of two recent frameworks: RPO and KTO.We evaluate RKO against common preference learning frameworks, including these two, as well as a baseline vanilla diffusion policy, on real-world cloth-folding tasks spanning multiple garments and preference settings. We show that preference-aligned policies (particularly RKO) achieve superior performance and sample efficiency compared to standard diffusion policy fine-tuning.These results highlight the importance and feasibility of structured preference learning for scaling personalized robot behavior in complex deformable object manipulation tasks.

</details>

---

## 113. Optimal Control of Microswimmers for Trajectory Tracking Using Bayesian Optimization

**Chinese Title**: Optimal Control of Microswimmers for Trajectory Tracking Using Bayesian Optimization

**Authors**: Lucas Palazzolo, MickaÃ«l Binois, LaÃ«titia Giraldi

**Date**: 2026-02-10 | **arXiv**: [2602.09563v1](http://arxiv.org/abs/2602.09563v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09563v1)

**Categories**: cs.RO, math.OC

<details><summary><b>Abstract</b></summary>

Trajectory tracking for microswimmers remains a key challenge in microrobotics, where low-Reynolds-number dynamics make control design particularly complex. In this work, we formulate the trajectory tracking problem as an optimal control problem and solve it using a combination of B-spline parametrization with Bayesian optimization, allowing the treatment of high computational costs without requiring complex gradient computations. Applied to a flagellated magnetic swimmer, the proposed method reproduces a variety of target trajectories, including biologically inspired paths observed in experimental studies. We further evaluate the approach on a three-sphere swimmer model, demonstrating that it can adapt to and partially compensate for wall-induced hydrodynamic effects. The proposed optimization strategy can be applied consistently across models of different fidelity, from low-dimensional ODE-based models to high-fidelity PDE-based simulations, showing its robustness and generality. These results highlight the potential of Bayesian optimization as a versatile tool for optimal control strategies in microscale locomotion under complex fluid-structure interactions.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Trajectory tracking for microswimmers remains a key challenge in microrobotics, where low-Reynolds-number dynamics make control design particularly complex. In this work, we formulate the trajectory tracking problem as an optimal control problem and solve it using a combination of B-spline parametrization with Bayesian optimization, allowing the treatment of high computational costs without requiring complex gradient computations.Applied to a flagellated magnetic swimmer, the proposed method reproduces a variety of target trajectories, including biologically inspired paths observed in experimental studies. We further evaluate the approach on a three-sphere swimmer model, demonstrating that it can adapt to and partially compensate for wall-induced hydrodynamic effects.The proposed optimization strategy can be applied consistently across models of different fidelity, from low-dimensional ODE-based models to high-fidelity PDE-based simulations, showing its robustness and generality. These results highlight the potential of Bayesian optimization as a versatile tool for optimal control strategies in microscale locomotion under complex fluid-structure interactions.

</details>

---

## 114. Finite-time Stable Pose Estimation on TSE(3) using Point Cloud and Velocity Sensors

**Chinese Title**: Finite-time Stable Pose Estimation on TSE(3) using Point Cloud and Velocity Sensors

**Authors**: Nazanin S. Hashkavaei, Abhijit Dongare, Neon Srinivasu, Amit K. Sanyal

**Date**: 2026-02-10 | **arXiv**: [2602.09414v1](http://arxiv.org/abs/2602.09414v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09414v1)

**Categories**: eess.SY, cs.RO

<details><summary><b>Abstract</b></summary>

This work presents a finite-time stable pose estimator (FTS-PE) for rigid bodies undergoing rotational and translational motion in three dimensions, using measurements from onboard sensors that provide position vectors to inertially-fixed points and body velocities. The FTS-PE is a full-state observer for the pose (position and orientation) and velocities and is obtained through a Lyapunov analysis that shows its stability in finite time and its robustness to bounded measurement noise. Further, this observer is designed directly on the state space, the tangent bundle of the Lie group of rigid body motions, SE(3), without using local coordinates or (dual) quaternion representations. Therefore, it can estimate arbitrary rigid body motions without encountering singularities or the unwinding phenomenon and be readily applied to autonomous vehicles. A version of this observer that does not need translational velocity measurements and uses only point clouds and angular velocity measurements from rate gyros, is also obtained. It is discretized using the framework of geometric mechanics for numerical and experimental implementations. The numerical simulations compare the FTS-PE with a dual-quaternion extended Kalman filter and our previously developed variational pose estimator (VPE). The experimental results are obtained using point cloud images and rate gyro measurements obtained from a Zed 2i stereo depth camera sensor. These results validate the stability and robustness of the FTS-PE.

</details>

<details><summary><b>Chinese Abstract</b></summary>

This work presents a finite-time stable pose estimator (FTS-PE) for rigid bodies undergoing rotational and translational motion in three dimensions, using measurements from onboard sensors that provide position vectors to inertially-fixed points and body velocities.The FTS-PE is a full-state observer for the pose (position and orientation) and velocities and is obtained through a Lyapunov analysis that shows its stability in finite time and its robustness to bounded measurement noise. Further, this observer is designed directly on the state space, the tangent bundle of the Lie group of rigid body motions, SE(3), without using local coordinates or (dual) quaternion representations.Therefore, it can estimate arbitrary rigid body motions without encountering singularities or the unwinding phenomenon and be readily applied to autonomous vehicles. A version of this observer that does not need translational velocity measurements and uses only point clouds and angular velocity measurements from rate gyros, is also obtained.It is discretized using the framework of geometric mechanics for numerical and experimental implementations. The numerical simulations compare the FTS-PE with a dual-quaternion extended Kalman filter and our previously developed variational pose estimator (VPE). The experimental results are obtained using point cloud images and rate gyro measurements obtained from a Zed 2i stereo depth camera sensor.These results validate the stability and robustness of the FTS-PE.

</details>

---

## 115. Phase-Aware Policy Learning for Skateboard Riding of Quadruped Robots via Feature-wise Linear Modulation

**Chinese Title**: Phase-Aware Policy Learning for Skateboard Riding of Quadruped Robots via Feature-wise Linear Modulation

**Authors**: Minsung Yoon, Jeil Jeong, Sung-Eui Yoon

**Date**: 2026-02-10 | **arXiv**: [2602.09370v1](http://arxiv.org/abs/2602.09370v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09370v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Skateboards offer a compact and efficient means of transportation as a type of personal mobility device. However, controlling them with legged robots poses several challenges for policy learning due to perception-driven interactions and multi-modal control objectives across distinct skateboarding phases. To address these challenges, we introduce Phase-Aware Policy Learning (PAPL), a reinforcement-learning framework tailored for skateboarding with quadruped robots. PAPL leverages the cyclic nature of skateboarding by integrating phase-conditioned Feature-wise Linear Modulation layers into actor and critic networks, enabling a unified policy that captures phase-dependent behaviors while sharing robot-specific knowledge across phases. Our evaluations in simulation validate command-tracking accuracy and conduct ablation studies quantifying each component's contribution. We also compare locomotion efficiency against leg and wheel-leg baselines and show real-world transferability.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Skateboards offer a compact and efficient means of transportation as a type of personal mobility device. However, controlling them with legged robots poses several challenges for policy learning due to perception-driven interactions and multi-modal control objectives across distinct skateboarding phases.To address these challenges, we introduce Phase-Aware Policy Learning (PAPL), a reinforcement-learning framework tailored for skateboarding with quadruped robots. PAPL leverages the cyclic nature of skateboarding by integrating phase-conditioned Feature-wise Linear Modulation layers into actor and critic networks, enabling a unified policy that captures phase-dependent behaviors while sharing robot-specific knowledge across phases.Our evaluations in simulation validate command-tracking accuracy and conduct ablation studies quantifying each component's contribution. We also compare locomotion efficiency against leg and wheel-leg baselines and show real-world transferability.

</details>

---

## 116. CAPER: Constrained and Procedural Reasoning for Robotic Scientific Experiments

**Chinese Title**: CAPER: Constrained and Procedural Reasoning for Robotic Scientific Experiments

**Authors**: Jinghan Yang, Jingyi Hou, Xinbo Yu, Wei He, Yifan Wu

**Date**: 2026-02-10 | **arXiv**: [2602.09367v1](http://arxiv.org/abs/2602.09367v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09367v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Robotic assistance in scientific laboratories requires procedurally correct long-horizon manipulation, reliable execution under limited supervision, and robustness in low-demonstration regimes. Such conditions greatly challenge end-to-end vision-language-action (VLA) models, whose assumptions of recoverable errors and data-driven policy learning often break down in protocol-sensitive experiments. We propose CAPER, a framework for Constrained And ProcEdural Reasoning for robotic scientific experiments, which explicitly restricts where learning and reasoning occur in the planning and control pipeline. Rather than strengthening end-to-end policies, CAPER enforces a responsibility-separated structure: task-level reasoning generates procedurally valid action sequences under explicit constraints, mid-level multimodal grounding realizes subtasks without delegating spatial decision-making to large language models, and low-level control adapts to physical uncertainty via reinforcement learning with minimal demonstrations. By encoding procedural commitments through interpretable intermediate representations, CAPER prevents execution-time violations of experimental logic, improving controllability, robustness, and data efficiency. Experiments on a scientific workflow benchmark and a public long-horizon manipulation dataset demonstrate consistent improvements in success rate and procedural correctness, particularly in low-data and long-horizon settings.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Robotic assistance in scientific laboratories requires procedurally correct long-horizon manipulation, reliable execution under limited supervision, and robustness in low-demonstration regimes. Such conditions greatly challenge end-to-end vision-language-action (VLA) models, whose assumptions of recoverable errors and data-driven policy learning often break down in protocol-sensitive experiments.We propose CAPER, a framework for Constrained And ProcEdural Reasoning for robotic scientific experiments, which explicitly restricts where learning and reasoning occur in the planning and control pipeline.Rather than strengthening end-to-end policies, CAPER enforces a responsibility-separated structure: task-level reasoning generates procedurally valid action sequences under explicit constraints, mid-level multimodal grounding realizes subtasks without delegating spatial decision-making to large language models, and low-level control adapts to physical uncertainty via reinforcement learning with minimal demonstrations.By encoding procedural commitments through interpretable intermediate representations, CAPER prevents execution-time violations of experimental logic, improving controllability, robustness, and data efficiency. Experiments on a scientific workflow benchmark and a public long-horizon manipulation dataset demonstrate consistent improvements in success rate and procedural correctness, particularly in low-data and long-horizon settings.

</details>

---

## 117. Features as Rewards: Scalable Supervision for Open-Ended Tasks via Interpretability

**Chinese Title**: Features as Rewards: Scalable Supervision for Open-Ended Tasks via Interpretability

**Authors**: Aaditya Vikram Prasad, Connor Watts, Jack Merullo, Dhruvil Gala, Owen Lewis et al.

**Date**: 2026-02-10 | **arXiv**: [2602.10067v1](http://arxiv.org/abs/2602.10067v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10067v1)

**Categories**: cs.LG

<details><summary><b>Abstract</b></summary>

Language models trained on large-scale datasets have been shown to learn features that encode abstract concepts such as factuality or intent. Such features are traditionally used for test-time monitoring or steering. We present an alternative affordance: features as scalable supervision for open-ended tasks. We consider the case of hallucination-reduction as a desirable, yet open-ended behavior and design a reinforcement learning (RL) pipeline, titled RLFR (Reinforcement Learning from Feature Rewards), that uses features as reward functions. Grounded in a novel probing framework that identifies candidate hallucinated claims, our pipeline teaches a model to intervene and correct its completions when it is uncertain of their factuality. Furthermore, the pipeline enables scalable test-time compute, guided once more by our reward features. This end-to-end process operationalized on Gemma-3-12B-IT results in a policy that is 58% less likely to hallucinate compared to the original model, while preserving performance on standard benchmarks. Taken together, by grounding supervision in the language of features, this paper introduces a novel paradigm in the use of interpretability for learning open-ended tasks.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Language models trained on large-scale datasets have been shown to learn features that encode abstract concepts such as factuality or intent. Such features are traditionally used for test-time monitoring or steering. We present an alternative affordance: features as scalable supervision for open-ended tasks.We consider the case of hallucination-reduction as a desirable, yet open-ended behavior and design a reinforcement learning (RL) pipeline, titled RLFR (Reinforcement Learning from Feature Rewards), that uses features as reward functions. Grounded in a novel probing framework that identifies candidate hallucinated claims, our pipeline teaches a model to intervene and correct its completions when it is uncertain of their factuality.Furthermore, the pipeline enables scalable test-time compute, guided once more by our reward features. This end-to-end process operationalized on Gemma-3-12B-IT results in a policy that is 58% less likely to hallucinate compared to the original model, while preserving performance on standard benchmarks.Taken together, by grounding supervision in the language of features, this paper introduces a novel paradigm in the use of interpretability for learning open-ended tasks.

</details>

---

## 118. Evaluating Disentangled Representations for Controllable Music Generation

**Chinese Title**: Evaluating Disentangled Representations for Controllable Music Generation

**Authors**: Laura IbÃ¡Ã±ez-MartÃ­nez, Chukwuemeka Nkama, Andrea Poltronieri, Xavier Serra, MartÃ­n Rocamora

**Date**: 2026-02-10 | **arXiv**: [2602.10058v1](http://arxiv.org/abs/2602.10058v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10058v1)

**Categories**: cs.SD, cs.LG, eess.AS

<details><summary><b>Abstract</b></summary>

Recent approaches in music generation rely on disentangled representations, often labeled as structure and timbre or local and global, to enable controllable synthesis. Yet the underlying properties of these embeddings remain underexplored. In this work, we evaluate such disentangled representations in a set of music audio models for controllable generation using a probing-based framework that goes beyond standard downstream tasks. The selected models reflect diverse unsupervised disentanglement strategies, including inductive biases, data augmentations, adversarial objectives, and staged training procedures. We further isolate specific strategies to analyze their effect. Our analysis spans four key axes: informativeness, equivariance, invariance, and disentanglement, which are assessed across datasets, tasks, and controlled transformations. Our findings reveal inconsistencies between intended and actual semantics of the embeddings, suggesting that current strategies fall short of producing truly disentangled representations, and prompting a re-examination of how controllability is approached in music generation.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Recent approaches in music generation rely on disentangled representations, often labeled as structure and timbre or local and global, to enable controllable synthesis. Yet the underlying properties of these embeddings remain underexplored. In this work, we evaluate such disentangled representations in a set of music audio models for controllable generation using a probing-based framework that goes beyond standard downstream tasks.The selected models reflect diverse unsupervised disentanglement strategies, including inductive biases, data augmentations, adversarial objectives, and staged training procedures. We further isolate specific strategies to analyze their effect. Our analysis spans four key axes: informativeness, equivariance, invariance, and disentanglement, which are assessed across datasets, tasks, and controlled transformations.Our findings reveal inconsistencies between intended and actual semantics of the embeddings, suggesting that current strategies fall short of producing truly disentangled representations, and prompting a re-examination of how controllability is approached in music generation.

</details>

---

## 119. Answer First, Reason Later: Aligning Search Relevance via Mode-Balanced Reinforcement Learning

**Chinese Title**: Answer First, Reason Later: Aligning Search Relevance via Mode-Balanced Reinforcement Learning

**Authors**: Shijie Zhang, Xiang Guo, Rujun Guo, Shaoyu Liu, Xiaozhao Wang et al.

**Date**: 2026-02-10 | **arXiv**: [2602.10006v1](http://arxiv.org/abs/2602.10006v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10006v1)

**Categories**: cs.LG

<details><summary><b>Abstract</b></summary>

Building a search relevance model that achieves both low latency and high performance is a long-standing challenge in the search industry. To satisfy the millisecond-level response requirements of online systems while retaining the interpretable reasoning traces of Large Language Models (LLMs), we propose a novel \textbf{Answer-First, Reason Later (AFRL)} paradigm. This paradigm requires the model to output the definitive relevance score in the very first token, followed by a structured logical explanation. Inspired by the success of reasoning models, we adopt a "Supervised Fine-Tuning (SFT) + Reinforcement Learning (RL)" pipeline to achieve AFRL. However, directly applying existing RL training often leads to \textbf{mode collapse} in the search relevance task, where the model forgets complex long-tail rules in pursuit of high rewards. From an information theory perspective: RL inherently minimizes the \textbf{Reverse KL divergence}, which tends to seek probability peaks (mode-seeking) and is prone to "reward hacking." On the other hand, SFT minimizes the \textbf{Forward KL divergence}, forcing the model to cover the data distribution (mode-covering) and effectively anchoring expert rules. Based on this insight, we propose a \textbf{Mode-Balanced Optimization} strategy, incorporating an SFT auxiliary loss into Stepwise-GRPO training to balance these two properties. Furthermore, we construct an automated instruction evolution system and a multi-stage curriculum to ensure expert-level data quality. Extensive experiments demonstrate that our 32B teacher model achieves state-of-the-art performance. Moreover, the AFRL architecture enables efficient knowledge distillation, successfully transferring expert-level logic to a 0.6B model, thereby reconciling reasoning depth with deployment latency.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Building a search relevance model that achieves both low latency and high performance is a long-standing challenge in the search industry. To satisfy the millisecond-level response requirements of online systems while retaining the interpretable reasoning traces of Large Language Models (LLMs), we propose a novel \textbf{Answer-First, Reason Later (AFRL)} paradigm.This paradigm requires the model to output the definitive relevance score in the very first token, followed by a structured logical explanation. Inspired by the success of reasoning models, we adopt a "Supervised Fine-Tuning (SFT) + Reinforcement Learning (RL)" pipeline to achieve AFRL.However, directly applying existing RL training often leads to \textbf{mode collapse} in the search relevance task, where the model forgets complex long-tail rules in pursuit of high rewards.From an information theory perspective: RL inherently minimizes the \textbf{Reverse KL divergence}, which tends to seek probability peaks (mode-seeking) and is prone to "reward hacking." On the other hand, SFT minimizes the \textbf{Forward KL divergence}, forcing the model to cover the data distribution (mode-covering) and effectively anchoring expert rules.Based on this insight, we propose a \textbf{Mode-Balanced Optimization} strategy, incorporating an SFT auxiliary loss into Stepwise-GRPO training to balance these two properties. Furthermore, we construct an automated instruction evolution system and a multi-stage curriculum to ensure expert-level data quality. Extensive experiments demonstrate that our 32B teacher model achieves state-of-the-art performance.Moreover, the AFRL architecture enables efficient knowledge distillation, successfully transferring expert-level logic to a 0.6B model, thereby reconciling reasoning depth with deployment latency.

</details>

---

## 120. Causal Identification in Multi-Task Demand Learning with Confounding

**Chinese Title**: Causal Identification in Multi-Task Demand Learning with Confounding

**Authors**: Varun Gupta, Vijay Kamble

**Date**: 2026-02-10 | **arXiv**: [2602.09969v1](http://arxiv.org/abs/2602.09969v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09969v1)

**Categories**: cs.LG, econ.EM, stat.ML

<details><summary><b>Abstract</b></summary>

We study a canonical multi-task demand learning problem motivated by retail pricing, in which a firm seeks to estimate heterogeneous linear price-response functions across a large collection of decision contexts. Each context is characterized by rich observable covariates yet typically exhibits only limited historical price variation, motivating the use of multi-task learning to borrow strength across tasks. A central challenge in this setting is endogeneity: historical prices are chosen by managers or algorithms and may be arbitrarily correlated with unobserved, task-level demand determinants. Under such confounding by latent fundamentals, commonly used approaches, such as pooled regression and meta-learning, fail to identify causal price effects.   We propose a new estimation framework that achieves causal identification despite arbitrary dependence between prices and latent task structure. Our approach, Decision-Conditioned Masked-Outcome Meta-Learning (DCMOML), involves carefully designing the information set of a meta-learner to leverage cross-task heterogeneity while accounting for endogenous decision histories. Under a mild restriction on price adaptivity in each task, we establish that this method identifies the conditional mean of the task-specific causal parameters given the designed information set. Our results provide guarantees for large-scale demand estimation with endogenous prices and small per-task samples, offering a principled foundation for deploying causal, data-driven pricing models in operational environments.

</details>

<details><summary><b>Chinese Abstract</b></summary>

We study a canonical multi-task demand learning problem motivated by retail pricing, in which a firm seeks to estimate heterogeneous linear price-response functions across a large collection of decision contexts. Each context is characterized by rich observable covariates yet typically exhibits only limited historical price variation, motivating the use of multi-task learning to borrow strength across tasks.A central challenge in this setting is endogeneity: historical prices are chosen by managers or algorithms and may be arbitrarily correlated with unobserved, task-level demand determinants. Under such confounding by latent fundamentals, commonly used approaches, such as pooled regression and meta-learning, fail to identify causal price effects.We propose a new estimation framework that achieves causal identification despite arbitrary dependence between prices and latent task structure. Our approach, Decision-Conditioned Masked-Outcome Meta-Learning (DCMOML), involves carefully designing the information set of a meta-learner to leverage cross-task heterogeneity while accounting for endogenous decision histories.Under a mild restriction on price adaptivity in each task, we establish that this method identifies the conditional mean of the task-specific causal parameters given the designed information set. Our results provide guarantees for large-scale demand estimation with endogenous prices and small per-task samples, offering a principled foundation for deploying causal, data-driven pricing models in operational environments.

</details>

---

## 121. Safeguarding Privacy: Privacy-Preserving Detection of Mind Wandering and Disengagement Using Federated Learning in Online Education

**Chinese Title**: Safeguarding Privacy: Privacy-Preserving Detection of Mind Wandering and Disengagement Using Federated Learning in Online Education

**Authors**: Anna Bodonhelyi, Mengdi Wang, Efe Bozkir, Babette BÃ¼hler, Enkelejda Kasneci

**Date**: 2026-02-10 | **arXiv**: [2602.09904v1](http://arxiv.org/abs/2602.09904v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09904v1)

**Categories**: cs.LG, cs.HC

<details><summary><b>Abstract</b></summary>

Since the COVID-19 pandemic, online courses have expanded access to education, yet the absence of direct instructor support challenges learners' ability to self-regulate attention and engagement. Mind wandering and disengagement can be detrimental to learning outcomes, making their automated detection via video-based indicators a promising approach for real-time learner support. However, machine learning-based approaches often require sharing sensitive data, raising privacy concerns. Federated learning offers a privacy-preserving alternative by enabling decentralized model training while also distributing computational load. We propose a framework exploiting cross-device federated learning to address different manifestations of behavioral and cognitive disengagement during remote learning, specifically behavioral disengagement, mind wandering, and boredom. We fit video-based cognitive disengagement detection models using facial expressions and gaze features. By adopting federated learning, we safeguard users' data privacy through privacy-by-design and introduce a novel solution with the potential for real-time learner support. We further address challenges posed by eyeglasses by incorporating related features, enhancing overall model performance. To validate the performance of our approach, we conduct extensive experiments on five datasets and benchmark multiple federated learning algorithms. Our results show great promise for privacy-preserving educational technologies promoting learner engagement.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Since the COVID-19 pandemic, online courses have expanded access to education, yet the absence of direct instructor support challenges learners' ability to self-regulate attention and engagement. Mind wandering and disengagement can be detrimental to learning outcomes, making their automated detection via video-based indicators a promising approach for real-time learner support.However, machine learning-based approaches often require sharing sensitive data, raising privacy concerns. Federated learning offers a privacy-preserving alternative by enabling decentralized model training while also distributing computational load.We propose a framework exploiting cross-device federated learning to address different manifestations of behavioral and cognitive disengagement during remote learning, specifically behavioral disengagement, mind wandering, and boredom. We fit video-based cognitive disengagement detection models using facial expressions and gaze features.By adopting federated learning, we safeguard users' data privacy through privacy-by-design and introduce a novel solution with the potential for real-time learner support. We further address challenges posed by eyeglasses by incorporating related features, enhancing overall model performance. To validate the performance of our approach, we conduct extensive experiments on five datasets and benchmark multiple federated learning algorithms.Our results show great promise for privacy-preserving educational technologies promoting learner engagement.

</details>

---

## 122. Differentiable Tripartite Modularity for Clustering Heterogeneous Graphs

**Chinese Title**: Differentiable Tripartite Modularity for Clustering Heterogeneous Graphs

**Authors**: BenoÃ®t Hurpeau

**Date**: 2026-02-10 | **arXiv**: [2602.09864v1](http://arxiv.org/abs/2602.09864v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09864v1)

**Categories**: cs.LG, cs.SI

<details><summary><b>Abstract</b></summary>

Clustering heterogeneous relational data remains a central challenge in graph learning, particularly when interactions involve more than two types of entities. While differentiable modularity objectives such as DMoN have enabled end-to-end community detection on homogeneous and bipartite graphs, extending these approaches to higher-order relational structures remains non-trivial.   In this work, we introduce a differentiable formulation of tripartite modularity for graphs composed of three node types connected through mediated interactions. Community structure is defined in terms of weighted co-paths across the tripartite graph, together with an exact factorized computation that avoids the explicit construction of dense third-order tensors. A structural normalization at pivot nodes is introduced to control extreme degree heterogeneity and ensure stable optimization.   The resulting objective can be optimized jointly with a graph neural network in an end-to-end manner, while retaining linear complexity in the number of edges. We validate the proposed framework on large-scale urban cadastral data, where it exhibits robust convergence behavior and produces spatially coherent partitions. These results highlight differentiable tripartite modularity as a generic methodological building block for unsupervised clustering of heterogeneous graphs.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Clustering heterogeneous relational data remains a central challenge in graph learning, particularly when interactions involve more than two types of entities. While differentiable modularity objectives such as DMoN have enabled end-to-end community detection on homogeneous and bipartite graphs, extending these approaches to higher-order relational structures remains non-trivial.In this work, we introduce a differentiable formulation of tripartite modularity for graphs composed of three node types connected through mediated interactions. Community structure is defined in terms of weighted co-paths across the tripartite graph, together with an exact factorized computation that avoids the explicit construction of dense third-order tensors.A structural normalization at pivot nodes is introduced to control extreme degree heterogeneity and ensure stable optimization. The resulting objective can be optimized jointly with a graph neural network in an end-to-end manner, while retaining linear complexity in the number of edges. We validate the proposed framework on large-scale urban cadastral data, where it exhibits robust convergence behavior and produces spatially coherent partitions.These results highlight differentiable tripartite modularity as a generic methodological building block for unsupervised clustering of heterogeneous graphs.

</details>

---

## 123. Toeplitz Based Spectral Methods for Data-driven Dynamical Systems

**Chinese Title**: Toeplitz Based Spectral Methods for Data-driven Dynamical Systems

**Authors**: Vladimir R. Kostic, Karim Lounici, Massimiliano Pontil

**Date**: 2026-02-10 | **arXiv**: [2602.09791v1](http://arxiv.org/abs/2602.09791v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09791v1)

**Categories**: math.DS, cs.LG, math.NA

<details><summary><b>Abstract</b></summary>

We introduce a Toeplitz-based framework for data-driven spectral estimation of linear evolution operators in dynamical systems. Focusing on transfer and Koopman operators from equilibrium trajectories without access to the underlying equations of motion, our method applies Toeplitz filters to the infinitesimal generator to extract eigenvalues, eigenfunctions, and spectral measures. Structural prior knowledge, such as self-adjointness or skew-symmetry, can be incorporated by design. The approach is statistically consistent and computationally efficient, leveraging both primal and dual algorithms commonly used in statistical learning. Numerical experiments on deterministic and chaotic systems demonstrate that the framework can recover spectral properties beyond the reach of standard data-driven methods.

</details>

<details><summary><b>Chinese Abstract</b></summary>

We introduce a Toeplitz-based framework for data-driven spectral estimation of linear evolution operators in dynamical systems. Focusing on transfer and Koopman operators from equilibrium trajectories without access to the underlying equations of motion, our method applies Toeplitz filters to the infinitesimal generator to extract eigenvalues, eigenfunctions, and spectral measures.Structural prior knowledge, such as self-adjointness or skew-symmetry, can be incorporated by design. The approach is statistically consistent and computationally efficient, leveraging both primal and dual algorithms commonly used in statistical learning. Numerical experiments on deterministic and chaotic systems demonstrate that the framework can recover spectral properties beyond the reach of standard data-driven methods.

</details>

---

## 124. When Less is More: The LLM Scaling Paradox in Context Compression

**Chinese Title**: When Less is More: The LLM Scaling Paradox in Context Compression

**Authors**: Ruishan Guo, Yibing Liu, Guoxin Ma, Yan Wang, Yueyang Zhang et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09789v1](http://arxiv.org/abs/2602.09789v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09789v1)

**Categories**: cs.LG

<details><summary><b>Abstract</b></summary>

Scaling up model parameters has long been a prevalent training paradigm driven by the assumption that larger models yield superior generation capabilities. However, under lossy context compression in a compressor-decoder setup, we observe a Size-Fidelity Paradox: increasing the compressor size can lessen the faithfulness of reconstructed contexts though training loss decreases. Through extensive experiments across models from 0.6B to 90B, we coin this paradox arising from two dominant factors: 1) knowledge overwriting: larger models increasingly replace source facts with their own prior beliefs, e.g., ``the white strawberry'' $\to$ ``the red strawberry''; and 2) semantic drift: larger models tend to paraphrase or restructure content instead of reproducing it verbatim, e.g., ``Alice hit Bob'' $\to$ ``Bob hit Alice''. By holding model size fixed, we reflect on the emergent properties of compressed context representations. We show that the culprit is not parameter count itself, but the excessive semantic capacity and amplified generative uncertainty that accompany scaling. Specifically, the increased rank of context embeddings facilitates prior knowledge intrusion, whereas higher entropy over token prediction distributions promotes rewriting. Our results complement existing evaluations over context compression paradigm, underpinning a breakdown in scaling laws for faithful preservation in open-ended generation.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Scaling up model parameters has long been a prevalent training paradigm driven by the assumption that larger models yield superior generation capabilities. However, under lossy context compression in a compressor-decoder setup, we observe a Size-Fidelity Paradox: increasing the compressor size can lessen the faithfulness of reconstructed contexts though training loss decreases.Through extensive experiments across models from 0.6B to 90B, we coin this paradox arising from two dominant factors: 1) knowledge overwriting: larger models increasingly replace source facts with their own prior beliefs, e.g., ``the white strawberry'' $\to$ ``the red strawberry''; and 2) semantic drift: larger models tend to paraphrase or restructure content instead of reproducing it verbatim, e.g., ``Alice hit Bob'' $\to$ ``Bob hit Alice''.By holding model size fixed, we reflect on the emergent properties of compressed context representations. We show that the culprit is not parameter count itself, but the excessive semantic capacity and amplified generative uncertainty that accompany scaling. Specifically, the increased rank of context embeddings facilitates prior knowledge intrusion, whereas higher entropy over token prediction distributions promotes rewriting.Our results complement existing evaluations over context compression paradigm, underpinning a breakdown in scaling laws for faithful preservation in open-ended generation.

</details>

---

## 125. Circuit Fingerprints: How Answer Tokens Encode Their Geometrical Path

**Chinese Title**: Circuit Fingerprints: How Answer Tokens Encode Their Geometrical Path

**Authors**: Andres Saurez, Neha Sengar, Dongsoo Har

**Date**: 2026-02-10 | **arXiv**: [2602.09784v1](http://arxiv.org/abs/2602.09784v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09784v1)

**Categories**: cs.LG, cs.CL

<details><summary><b>Abstract</b></summary>

Circuit discovery and activation steering in transformers have developed as separate research threads, yet both operate on the same representational space. Are they two views of the same underlying structure? We show they follow a single geometric principle: answer tokens, processed in isolation, encode the directions that would produce them. This Circuit Fingerprint hypothesis enables circuit discovery without gradients or causal intervention -- recovering comparable structure to gradient-based methods through geometric alignment alone. We validate this on standard benchmarks (IOI, SVA, MCQA) across four model families, achieving circuit discovery performance comparable to gradient-based methods. The same directions that identify circuit components also enable controlled steering -- achieving 69.8\% emotion classification accuracy versus 53.1\% for instruction prompting while preserving factual accuracy. Beyond method development, this read-write duality reveals that transformer circuits are fundamentally geometric structures: interpretability and controllability are two facets of the same object.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Circuit discovery and activation steering in transformers have developed as separate research threads, yet both operate on the same representational space. Are they two views of the same underlying structure? We show they follow a single geometric principle: answer tokens, processed in isolation, encode the directions that would produce them.This Circuit Fingerprint hypothesis enables circuit discovery without gradients or causal intervention -- recovering comparable structure to gradient-based methods through geometric alignment alone. We validate this on standard benchmarks (IOI, SVA, MCQA) across four model families, achieving circuit discovery performance comparable to gradient-based methods.The same directions that identify circuit components also enable controlled steering -- achieving 69.8\% emotion classification accuracy versus 53.1\% for instruction prompting while preserving factual accuracy. Beyond method development, this read-write duality reveals that transformer circuits are fundamentally geometric structures: interpretability and controllability are two facets of the same object.

</details>

---

## 126. BRAVA-GNN: Betweenness Ranking Approximation Via Degree MAss Inspired Graph Neural Network

**Chinese Title**: BRAVA-GNN: Betweenness Ranking Approximation Via Degree MAss Inspired Graph Neural Network

**Authors**: Justin Dachille, Aurora Rossi, Sunil Kumar Maurya, Frederik Mallmann-Trenn, Xin Liu et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09716v1](http://arxiv.org/abs/2602.09716v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09716v1)

**Categories**: cs.LG

<details><summary><b>Abstract</b></summary>

Computing node importance in networks is a long-standing fundamental problem that has driven extensive study of various centrality measures. A particularly well-known centrality measure is betweenness centrality, which becomes computationally prohibitive on large-scale networks. Graph Neural Network (GNN) models have thus been proposed to predict node rankings according to their relative betweenness centrality. However, state-of-the-art methods fail to generalize to high-diameter graphs such as road networks. We propose BRAVA-GNN, a lightweight GNN architecture that leverages the empirically observed correlation linking betweenness centrality to degree-based quantities, in particular multi-hop degree mass. This correlation motivates the use of degree masses as size-invariant node features and synthetic training graphs that closely match the degree distributions of real networks. Furthermore, while previous work relies on scale-free synthetic graphs, we leverage the hyperbolic random graph model, which reproduces power-law exponents outside the scale-free regime, better capturing the structure of real-world graphs like road networks. This design enables BRAVA-GNN to generalize across diverse graph families while using 54x fewer parameters than the most lightweight existing GNN baseline. Extensive experiments on 19 real-world networks, spanning social, web, email, and road graphs, show that BRAVA-GNN achieves up to 214% improvement in Kendall-Tau correlation and up to 70x speedup in inference time over state-of-the-art GNN-based approaches, particularly on challenging road networks.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Computing node importance in networks is a long-standing fundamental problem that has driven extensive study of various centrality measures. A particularly well-known centrality measure is betweenness centrality, which becomes computationally prohibitive on large-scale networks. Graph Neural Network (GNN) models have thus been proposed to predict node rankings according to their relative betweenness centrality.However, state-of-the-art methods fail to generalize to high-diameter graphs such as road networks. We propose BRAVA-GNN, a lightweight GNN architecture that leverages the empirically observed correlation linking betweenness centrality to degree-based quantities, in particular multi-hop degree mass.This correlation motivates the use of degree masses as size-invariant node features and synthetic training graphs that closely match the degree distributions of real networks. Furthermore, while previous work relies on scale-free synthetic graphs, we leverage the hyperbolic random graph model, which reproduces power-law exponents outside the scale-free regime, better capturing the structure of real-world graphs like road networks.This design enables BRAVA-GNN to generalize across diverse graph families while using 54x fewer parameters than the most lightweight existing GNN baseline.Extensive experiments on 19 real-world networks, spanning social, web, email, and road graphs, show that BRAVA-GNN achieves up to 214% improvement in Kendall-Tau correlation and up to 70x speedup in inference time over state-of-the-art GNN-based approaches, particularly on challenging road networks.

</details>

---

## 127. LLM-FS: Zero-Shot Feature Selection for Effective and Interpretable Malware Detection

**Chinese Title**: LLM-FS: Zero-Shot Feature Selection for Effective and Interpretable Malware Detection

**Authors**: Naveen Gill, Ajvad Haneef K, Madhu Kumar S D

**Date**: 2026-02-10 | **arXiv**: [2602.09634v1](http://arxiv.org/abs/2602.09634v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09634v1)

**Categories**: cs.LG, cs.CR

<details><summary><b>Abstract</b></summary>

Feature selection (FS) remains essential for building accurate and interpretable detection models, particularly in high-dimensional malware datasets. Conventional FS methods such as Extra Trees, Variance Threshold, Tree-based models, Chi-Squared tests, ANOVA, Random Selection, and Sequential Attention rely primarily on statistical heuristics or model-driven importance scores, often overlooking the semantic context of features. Motivated by recent progress in LLM-driven FS, we investigate whether large language models (LLMs) can guide feature selection in a zero-shot setting, using only feature names and task descriptions, as a viable alternative to traditional approaches. We evaluate multiple LLMs (GPT-5.0, GPT-4.0, Gemini-2.5 etc.) on the EMBOD dataset (a fusion of EMBER and BODMAS benchmark datasets), comparing them against established FS methods across several classifiers, including Random Forest, Extra Trees, MLP, and KNN. Performance is assessed using accuracy, precision, recall, F1, AUC, MCC, and runtime. Our results demonstrate that LLM-guided zero-shot feature selection achieves competitive performance with traditional FS methods while offering additional advantages in interpretability, stability, and reduced dependence on labeled data. These findings position zero-shot LLM-based FS as a promising alternative strategy for effective and interpretable malware detection, paving the way for knowledge-guided feature selection in security-critical applications

</details>

<details><summary><b>Chinese Abstract</b></summary>

Feature selection (FS) remains essential for building accurate and interpretable detection models, particularly in high-dimensional malware datasets. Conventional FS methods such as Extra Trees, Variance Threshold, Tree-based models, Chi-Squared tests, ANOVA, Random Selection, and Sequential Attention rely primarily on statistical heuristics or model-driven importance scores, often overlooking the semantic context of features.Motivated by recent progress in LLM-driven FS, we investigate whether large language models (LLMs) can guide feature selection in a zero-shot setting, using only feature names and task descriptions, as a viable alternative to traditional approaches.We evaluate multiple LLMs (GPT-5.0, GPT-4.0, Gemini-2.5 etc.) on the EMBOD dataset (a fusion of EMBER and BODMAS benchmark datasets), comparing them against established FS methods across several classifiers, including Random Forest, Extra Trees, MLP, and KNN. Performance is assessed using accuracy, precision, recall, F1, AUC, MCC, and runtime.Our results demonstrate that LLM-guided zero-shot feature selection achieves competitive performance with traditional FS methods while offering additional advantages in interpretability, stability, and reduced dependence on labeled data.These findings position zero-shot LLM-based FS as a promising alternative strategy for effective and interpretable malware detection, paving the way for knowledge-guided feature selection in security-critical applications

</details>

---

## 128. AlignTune: Modular Toolkit for Post-Training Alignment of Large Language Models

**Chinese Title**: AlignTune: Modular Toolkit for Post-Training Alignment of Large Language Models

**Authors**: R E Zera Marveen Lyngkhoi, Chirag Chawla, Pratinav Seth, Utsav Avaiya, Soham Bhattacharjee et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09621v1](http://arxiv.org/abs/2602.09621v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09621v1)

**Categories**: cs.CL, cs.LG

<details><summary><b>Abstract</b></summary>

Post-training alignment is central to deploying large language models (LLMs), yet practical workflows remain split across backend-specific tools and ad-hoc glue code, making experiments hard to reproduce. We identify backend interference, reward fragmentation, and irreproducible pipelines as key obstacles in alignment research. We introduce AlignTune, a modular toolkit exposing a unified interface for supervised fine-tuning (SFT) and RLHF-style optimization with interchangeable TRL and Unsloth backends. AlignTune standardizes configuration, provides an extensible reward layer (rule-based and learned), and integrates evaluation over standard benchmarks and custom tasks. By isolating backend-specific logic behind a single factory boundary, AlignTune enables controlled comparisons and reproducible alignment experiments.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Post-training alignment is central to deploying large language models (LLMs), yet practical workflows remain split across backend-specific tools and ad-hoc glue code, making experiments hard to reproduce. We identify backend interference, reward fragmentation, and irreproducible pipelines as key obstacles in alignment research.We introduce AlignTune, a modular toolkit exposing a unified interface for supervised fine-tuning (SFT) and RLHF-style optimization with interchangeable TRL and Unsloth backends. AlignTune standardizes configuration, provides an extensible reward layer (rule-based and learned), and integrates evaluation over standard benchmarks and custom tasks.By isolating backend-specific logic behind a single factory boundary, AlignTune enables controlled comparisons and reproducible alignment experiments.

</details>

---

## 129. Training deep physical neural networks with local physical information bottleneck

**Chinese Title**: Training deep physical neural networks with local physical information bottleneck

**Authors**: Hao Wang, Ziao Wang, Xiangpeng Liang, Han Zhao, Jianqi Hu et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09569v1](http://arxiv.org/abs/2602.09569v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09569v1)

**Categories**: cs.LG, physics.app-ph

<details><summary><b>Abstract</b></summary>

Deep learning has revolutionized modern society but faces growing energy and latency constraints. Deep physical neural networks (PNNs) are interconnected computing systems that directly exploit analog dynamics for energy-efficient, ultrafast AI execution. Realizing this potential, however, requires universal training methods tailored to physical intricacies. Here, we present the Physical Information Bottleneck (PIB), a general and efficient framework that integrates information theory and local learning, enabling deep PNNs to learn under arbitrary physical dynamics. By allocating matrix-based information bottlenecks to each unit, we demonstrate supervised, unsupervised, and reinforcement learning across electronic memristive chips and optical computing platforms. PIB also adapts to severe hardware faults and allows for parallel training via geographically distributed resources. Bypassing auxiliary digital models and contrastive measurements, PIB recasts PNN training as an intrinsic, scalable information-theoretic process compatible with diverse physical substrates.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Deep learning has revolutionized modern society but faces growing energy and latency constraints. Deep physical neural networks (PNNs) are interconnected computing systems that directly exploit analog dynamics for energy-efficient, ultrafast AI execution. Realizing this potential, however, requires universal training methods tailored to physical intricacies.Here, we present the Physical Information Bottleneck (PIB), a general and efficient framework that integrates information theory and local learning, enabling deep PNNs to learn under arbitrary physical dynamics. By allocating matrix-based information bottlenecks to each unit, we demonstrate supervised, unsupervised, and reinforcement learning across electronic memristive chips and optical computing platforms.PIB also adapts to severe hardware faults and allows for parallel training via geographically distributed resources. Bypassing auxiliary digital models and contrastive measurements, PIB recasts PNN training as an intrinsic, scalable information-theoretic process compatible with diverse physical substrates.

</details>

---

## 130. Learning with Multiple Correct Answers -- A Trichotomy of Regret Bounds under Different Feedback Models

**Chinese Title**: Learning with Multiple Correct Answers -- A Trichotomy of Regret Bounds under Different Feedback Models

**Authors**: Alireza F. Pour, Farnam Mansouri, Shai Ben-David

**Date**: 2026-02-10 | **arXiv**: [2602.09402v1](http://arxiv.org/abs/2602.09402v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09402v1)

**Categories**: cs.LG

<details><summary><b>Abstract</b></summary>

We study an online learning problem with multiple correct answers, where each instance admits a set of valid labels, and in each round the learner must output a valid label for the queried example. This setting is motivated by language generation tasks, in which a prompt may admit many acceptable completions, but not every completion is acceptable. We study this problem under three feedback models. For each model, we characterize the optimal mistake bound in the realizable setting using an appropriate combinatorial dimension. We then establish a trichotomy of regret bounds across the three models in the agnostic setting. Our results also imply sample complexity bounds for the batch setup that depend on the respective combinatorial dimensions.

</details>

<details><summary><b>Chinese Abstract</b></summary>

We study an online learning problem with multiple correct answers, where each instance admits a set of valid labels, and in each round the learner must output a valid label for the queried example. This setting is motivated by language generation tasks, in which a prompt may admit many acceptable completions, but not every completion is acceptable. We study this problem under three feedback models.For each model, we characterize the optimal mistake bound in the realizable setting using an appropriate combinatorial dimension. We then establish a trichotomy of regret bounds across the three models in the agnostic setting. Our results also imply sample complexity bounds for the batch setup that depend on the respective combinatorial dimensions.

</details>

---

## 131. Sparse Layer Sharpness-Aware Minimization for Efficient Fine-Tuning

**Chinese Title**: Sparse Layer Sharpness-Aware Minimization for Efficient Fine-Tuning

**Authors**: Yifei Cheng, Xianglin Yang, Guoxia Wang, Chao Huang, Fei Ma et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09395v1](http://arxiv.org/abs/2602.09395v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09395v1)

**Categories**: cs.LG

<details><summary><b>Abstract</b></summary>

Sharpness-aware minimization (SAM) seeks the minima with a flat loss landscape to improve the generalization performance in machine learning tasks, including fine-tuning. However, its extra parameter perturbation step doubles the computation cost, which becomes the bottleneck of SAM in the practical implementation. In this work, we propose an approach SL-SAM to break this bottleneck by introducing the sparse technique to layers. Our key innovation is to frame the dynamic selection of layers for both the gradient ascent (perturbation) and descent (update) steps as a multi-armed bandit problem. At the beginning of each iteration, SL-SAM samples a part of the layers of the model according to the gradient norm to participate in the backpropagation of the following parameter perturbation and update steps, thereby reducing the computation complexity. We then provide the analysis to guarantee the convergence of SL-SAM. In the experiments of fine-tuning models in several tasks, SL-SAM achieves the performances comparable to the state-of-the-art baselines, including a \#1 rank on LLM fine-tuning. Meanwhile, SL-SAM significantly reduces the ratio of active parameters in backpropagation compared to vanilla SAM (SL-SAM activates 47\%, 22\% and 21\% parameters on the vision, moderate and large language model respectively while vanilla SAM always activates 100\%), verifying the efficiency of our proposed algorithm.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Sharpness-aware minimization (SAM) seeks the minima with a flat loss landscape to improve the generalization performance in machine learning tasks, including fine-tuning. However, its extra parameter perturbation step doubles the computation cost, which becomes the bottleneck of SAM in the practical implementation. In this work, we propose an approach SL-SAM to break this bottleneck by introducing the sparse technique to layers.Our key innovation is to frame the dynamic selection of layers for both the gradient ascent (perturbation) and descent (update) steps as a multi-armed bandit problem. At the beginning of each iteration, SL-SAM samples a part of the layers of the model according to the gradient norm to participate in the backpropagation of the following parameter perturbation and update steps, thereby reducing the computation complexity.We then provide the analysis to guarantee the convergence of SL-SAM. In the experiments of fine-tuning models in several tasks, SL-SAM achieves the performances comparable to the state-of-the-art baselines, including a \#1 rank on LLM fine-tuning.Meanwhile, SL-SAM significantly reduces the ratio of active parameters in backpropagation compared to vanilla SAM (SL-SAM activates 47\%, 22\% and 21\% parameters on the vision, moderate and large language model respectively while vanilla SAM always activates 100\%), verifying the efficiency of our proposed algorithm.

</details>

---

## 132. Latent PoincarÃ© Shaping for Agentic Reinforcement Learning

**Chinese Title**: Latent PoincarÃ© Shaping for Agentic Reinforcement Learning

**Authors**: Hanchen Xia, Baoyou Chen, Zelin Zang, Yutang Ge, Guojiang Zhao et al.

**Date**: 2026-02-10 | **arXiv**: [2602.09375v1](http://arxiv.org/abs/2602.09375v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09375v1)

**Categories**: cs.LG

<details><summary><b>Abstract</b></summary>

We propose LaPha, a method for training AlphaZero-like LLM agents in a PoincarÃ© latent space. Under LaPha, the search process can be visualized as a tree rooted at the prompt and growing outward from the origin toward the boundary of the PoincarÃ© ball, where negative curvature provides exponentially increasing capacity with radius. Using hyperbolic geodesic distance to rule-verified correctness, we define a node potential and assign dense process rewards by potential differences. We further attach a lightweight value head on the same shared latent space, enabling self-guided test-time scaling with almost no additional overhead. On MATH-500, LaPha improves Qwen2.5-Math-1.5B from 66.0% to 88.2%. With value-head-guided search, LaPha-1.5B reaches 56.7% accuracy on AIME'24, and LaPha-7B further achieves 60.0% on AIME'24 and 53.3% on AIME'25.

</details>

<details><summary><b>Chinese Abstract</b></summary>

We propose LaPha, a method for training AlphaZero-like LLM agents in a PoincarÃ© latent space. Under LaPha, the search process can be visualized as a tree rooted at the prompt and growing outward from the origin toward the boundary of the PoincarÃ© ball, where negative curvature provides exponentially increasing capacity with radius.Using hyperbolic geodesic distance to rule-verified correctness, we define a node potential and assign dense process rewards by potential differences. We further attach a lightweight value head on the same shared latent space, enabling self-guided test-time scaling with almost no additional overhead. On MATH-500, LaPha improves Qwen2.5-Math-1.5B from 66.0% to 88.2%.With value-head-guided search, LaPha-1.5B reaches 56.7% accuracy on AIME'24, and LaPha-7B further achieves 60.0% on AIME'24 and 53.3% on AIME'25.

</details>

---

## 133. Statistical Roughness-Informed Machine Unlearning

**Chinese Title**: Statistical Roughness-Informed Machine Unlearning

**Authors**: Mohammad Partohaghighi, Roummel Marcia, Bruce J. West, YangQuan Chen

**Date**: 2026-02-10 | **arXiv**: [2602.09304v1](http://arxiv.org/abs/2602.09304v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09304v1)

**Categories**: cs.LG

<details><summary><b>Abstract</b></summary>

Machine unlearning aims to remove the influence of a designated forget set from a trained model while preserving utility on the retained data. In modern deep networks, approximate unlearning frequently fails under large or adversarial deletions due to pronounced layer-wise heterogeneity: some layers exhibit stable, well-regularized representations while others are brittle, undertrained, or overfit, so naive update allocation can trigger catastrophic forgetting or unstable dynamics. We propose Statistical-Roughness Adaptive Gradient Unlearning (SRAGU), a mechanism-first unlearning algorithm that reallocates unlearning updates using layer-wise statistical roughness operationalized via heavy-tailed spectral diagnostics of layer weight matrices. Starting from an Adaptive Gradient Unlearning (AGU) sensitivity signal computed on the forget set, SRAGU estimates a WeightWatcher-style heavy-tailed exponent for each layer, maps it to a bounded spectral stability weight, and uses this stability signal to spectrally reweight the AGU sensitivities before applying the same minibatch update form. This concentrates unlearning motion in spectrally stable layers while damping updates in unstable or overfit layers, improving stability under hard deletions. We evaluate unlearning via behavioral alignment to a gold retrained reference model trained from scratch on the retained data, using empirical prediction-divergence and KL-to-gold proxies on a forget-focused query set; we additionally report membership inference auditing as a complementary leakage signal, treating forget-set points as should-be-forgotten members during evaluation.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Machine unlearning aims to remove the influence of a designated forget set from a trained model while preserving utility on the retained data.In modern deep networks, approximate unlearning frequently fails under large or adversarial deletions due to pronounced layer-wise heterogeneity: some layers exhibit stable, well-regularized representations while others are brittle, undertrained, or overfit, so naive update allocation can trigger catastrophic forgetting or unstable dynamics.We propose Statistical-Roughness Adaptive Gradient Unlearning (SRAGU), a mechanism-first unlearning algorithm that reallocates unlearning updates using layer-wise statistical roughness operationalized via heavy-tailed spectral diagnostics of layer weight matrices.Starting from an Adaptive Gradient Unlearning (AGU) sensitivity signal computed on the forget set, SRAGU estimates a WeightWatcher-style heavy-tailed exponent for each layer, maps it to a bounded spectral stability weight, and uses this stability signal to spectrally reweight the AGU sensitivities before applying the same minibatch update form.This concentrates unlearning motion in spectrally stable layers while damping updates in unstable or overfit layers, improving stability under hard deletions.We evaluate unlearning via behavioral alignment to a gold retrained reference model trained from scratch on the retained data, using empirical prediction-divergence and KL-to-gold proxies on a forget-focused query set; we additionally report membership inference auditing as a complementary leakage signal, treating forget-set points as should-be-forgotten members during evaluation.

</details>

---

## 134. The Laplacian Mechanism Improves Transformers by Reshaping Token Geometry

**Chinese Title**: The Laplacian Mechanism Improves Transformers by Reshaping Token Geometry

**Authors**: Yuchong Zhang, Vardan Papyan

**Date**: 2026-02-10 | **arXiv**: [2602.09297v1](http://arxiv.org/abs/2602.09297v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09297v1)

**Categories**: cs.LG

<details><summary><b>Abstract</b></summary>

Transformers leverage attention, the residual connection, and layer normalization to control the variance of token representations. We propose to modify attention into a Laplacian mechanism that gives the model more direct control over token variance. We conjecture that this helps transformers achieve the ideal token geometry. To investigate our conjecture, we first show that incorporating the Laplacian mechanism into transformers induces consistent improvements across benchmarks in computer vision and language. Next, we study how the Laplacian mechanism impacts the geometry of token representations using various tools: 1) principal component analysis, 2) cosine similarity metric, 3) analysis of variance, and 4) Neural Collapse metrics. Our investigation shows that the Laplacian mechanism reshapes token embeddings toward a geometry of maximal separability: tokens collapse according to their classes, and the class means exhibit Neural Collapse.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Transformers leverage attention, the residual connection, and layer normalization to control the variance of token representations. We propose to modify attention into a Laplacian mechanism that gives the model more direct control over token variance. We conjecture that this helps transformers achieve the ideal token geometry.To investigate our conjecture, we first show that incorporating the Laplacian mechanism into transformers induces consistent improvements across benchmarks in computer vision and language. Next, we study how the Laplacian mechanism impacts the geometry of token representations using various tools: 1) principal component analysis, 2) cosine similarity metric, 3) analysis of variance, and 4) Neural Collapse metrics.Our investigation shows that the Laplacian mechanism reshapes token embeddings toward a geometry of maximal separability: tokens collapse according to their classes, and the class means exhibit Neural Collapse.

</details>

---

## 135. Positive-Unlabelled Active Learning to Curate a Dataset for Orca Resident Interpretation

**Chinese Title**: Positive-Unlabelled Active Learning to Curate a Dataset for Orca Resident Interpretation

**Authors**: Bret Nestor, Bohan Yao, Jasmine Moore, Jasper Kanes

**Date**: 2026-02-10 | **arXiv**: [2602.09295v1](http://arxiv.org/abs/2602.09295v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09295v1)

**Categories**: cs.LG, cs.SD

<details><summary><b>Abstract</b></summary>

This work presents the largest curation of Southern Resident Killer Whale (SRKW) acoustic data to date, also containing other marine mammals in their environment. We systematically search all available public archival hydrophone data within the SRKW habitat (over 30 years of audio data). The search consists of a weakly-supervised, positive-unlabelled, active learning strategy to identify all instances of marine mammals. The resulting transformer-based detectors outperform state-of-the-art detectors on the DEEPAL, DCLDE-2026, and two newly introduced expert-annotated datasets in terms of accuracy, energy efficiency, and speed. The detection model has a specificity of 0-28.8% at 95% sensitivity. Our multiclass species classifier obtains a top-1 accuracy of 42.1% (11 train classes, 4 test classes) and our ecotype classifier obtains a top-1 accuracy of 43.0% (4 train classes, 5 test classes) on the DCLDE-2026 dataset.   We yield 919 hours of SRKW data, 230 hours of Bigg's orca data, 1374 hours of orca data from unlabelled ecotypes, 1501 hours of humpback data, 88 hours of sea lion data, 246 hours of pacific white-sided dolphin data, and over 784 hours of unspecified marine mammal data. This SRKW dataset is larger than DCLDE-2026, Ocean Networks Canada, and OrcaSound combined. The curated species labels are available under CC-BY 4.0 license, and the corresponding audio data are available under the licenses of the original owners. The comprehensive nature of this dataset makes it suitable for unsupervised machine translation, habitat usage surveys, and conservation endeavours for this critically endangered ecotype.

</details>

<details><summary><b>Chinese Abstract</b></summary>

This work presents the largest curation of Southern Resident Killer Whale (SRKW) acoustic data to date, also containing other marine mammals in their environment. We systematically search all available public archival hydrophone data within the SRKW habitat (over 30 years of audio data). The search consists of a weakly-supervised, positive-unlabelled, active learning strategy to identify all instances of marine mammals.The resulting transformer-based detectors outperform state-of-the-art detectors on the DEEPAL, DCLDE-2026, and two newly introduced expert-annotated datasets in terms of accuracy, energy efficiency, and speed. The detection model has a specificity of 0-28.8% at 95% sensitivity.Our multiclass species classifier obtains a top-1 accuracy of 42.1% (11 train classes, 4 test classes) and our ecotype classifier obtains a top-1 accuracy of 43.0% (4 train classes, 5 test classes) on the DCLDE-2026 dataset.We yield 919 hours of SRKW data, 230 hours of Bigg's orca data, 1374 hours of orca data from unlabelled ecotypes, 1501 hours of humpback data, 88 hours of sea lion data, 246 hours of pacific white-sided dolphin data, and over 784 hours of unspecified marine mammal data. This SRKW dataset is larger than DCLDE-2026, Ocean Networks Canada, and OrcaSound combined.The curated species labels are available under CC-BY 4.0 license, and the corresponding audio data are available under the licenses of the original owners. The comprehensive nature of this dataset makes it suitable for unsupervised machine translation, habitat usage surveys, and conservation endeavours for this critically endangered ecotype.

</details>

---

## 136. Rethinking Global Text Conditioning in Diffusion Transformers

**Chinese Title**: Rethinking Global Text Conditioning in Diffusion Transformers

**Authors**: Nikita Starodubcev, Daniil Pakhomov, Zongze Wu, Ilya Drobyshevskiy, Yuchen Liu et al.

**Date**: 2026-02-09 | **arXiv**: [2602.09268v1](http://arxiv.org/abs/2602.09268v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09268v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Diffusion transformers typically incorporate textual information via attention layers and a modulation mechanism using a pooled text embedding. Nevertheless, recent approaches discard modulation-based text conditioning and rely exclusively on attention. In this paper, we address whether modulation-based text conditioning is necessary and whether it can provide any performance advantage. Our analysis shows that, in its conventional usage, the pooled embedding contributes little to overall performance, suggesting that attention alone is generally sufficient for faithfully propagating prompt information. However, we reveal that the pooled embedding can provide significant gains when used from a different perspective-serving as guidance and enabling controllable shifts toward more desirable properties. This approach is training-free, simple to implement, incurs negligible runtime overhead, and can be applied to various diffusion models, bringing improvements across diverse tasks, including text-to-image/video generation and image editing.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Diffusion transformers typically incorporate textual information via attention layers and a modulation mechanism using a pooled text embedding. Nevertheless, recent approaches discard modulation-based text conditioning and rely exclusively on attention. In this paper, we address whether modulation-based text conditioning is necessary and whether it can provide any performance advantage.Our analysis shows that, in its conventional usage, the pooled embedding contributes little to overall performance, suggesting that attention alone is generally sufficient for faithfully propagating prompt information. However, we reveal that the pooled embedding can provide significant gains when used from a different perspective-serving as guidance and enabling controllable shifts toward more desirable properties.This approach is training-free, simple to implement, incurs negligible runtime overhead, and can be applied to various diffusion models, bringing improvements across diverse tasks, including text-to-image/video generation and image editing.

</details>

---

## 137. VLM-Guided Iterative Refinement for Surgical Image Segmentation with Foundation Models

**Chinese Title**: VLM-Guided Iterative Refinement for Surgical Image Segmentation with Foundation Models

**Authors**: Ange Lou, Yamin Li, Qi Chang, Nan Xi, Luyuan Xie et al.

**Date**: 2026-02-09 | **arXiv**: [2602.09252v1](http://arxiv.org/abs/2602.09252v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09252v1)

**Categories**: cs.CV, cs.AI, cs.MA

<details><summary><b>Abstract</b></summary>

Surgical image segmentation is essential for robot-assisted surgery and intraoperative guidance. However, existing methods are constrained to predefined categories, produce one-shot predictions without adaptive refinement, and lack mechanisms for clinician interaction. We propose IR-SIS, an iterative refinement system for surgical image segmentation that accepts natural language descriptions. IR-SIS leverages a fine-tuned SAM3 for initial segmentation, employs a Vision-Language Model to detect instruments and assess segmentation quality, and applies an agentic workflow that adaptively selects refinement strategies. The system supports clinician-in-the-loop interaction through natural language feedback. We also construct a multi-granularity language-annotated dataset from EndoVis2017 and EndoVis2018 benchmarks. Experiments demonstrate state-of-the-art performance on both in-domain and out-of-distribution data, with clinician interaction providing additional improvements. Our work establishes the first language-based surgical segmentation framework with adaptive self-refinement capabilities.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Surgical image segmentation is essential for robot-assisted surgery and intraoperative guidance. However, existing methods are constrained to predefined categories, produce one-shot predictions without adaptive refinement, and lack mechanisms for clinician interaction. We propose IR-SIS, an iterative refinement system for surgical image segmentation that accepts natural language descriptions.IR-SIS leverages a fine-tuned SAM3 for initial segmentation, employs a Vision-Language Model to detect instruments and assess segmentation quality, and applies an agentic workflow that adaptively selects refinement strategies. The system supports clinician-in-the-loop interaction through natural language feedback. We also construct a multi-granularity language-annotated dataset from EndoVis2017 and EndoVis2018 benchmarks.Experiments demonstrate state-of-the-art performance on both in-domain and out-of-distribution data, with clinician interaction providing additional improvements. Our work establishes the first language-based surgical segmentation framework with adaptive self-refinement capabilities.

</details>

---

## 138. VLM-UQBench: A Benchmark for Modality-Specific and Cross-Modality Uncertainties in Vision Language Models

**Chinese Title**: VLM-UQBench: A Benchmark for Modality-Specific and Cross-Modality Uncertainties in Vision Language Models

**Authors**: Chenyu Wang, Tianle Chen, H. M. Sabbir Ahmad, Kayhan Batmanghelich, Wenchao Li

**Date**: 2026-02-09 | **arXiv**: [2602.09214v1](http://arxiv.org/abs/2602.09214v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09214v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Uncertainty quantification (UQ) is vital for ensuring that vision-language models (VLMs) behave safely and reliably. A central challenge is to localize uncertainty to its source, determining whether it arises from the image, the text, or misalignment between the two. We introduce VLM-UQBench, a benchmark for modality-specific and cross-modal data uncertainty in VLMs, It consists of 600 real-world samples drawn from the VizWiz dataset, curated into clean, image-, text-, and cross-modal uncertainty subsets, and a scalable perturbation pipeline with 8 visual, 5 textual, and 3 cross-modal perturbations. We further propose two simple metrics that quantify the sensitivity of UQ scores to these perturbations and their correlation with hallucinations, and use them to evaluate a range of UQ methods across four VLMs and three datasets. Empirically, we find that: (i) existing UQ methods exhibit strong modality-specific specialization and substantial dependence on the underlying VLM, (ii) modality-specific uncertainty frequently co-occurs with hallucinations while current UQ scores provide only weak and inconsistent risk signals, and (iii) although UQ methods can rival reasoning-based chain-of-thought baselines on overt, group-level ambiguity, they largely fail to detect the subtle, instance-level ambiguity introduced by our perturbation pipeline. These results highlight a significant gap between current UQ practices and the fine-grained, modality-aware uncertainty required for reliable VLM deployment.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Uncertainty quantification (UQ) is vital for ensuring that vision-language models (VLMs) behave safely and reliably. A central challenge is to localize uncertainty to its source, determining whether it arises from the image, the text, or misalignment between the two.We introduce VLM-UQBench, a benchmark for modality-specific and cross-modal data uncertainty in VLMs, It consists of 600 real-world samples drawn from the VizWiz dataset, curated into clean, image-, text-, and cross-modal uncertainty subsets, and a scalable perturbation pipeline with 8 visual, 5 textual, and 3 cross-modal perturbations.We further propose two simple metrics that quantify the sensitivity of UQ scores to these perturbations and their correlation with hallucinations, and use them to evaluate a range of UQ methods across four VLMs and three datasets.Empirically, we find that: (i) existing UQ methods exhibit strong modality-specific specialization and substantial dependence on the underlying VLM, (ii) modality-specific uncertainty frequently co-occurs with hallucinations while current UQ scores provide only weak and inconsistent risk signals, and (iii) although UQ methods can rival reasoning-based chain-of-thought baselines on overt, group-level ambiguity, they largely fail to detect the subtle, instance-level ambiguity introduced by our perturbation pipeline.These results highlight a significant gap between current UQ practices and the fine-grained, modality-aware uncertainty required for reliable VLM deployment.

</details>

---

## 139. Wearable environmental sensing to forecast how legged systems will interact with upcoming terrain

**Chinese Title**: Wearable environmental sensing to forecast how legged systems will interact with upcoming terrain

**Authors**: Michael D. Murray, James Tung, Richard W. Nuckols

**Date**: 2026-02-09 | **arXiv**: [2602.09209v1](http://arxiv.org/abs/2602.09209v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09209v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Computer-vision (CV) has been used for environmental classification during gait and is often used to inform control in assistive systems; however, the ability to predict how the foot will contact a changing environment is underexplored. We evaluated the feasibility of forecasting the anterior-posterior (AP) foot center-of-pressure (COP) and time-of-impact (TOI) prior to foot-strike on a level-ground to stair-ascent transition. Eight subjects wore an RGB-D camera on their right shank and instrumented insoles while performing the task of stepping onto the stairs. We trained a CNN-RNN to forecast the COP and TOI continuously within a 250ms window prior to foot-strike, termed the forecast horizon (FH). The COP mean-absolute-error (MAE) at 150, 100, and 50ms FH was 29.42mm, 26.82, and 23.72mm respectively. The TOI MAE was 21.14, 20.08, and 17.73ms for 150, 100, and 50ms respectively. While torso velocity had no effect on the error in either task, faster toe-swing speeds prior to foot-strike were found to improve the prediction accuracy in the COP case, however, was insignificant in the TOI case. Further, more anterior foot-strikes were found to reduce COP prediction accuracy but did not affect the TOI prediction accuracy. We also found that our lightweight model was capable at running at 60 FPS on either a consumer grade laptop or an edge computing device. This study demonstrates that forecasting COP and TOI from visual data was feasible using a lightweight model, which may have important implications for anticipatory control in assistive systems.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Computer-vision (CV) has been used for environmental classification during gait and is often used to inform control in assistive systems; however, the ability to predict how the foot will contact a changing environment is underexplored. We evaluated the feasibility of forecasting the anterior-posterior (AP) foot center-of-pressure (COP) and time-of-impact (TOI) prior to foot-strike on a level-ground to stair-ascent transition.Eight subjects wore an RGB-D camera on their right shank and instrumented insoles while performing the task of stepping onto the stairs. We trained a CNN-RNN to forecast the COP and TOI continuously within a 250ms window prior to foot-strike, termed the forecast horizon (FH). The COP mean-absolute-error (MAE) at 150, 100, and 50ms FH was 29.42mm, 26.82, and 23.72mm respectively.The TOI MAE was 21.14, 20.08, and 17.73ms for 150, 100, and 50ms respectively. While torso velocity had no effect on the error in either task, faster toe-swing speeds prior to foot-strike were found to improve the prediction accuracy in the COP case, however, was insignificant in the TOI case. Further, more anterior foot-strikes were found to reduce COP prediction accuracy but did not affect the TOI prediction accuracy.We also found that our lightweight model was capable at running at 60 FPS on either a consumer grade laptop or an edge computing device. This study demonstrates that forecasting COP and TOI from visual data was feasible using a lightweight model, which may have important implications for anticipatory control in assistive systems.

</details>

---

## 140. All-in-One Conditioning for Text-to-Image Synthesis

**Chinese Title**: All-in-One Conditioning for Text-to-Image Synthesis

**Authors**: Hirunima Jayasekara, Chuong Huynh, Yixuan Ren, Christabel Acquaye, Abhinav Shrivastava

**Date**: 2026-02-09 | **arXiv**: [2602.09165v1](http://arxiv.org/abs/2602.09165v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09165v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Accurate interpretation and visual representation of complex prompts involving multiple objects, attributes, and spatial relationships is a critical challenge in text-to-image synthesis. Despite recent advancements in generating photorealistic outputs, current models often struggle with maintaining semantic fidelity and structural coherence when processing intricate textual inputs. We propose a novel approach that grounds text-to-image synthesis within the framework of scene graph structures, aiming to enhance the compositional abilities of existing models. Eventhough, prior approaches have attempted to address this by using pre-defined layout maps derived from prompts, such rigid constraints often limit compositional flexibility and diversity. In contrast, we introduce a zero-shot, scene graph-based conditioning mechanism that generates soft visual guidance during inference. At the core of our method is the Attribute-Size-Quantity-Location (ASQL) Conditioner, which produces visual conditions via a lightweight language model and guides diffusion-based generation through inference-time optimization. This enables the model to maintain text-image alignment while supporting lightweight, coherent, and diverse image synthesis.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Accurate interpretation and visual representation of complex prompts involving multiple objects, attributes, and spatial relationships is a critical challenge in text-to-image synthesis. Despite recent advancements in generating photorealistic outputs, current models often struggle with maintaining semantic fidelity and structural coherence when processing intricate textual inputs.We propose a novel approach that grounds text-to-image synthesis within the framework of scene graph structures, aiming to enhance the compositional abilities of existing models. Eventhough, prior approaches have attempted to address this by using pre-defined layout maps derived from prompts, such rigid constraints often limit compositional flexibility and diversity.In contrast, we introduce a zero-shot, scene graph-based conditioning mechanism that generates soft visual guidance during inference. At the core of our method is the Attribute-Size-Quantity-Location (ASQL) Conditioner, which produces visual conditions via a lightweight language model and guides diffusion-based generation through inference-time optimization.This enables the model to maintain text-image alignment while supporting lightweight, coherent, and diverse image synthesis.

</details>

---

## 141. SemanticMoments: Training-Free Motion Similarity via Third Moment Features

**Chinese Title**: SemanticMoments: Training-Free Motion Similarity via Third Moment Features

**Authors**: Saar Huberman, Kfir Goldberg, Or Patashnik, Sagie Benaim, Ron Mokady

**Date**: 2026-02-09 | **arXiv**: [2602.09146v1](http://arxiv.org/abs/2602.09146v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09146v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Retrieving videos based on semantic motion is a fundamental, yet unsolved, problem. Existing video representation approaches overly rely on static appearance and scene context rather than motion dynamics, a bias inherited from their training data and objectives. Conversely, traditional motion-centric inputs like optical flow lack the semantic grounding needed to understand high-level motion. To demonstrate this inherent bias, we introduce the SimMotion benchmarks, combining controlled synthetic data with a new human-annotated real-world dataset. We show that existing models perform poorly on these benchmarks, often failing to disentangle motion from appearance. To address this gap, we propose SemanticMoments, a simple, training-free method that computes temporal statistics (specifically, higher-order moments) over features from pre-trained semantic models. Across our benchmarks, SemanticMoments consistently outperforms existing RGB, flow, and text-supervised methods. This demonstrates that temporal statistics in a semantic feature space provide a scalable and perceptually grounded foundation for motion-centric video understanding.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Retrieving videos based on semantic motion is a fundamental, yet unsolved, problem. Existing video representation approaches overly rely on static appearance and scene context rather than motion dynamics, a bias inherited from their training data and objectives. Conversely, traditional motion-centric inputs like optical flow lack the semantic grounding needed to understand high-level motion.To demonstrate this inherent bias, we introduce the SimMotion benchmarks, combining controlled synthetic data with a new human-annotated real-world dataset. We show that existing models perform poorly on these benchmarks, often failing to disentangle motion from appearance.To address this gap, we propose SemanticMoments, a simple, training-free method that computes temporal statistics (specifically, higher-order moments) over features from pre-trained semantic models. Across our benchmarks, SemanticMoments consistently outperforms existing RGB, flow, and text-supervised methods.This demonstrates that temporal statistics in a semantic feature space provide a scalable and perceptually grounded foundation for motion-centric video understanding.

</details>

---

## 142. Autoregressive Image Generation with Masked Bit Modeling

**Chinese Title**: Autoregressive Image Generation with Masked Bit Modeling

**Authors**: Qihang Yu, Qihao Liu, Ju He, Xinyang Zhang, Yang Liu et al.

**Date**: 2026-02-09 | **arXiv**: [2602.09024v1](http://arxiv.org/abs/2602.09024v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09024v1)

**Project**: https://bar-gen.github.io/  **Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook sizes. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at https://bar-gen.github.io/

</details>

<details><summary><b>Chinese Abstract</b></summary>

This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio).We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook.To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook sizes. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits.BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at https://bar-gen.github.io/

</details>

---

## 143. WorldCompass: Reinforcement Learning for Long-Horizon World Models

**Chinese Title**: WorldCompass: Reinforcement Learning for Long-Horizon World Models

**Authors**: Zehan Wang, Tengfei Wang, Haiyu Zhang, Xuhui Zuo, Junta Wu et al.

**Date**: 2026-02-09 | **arXiv**: [2602.09022v1](http://arxiv.org/abs/2602.09022v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09022v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively "steer" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.

</details>

<details><summary><b>Chinese Abstract</b></summary>

This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals.To effectively "steer" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals.2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity.Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.

</details>

---

## 144. $Ï‡_{0}$: Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies

**Chinese Title**: $Ï‡_{0}$: Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies

**Authors**: Checheng Yu, Chonghao Sima, Gangcheng Jiang, Hai Zhang, Haoguang Mai et al.

**Date**: 2026-02-09 | **arXiv**: [2602.09021v1](http://arxiv.org/abs/2602.09021v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09021v1)

**Categories**: cs.RO, cs.CV

<details><summary><b>Abstract</b></summary>

High-reliability long-horizon robotic manipulation has traditionally relied on large-scale data and compute to understand complex real-world dynamics. However, we identify that the primary bottleneck to real-world robustness is not resource scale alone, but the distributional shift among the human demonstration distribution, the inductive bias learned by the policy, and the test-time execution distribution -- a systematic inconsistency that causes compounding errors in multi-stage tasks. To mitigate these inconsistencies, we propose $Ï‡_{0}$, a resource-efficient framework with effective modules designated to achieve production-level robustness in robotic manipulation. Our approach builds off three technical pillars: (i) Model Arithmetic, a weight-space merging strategy that efficiently soaks up diverse distributions of different demonstrations, varying from object appearance to state variations; (ii) Stage Advantage, a stage-aware advantage estimator that provides stable, dense progress signals, overcoming the numerical instability of prior non-stage approaches; and (iii) Train-Deploy Alignment, which bridges the distribution gap via spatio-temporal augmentation, heuristic DAgger corrections, and temporal chunk-wise smoothing. $Ï‡_{0}$ enables two sets of dual-arm robots to collaboratively orchestrate long-horizon garment manipulation, spanning tasks from flattening, folding, to hanging different clothes. Our method exhibits high-reliability autonomy; we are able to run the system from arbitrary initial state for consecutive 24 hours non-stop. Experiments validate that $Ï‡_{0}$ surpasses the state-of-the-art $Ï€_{0.5}$ in success rate by nearly 250%, with only 20-hour data and 8 A100 GPUs. Code, data and models will be released to facilitate the community.

</details>

<details><summary><b>Chinese Abstract</b></summary>

High-reliability long-horizon robotic manipulation has traditionally relied on large-scale data and compute to understand complex real-world dynamics.However, we identify that the primary bottleneck to real-world robustness is not resource scale alone, but the distributional shift among the human demonstration distribution, the inductive bias learned by the policy, and the test-time execution distribution -- a systematic inconsistency that causes compounding errors in multi-stage tasks.To mitigate these inconsistencies, we propose $Ï‡_{0}$, a resource-efficient framework with effective modules designated to achieve production-level robustness in robotic manipulation.Our approach builds off three technical pillars: (i) Model Arithmetic, a weight-space merging strategy that efficiently soaks up diverse distributions of different demonstrations, varying from object appearance to state variations; (ii) Stage Advantage, a stage-aware advantage estimator that provides stable, dense progress signals, overcoming the numerical instability of prior non-stage approaches; and (iii) Train-Deploy Alignment, which bridges the distribution gap via spatio-temporal augmentation, heuristic DAgger corrections, and temporal chunk-wise smoothing.$Ï‡_{0}$ enables two sets of dual-arm robots to collaboratively orchestrate long-horizon garment manipulation, spanning tasks from flattening, folding, to hanging different clothes. Our method exhibits high-reliability autonomy; we are able to run the system from arbitrary initial state for consecutive 24 hours non-stop.Experiments validate that $Ï‡_{0}$ surpasses the state-of-the-art $Ï€_{0.5}$ in success rate by nearly 250%, with only 20-hour data and 8 A100 GPUs. Code, data and models will be released to facilitate the community.

</details>

---

## 145. Robustness Is a Function, Not a Number: A Factorized Comprehensive Study of OOD Robustness in Vision-Based Driving

**Chinese Title**: Robustness Is a Function, Not a Number: A Factorized Comprehensive Study of OOD Robustness in Vision-Based Driving

**Authors**: Amir Mallak, Alaa Maalouf

**Date**: 2026-02-09 | **arXiv**: [2602.09018v1](http://arxiv.org/abs/2602.09018v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09018v1)

**Categories**: cs.RO, cs.AI, cs.CV, cs.LG

<details><summary><b>Abstract</b></summary>

Out of distribution (OOD) robustness in autonomous driving is often reduced to a single number, hiding what breaks a policy. We decompose environments along five axes: scene (rural/urban), season, weather, time (day/night), and agent mix; and measure performance under controlled $k$-factor perturbations ($k \in \{0,1,2,3\}$). Using closed loop control in VISTA, we benchmark FC, CNN, and ViT policies, train compact ViT heads on frozen foundation-model (FM) features, and vary ID support in scale, diversity, and temporal context. (1) ViT policies are markedly more OOD-robust than comparably sized CNN/FC, and FM features yield state-of-the-art success at a latency cost. (2) Naive temporal inputs (multi-frame) do not beat the best single-frame baseline. (3) The largest single factor drops are rural $\rightarrow$ urban and day $\rightarrow$ night ($\sim 31\%$ each); actor swaps $\sim 10\%$, moderate rain $\sim 7\%$; season shifts can be drastic, and combining a time flip with other changes further degrades performance. (4) FM-feature policies stay above $85\%$ under three simultaneous changes; non-FM single-frame policies take a large first-shift hit, and all no-FM models fall below $50\%$ by three changes. (5) Interactions are non-additive: some pairings partially offset, whereas season-time combinations are especially harmful. (6) Training on winter/snow is most robust to single-factor shifts, while a rural+summer baseline gives the best overall OOD performance. (7) Scaling traces/views improves robustness ($+11.8$ points from $5$ to $14$ traces), yet targeted exposure to hard conditions can substitute for scale. (8) Using multiple ID environments broadens coverage and strengthens weak cases (urban OOD $60.6\% \rightarrow 70.1\%$) with a small ID drop; single-ID preserves peak performance but in a narrow domain. These results yield actionable design rules for OOD-robust driving policies.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Out of distribution (OOD) robustness in autonomous driving is often reduced to a single number, hiding what breaks a policy. We decompose environments along five axes: scene (rural/urban), season, weather, time (day/night), and agent mix; and measure performance under controlled $k$-factor perturbations ($k \in \{0,1,2,3\}$).Using closed loop control in VISTA, we benchmark FC, CNN, and ViT policies, train compact ViT heads on frozen foundation-model (FM) features, and vary ID support in scale, diversity, and temporal context. (1) ViT policies are markedly more OOD-robust than comparably sized CNN/FC, and FM features yield state-of-the-art success at a latency cost. (2) Naive temporal inputs (multi-frame) do not beat the best single-frame baseline.(3) The largest single factor drops are rural $\rightarrow$ urban and day $\rightarrow$ night ($\sim 31\%$ each); actor swaps $\sim 10\%$, moderate rain $\sim 7\%$; season shifts can be drastic, and combining a time flip with other changes further degrades performance.(4) FM-feature policies stay above $85\%$ under three simultaneous changes; non-FM single-frame policies take a large first-shift hit, and all no-FM models fall below $50\%$ by three changes. (5) Interactions are non-additive: some pairings partially offset, whereas season-time combinations are especially harmful.(6) Training on winter/snow is most robust to single-factor shifts, while a rural+summer baseline gives the best overall OOD performance. (7) Scaling traces/views improves robustness ($+11.8$ points from $5$ to $14$ traces), yet targeted exposure to hard conditions can substitute for scale.(8) Using multiple ID environments broadens coverage and strengthens weak cases (urban OOD $60.6\% \rightarrow 70.1\%$) with a small ID drop; single-ID preserves peak performance but in a narrow domain. These results yield actionable design rules for OOD-robust driving policies.

</details>

---

## 146. ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation

**Chinese Title**: ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation

**Authors**: Zihan Yang, Shuyuan Tu, Licheng Zhang, Qi Dai, Yu-Gang Jiang et al.

**Date**: 2026-02-09 | **arXiv**: [2602.09014v1](http://arxiv.org/abs/2602.09014v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09014v1)

**Categories**: cs.CV, cs.AI

<details><summary><b>Abstract</b></summary>

Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime.However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories.Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step.Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters.This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.

</details>

---

## 147. Dexterous Manipulation Policies from RGB Human Videos via 4D Hand-Object Trajectory Reconstruction

**Chinese Title**: Dexterous Manipulation Policies from RGB Human Videos via 4D Hand-Object Trajectory Reconstruction

**Authors**: Hongyi Chen, Tony Dong, Tiancheng Wu, Liquan Wang, Yash Jangir et al.

**Date**: 2026-02-09 | **arXiv**: [2602.09013v1](http://arxiv.org/abs/2602.09013v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09013v1)

**Categories**: cs.RO, cs.CV

<details><summary><b>Abstract</b></summary>

Multi-finger robotic hand manipulation and grasping are challenging due to the high-dimensional action space and the difficulty of acquiring large-scale training data. Existing approaches largely rely on human teleoperation with wearable devices or specialized sensing equipment to capture hand-object interactions, which limits scalability. In this work, we propose VIDEOMANIP, a device-free framework that learns dexterous manipulation directly from RGB human videos. Leveraging recent advances in computer vision, VIDEOMANIP reconstructs explicit 4D robot-object trajectories from monocular videos by estimating human hand poses, object meshes, and retargets the reconstructed human motions to robotic hands for manipulation learning. To make the reconstructed robot data suitable for dexterous manipulation training, we introduce hand-object contact optimization with interaction-centric grasp modeling, as well as a demonstration synthesis strategy that generates diverse training trajectories from a single video, enabling generalizable policy learning without additional robot demonstrations. In simulation, the learned grasping model achieves a 70.25% success rate across 20 diverse objects using the Inspire Hand. In the real world, manipulation policies trained from RGB videos achieve an average 62.86% success rate across seven tasks using the LEAP Hand, outperforming retargeting-based methods by 15.87%. Project videos are available at videomanip.github.io.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Multi-finger robotic hand manipulation and grasping are challenging due to the high-dimensional action space and the difficulty of acquiring large-scale training data. Existing approaches largely rely on human teleoperation with wearable devices or specialized sensing equipment to capture hand-object interactions, which limits scalability.In this work, we propose VIDEOMANIP, a device-free framework that learns dexterous manipulation directly from RGB human videos. Leveraging recent advances in computer vision, VIDEOMANIP reconstructs explicit 4D robot-object trajectories from monocular videos by estimating human hand poses, object meshes, and retargets the reconstructed human motions to robotic hands for manipulation learning.To make the reconstructed robot data suitable for dexterous manipulation training, we introduce hand-object contact optimization with interaction-centric grasp modeling, as well as a demonstration synthesis strategy that generates diverse training trajectories from a single video, enabling generalizable policy learning without additional robot demonstrations.In simulation, the learned grasping model achieves a 70.25% success rate across 20 diverse objects using the Inspire Hand. In the real world, manipulation policies trained from RGB videos achieve an average 62.86% success rate across seven tasks using the LEAP Hand, outperforming retargeting-based methods by 15.87%. Project videos are available at videomanip.github.io.

</details>

---

## 148. GEBench: Benchmarking Image Generation Models as GUI Environments

**Chinese Title**: GEBench: Benchmarking Image Generation Models as GUI Environments

**Authors**: Haodong Li, Jingwei Wu, Quan Sun, Guopeng Li, Juanxi Tian et al.

**Date**: 2026-02-09 | **arXiv**: [2602.09007v2](http://arxiv.org/abs/2602.09007v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.09007v2)

**Code**: https://github.com/stepfun-ai/GEBench.

**Categories**: cs.AI, cs.CV

<details><summary><b>Abstract</b></summary>

Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored.To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization.To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences.Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.

</details>

---

## 149. WorldArena: A Unified Benchmark for Evaluating Perception and Functional Utility of Embodied World Models

**Chinese Title**: WorldArena: A Unified Benchmark for Evaluating Perception and Functional Utility of Embodied World Models

**Authors**: Yu Shang, Zhuohang Li, Yiding Ma, Weikang Su, Xin Jin et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08971v1](http://arxiv.org/abs/2602.08971v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08971v1)

**Project**: https://worldarena.ai,  **Categories**: cs.CV, cs.RO

<details><summary><b>Abstract</b></summary>

While world models have emerged as a cornerstone of embodied intelligence by enabling agents to reason about environmental dynamics through action-conditioned prediction, their evaluation remains fragmented. Current evaluation of embodied world models has largely focused on perceptual fidelity (e.g., video generation quality), overlooking the functional utility of these models in downstream decision-making tasks. In this work, we introduce WorldArena, a unified benchmark designed to systematically evaluate embodied world models across both perceptual and functional dimensions. WorldArena assesses models through three dimensions: video perception quality, measured with 16 metrics across six sub-dimensions; embodied task functionality, which evaluates world models as data engines, policy evaluators, and action planners integrating with subjective human evaluation. Furthermore, we propose EWMScore, a holistic metric integrating multi-dimensional performance into a single interpretable index. Through extensive experiments on 14 representative models, we reveal a significant perception-functionality gap, showing that high visual quality does not necessarily translate into strong embodied task capability. WorldArena benchmark with the public leaderboard is released at https://worldarena.ai, providing a framework for tracking progress toward truly functional world models in embodied AI.

</details>

<details><summary><b>Chinese Abstract</b></summary>

While world models have emerged as a cornerstone of embodied intelligence by enabling agents to reason about environmental dynamics through action-conditioned prediction, their evaluation remains fragmented. Current evaluation of embodied world models has largely focused on perceptual fidelity (e.g., video generation quality), overlooking the functional utility of these models in downstream decision-making tasks.In this work, we introduce WorldArena, a unified benchmark designed to systematically evaluate embodied world models across both perceptual and functional dimensions.WorldArena assesses models through three dimensions: video perception quality, measured with 16 metrics across six sub-dimensions; embodied task functionality, which evaluates world models as data engines, policy evaluators, and action planners integrating with subjective human evaluation. Furthermore, we propose EWMScore, a holistic metric integrating multi-dimensional performance into a single interpretable index.Through extensive experiments on 14 representative models, we reveal a significant perception-functionality gap, showing that high visual quality does not necessarily translate into strong embodied task capability. WorldArena benchmark with the public leaderboard is released at https://worldarena.ai, providing a framework for tracking progress toward truly functional world models in embodied AI.

</details>

---

## 150. Modeling 3D Pedestrian-Vehicle Interactions for Vehicle-Conditioned Pose Forecasting

**Chinese Title**: Modeling 3D Pedestrian-Vehicle Interactions for Vehicle-Conditioned Pose Forecasting

**Authors**: Guangxun Zhu, Xuan Liu, Nicolas Pugeault, Chongfeng Wei, Edmond S. L. Ho

**Date**: 2026-02-09 | **arXiv**: [2602.08962v1](http://arxiv.org/abs/2602.08962v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08962v1)

**Code**: https://github.com/GuangxunZhu/VehCondPose3D

**Categories**: cs.CV, cs.RO

<details><summary><b>Abstract</b></summary>

Accurately predicting pedestrian motion is crucial for safe and reliable autonomous driving in complex urban environments. In this work, we present a 3D vehicle-conditioned pedestrian pose forecasting framework that explicitly incorporates surrounding vehicle information. To support this, we enhance the Waymo-3DSkelMo dataset with aligned 3D vehicle bounding boxes, enabling realistic modeling of multi-agent pedestrian-vehicle interactions. We introduce a sampling scheme to categorize scenes by pedestrian and vehicle count, facilitating training across varying interaction complexities. Our proposed network adapts the TBIFormer architecture with a dedicated vehicle encoder and pedestrian-vehicle interaction cross-attention module to fuse pedestrian and vehicle features, allowing predictions to be conditioned on both historical pedestrian motion and surrounding vehicles. Extensive experiments demonstrate substantial improvements in forecasting accuracy and validate different approaches for modeling pedestrian-vehicle interactions, highlighting the importance of vehicle-aware 3D pose prediction for autonomous driving. Code is available at: https://github.com/GuangxunZhu/VehCondPose3D

</details>

<details><summary><b>Chinese Abstract</b></summary>

Accurately predicting pedestrian motion is crucial for safe and reliable autonomous driving in complex urban environments. In this work, we present a 3D vehicle-conditioned pedestrian pose forecasting framework that explicitly incorporates surrounding vehicle information. To support this, we enhance the Waymo-3DSkelMo dataset with aligned 3D vehicle bounding boxes, enabling realistic modeling of multi-agent pedestrian-vehicle interactions.We introduce a sampling scheme to categorize scenes by pedestrian and vehicle count, facilitating training across varying interaction complexities. Our proposed network adapts the TBIFormer architecture with a dedicated vehicle encoder and pedestrian-vehicle interaction cross-attention module to fuse pedestrian and vehicle features, allowing predictions to be conditioned on both historical pedestrian motion and surrounding vehicles.Extensive experiments demonstrate substantial improvements in forecasting accuracy and validate different approaches for modeling pedestrian-vehicle interactions, highlighting the importance of vehicle-aware 3D pose prediction for autonomous driving. Code is available at: https://github.com/GuangxunZhu/VehCondPose3D

</details>

---

## 151. MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE

**Chinese Title**: MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE

**Authors**: Ruijie Zhu, Jiahao Lu, Wenbo Hu, Xiaoguang Han, Jianfei Cai et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08961v1](http://arxiv.org/abs/2602.08961v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08961v1)

**Project**: https://ruijiezhu94.github.io/MotionCrafter_Page  **Categories**: cs.CV, cs.AI, cs.CG, cs.LG

<details><summary><b>Abstract</b></summary>

We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page

</details>

<details><summary><b>Chinese Abstract</b></summary>

We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation.Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality.Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page

</details>

---

## 152. Grow with the Flow: 4D Reconstruction of Growing Plants with Gaussian Flow Fields

**Chinese Title**: Grow with the Flow: 4D Reconstruction of Growing Plants with Gaussian Flow Fields

**Authors**: Weihan Luo, Lily Goli, Sherwin Bahmani, Felix Taubner, Andrea Tagliasacchi et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08958v2](http://arxiv.org/abs/2602.08958v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.08958v2)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Modeling the time-varying 3D appearance of plants during their growth poses unique challenges: unlike many dynamic scenes, plants generate new geometry over time as they expand, branch, and differentiate. Recent motion modeling techniques are ill-suited to this problem setting. For example, deformation fields cannot introduce new geometry, and 4D Gaussian splatting constrains motion to a linear trajectory in space and time and cannot track the same set of Gaussians over time. Here, we introduce a 3D Gaussian flow field representation that models plant growth as a time-varying derivative over Gaussian parameters -- position, scale, orientation, color, and opacity -- enabling nonlinear and continuous-time growth dynamics. To initialize a sufficient set of Gaussian primitives, we reconstruct the mature plant and learn a process of reverse growth, effectively simulating the plant's developmental history in reverse. Our approach achieves superior image quality and geometric accuracy compared to prior methods on multi-view timelapse datasets of plant growth, providing a new approach for appearance modeling of growing 3D structures.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Modeling the time-varying 3D appearance of plants during their growth poses unique challenges: unlike many dynamic scenes, plants generate new geometry over time as they expand, branch, and differentiate. Recent motion modeling techniques are ill-suited to this problem setting.For example, deformation fields cannot introduce new geometry, and 4D Gaussian splatting constrains motion to a linear trajectory in space and time and cannot track the same set of Gaussians over time. Here, we introduce a 3D Gaussian flow field representation that models plant growth as a time-varying derivative over Gaussian parameters -- position, scale, orientation, color, and opacity -- enabling nonlinear and continuous-time growth dynamics.To initialize a sufficient set of Gaussian primitives, we reconstruct the mature plant and learn a process of reverse growth, effectively simulating the plant's developmental history in reverse. Our approach achieves superior image quality and geometric accuracy compared to prior methods on multi-view timelapse datasets of plant growth, providing a new approach for appearance modeling of growing 3D structures.

</details>

---

## 153. Analysis of Converged 3D Gaussian Splatting Solutions: Density Effects and Prediction Limit

**Chinese Title**: Analysis of Converged 3D Gaussian Splatting Solutions: Density Effects and Prediction Limit

**Authors**: Zhendong Wang, Cihan Ruan, Jingchuan Xiao, Chuqing Shi, Wei Jiang et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08909v1](http://arxiv.org/abs/2602.08909v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08909v1)

**Categories**: cs.CV, cs.LG

<details><summary><b>Abstract</b></summary>

We investigate what structure emerges in 3D Gaussian Splatting (3DGS) solutions from standard multi-view optimization. We term these Rendering-Optimal References (RORs) and analyze their statistical properties, revealing stable patterns: mixture-structured scales and bimodal radiance across diverse scenes. To understand what determines these parameters, we apply learnability probes by training predictors to reconstruct RORs from point clouds without rendering supervision. Our analysis uncovers fundamental density-stratification. Dense regions exhibit geometry-correlated parameters amenable to render-free prediction, while sparse regions show systematic failure across architectures. We formalize this through variance decomposition, demonstrating that visibility heterogeneity creates covariance-dominated coupling between geometric and appearance parameters in sparse regions. This reveals the dual character of RORs: geometric primitives where point clouds suffice, and view synthesis primitives where multi-view constraints are essential. We provide density-aware strategies that improve training robustness and discuss architectural implications for systems that adaptively balance feed-forward prediction and rendering-based refinement.

</details>

<details><summary><b>Chinese Abstract</b></summary>

We investigate what structure emerges in 3D Gaussian Splatting (3DGS) solutions from standard multi-view optimization. We term these Rendering-Optimal References (RORs) and analyze their statistical properties, revealing stable patterns: mixture-structured scales and bimodal radiance across diverse scenes.To understand what determines these parameters, we apply learnability probes by training predictors to reconstruct RORs from point clouds without rendering supervision. Our analysis uncovers fundamental density-stratification. Dense regions exhibit geometry-correlated parameters amenable to render-free prediction, while sparse regions show systematic failure across architectures.We formalize this through variance decomposition, demonstrating that visibility heterogeneity creates covariance-dominated coupling between geometric and appearance parameters in sparse regions. This reveals the dual character of RORs: geometric primitives where point clouds suffice, and view synthesis primitives where multi-view constraints are essential.We provide density-aware strategies that improve training robustness and discuss architectural implications for systems that adaptively balance feed-forward prediction and rendering-based refinement.

</details>

---

## 154. Designing Multi-Robot Ground Video Sensemaking with Public Safety Professionals

**Chinese Title**: Designing Multi-Robot Ground Video Sensemaking with Public Safety Professionals

**Authors**: Puqi Zhou, Ali Asgarov, Aafiya Hussain, Wonjoon Park, Amit Paudyal et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08882v2](http://arxiv.org/abs/2602.08882v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.08882v2)

**Categories**: cs.HC, cs.CV

<details><summary><b>Abstract</b></summary>

Videos from fleets of ground robots can advance public safety by providing scalable situational awareness and reducing professionals' burden. Yet little is known about how to design and integrate multi-robot videos into public safety workflows. Collaborating with six police agencies, we examined how such videos could be made practical. In Study 1, we presented the first testbed for multi-robot ground video sensemaking. The testbed includes 38 events-of-interest (EoI) relevant to public safety, a dataset of 20 robot patrol videos (10 day/night pairs) covering EoI types, and 6 design requirements aimed at improving current video sensemaking practices. In Study 2, we built MRVS, a tool that augments multi-robot patrol video streams with a prompt-engineered video understanding model. Participants reported reduced manual workload and greater confidence with LLM-based explanations, while noting concerns about false alarms and privacy. We conclude with implications for designing future multi-robot video sensemaking tools.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Videos from fleets of ground robots can advance public safety by providing scalable situational awareness and reducing professionals' burden. Yet little is known about how to design and integrate multi-robot videos into public safety workflows. Collaborating with six police agencies, we examined how such videos could be made practical. In Study 1, we presented the first testbed for multi-robot ground video sensemaking.The testbed includes 38 events-of-interest (EoI) relevant to public safety, a dataset of 20 robot patrol videos (10 day/night pairs) covering EoI types, and 6 design requirements aimed at improving current video sensemaking practices. In Study 2, we built MRVS, a tool that augments multi-robot patrol video streams with a prompt-engineered video understanding model.Participants reported reduced manual workload and greater confidence with LLM-based explanations, while noting concerns about false alarms and privacy. We conclude with implications for designing future multi-robot video sensemaking tools.

</details>

---

## 155. TiFRe: Text-guided Video Frame Reduction for Efficient Video Multi-modal Large Language Models

**Chinese Title**: TiFRe: Text-guided Video Frame Reduction for Efficient Video Multi-modal Large Language Models

**Authors**: Xiangtian Zheng, Zishuo Wang, Yuxin Peng

**Date**: 2026-02-09 | **arXiv**: [2602.08861v1](http://arxiv.org/abs/2602.08861v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08861v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

With the rapid development of Large Language Models (LLMs), Video Multi-Modal Large Language Models (Video MLLMs) have achieved remarkable performance in video-language tasks such as video understanding and question answering. However, Video MLLMs face high computational costs, particularly in processing numerous video frames as input, which leads to significant attention computation overhead. A straightforward approach to reduce computational costs is to decrease the number of input video frames. However, simply selecting key frames at a fixed frame rate (FPS) often overlooks valuable information in non-key frames, resulting in notable performance degradation. To address this, we propose Text-guided Video Frame Reduction (TiFRe), a framework that reduces input frames while preserving essential video information. TiFRe uses a Text-guided Frame Sampling (TFS) strategy to select key frames based on user input, which is processed by an LLM to generate a CLIP-style prompt. Pre-trained CLIP encoders calculate the semantic similarity between the prompt and each frame, selecting the most relevant frames as key frames. To preserve video semantics, TiFRe employs a Frame Matching and Merging (FMM) mechanism, which integrates non-key frame information into the selected key frames, minimizing information loss. Experiments show that TiFRe effectively reduces computational costs while improving performance on video-language tasks.

</details>

<details><summary><b>Chinese Abstract</b></summary>

With the rapid development of Large Language Models (LLMs), Video Multi-Modal Large Language Models (Video MLLMs) have achieved remarkable performance in video-language tasks such as video understanding and question answering. However, Video MLLMs face high computational costs, particularly in processing numerous video frames as input, which leads to significant attention computation overhead.A straightforward approach to reduce computational costs is to decrease the number of input video frames. However, simply selecting key frames at a fixed frame rate (FPS) often overlooks valuable information in non-key frames, resulting in notable performance degradation. To address this, we propose Text-guided Video Frame Reduction (TiFRe), a framework that reduces input frames while preserving essential video information.TiFRe uses a Text-guided Frame Sampling (TFS) strategy to select key frames based on user input, which is processed by an LLM to generate a CLIP-style prompt. Pre-trained CLIP encoders calculate the semantic similarity between the prompt and each frame, selecting the most relevant frames as key frames.To preserve video semantics, TiFRe employs a Frame Matching and Merging (FMM) mechanism, which integrates non-key frame information into the selected key frames, minimizing information loss. Experiments show that TiFRe effectively reduces computational costs while improving performance on video-language tasks.

</details>

---

## 156. VideoVeritas: AI-Generated Video Detection via Perception Pretext Reinforcement Learning

**Chinese Title**: VideoVeritas: AI-Generated Video Detection via Perception Pretext Reinforcement Learning

**Authors**: Hao Tan, Jun Lan, Senyuan Shi, Zichang Tan, Zijian Yu et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08828v1](http://arxiv.org/abs/2602.08828v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08828v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

The growing capability of video generation poses escalating security risks, making reliable detection increasingly essential. In this paper, we introduce VideoVeritas, a framework that integrates fine-grained perception and fact-based reasoning. We observe that while current multi-modal large language models (MLLMs) exhibit strong reasoning capacity, their granular perception ability remains limited. To mitigate this, we introduce Joint Preference Alignment and Perception Pretext Reinforcement Learning (PPRL). Specifically, rather than directly optimizing for detection task, we adopt general spatiotemporal grounding and self-supervised object counting in the RL stage, enhancing detection performance with simple perception pretext tasks. To facilitate robust evaluation, we further introduce MintVid, a light yet high-quality dataset containing 3K videos from 9 state-of-the-art generators, along with a real-world collected subset that has factual errors in content. Experimental results demonstrate that existing methods tend to bias towards either superficial reasoning or mechanical analysis, while VideoVeritas achieves more balanced performance across diverse benchmarks.

</details>

<details><summary><b>Chinese Abstract</b></summary>

The growing capability of video generation poses escalating security risks, making reliable detection increasingly essential. In this paper, we introduce VideoVeritas, a framework that integrates fine-grained perception and fact-based reasoning. We observe that while current multi-modal large language models (MLLMs) exhibit strong reasoning capacity, their granular perception ability remains limited.To mitigate this, we introduce Joint Preference Alignment and Perception Pretext Reinforcement Learning (PPRL). Specifically, rather than directly optimizing for detection task, we adopt general spatiotemporal grounding and self-supervised object counting in the RL stage, enhancing detection performance with simple perception pretext tasks.To facilitate robust evaluation, we further introduce MintVid, a light yet high-quality dataset containing 3K videos from 9 state-of-the-art generators, along with a real-world collected subset that has factual errors in content. Experimental results demonstrate that existing methods tend to bias towards either superficial reasoning or mechanical analysis, while VideoVeritas achieves more balanced performance across diverse benchmarks.

</details>

---

## 157. Any-to-All MRI Synthesis: A Unified Foundation Model for Nasopharyngeal Carcinoma and Its Downstream Applications

**Chinese Title**: Any-to-All MRI Synthesis: A Unified Foundation Model for Nasopharyngeal Carcinoma and Its Downstream Applications

**Authors**: Yao Pu, Yiming Shi, Zhenxi Zhang, Peixin Yu, Yitao Zhuang et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08822v1](http://arxiv.org/abs/2602.08822v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08822v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Magnetic resonance imaging (MRI) is essential for nasopharyngeal carcinoma (NPC) radiotherapy (RT), but practical constraints, such as patient discomfort, long scan times, and high costs often lead to incomplete modalities in clinical practice, compromising RT planning accuracy. Traditional MRI synthesis methods are modality-specific, limited in anatomical adaptability, and lack clinical interpretability-failing to meet NPC's RT needs. Here, we developed a unified foundation model integrating contrastive visual representation learning and vision-language alignment (VLA) to enable any-to-all MRI synthesis. The model uses a contrastive encoder for modality-invariant representations and a CLIP-based text-informed decoder for semantically consistent synthesis, supporting any-to-all MRI synthesis via one unified foundation model. Trained on 40,825 images from 13 institutions, it achieves consistently high performance (average SSIM 0.90, PSNR 27) across 26 internal/external validation sites (15,748 images), with superior synthesis fidelity and robustness to noise and domain shifts. Meanwhile, its unified representation enhances downstream RT-relevant tasks (e.g., segmentation). This work advances digital medicine solutions for NPC care by leveraging foundation models to bridge technical synthesis and clinical utility.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Magnetic resonance imaging (MRI) is essential for nasopharyngeal carcinoma (NPC) radiotherapy (RT), but practical constraints, such as patient discomfort, long scan times, and high costs often lead to incomplete modalities in clinical practice, compromising RT planning accuracy. Traditional MRI synthesis methods are modality-specific, limited in anatomical adaptability, and lack clinical interpretability-failing to meet NPC's RT needs.Here, we developed a unified foundation model integrating contrastive visual representation learning and vision-language alignment (VLA) to enable any-to-all MRI synthesis. The model uses a contrastive encoder for modality-invariant representations and a CLIP-based text-informed decoder for semantically consistent synthesis, supporting any-to-all MRI synthesis via one unified foundation model.Trained on 40,825 images from 13 institutions, it achieves consistently high performance (average SSIM 0.90, PSNR 27) across 26 internal/external validation sites (15,748 images), with superior synthesis fidelity and robustness to noise and domain shifts. Meanwhile, its unified representation enhances downstream RT-relevant tasks (e.g., segmentation).This work advances digital medicine solutions for NPC care by leveraging foundation models to bridge technical synthesis and clinical utility.

</details>

---

## 158. Omni-Video 2: Scaling MLLM-Conditioned Diffusion for Unified Video Generation and Editing

**Chinese Title**: Omni-Video 2: Scaling MLLM-Conditioned Diffusion for Unified Video Generation and Editing

**Authors**: Hao Yang, Zhiyu Tan, Jia Gong, Luozheng Qin, Hesen Chen et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08820v1](http://arxiv.org/abs/2602.08820v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08820v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

We present Omni-Video 2, a scalable and computationally efficient model that connects pretrained multimodal large-language models (MLLMs) with video diffusion models for unified video generation and editing. Our key idea is to exploit the understanding and reasoning capabilities of MLLMs to produce explicit target captions to interpret user instructions. In this way, the rich contextual representations from the understanding model are directly used to guide the generative process, thereby improving performance on complex and compositional editing. Moreover, a lightweight adapter is developed to inject multimodal conditional tokens into pretrained text-to-video diffusion models, allowing maximum reuse of their powerful generative priors in a parameter-efficient manner. Benefiting from these designs, we scale up Omni-Video 2 to a 14B video diffusion model on meticulously curated training data with quality, supporting high quality text-to-video generation and various video editing tasks such as object removal, addition, background change, complex motion editing, \emph{etc.} We evaluate the performance of Omni-Video 2 on the FiVE benchmark for fine-grained video editing and the VBench benchmark for text-to-video generation. The results demonstrate its superior ability to follow complex compositional instructions in video editing, while also achieving competitive or superior quality in video generation tasks.

</details>

<details><summary><b>Chinese Abstract</b></summary>

We present Omni-Video 2, a scalable and computationally efficient model that connects pretrained multimodal large-language models (MLLMs) with video diffusion models for unified video generation and editing. Our key idea is to exploit the understanding and reasoning capabilities of MLLMs to produce explicit target captions to interpret user instructions.In this way, the rich contextual representations from the understanding model are directly used to guide the generative process, thereby improving performance on complex and compositional editing. Moreover, a lightweight adapter is developed to inject multimodal conditional tokens into pretrained text-to-video diffusion models, allowing maximum reuse of their powerful generative priors in a parameter-efficient manner.Benefiting from these designs, we scale up Omni-Video 2 to a 14B video diffusion model on meticulously curated training data with quality, supporting high quality text-to-video generation and various video editing tasks such as object removal, addition, background change, complex motion editing, \emph{etc.} We evaluate the performance of Omni-Video 2 on the FiVE benchmark for fine-grained video editing and the VBench benchmark for text-to-video generation.The results demonstrate its superior ability to follow complex compositional instructions in video editing, while also achieving competitive or superior quality in video generation tasks.

</details>

---

## 159. Addressing data annotation scarcity in Brain Tumor Segmentation on 3D MRI scan Using a Semi-Supervised Teacher-Student Framework

**Chinese Title**: Addressing data annotation scarcity in Brain Tumor Segmentation on 3D MRI scan Using a Semi-Supervised Teacher-Student Framework

**Authors**: Jiaming Liu, Cheng Ding, Daoqiang Zhang

**Date**: 2026-02-09 | **arXiv**: [2602.08797v1](http://arxiv.org/abs/2602.08797v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08797v1)

**Categories**: cs.CV, cs.AI

<details><summary><b>Abstract</b></summary>

Accurate brain tumor segmentation from MRI is limited by expensive annotations and data heterogeneity across scanners and sites. We propose a semi-supervised teacher-student framework that combines an uncertainty-aware pseudo-labeling teacher with a progressive, confidence-based curriculum for the student. The teacher produces probabilistic masks and per-pixel uncertainty; unlabeled scans are ranked by image-level confidence and introduced in stages, while a dual-loss objective trains the student to learn from high-confidence regions and unlearn low-confidence ones. Agreement-based refinement further improves pseudo-label quality. On BraTS 2021, validation DSC increased from 0.393 (10% data) to 0.872 (100%), with the largest gains in early stages, demonstrating data efficiency. The teacher reached a validation DSC of 0.922, and the student surpassed the teacher on tumor subregions (e.g., NCR/NET 0.797 and Edema 0.980); notably, the student recovered the Enhancing class (DSC 0.620) where the teacher failed. These results show that confidence-driven curricula and selective unlearning provide robust segmentation under limited supervision and noisy pseudo-labels.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Accurate brain tumor segmentation from MRI is limited by expensive annotations and data heterogeneity across scanners and sites. We propose a semi-supervised teacher-student framework that combines an uncertainty-aware pseudo-labeling teacher with a progressive, confidence-based curriculum for the student.The teacher produces probabilistic masks and per-pixel uncertainty; unlabeled scans are ranked by image-level confidence and introduced in stages, while a dual-loss objective trains the student to learn from high-confidence regions and unlearn low-confidence ones. Agreement-based refinement further improves pseudo-label quality.On BraTS 2021, validation DSC increased from 0.393 (10% data) to 0.872 (100%), with the largest gains in early stages, demonstrating data efficiency. The teacher reached a validation DSC of 0.922, and the student surpassed the teacher on tumor subregions (e.g., NCR/NET 0.797 and Edema 0.980); notably, the student recovered the Enhancing class (DSC 0.620) where the teacher failed.These results show that confidence-driven curricula and selective unlearning provide robust segmentation under limited supervision and noisy pseudo-labels.

</details>

---

## 160. MOVA: Towards Scalable and Synchronized Video-Audio Generation

**Chinese Title**: MOVA: Towards Scalable and Synchronized Video-Audio Generation

**Authors**: SII-OpenMOSS Team,  :, Donghua Yu, Mingshu Chen, Qi Chen et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08794v2](http://arxiv.org/abs/2602.08794v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.08794v2)

**Categories**: cs.CV, cs.SD

<details><summary><b>Abstract</b></summary>

Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training.Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music.MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.

</details>

---

## 161. Multimodal Learning for Arcing Detection in Pantograph-Catenary Systems

**Chinese Title**: Multimodal Learning for Arcing Detection in Pantograph-Catenary Systems

**Authors**: Hao Dong, Eleni Chatzi, Olga Fink

**Date**: 2026-02-09 | **arXiv**: [2602.08792v1](http://arxiv.org/abs/2602.08792v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08792v1)

**Categories**: cs.CV, cs.AI, cs.LG

<details><summary><b>Abstract</b></summary>

The pantograph-catenary interface is essential for ensuring uninterrupted and reliable power delivery in electrified rail systems. However, electrical arcing at this interface poses serious risks, including accelerated wear of contact components, degraded system performance, and potential service disruptions. Detecting arcing events at the pantograph-catenary interface is challenging due to their transient nature, noisy operating environment, data scarcity, and the difficulty of distinguishing arcs from other similar transient phenomena. To address these challenges, we propose a novel multimodal framework that combines high-resolution image data with force measurements to more accurately and robustly detect arcing events. First, we construct two arcing detection datasets comprising synchronized visual and force measurements. One dataset is built from data provided by the Swiss Federal Railways (SBB), and the other is derived from publicly available videos of arcing events in different railway systems and synthetic force data that mimic the characteristics observed in the real dataset. Leveraging these datasets, we propose MultiDeepSAD, an extension of the DeepSAD algorithm for multiple modalities with a new loss formulation. Additionally, we introduce tailored pseudo-anomaly generation techniques specific to each data type, such as synthetic arc-like artifacts in images and simulated force irregularities, to augment training data and improve the discriminative ability of the model. Through extensive experiments and ablation studies, we demonstrate that our framework significantly outperforms baseline approaches, exhibiting enhanced sensitivity to real arcing events even under domain shifts and limited availability of real arcing observations.

</details>

<details><summary><b>Chinese Abstract</b></summary>

The pantograph-catenary interface is essential for ensuring uninterrupted and reliable power delivery in electrified rail systems. However, electrical arcing at this interface poses serious risks, including accelerated wear of contact components, degraded system performance, and potential service disruptions.Detecting arcing events at the pantograph-catenary interface is challenging due to their transient nature, noisy operating environment, data scarcity, and the difficulty of distinguishing arcs from other similar transient phenomena. To address these challenges, we propose a novel multimodal framework that combines high-resolution image data with force measurements to more accurately and robustly detect arcing events.First, we construct two arcing detection datasets comprising synchronized visual and force measurements. One dataset is built from data provided by the Swiss Federal Railways (SBB), and the other is derived from publicly available videos of arcing events in different railway systems and synthetic force data that mimic the characteristics observed in the real dataset.Leveraging these datasets, we propose MultiDeepSAD, an extension of the DeepSAD algorithm for multiple modalities with a new loss formulation. Additionally, we introduce tailored pseudo-anomaly generation techniques specific to each data type, such as synthetic arc-like artifacts in images and simulated force irregularities, to augment training data and improve the discriminative ability of the model.Through extensive experiments and ablation studies, we demonstrate that our framework significantly outperforms baseline approaches, exhibiting enhanced sensitivity to real arcing events even under domain shifts and limited availability of real arcing observations.

</details>

---

## 162. VedicTHG: Symbolic Vedic Computation for Low-Resource Talking-Head Generation in Educational Avatars

**Chinese Title**: VedicTHG: Symbolic Vedic Computation for Low-Resource Talking-Head Generation in Educational Avatars

**Authors**: Vineet Kumar Rakesh, Ahana Bhattacharjee, Soumya Mazumdar, Tapas Samanta, Hemendra Kumar Pandey et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08775v1](http://arxiv.org/abs/2602.08775v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08775v1)

**Project**: https://vineetkumarrakesh.github.io/vedicthg  **Categories**: cs.CV, cs.CG

<details><summary><b>Abstract</b></summary>

Talking-head avatars are increasingly adopted in educational technology to deliver content with social presence and improved engagement. However, many recent talking-head generation (THG) methods rely on GPU-centric neural rendering, large training sets, or high-capacity diffusion models, which limits deployment in offline or resource-constrained learning environments. A deterministic and CPU-oriented THG framework is described, termed Symbolic Vedic Computation, that converts speech to a time-aligned phoneme stream, maps phonemes to a compact viseme inventory, and produces smooth viseme trajectories through symbolic coarticulation inspired by Vedic sutra Urdhva Tiryakbhyam. A lightweight 2D renderer performs region-of-interest (ROI) warping and mouth compositing with stabilization to support real-time synthesis on commodity CPUs. Experiments report synchronization accuracy, temporal stability, and identity consistency under CPU-only execution, alongside benchmarking against representative CPU-feasible baselines. Results indicate that acceptable lip-sync quality can be achieved while substantially reducing computational load and latency, supporting practical educational avatars on low-end hardware. GitHub: https://vineetkumarrakesh.github.io/vedicthg

</details>

<details><summary><b>Chinese Abstract</b></summary>

Talking-head avatars are increasingly adopted in educational technology to deliver content with social presence and improved engagement. However, many recent talking-head generation (THG) methods rely on GPU-centric neural rendering, large training sets, or high-capacity diffusion models, which limits deployment in offline or resource-constrained learning environments.A deterministic and CPU-oriented THG framework is described, termed Symbolic Vedic Computation, that converts speech to a time-aligned phoneme stream, maps phonemes to a compact viseme inventory, and produces smooth viseme trajectories through symbolic coarticulation inspired by Vedic sutra Urdhva Tiryakbhyam.A lightweight 2D renderer performs region-of-interest (ROI) warping and mouth compositing with stabilization to support real-time synthesis on commodity CPUs. Experiments report synchronization accuracy, temporal stability, and identity consistency under CPU-only execution, alongside benchmarking against representative CPU-feasible baselines.Results indicate that acceptable lip-sync quality can be achieved while substantially reducing computational load and latency, supporting practical educational avatars on low-end hardware. GitHub: https://vineetkumarrakesh.github.io/vedicthg

</details>

---

## 163. MVAnimate: Enhancing Character Animation with Multi-View Optimization

**Chinese Title**: MVAnimate: Enhancing Character Animation with Multi-View Optimization

**Authors**: Tianyu Sun, Zhoujie Fu, Bang Zhang, Guosheng Lin

**Date**: 2026-02-09 | **arXiv**: [2602.08753v1](http://arxiv.org/abs/2602.08753v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08753v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

The demand for realistic and versatile character animation has surged, driven by its wide-ranging applications in various domains. However, the animation generation algorithms modeling human pose with 2D or 3D structures all face various problems, including low-quality output content and training data deficiency, preventing the related algorithms from generating high-quality animation videos. Therefore, we introduce MVAnimate, a novel framework that synthesizes both 2D and 3D information of dynamic figures based on multi-view prior information, to enhance the generated video quality. Our approach leverages multi-view prior information to produce temporally consistent and spatially coherent animation outputs, demonstrating improvements over existing animation methods. Our MVAnimate also optimizes the multi-view videos of the target character, enhancing the video quality from different views. Experimental results on diverse datasets highlight the robustness of our method in handling various motion patterns and appearances.

</details>

<details><summary><b>Chinese Abstract</b></summary>

The demand for realistic and versatile character animation has surged, driven by its wide-ranging applications in various domains. However, the animation generation algorithms modeling human pose with 2D or 3D structures all face various problems, including low-quality output content and training data deficiency, preventing the related algorithms from generating high-quality animation videos.Therefore, we introduce MVAnimate, a novel framework that synthesizes both 2D and 3D information of dynamic figures based on multi-view prior information, to enhance the generated video quality. Our approach leverages multi-view prior information to produce temporally consistent and spatially coherent animation outputs, demonstrating improvements over existing animation methods.Our MVAnimate also optimizes the multi-view videos of the target character, enhancing the video quality from different views. Experimental results on diverse datasets highlight the robustness of our method in handling various motion patterns and appearances.

</details>

---

## 164. Shifting the Breaking Point of Flow Matching for Multi-Instance Editing

**Chinese Title**: Shifting the Breaking Point of Flow Matching for Multi-Instance Editing

**Authors**: Carmine Zaccagnino, Fabio Quattrini, Enis Simsar, Marta TintorÃ© Gazulla, Rita Cucchiara et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08749v2](http://arxiv.org/abs/2602.08749v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.08749v2)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Flow matching models have recently emerged as an efficient alternative to diffusion, especially for text-guided image generation and editing, offering faster inference through continuous-time dynamics. However, existing flow-based editors predominantly support global or single-instruction edits and struggle with multi-instance scenarios, where multiple parts of a reference input must be edited independently without semantic interference. We identify this limitation as a consequence of globally conditioned velocity fields and joint attention mechanisms, which entangle concurrent edits. To address this issue, we introduce Instance-Disentangled Attention, a mechanism that partitions joint attention operations, enforcing binding between instance-specific textual instructions and spatial regions during velocity field estimation. We evaluate our approach on both natural image editing and a newly introduced benchmark of text-dense infographics with region-level editing instructions. Experimental results demonstrate that our approach promotes edit disentanglement and locality while preserving global output coherence, enabling single-pass, instance-level editing.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Flow matching models have recently emerged as an efficient alternative to diffusion, especially for text-guided image generation and editing, offering faster inference through continuous-time dynamics. However, existing flow-based editors predominantly support global or single-instruction edits and struggle with multi-instance scenarios, where multiple parts of a reference input must be edited independently without semantic interference.We identify this limitation as a consequence of globally conditioned velocity fields and joint attention mechanisms, which entangle concurrent edits. To address this issue, we introduce Instance-Disentangled Attention, a mechanism that partitions joint attention operations, enforcing binding between instance-specific textual instructions and spatial regions during velocity field estimation.We evaluate our approach on both natural image editing and a newly introduced benchmark of text-dense infographics with region-level editing instructions. Experimental results demonstrate that our approach promotes edit disentanglement and locality while preserving global output coherence, enabling single-pass, instance-level editing.

</details>

---

## 165. From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models

**Chinese Title**: From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models

**Authors**: Masanari Oi, Koki Maeda, Ryuto Koike, Daisuke Oba, Nakamasa Inoue et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08735v2](http://arxiv.org/abs/2602.08735v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.08735v2)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

While multimodal large language models (MLLMs) have made substantial progress in single-image spatial reasoning, multi-image spatial reasoning, which requires integration of information from multiple viewpoints, remains challenging. Cognitive studies suggest that humans address such tasks through two mechanisms: cross-view correspondence, which identifies regions across different views that correspond to the same physical locations, and stepwise viewpoint transformation, which composes relative viewpoint changes sequentially. However, existing studies incorporate these mechanisms only partially and often implicitly, without explicit supervision for both. We propose Human-Aware Training for Cross-view correspondence and viewpoint cHange (HATCH), a training framework with two complementary objectives: (1) Patch-Level Spatial Alignment, which encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning, which requires the model to generate explicit viewpoint transition actions before predicting the final answer. Experiments on three benchmarks demonstrate that HATCH consistently outperforms baselines of comparable size by a clear margin and achieves competitive results against much larger models, while preserving single-image reasoning capabilities.

</details>

<details><summary><b>Chinese Abstract</b></summary>

While multimodal large language models (MLLMs) have made substantial progress in single-image spatial reasoning, multi-image spatial reasoning, which requires integration of information from multiple viewpoints, remains challenging.Cognitive studies suggest that humans address such tasks through two mechanisms: cross-view correspondence, which identifies regions across different views that correspond to the same physical locations, and stepwise viewpoint transformation, which composes relative viewpoint changes sequentially. However, existing studies incorporate these mechanisms only partially and often implicitly, without explicit supervision for both.We propose Human-Aware Training for Cross-view correspondence and viewpoint cHange (HATCH), a training framework with two complementary objectives: (1) Patch-Level Spatial Alignment, which encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning, which requires the model to generate explicit viewpoint transition actions before predicting the final answer.Experiments on three benchmarks demonstrate that HATCH consistently outperforms baselines of comparable size by a clear margin and achieves competitive results against much larger models, while preserving single-image reasoning capabilities.

</details>

---

## 166. Closing the Confusion Loop: CLIP-Guided Alignment for Source-Free Domain Adaptation

**Chinese Title**: Closing the Confusion Loop: CLIP-Guided Alignment for Source-Free Domain Adaptation

**Authors**: Shanshan Wang, Ziying Feng, Xiaozheng Shen, Xun Yang, Pichao Wang et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08730v1](http://arxiv.org/abs/2602.08730v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08730v1)

**Code**: https://github.com/soloiro/CGA

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Source-Free Domain Adaptation (SFDA) tackles the problem of adapting a pre-trained source model to an unlabeled target domain without accessing any source data, which is quite suitable for the field of data security. Although recent advances have shown that pseudo-labeling strategies can be effective, they often fail in fine-grained scenarios due to subtle inter-class similarities. A critical but underexplored issue is the presence of asymmetric and dynamic class confusion, where visually similar classes are unequally and inconsistently misclassified by the source model. Existing methods typically ignore such confusion patterns, leading to noisy pseudo-labels and poor target discrimination. To address this, we propose CLIP-Guided Alignment(CGA), a novel framework that explicitly models and mitigates class confusion in SFDA. Generally, our method consists of three parts: (1) MCA: detects first directional confusion pairs by analyzing the predictions of the source model in the target domain; (2) MCC: leverages CLIP to construct confusion-aware textual prompts (e.g. a truck that looks like a bus), enabling more context-sensitive pseudo-labeling; and (3) FAM: builds confusion-guided feature banks for both CLIP and the source model and aligns them using contrastive learning to reduce ambiguity in the representation space. Extensive experiments on various datasets demonstrate that CGA consistently outperforms state-of-the-art SFDA methods, with especially notable gains in confusion-prone and fine-grained scenarios. Our results highlight the importance of explicitly modeling inter-class confusion for effective source-free adaptation. Our code can be find at https://github.com/soloiro/CGA

</details>

<details><summary><b>Chinese Abstract</b></summary>

Source-Free Domain Adaptation (SFDA) tackles the problem of adapting a pre-trained source model to an unlabeled target domain without accessing any source data, which is quite suitable for the field of data security. Although recent advances have shown that pseudo-labeling strategies can be effective, they often fail in fine-grained scenarios due to subtle inter-class similarities.A critical but underexplored issue is the presence of asymmetric and dynamic class confusion, where visually similar classes are unequally and inconsistently misclassified by the source model. Existing methods typically ignore such confusion patterns, leading to noisy pseudo-labels and poor target discrimination. To address this, we propose CLIP-Guided Alignment(CGA), a novel framework that explicitly models and mitigates class confusion in SFDA.Generally, our method consists of three parts: (1) MCA: detects first directional confusion pairs by analyzing the predictions of the source model in the target domain; (2) MCC: leverages CLIP to construct confusion-aware textual prompts (e.g.a truck that looks like a bus), enabling more context-sensitive pseudo-labeling; and (3) FAM: builds confusion-guided feature banks for both CLIP and the source model and aligns them using contrastive learning to reduce ambiguity in the representation space.Extensive experiments on various datasets demonstrate that CGA consistently outperforms state-of-the-art SFDA methods, with especially notable gains in confusion-prone and fine-grained scenarios. Our results highlight the importance of explicitly modeling inter-class confusion for effective source-free adaptation. Our code can be find at https://github.com/soloiro/CGA

</details>

---

## 167. SynSacc: A Blender-to-V2E Pipeline for Synthetic Neuromorphic Eye-Movement Data and Sim-to-Real Spiking Model Training

**Chinese Title**: SynSacc: A Blender-to-V2E Pipeline for Synthetic Neuromorphic Eye-Movement Data and Sim-to-Real Spiking Model Training

**Authors**: Khadija Iddrisu, Waseem Shariff, Suzanne Little, Noel OConnor

**Date**: 2026-02-09 | **arXiv**: [2602.08726v1](http://arxiv.org/abs/2602.08726v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08726v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

The study of eye movements, particularly saccades and fixations, are fundamental to understanding the mechanisms of human cognition and perception. Accurate classification of these movements requires sensing technologies capable of capturing rapid dynamics without distortion. Event cameras, also known as Dynamic Vision Sensors (DVS), provide asynchronous recordings of changes in light intensity, thereby eliminating motion blur inherent in conventional frame-based cameras and offering superior temporal resolution and data efficiency. In this study, we introduce a synthetic dataset generated with Blender to simulate saccades and fixations under controlled conditions. Leveraging Spiking Neural Networks (SNNs), we evaluate its robustness by training two architectures and finetuning on real event data. The proposed models achieve up to 0.83 accuracy and maintain consistent performance across varying temporal resolutions, demonstrating stability in eye movement classification. Moreover, the use of SNNs with synthetic event streams yields substantial computational efficiency gains over artificial neural network (ANN) counterparts, underscoring the utility of synthetic data augmentation in advancing event-based vision. All code and datasets associated with this work is available at https: //github.com/Ikhadija-5/SynSacc-Dataset.

</details>

<details><summary><b>Chinese Abstract</b></summary>

The study of eye movements, particularly saccades and fixations, are fundamental to understanding the mechanisms of human cognition and perception. Accurate classification of these movements requires sensing technologies capable of capturing rapid dynamics without distortion.Event cameras, also known as Dynamic Vision Sensors (DVS), provide asynchronous recordings of changes in light intensity, thereby eliminating motion blur inherent in conventional frame-based cameras and offering superior temporal resolution and data efficiency. In this study, we introduce a synthetic dataset generated with Blender to simulate saccades and fixations under controlled conditions.Leveraging Spiking Neural Networks (SNNs), we evaluate its robustness by training two architectures and finetuning on real event data. The proposed models achieve up to 0.83 accuracy and maintain consistent performance across varying temporal resolutions, demonstrating stability in eye movement classification.Moreover, the use of SNNs with synthetic event streams yields substantial computational efficiency gains over artificial neural network (ANN) counterparts, underscoring the utility of synthetic data augmentation in advancing event-based vision. All code and datasets associated with this work is available at https: //github.com/Ikhadija-5/SynSacc-Dataset.

</details>

---

## 168. Zero-shot System for Automatic Body Region Detection for Volumetric CT and MR Images

**Chinese Title**: Zero-shot System for Automatic Body Region Detection for Volumetric CT and MR Images

**Authors**: Farnaz Khun Jush, Grit Werner, Mark Klemens, Matthias Lenga

**Date**: 2026-02-09 | **arXiv**: [2602.08717v1](http://arxiv.org/abs/2602.08717v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08717v1)

**Categories**: cs.CV, cs.AI

<details><summary><b>Abstract</b></summary>

Reliable identification of anatomical body regions is a prerequisite for many automated medical imaging workflows, yet existing solutions remain heavily dependent on unreliable DICOM metadata. Current solutions mainly use supervised learning, which limits their applicability in many real-world scenarios. In this work, we investigate whether body region detection in volumetric CT and MR images can be achieved in a fully zero-shot manner by using knowledge embedded in large pre-trained foundation models. We propose and systematically evaluate three training-free pipelines: (1) a segmentation-driven rule-based system leveraging pre-trained multi-organ segmentation models, (2) a Multimodal Large Language Model (MLLM) guided by radiologist-defined rules, and (3) a segmentation-aware MLLM that combines visual input with explicit anatomical evidence. All methods are evaluated on 887 heterogeneous CT and MR scans with manually verified anatomical region labels. The segmentation-driven rule-based approach achieves the strongest and most consistent performance, with weighted F1-scores of 0.947 (CT) and 0.914 (MR), demonstrating robustness across modalities and atypical scan coverage. The MLLM performs competitively in visually distinctive regions, while the segmentation-aware MLLM reveals fundamental limitations.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Reliable identification of anatomical body regions is a prerequisite for many automated medical imaging workflows, yet existing solutions remain heavily dependent on unreliable DICOM metadata. Current solutions mainly use supervised learning, which limits their applicability in many real-world scenarios.In this work, we investigate whether body region detection in volumetric CT and MR images can be achieved in a fully zero-shot manner by using knowledge embedded in large pre-trained foundation models.We propose and systematically evaluate three training-free pipelines: (1) a segmentation-driven rule-based system leveraging pre-trained multi-organ segmentation models, (2) a Multimodal Large Language Model (MLLM) guided by radiologist-defined rules, and (3) a segmentation-aware MLLM that combines visual input with explicit anatomical evidence.All methods are evaluated on 887 heterogeneous CT and MR scans with manually verified anatomical region labels. The segmentation-driven rule-based approach achieves the strongest and most consistent performance, with weighted F1-scores of 0.947 (CT) and 0.914 (MR), demonstrating robustness across modalities and atypical scan coverage.The MLLM performs competitively in visually distinctive regions, while the segmentation-aware MLLM reveals fundamental limitations.

</details>

---

## 169. TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions

**Chinese Title**: TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions

**Authors**: Linli Yao, Yuancheng Wei, Yaojie Zhang, Lei Li, Xinlong Chen et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08711v1](http://arxiv.org/abs/2602.08711v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08711v1)

**Code**: https://github.com/yaolinli/TimeChat-Captioner.

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

This paper proposes Omni Dense Captioning, a novel task designed to generate continuous, fine-grained, and structured audio-visual narratives with explicit timestamps. To ensure dense semantic coverage, we introduce a six-dimensional structural schema to create "script-like" captions, enabling readers to vividly imagine the video content scene by scene, akin to a cinematographic screenplay. To facilitate research, we construct OmniDCBench, a high-quality, human-annotated benchmark, and propose SodaM, a unified metric that evaluates time-aware detailed descriptions while mitigating scene boundary ambiguity. Furthermore, we construct a training dataset, TimeChatCap-42K, and present TimeChat-Captioner-7B, a strong baseline trained via SFT and GRPO with task-specific rewards. Extensive experiments demonstrate that TimeChat-Captioner-7B achieves state-of-the-art performance, surpassing Gemini-2.5-Pro, while its generated dense descriptions significantly boost downstream capabilities in audio-visual reasoning (DailyOmni and WorldSense) and temporal grounding (Charades-STA). All datasets, models, and code will be made publicly available at https://github.com/yaolinli/TimeChat-Captioner.

</details>

<details><summary><b>Chinese Abstract</b></summary>

This paper proposes Omni Dense Captioning, a novel task designed to generate continuous, fine-grained, and structured audio-visual narratives with explicit timestamps. To ensure dense semantic coverage, we introduce a six-dimensional structural schema to create "script-like" captions, enabling readers to vividly imagine the video content scene by scene, akin to a cinematographic screenplay.To facilitate research, we construct OmniDCBench, a high-quality, human-annotated benchmark, and propose SodaM, a unified metric that evaluates time-aware detailed descriptions while mitigating scene boundary ambiguity. Furthermore, we construct a training dataset, TimeChatCap-42K, and present TimeChat-Captioner-7B, a strong baseline trained via SFT and GRPO with task-specific rewards.Extensive experiments demonstrate that TimeChat-Captioner-7B achieves state-of-the-art performance, surpassing Gemini-2.5-Pro, while its generated dense descriptions significantly boost downstream capabilities in audio-visual reasoning (DailyOmni and WorldSense) and temporal grounding (Charades-STA). All datasets, models, and code will be made publicly available at https://github.com/yaolinli/TimeChat-Captioner.

</details>

---

## 170. Low-Light Video Enhancement with An Effective Spatial-Temporal Decomposition Paradigm

**Chinese Title**: Low-Light Video Enhancement with An Effective Spatial-Temporal Decomposition Paradigm

**Authors**: Xiaogang Xu, Kun Zhou, Tao Hu, Jiafei Wu, Ruixing Wang et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08699v1](http://arxiv.org/abs/2602.08699v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08699v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Low-Light Video Enhancement (LLVE) seeks to restore dynamic or static scenes plagued by severe invisibility and noise. In this paper, we present an innovative video decomposition strategy that incorporates view-independent and view-dependent components to enhance the performance of LLVE. The framework is called View-aware Low-light Video Enhancement (VLLVE). We leverage dynamic cross-frame correspondences for the view-independent term (which primarily captures intrinsic appearance) and impose a scene-level continuity constraint on the view-dependent term (which mainly describes the shading condition) to achieve consistent and satisfactory decomposition results. To further ensure consistent decomposition, we introduce a dual-structure enhancement network featuring a cross-frame interaction mechanism. By supervising different frames simultaneously, this network encourages them to exhibit matching decomposition features. This mechanism can seamlessly integrate with encoder-decoder single-frame networks, incurring minimal additional parameter costs. Building upon VLLVE, we propose a more comprehensive decomposition strategy by introducing an additive residual term, resulting in VLLVE++. This residual term can simulate scene-adaptive degradations, which are difficult to model using a decomposition formulation for common scenes, thereby further enhancing the ability to capture the overall content of videos. In addition, VLLVE++ enables bidirectional learning for both enhancement and degradation-aware correspondence refinement (end-to-end manner), effectively increasing reliable correspondences while filtering out incorrect ones. Notably, VLLVE++ demonstrates strong capability in handling challenging cases, such as real-world scenes and videos with high dynamics. Extensive experiments are conducted on widely recognized LLVE benchmarks.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Low-Light Video Enhancement (LLVE) seeks to restore dynamic or static scenes plagued by severe invisibility and noise. In this paper, we present an innovative video decomposition strategy that incorporates view-independent and view-dependent components to enhance the performance of LLVE. The framework is called View-aware Low-light Video Enhancement (VLLVE).We leverage dynamic cross-frame correspondences for the view-independent term (which primarily captures intrinsic appearance) and impose a scene-level continuity constraint on the view-dependent term (which mainly describes the shading condition) to achieve consistent and satisfactory decomposition results. To further ensure consistent decomposition, we introduce a dual-structure enhancement network featuring a cross-frame interaction mechanism.By supervising different frames simultaneously, this network encourages them to exhibit matching decomposition features. This mechanism can seamlessly integrate with encoder-decoder single-frame networks, incurring minimal additional parameter costs. Building upon VLLVE, we propose a more comprehensive decomposition strategy by introducing an additive residual term, resulting in VLLVE++.This residual term can simulate scene-adaptive degradations, which are difficult to model using a decomposition formulation for common scenes, thereby further enhancing the ability to capture the overall content of videos. In addition, VLLVE++ enables bidirectional learning for both enhancement and degradation-aware correspondence refinement (end-to-end manner), effectively increasing reliable correspondences while filtering out incorrect ones.Notably, VLLVE++ demonstrates strong capability in handling challenging cases, such as real-world scenes and videos with high dynamics. Extensive experiments are conducted on widely recognized LLVE benchmarks.

</details>

---

## 171. OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence

**Chinese Title**: OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence

**Authors**: Feilong Tang, Xiang An, Yunyao Yan, Yin Xie, Bin Qin et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08683v1](http://arxiv.org/abs/2602.08683v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08683v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Hypothesis. Artificial general intelligence is, at its core, a compression problem. Effective compression demands resonance: deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information, the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs.   Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification, OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts, jointly capturing object permanence and motion dynamics.   Evidence. The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM, it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data. Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Hypothesis. Artificial general intelligence is, at its core, a compression problem. Effective compression demands resonance: deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information, the surprise, is sparse.Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs. Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning.By adopting Codec Patchification, OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts, jointly capturing object permanence and motion dynamics. Evidence.The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM, it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data.Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists.

</details>

---

## 172. ALIVE: Animate Your World with Lifelike Audio-Video Generation

**Chinese Title**: ALIVE: Animate Your World with Lifelike Audio-Video Generation

**Authors**: Ying Guo, Qijun Gan, Yifu Zhang, Jinlai Liu, Yifei Hu et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08682v2](http://arxiv.org/abs/2602.08682v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.08682v2)

**Code**: https://github.com/FoundationVision/Alive.

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Video generation is rapidly evolving towards unified audio-video generation. In this paper, we present ALIVE, a generation model that adapts a pretrained Text-to-Video (T2V) model to Sora-style audio-video generation and animation. In particular, the model unlocks the Text-to-Video&Audio (T2VA) and Reference-to-Video&Audio (animation) capabilities compared to the T2V foundation models. To support the audio-visual synchronization and reference animation, we augment the popular MMDiT architecture with a joint audio-video branch which includes TA-CrossAttn for temporally-aligned cross-modal fusion and UniTemp-RoPE for precise audio-visual alignment. Meanwhile, a comprehensive data pipeline consisting of audio-video captioning, quality control, etc., is carefully designed to collect high-quality finetuning data. Additionally, we introduce a new benchmark to perform a comprehensive model test and comparison. After continue pretraining and finetuning on million-level high-quality data, ALIVE demonstrates outstanding performance, consistently outperforming open-source models and matching or surpassing state-of-the-art commercial solutions. With detailed recipes and benchmarks, we hope ALIVE helps the community develop audio-video generation models more efficiently. Official page: https://github.com/FoundationVision/Alive.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Video generation is rapidly evolving towards unified audio-video generation. In this paper, we present ALIVE, a generation model that adapts a pretrained Text-to-Video (T2V) model to Sora-style audio-video generation and animation. In particular, the model unlocks the Text-to-Video&Audio (T2VA) and Reference-to-Video&Audio (animation) capabilities compared to the T2V foundation models.To support the audio-visual synchronization and reference animation, we augment the popular MMDiT architecture with a joint audio-video branch which includes TA-CrossAttn for temporally-aligned cross-modal fusion and UniTemp-RoPE for precise audio-visual alignment. Meanwhile, a comprehensive data pipeline consisting of audio-video captioning, quality control, etc., is carefully designed to collect high-quality finetuning data.Additionally, we introduce a new benchmark to perform a comprehensive model test and comparison. After continue pretraining and finetuning on million-level high-quality data, ALIVE demonstrates outstanding performance, consistently outperforming open-source models and matching or surpassing state-of-the-art commercial solutions.With detailed recipes and benchmarks, we hope ALIVE helps the community develop audio-video generation models more efficiently. Official page: https://github.com/FoundationVision/Alive.

</details>

---

## 173. WiFlow: A Lightweight WiFi-based Continuous Human Pose Estimation Network with Spatio-Temporal Feature Decoupling

**Chinese Title**: WiFlow: A Lightweight WiFi-based Continuous Human Pose Estimation Network with Spatio-Temporal Feature Decoupling

**Authors**: Yi Dao, Lankai Zhang, Hao Liu, Haiwei Zhang, Wenbo Wang

**Date**: 2026-02-09 | **arXiv**: [2602.08661v1](http://arxiv.org/abs/2602.08661v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08661v1)

**Code**: https://github.com/DY2434/WiFlow-WiFi-Pose-Estimation-with-Spatio-Temporal-Decoupling.git.

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Human pose estimation is fundamental to intelligent perception in the Internet of Things (IoT), enabling applications ranging from smart healthcare to human-computer interaction. While WiFi-based methods have gained traction, they often struggle with continuous motion and high computational overhead. This work presents WiFlow, a novel framework for continuous human pose estimation using WiFi signals. Unlike vision-based approaches such as two-dimensional deep residual networks that treat Channel State Information (CSI) as images, WiFlow employs an encoder-decoder architecture. The encoder captures spatio-temporal features of CSI using temporal and asymmetric convolutions, preserving the original sequential structure of signals. It then refines keypoint features of human bodies to be tracked and capture their structural dependencies via axial attention. The decoder subsequently maps the encoded high-dimensional features into keypoint coordinates. Trained on a self-collected dataset of 360,000 synchronized CSI-pose samples from 5 subjects performing continuous sequences of 8 daily activities, WiFlow achieves a Percentage of Correct Keypoints (PCK) of 97.00% at a threshold of 20% (PCK@20) and 99.48% at PCK@50, with a mean per-joint position error of 0.008m. With only 4.82M parameters, WiFlow significantly reduces model complexity and computational cost, establishing a new performance baseline for practical WiFi-based human pose estimation. Our code and datasets are available at https://github.com/DY2434/WiFlow-WiFi-Pose-Estimation-with-Spatio-Temporal-Decoupling.git.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Human pose estimation is fundamental to intelligent perception in the Internet of Things (IoT), enabling applications ranging from smart healthcare to human-computer interaction. While WiFi-based methods have gained traction, they often struggle with continuous motion and high computational overhead. This work presents WiFlow, a novel framework for continuous human pose estimation using WiFi signals.Unlike vision-based approaches such as two-dimensional deep residual networks that treat Channel State Information (CSI) as images, WiFlow employs an encoder-decoder architecture. The encoder captures spatio-temporal features of CSI using temporal and asymmetric convolutions, preserving the original sequential structure of signals.It then refines keypoint features of human bodies to be tracked and capture their structural dependencies via axial attention. The decoder subsequently maps the encoded high-dimensional features into keypoint coordinates.Trained on a self-collected dataset of 360,000 synchronized CSI-pose samples from 5 subjects performing continuous sequences of 8 daily activities, WiFlow achieves a Percentage of Correct Keypoints (PCK) of 97.00% at a threshold of 20% (PCK@20) and 99.48% at PCK@50, with a mean per-joint position error of 0.008m.With only 4.82M parameters, WiFlow significantly reduces model complexity and computational cost, establishing a new performance baseline for practical WiFi-based human pose estimation. Our code and datasets are available at https://github.com/DY2434/WiFlow-WiFi-Pose-Estimation-with-Spatio-Temporal-Decoupling.git.

</details>

---

## 174. Revisiting [CLS] and Patch Token Interaction in Vision Transformers

**Chinese Title**: Revisiting [CLS] and Patch Token Interaction in Vision Transformers

**Authors**: Alexis Marouani, Oriane SimÃ©oni, HervÃ© JÃ©gou, Piotr Bojanowski, Huy V. Vo

**Date**: 2026-02-09 | **arXiv**: [2602.08626v1](http://arxiv.org/abs/2602.08626v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08626v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Vision Transformers have emerged as powerful, scalable and versatile representation learners. To capture both global and local features, a learnable [CLS] class token is typically prepended to the input sequence of patch tokens. Despite their distinct nature, both token types are processed identically throughout the model. In this work, we investigate the friction between global and local feature learning under different pre-training strategies by analyzing the interactions between class and patch tokens. Our analysis reveals that standard normalization layers introduce an implicit differentiation between these token types. Building on this insight, we propose specialized processing paths that selectively disentangle the computational flow of class and patch tokens, particularly within normalization layers and early query-key-value projections. This targeted specialization leads to significantly improved patch representation quality for dense prediction tasks. Our experiments demonstrate segmentation performance gains of over 2 mIoU points on standard benchmarks, while maintaining strong classification accuracy. The proposed modifications introduce only an 8% increase in parameters, with no additional computational overhead. Through comprehensive ablations, we provide insights into which architectural components benefit most from specialization and how our approach generalizes across model scales and learning frameworks.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Vision Transformers have emerged as powerful, scalable and versatile representation learners. To capture both global and local features, a learnable [CLS] class token is typically prepended to the input sequence of patch tokens. Despite their distinct nature, both token types are processed identically throughout the model.In this work, we investigate the friction between global and local feature learning under different pre-training strategies by analyzing the interactions between class and patch tokens. Our analysis reveals that standard normalization layers introduce an implicit differentiation between these token types.Building on this insight, we propose specialized processing paths that selectively disentangle the computational flow of class and patch tokens, particularly within normalization layers and early query-key-value projections. This targeted specialization leads to significantly improved patch representation quality for dense prediction tasks.Our experiments demonstrate segmentation performance gains of over 2 mIoU points on standard benchmarks, while maintaining strong classification accuracy. The proposed modifications introduce only an 8% increase in parameters, with no additional computational overhead.Through comprehensive ablations, we provide insights into which architectural components benefit most from specialization and how our approach generalizes across model scales and learning frameworks.

</details>

---

## 175. Improving Reconstruction of Representation Autoencoder

**Chinese Title**: Improving Reconstruction of Representation Autoencoder

**Authors**: Siyu Liu, Chujie Qin, Hubery Yin, Qixin Yan, Zheng-Peng Duan et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08620v1](http://arxiv.org/abs/2602.08620v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08620v1)

**Code**: https://github.com/modyu-liu/LVRAE.

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Recent work leverages Vision Foundation Models as image encoders to boost the generative performance of latent diffusion models (LDMs), as their semantic feature distributions are easy to learn. However, such semantic features often lack low-level information (\eg, color and texture), leading to degraded reconstruction fidelity, which has emerged as a primary bottleneck in further scaling LDMs. To address this limitation, we propose LV-RAE, a representation autoencoder that augments semantic features with missing low-level information, enabling high-fidelity reconstruction while remaining highly aligned with the semantic distribution. We further observe that the resulting high-dimensional, information-rich latent make decoders sensitive to latent perturbations, causing severe artifacts when decoding generated latent and consequently degrading generation quality. Our analysis suggests that this sensitivity primarily stems from excessive decoder responses along directions off the data manifold. Building on these insights, we propose fine-tuning the decoder to increase its robustness and smoothing the generated latent via controlled noise injection, thereby enhancing generation quality. Experiments demonstrate that LV-RAE significantly improves reconstruction fidelity while preserving the semantic abstraction and achieving strong generative quality. Our code is available at https://github.com/modyu-liu/LVRAE.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Recent work leverages Vision Foundation Models as image encoders to boost the generative performance of latent diffusion models (LDMs), as their semantic feature distributions are easy to learn. However, such semantic features often lack low-level information (\eg, color and texture), leading to degraded reconstruction fidelity, which has emerged as a primary bottleneck in further scaling LDMs.To address this limitation, we propose LV-RAE, a representation autoencoder that augments semantic features with missing low-level information, enabling high-fidelity reconstruction while remaining highly aligned with the semantic distribution.We further observe that the resulting high-dimensional, information-rich latent make decoders sensitive to latent perturbations, causing severe artifacts when decoding generated latent and consequently degrading generation quality. Our analysis suggests that this sensitivity primarily stems from excessive decoder responses along directions off the data manifold.Building on these insights, we propose fine-tuning the decoder to increase its robustness and smoothing the generated latent via controlled noise injection, thereby enhancing generation quality. Experiments demonstrate that LV-RAE significantly improves reconstruction fidelity while preserving the semantic abstraction and achieving strong generative quality. Our code is available at https://github.com/modyu-liu/LVRAE.

</details>

---

## 176. Inspiration Seeds: Learning Non-Literal Visual Combinations for Generative Exploration

**Chinese Title**: Inspiration Seeds: Learning Non-Literal Visual Combinations for Generative Exploration

**Authors**: Kfir Goldberg, Elad Richardson, Yael Vinker

**Date**: 2026-02-09 | **arXiv**: [2602.08615v1](http://arxiv.org/abs/2602.08615v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08615v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

While generative models have become powerful tools for image synthesis, they are typically optimized for executing carefully crafted textual prompts, offering limited support for the open-ended visual exploration that often precedes idea formation. In contrast, designers frequently draw inspiration from loosely connected visual references, seeking emergent connections that spark new ideas. We propose Inspiration Seeds, a generative framework that shifts image generation from final execution to exploratory ideation. Given two input images, our model produces diverse, visually coherent compositions that reveal latent relationships between inputs, without relying on user-specified text prompts. Our approach is feed-forward, trained on synthetic triplets of decomposed visual aspects derived entirely through visual means: we use CLIP Sparse Autoencoders to extract editing directions in CLIP latent space and isolate concept pairs. By removing the reliance on language and enabling fast, intuitive recombination, our method supports visual ideation at the early and ambiguous stages of creative work.

</details>

<details><summary><b>Chinese Abstract</b></summary>

While generative models have become powerful tools for image synthesis, they are typically optimized for executing carefully crafted textual prompts, offering limited support for the open-ended visual exploration that often precedes idea formation. In contrast, designers frequently draw inspiration from loosely connected visual references, seeking emergent connections that spark new ideas.We propose Inspiration Seeds, a generative framework that shifts image generation from final execution to exploratory ideation. Given two input images, our model produces diverse, visually coherent compositions that reveal latent relationships between inputs, without relying on user-specified text prompts.Our approach is feed-forward, trained on synthetic triplets of decomposed visual aspects derived entirely through visual means: we use CLIP Sparse Autoencoders to extract editing directions in CLIP latent space and isolate concept pairs. By removing the reliance on language and enabling fast, intuitive recombination, our method supports visual ideation at the early and ambiguous stages of creative work.

</details>

---

## 177. Overview and Comparison of AVS Point Cloud Compression Standard

**Chinese Title**: Overview and Comparison of AVS Point Cloud Compression Standard

**Authors**: Wei Gao, Wenxu Gao, Xingming Mu, Changhao Peng, Ge Li

**Date**: 2026-02-09 | **arXiv**: [2602.08613v1](http://arxiv.org/abs/2602.08613v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08613v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Point cloud is a prevalent 3D data representation format with significant application values in immersive media, autonomous driving, digital heritage protection, etc. However, the large data size of point clouds poses challenges to transmission and storage, which influences the wide deployments. Therefore, point cloud compression plays a crucial role in practical applications for both human and machine perception optimization. To this end, the Moving Picture Experts Group (MPEG) has established two standards for point cloud compression, including Geometry-based Point Cloud Compression (G-PCC) and Video-based Point Cloud Compression (V-PCC). In the meantime, the Audio Video coding Standard (AVS) Workgroup of China also have launched and completed the development for its first generation point cloud compression standard, namely AVS PCC. This new standardization effort has adopted many new coding tools and techniques, which are different from the other counterpart standards. This paper reviews the AVS PCC standard from two perspectives, i.e., the related technologies and performance comparisons.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Point cloud is a prevalent 3D data representation format with significant application values in immersive media, autonomous driving, digital heritage protection, etc. However, the large data size of point clouds poses challenges to transmission and storage, which influences the wide deployments. Therefore, point cloud compression plays a crucial role in practical applications for both human and machine perception optimization.To this end, the Moving Picture Experts Group (MPEG) has established two standards for point cloud compression, including Geometry-based Point Cloud Compression (G-PCC) and Video-based Point Cloud Compression (V-PCC). In the meantime, the Audio Video coding Standard (AVS) Workgroup of China also have launched and completed the development for its first generation point cloud compression standard, namely AVS PCC.This new standardization effort has adopted many new coding tools and techniques, which are different from the other counterpart standards. This paper reviews the AVS PCC standard from two perspectives, i.e., the related technologies and performance comparisons.

</details>

---

## 178. SemiNFT: Learning to Transfer Presets from Imitation to Appreciation via Hybrid-Sample Reinforcement Learning

**Chinese Title**: SemiNFT: Learning to Transfer Presets from Imitation to Appreciation via Hybrid-Sample Reinforcement Learning

**Authors**: Melany Yang, Yuhang Yu, Diwang Weng, Jinwei Chen, Wei Dong

**Date**: 2026-02-09 | **arXiv**: [2602.08582v1](http://arxiv.org/abs/2602.08582v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08582v1)

**Project**: https://melanyyang.github.io/SemiNFT/.  **Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Photorealistic color retouching plays a vital role in visual content creation, yet manual retouching remains inaccessible to non-experts due to its reliance on specialized expertise. Reference-based methods offer a promising alternative by transferring the preset color of a reference image to a source image. However, these approaches often operate as novice learners, performing global color mappings derived from pixel-level statistics, without a true understanding of semantic context or human aesthetics. To address this issue, we propose SemiNFT, a Diffusion Transformer (DiT)-based retouching framework that mirrors the trajectory of human artistic training: beginning with rigid imitation and evolving into intuitive creation. Specifically, SemiNFT is first taught with paired triplets to acquire basic structural preservation and color mapping skills, and then advanced to reinforcement learning (RL) on unpaired data to cultivate nuanced aesthetic perception. Crucially, during the RL stage, to prevent catastrophic forgetting of old skills, we design a hybrid online-offline reward mechanism that anchors aesthetic exploration with structural review. % experiments Extensive experiments show that SemiNFT not only outperforms state-of-the-art methods on standard preset transfer benchmarks but also demonstrates remarkable intelligence in zero-shot tasks, such as black-and-white photo colorization and cross-domain (anime-to-photo) preset transfer. These results confirm that SemiNFT transcends simple statistical matching and achieves a sophisticated level of aesthetic comprehension. Our project can be found at https://melanyyang.github.io/SemiNFT/.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Photorealistic color retouching plays a vital role in visual content creation, yet manual retouching remains inaccessible to non-experts due to its reliance on specialized expertise. Reference-based methods offer a promising alternative by transferring the preset color of a reference image to a source image.However, these approaches often operate as novice learners, performing global color mappings derived from pixel-level statistics, without a true understanding of semantic context or human aesthetics. To address this issue, we propose SemiNFT, a Diffusion Transformer (DiT)-based retouching framework that mirrors the trajectory of human artistic training: beginning with rigid imitation and evolving into intuitive creation.Specifically, SemiNFT is first taught with paired triplets to acquire basic structural preservation and color mapping skills, and then advanced to reinforcement learning (RL) on unpaired data to cultivate nuanced aesthetic perception. Crucially, during the RL stage, to prevent catastrophic forgetting of old skills, we design a hybrid online-offline reward mechanism that anchors aesthetic exploration with structural review.% experiments Extensive experiments show that SemiNFT not only outperforms state-of-the-art methods on standard preset transfer benchmarks but also demonstrates remarkable intelligence in zero-shot tasks, such as black-and-white photo colorization and cross-domain (anime-to-photo) preset transfer. These results confirm that SemiNFT transcends simple statistical matching and achieves a sophisticated level of aesthetic comprehension.Our project can be found at https://melanyyang.github.io/SemiNFT/.

</details>

---

## 179. retinalysis-vascx: An explainable software toolbox for the extraction of retinal vascular biomarkers

**Chinese Title**: retinalysis-vascx: An explainable software toolbox for the extraction of retinal vascular biomarkers

**Authors**: Jose D. Vargas Quiros, Michael J. Beyeler, Sofia Ortin Vela, EyeNED Reading Center, Sven Bergmann et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08580v1](http://arxiv.org/abs/2602.08580v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08580v1)

**Categories**: q-bio.TO, cs.CV

<details><summary><b>Abstract</b></summary>

The automatic extraction of retinal vascular biomarkers from color fundus images (CFI) is essential for large-scale studies of the retinal vasculature. We present VascX, an open-source Python toolbox designed for the automated extraction of biomarkers from artery and vein segmentations. The VascX workflow processes vessel segmentation masks into skeletons to build undirected and directed vessel graphs, which are then used to resolve segments into continuous vessels. This architecture enables the calculation of a comprehensive suite of biomarkers, including vascular density, bifurcation angles, central retinal equivalents (CREs), tortuosity, and temporal angles, alongside image quality metrics.   A distinguishing feature of VascX is its region awareness; by utilizing the fovea, optic disc, and CFI boundaries as anatomical landmarks, the tool ensures spatially standardized measurements and identifies when specific biomarkers are not computable. Spatially localized biomarkers are calculated over grids relative to these landmarks, facilitating precise clinical analysis. Released via GitHub and PyPI, VascX provides an explainable and modifiable framework that supports reproducible vascular research through integrated visualizations. By enabling the rapid extraction of established biomarkers and the development of new ones, VascX advances the field of oculomics, offering a robust, computationally efficient solution for scalable deployment in large-scale clinical and epidemiological databases.

</details>

<details><summary><b>Chinese Abstract</b></summary>

The automatic extraction of retinal vascular biomarkers from color fundus images (CFI) is essential for large-scale studies of the retinal vasculature. We present VascX, an open-source Python toolbox designed for the automated extraction of biomarkers from artery and vein segmentations.The VascX workflow processes vessel segmentation masks into skeletons to build undirected and directed vessel graphs, which are then used to resolve segments into continuous vessels. This architecture enables the calculation of a comprehensive suite of biomarkers, including vascular density, bifurcation angles, central retinal equivalents (CREs), tortuosity, and temporal angles, alongside image quality metrics.A distinguishing feature of VascX is its region awareness; by utilizing the fovea, optic disc, and CFI boundaries as anatomical landmarks, the tool ensures spatially standardized measurements and identifies when specific biomarkers are not computable. Spatially localized biomarkers are calculated over grids relative to these landmarks, facilitating precise clinical analysis.Released via GitHub and PyPI, VascX provides an explainable and modifiable framework that supports reproducible vascular research through integrated visualizations. By enabling the rapid extraction of established biomarkers and the development of new ones, VascX advances the field of oculomics, offering a robust, computationally efficient solution for scalable deployment in large-scale clinical and epidemiological databases.

</details>

---

## 180. FLAG-4D: Flow-Guided Local-Global Dual-Deformation Model for 4D Reconstruction

**Chinese Title**: FLAG-4D: Flow-Guided Local-Global Dual-Deformation Model for 4D Reconstruction

**Authors**: Guan Yuan Tan, Ngoc Tuan Vu, Arghya Pal, Sailaja Rajanala, Raphael Phan C. -W. et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08558v1](http://arxiv.org/abs/2602.08558v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08558v1)

**Categories**: cs.CV, cs.GT

<details><summary><b>Abstract</b></summary>

We introduce FLAG-4D, a novel framework for generating novel views of dynamic scenes by reconstructing how 3D Gaussian primitives evolve through space and time. Existing methods typically rely on a single Multilayer Perceptron (MLP) to model temporal deformations, and they often struggle to capture complex point motions and fine-grained dynamic details consistently over time, especially from sparse input views. Our approach, FLAG-4D, overcomes this by employing a dual-deformation network that dynamically warps a canonical set of 3D Gaussians over time into new positions and anisotropic shapes. This dual-deformation network consists of an Instantaneous Deformation Network (IDN) for modeling fine-grained, local deformations and a Global Motion Network (GMN) for capturing long-range dynamics, refined through mutual learning. To ensure these deformations are both accurate and temporally smooth, FLAG-4D incorporates dense motion features from a pretrained optical flow backbone. We fuse these motion cues from adjacent timeframes and use a deformation-guided attention mechanism to align this flow information with the current state of each evolving 3D Gaussian. Extensive experiments demonstrate that FLAG-4D achieves higher-fidelity and more temporally coherent reconstructions with finer detail preservation than state-of-the-art methods.

</details>

<details><summary><b>Chinese Abstract</b></summary>

We introduce FLAG-4D, a novel framework for generating novel views of dynamic scenes by reconstructing how 3D Gaussian primitives evolve through space and time. Existing methods typically rely on a single Multilayer Perceptron (MLP) to model temporal deformations, and they often struggle to capture complex point motions and fine-grained dynamic details consistently over time, especially from sparse input views.Our approach, FLAG-4D, overcomes this by employing a dual-deformation network that dynamically warps a canonical set of 3D Gaussians over time into new positions and anisotropic shapes. This dual-deformation network consists of an Instantaneous Deformation Network (IDN) for modeling fine-grained, local deformations and a Global Motion Network (GMN) for capturing long-range dynamics, refined through mutual learning.To ensure these deformations are both accurate and temporally smooth, FLAG-4D incorporates dense motion features from a pretrained optical flow backbone. We fuse these motion cues from adjacent timeframes and use a deformation-guided attention mechanism to align this flow information with the current state of each evolving 3D Gaussian.Extensive experiments demonstrate that FLAG-4D achieves higher-fidelity and more temporally coherent reconstructions with finer detail preservation than state-of-the-art methods.

</details>

---

## 181. GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing

**Chinese Title**: GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing

**Authors**: Shih-Fang Chen, Jun-Cheng Chen, I-Hong Jhuo, Yen-Yu Lin

**Date**: 2026-02-09 | **arXiv**: [2602.08550v1](http://arxiv.org/abs/2602.08550v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08550v1)

**Categories**: cs.CV, cs.AI, cs.LG, cs.MM, eess.IV

<details><summary><b>Abstract</b></summary>

Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geometric cues, which makes them susceptible to partial occlusion, distractors, and variations in geometry and appearance. To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to enable geometric cue inference from only a few 2D images. To tackle the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing with null-space constrained updates that incorporate geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios. Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geometric cues, which makes them susceptible to partial occlusion, distractors, and variations in geometry and appearance.To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to enable geometric cue inference from only a few 2D images.To tackle the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing with null-space constrained updates that incorporate geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios.Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking.

</details>

---

## 182. TIBR4D: Tracing-Guided Iterative Boundary Refinement for Efficient 4D Gaussian Segmentation

**Chinese Title**: TIBR4D: Tracing-Guided Iterative Boundary Refinement for Efficient 4D Gaussian Segmentation

**Authors**: He Wu, Xia Yan, Yanghui Xu, Liegang Xia, Jiazhou Chen

**Date**: 2026-02-09 | **arXiv**: [2602.08540v1](http://arxiv.org/abs/2602.08540v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08540v1)

**Categories**: cs.CV, cs.GR

<details><summary><b>Abstract</b></summary>

Object-level segmentation in dynamic 4D Gaussian scenes remains challenging due to complex motion, occlusions, and ambiguous boundaries. In this paper, we present an efficient learning-free 4D Gaussian segmentation framework that lifts video segmentation masks to 4D spaces, whose core is a two-stage iterative boundary refinement, TIBR4D. The first stage is an Iterative Gaussian Instance Tracing (IGIT) at the temporal segment level. It progressively refines Gaussian-to-instance probabilities through iterative tracing, and extracts corresponding Gaussian point clouds that better handle occlusions and preserve completeness of object structures compared to existing one-shot threshold-based methods. The second stage is a frame-wise Gaussian Rendering Range Control (RCC) via suppressing highly uncertain Gaussians near object boundaries while retaining their core contributions for more accurate boundaries. Furthermore, a temporal segmentation merging strategy is proposed for IGIT to balance identity consistency and dynamic awareness. Longer segments enforce stronger multi-frame constraints for stable identities, while shorter segments allow identity changes to be captured promptly. Experiments on HyperNeRF and Neu3D demonstrate that our method produces accurate object Gaussian point clouds with clearer boundaries and higher efficiency compared to SOTA methods.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Object-level segmentation in dynamic 4D Gaussian scenes remains challenging due to complex motion, occlusions, and ambiguous boundaries. In this paper, we present an efficient learning-free 4D Gaussian segmentation framework that lifts video segmentation masks to 4D spaces, whose core is a two-stage iterative boundary refinement, TIBR4D. The first stage is an Iterative Gaussian Instance Tracing (IGIT) at the temporal segment level.It progressively refines Gaussian-to-instance probabilities through iterative tracing, and extracts corresponding Gaussian point clouds that better handle occlusions and preserve completeness of object structures compared to existing one-shot threshold-based methods.The second stage is a frame-wise Gaussian Rendering Range Control (RCC) via suppressing highly uncertain Gaussians near object boundaries while retaining their core contributions for more accurate boundaries. Furthermore, a temporal segmentation merging strategy is proposed for IGIT to balance identity consistency and dynamic awareness.Longer segments enforce stronger multi-frame constraints for stable identities, while shorter segments allow identity changes to be captured promptly. Experiments on HyperNeRF and Neu3D demonstrate that our method produces accurate object Gaussian point clouds with clearer boundaries and higher efficiency compared to SOTA methods.

</details>

---

## 183. Thegra: Graph-based SLAM for Thermal Imagery

**Chinese Title**: Thegra: Graph-based SLAM for Thermal Imagery

**Authors**: Anastasiia Kornilova, Ivan Moskalenko, Arabella Gromova, Gonzalo Ferrer, Alexander Menshchikov

**Date**: 2026-02-09 | **arXiv**: [2602.08531v1](http://arxiv.org/abs/2602.08531v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08531v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Thermal imaging provides a practical sensing modality for visual SLAM in visually degraded environments such as low illumination, smoke, or adverse weather. However, thermal imagery often exhibits low texture, low contrast, and high noise, complicating feature-based SLAM. In this work, we propose a sparse monocular graph-based SLAM system for thermal imagery that leverages general-purpose learned features -- the SuperPoint detector and LightGlue matcher, trained on large-scale visible-spectrum data to improve cross-domain generalization. To adapt these components to thermal data, we introduce a preprocessing pipeline to enhance input suitability and modify core SLAM modules to handle sparse and outlier-prone feature matches. We further incorporate keypoint confidence scores from SuperPoint into a confidence-weighted factor graph to improve estimation robustness. Evaluations on public thermal datasets demonstrate that the proposed system achieves reliable performance without requiring dataset-specific training or fine-tuning a desired feature detector, given the scarcity of quality thermal data. Code will be made available upon publication.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Thermal imaging provides a practical sensing modality for visual SLAM in visually degraded environments such as low illumination, smoke, or adverse weather. However, thermal imagery often exhibits low texture, low contrast, and high noise, complicating feature-based SLAM.In this work, we propose a sparse monocular graph-based SLAM system for thermal imagery that leverages general-purpose learned features -- the SuperPoint detector and LightGlue matcher, trained on large-scale visible-spectrum data to improve cross-domain generalization.To adapt these components to thermal data, we introduce a preprocessing pipeline to enhance input suitability and modify core SLAM modules to handle sparse and outlier-prone feature matches. We further incorporate keypoint confidence scores from SuperPoint into a confidence-weighted factor graph to improve estimation robustness.Evaluations on public thermal datasets demonstrate that the proposed system achieves reliable performance without requiring dataset-specific training or fine-tuning a desired feature detector, given the scarcity of quality thermal data. Code will be made available upon publication.

</details>

---

## 184. GeoFocus: Blending Efficient Global-to-Local Perception for Multimodal Geometry Problem-Solving

**Chinese Title**: GeoFocus: Blending Efficient Global-to-Local Perception for Multimodal Geometry Problem-Solving

**Authors**: Linger Deng, Yuliang Liu, Wenwen Yu, Zujia Zhang, Jianzhong Ju et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08524v1](http://arxiv.org/abs/2602.08524v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08524v1)

**Code**: https://github.com/dle666/GeoFocus

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Geometry problem-solving remains a significant challenge for Large Multimodal Models (LMMs), requiring not only global shape recognition but also attention to intricate local relationships related to geometric theory. To address this, we propose GeoFocus, a novel framework comprising two core modules. 1) Critical Local Perceptor, which automatically identifies and emphasizes critical local structure (e.g., angles, parallel lines, comparative distances) through thirteen theory-based perception templates, boosting critical local feature coverage by 61% compared to previous methods. 2) VertexLang, a compact topology formal language, encodes global figures through vertex coordinates and connectivity relations. By replacing bulky code-based encodings, VertexLang reduces global perception training time by 20% while improving topology recognition accuracy. When evaluated in Geo3K, GeoQA, and FormalGeo7K, GeoFocus achieves a 4.7% accuracy improvement over leading specialized models and demonstrates superior robustness in MATHVERSE under diverse visual conditions. Project Page -- https://github.com/dle666/GeoFocus

</details>

<details><summary><b>Chinese Abstract</b></summary>

Geometry problem-solving remains a significant challenge for Large Multimodal Models (LMMs), requiring not only global shape recognition but also attention to intricate local relationships related to geometric theory. To address this, we propose GeoFocus, a novel framework comprising two core modules.1) Critical Local Perceptor, which automatically identifies and emphasizes critical local structure (e.g., angles, parallel lines, comparative distances) through thirteen theory-based perception templates, boosting critical local feature coverage by 61% compared to previous methods. 2) VertexLang, a compact topology formal language, encodes global figures through vertex coordinates and connectivity relations.By replacing bulky code-based encodings, VertexLang reduces global perception training time by 20% while improving topology recognition accuracy. When evaluated in Geo3K, GeoQA, and FormalGeo7K, GeoFocus achieves a 4.7% accuracy improvement over leading specialized models and demonstrates superior robustness in MATHVERSE under diverse visual conditions. Project Page -- https://github.com/dle666/GeoFocus

</details>

---

## 185. Are Vision Foundation Models Foundational for Electron Microscopy Image Segmentation?

**Chinese Title**: Are Vision Foundation Models Foundational for Electron Microscopy Image Segmentation?

**Authors**: Caterina Fuster-BarcelÃ³, Virginie Uhlmann

**Date**: 2026-02-09 | **arXiv**: [2602.08505v1](http://arxiv.org/abs/2602.08505v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08505v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Although vision foundation models (VFMs) are increasingly reused for biomedical image analysis, it remains unclear whether the latent representations they provide are general enough to support effective transfer and reuse across heterogeneous microscopy image datasets. Here, we study this question for the problem of mitochondria segmentation in electron microscopy (EM) images, using two popular public EM datasets (Lucchi++ and VNC) and three recent representative VFMs (DINOv2, DINOv3, and OpenCLIP). We evaluate two practical model adaptation regimes: a frozen-backbone setting in which only a lightweight segmentation head is trained on top of the VFM, and parameter-efficient fine-tuning (PEFT) via Low-Rank Adaptation (LoRA) in which the VFM is fine-tuned in a targeted manner to a specific dataset. Across all backbones, we observe that training on a single EM dataset yields good segmentation performance (quantified as foreground Intersection-over-Union), and that LoRA consistently improves in-domain performance. In contrast, training on multiple EM datasets leads to severe performance degradation for all models considered, with only marginal gains from PEFT. Exploration of the latent representation space through various techniques (PCA, FrÃ©chet Dinov2 distance, and linear probes) reveals a pronounced and persistent domain mismatch between the two considered EM datasets in spite of their visual similarity, which is consistent with the observed failure of paired training. These results suggest that, while VFMs can deliver competitive results for EM segmentation within a single domain under lightweight adaptation, current PEFT strategies are insufficient to obtain a single robust model across heterogeneous EM datasets without additional domain-alignment mechanisms.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Although vision foundation models (VFMs) are increasingly reused for biomedical image analysis, it remains unclear whether the latent representations they provide are general enough to support effective transfer and reuse across heterogeneous microscopy image datasets.Here, we study this question for the problem of mitochondria segmentation in electron microscopy (EM) images, using two popular public EM datasets (Lucchi++ and VNC) and three recent representative VFMs (DINOv2, DINOv3, and OpenCLIP).We evaluate two practical model adaptation regimes: a frozen-backbone setting in which only a lightweight segmentation head is trained on top of the VFM, and parameter-efficient fine-tuning (PEFT) via Low-Rank Adaptation (LoRA) in which the VFM is fine-tuned in a targeted manner to a specific dataset.Across all backbones, we observe that training on a single EM dataset yields good segmentation performance (quantified as foreground Intersection-over-Union), and that LoRA consistently improves in-domain performance. In contrast, training on multiple EM datasets leads to severe performance degradation for all models considered, with only marginal gains from PEFT.Exploration of the latent representation space through various techniques (PCA, FrÃ©chet Dinov2 distance, and linear probes) reveals a pronounced and persistent domain mismatch between the two considered EM datasets in spite of their visual similarity, which is consistent with the observed failure of paired training.These results suggest that, while VFMs can deliver competitive results for EM segmentation within a single domain under lightweight adaptation, current PEFT strategies are insufficient to obtain a single robust model across heterogeneous EM datasets without additional domain-alignment mechanisms.

</details>

---

## 186. Learning Self-Correction in Vision-Language Models via Rollout Augmentation

**Chinese Title**: Learning Self-Correction in Vision-Language Models via Rollout Augmentation

**Authors**: Yi Ding, Ziliang Qiu, Bolian Li, Ruqi Zhang

**Date**: 2026-02-09 | **arXiv**: [2602.08503v1](http://arxiv.org/abs/2602.08503v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08503v1)

**Categories**: cs.CV, cs.CL, cs.LG

<details><summary><b>Abstract</b></summary>

Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only $0.72\times$ training time per step.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse.To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision.Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability.Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only $0.72\times$ training time per step.

</details>

---

## 187. Understanding Image2Video Domain Shift in Food Segmentation: An Instance-level Analysis on Apples

**Chinese Title**: Understanding Image2Video Domain Shift in Food Segmentation: An Instance-level Analysis on Apples

**Authors**: Keonvin Park, Aditya Pal, Jin Hong Mok

**Date**: 2026-02-09 | **arXiv**: [2602.08491v2](http://arxiv.org/abs/2602.08491v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.08491v2)

**Categories**: cs.CV, cs.LG

<details><summary><b>Abstract</b></summary>

Food segmentation models trained on static images have achieved strong performance on benchmark datasets; however, their reliability in video settings remains poorly understood. In real-world applications such as food monitoring and instance counting, segmentation outputs must be temporally consistent, yet image-trained models often break down when deployed on videos. In this work, we analyze this failure through an instance segmentation and tracking perspective, focusing on apples as a representative food category. Models are trained solely on image-level food segmentation data and evaluated on video sequences using an instance segmentation with tracking-by-matching framework, enabling object-level temporal analysis. Our results reveal that high frame-wise segmentation accuracy does not translate to stable instance identities over time. Temporal appearance variations, particularly illumination changes, specular reflections, and texture ambiguity, lead to mask flickering and identity fragmentation, resulting in significant errors in apple counting. These failures are largely overlooked by conventional image-based metrics, which substantially overestimate real-world video performance. Beyond diagnosing the problem, we examine practical remedies that do not require full video supervision, including post-hoc temporal regularization and self-supervised temporal consistency objectives. Our findings suggest that the root cause of failure lies in image-centric training objectives that ignore temporal coherence, rather than model capacity. This study highlights a critical evaluation gap in food segmentation research and motivates temporally-aware learning and evaluation protocols for video-based food analysis.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Food segmentation models trained on static images have achieved strong performance on benchmark datasets; however, their reliability in video settings remains poorly understood. In real-world applications such as food monitoring and instance counting, segmentation outputs must be temporally consistent, yet image-trained models often break down when deployed on videos.In this work, we analyze this failure through an instance segmentation and tracking perspective, focusing on apples as a representative food category. Models are trained solely on image-level food segmentation data and evaluated on video sequences using an instance segmentation with tracking-by-matching framework, enabling object-level temporal analysis.Our results reveal that high frame-wise segmentation accuracy does not translate to stable instance identities over time. Temporal appearance variations, particularly illumination changes, specular reflections, and texture ambiguity, lead to mask flickering and identity fragmentation, resulting in significant errors in apple counting.These failures are largely overlooked by conventional image-based metrics, which substantially overestimate real-world video performance. Beyond diagnosing the problem, we examine practical remedies that do not require full video supervision, including post-hoc temporal regularization and self-supervised temporal consistency objectives.Our findings suggest that the root cause of failure lies in image-centric training objectives that ignore temporal coherence, rather than model capacity. This study highlights a critical evaluation gap in food segmentation research and motivates temporally-aware learning and evaluation protocols for video-based food analysis.

</details>

---

## 188. Reliability-aware Execution Gating for Near-field and Off-axis Vision-guided Robotic Alignment

**Chinese Title**: Reliability-aware Execution Gating for Near-field and Off-axis Vision-guided Robotic Alignment

**Authors**: Ning Hu, Senhao Cao, Maochen Li

**Date**: 2026-02-09 | **arXiv**: [2602.08466v1](http://arxiv.org/abs/2602.08466v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08466v1)

**Categories**: cs.RO, cs.CV

<details><summary><b>Abstract</b></summary>

Vision-guided robotic systems are increasingly deployed in precision alignment tasks that require reliable execution under near-field and off-axis configurations. While recent advances in pose estimation have significantly improved numerical accuracy, practical robotic systems still suffer from frequent execution failures even when pose estimates appear accurate. This gap suggests that pose accuracy alone is insufficient to guarantee execution-level reliability. In this paper, we reveal that such failures arise from a deterministic geometric error amplification mechanism, in which small pose estimation errors are magnified through system structure and motion execution, leading to unstable or failed alignment. Rather than modifying pose estimation algorithms, we propose a Reliability-aware Execution Gating mechanism that operates at the execution level. The proposed approach evaluates geometric consistency and configuration risk before execution, and selectively rejects or scales high-risk pose updates. We validate the proposed method on a real UR5 robotic platform performing single-step visual alignment tasks under varying camera-target distances and off-axis configurations. Experimental results demonstrate that the proposed execution gating significantly improves task success rates, reduces execution variance, and suppresses tail-risk behavior, while leaving average pose accuracy largely unchanged. Importantly, the proposed mechanism is estimator-agnostic and can be readily integrated with both classical geometry-based and learning-based pose estimation pipelines. These results highlight the importance of execution-level reliability modeling and provide a practical solution for improving robustness in near-field vision-guided robotic systems.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Vision-guided robotic systems are increasingly deployed in precision alignment tasks that require reliable execution under near-field and off-axis configurations. While recent advances in pose estimation have significantly improved numerical accuracy, practical robotic systems still suffer from frequent execution failures even when pose estimates appear accurate.This gap suggests that pose accuracy alone is insufficient to guarantee execution-level reliability. In this paper, we reveal that such failures arise from a deterministic geometric error amplification mechanism, in which small pose estimation errors are magnified through system structure and motion execution, leading to unstable or failed alignment.Rather than modifying pose estimation algorithms, we propose a Reliability-aware Execution Gating mechanism that operates at the execution level. The proposed approach evaluates geometric consistency and configuration risk before execution, and selectively rejects or scales high-risk pose updates.We validate the proposed method on a real UR5 robotic platform performing single-step visual alignment tasks under varying camera-target distances and off-axis configurations. Experimental results demonstrate that the proposed execution gating significantly improves task success rates, reduces execution variance, and suppresses tail-risk behavior, while leaving average pose accuracy largely unchanged.Importantly, the proposed mechanism is estimator-agnostic and can be readily integrated with both classical geometry-based and learning-based pose estimation pipelines. These results highlight the importance of execution-level reliability modeling and provide a practical solution for improving robustness in near-field vision-guided robotic systems.

</details>

---

## 189. TriC-Motion: Tri-Domain Causal Modeling Grounded Text-to-Motion Generation

**Chinese Title**: TriC-Motion: Tri-Domain Causal Modeling Grounded Text-to-Motion Generation

**Authors**: Yiyang Cao, Yunze Deng, Ziyu Lin, Bin Feng, Xinggang Wang et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08462v1](http://arxiv.org/abs/2602.08462v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08462v1)

**Project**: https://caoyiyang1105.github.io/TriC-Motion/.  **Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Text-to-motion generation, a rapidly evolving field in computer vision, aims to produce realistic and text-aligned motion sequences. Current methods primarily focus on spatial-temporal modeling or independent frequency domain analysis, lacking a unified framework for joint optimization across spatial, temporal, and frequency domains. This limitation hinders the model's ability to leverage information from all domains simultaneously, leading to suboptimal generation quality. Additionally, in motion generation frameworks, motion-irrelevant cues caused by noise are often entangled with features that contribute positively to generation, thereby leading to motion distortion. To address these issues, we propose Tri-Domain Causal Text-to-Motion Generation (TriC-Motion), a novel diffusion-based framework integrating spatial-temporal-frequency-domain modeling with causal intervention. TriC-Motion includes three core modeling modules for domain-specific modeling, namely Temporal Motion Encoding, Spatial Topology Modeling, and Hybrid Frequency Analysis. After comprehensive modeling, a Score-guided Tri-domain Fusion module integrates valuable information from the triple domains, simultaneously ensuring temporal consistency, spatial topology, motion trends, and dynamics. Moreover, the Causality-based Counterfactual Motion Disentangler is meticulously designed to expose motion-irrelevant cues to eliminate noise, disentangling the real modeling contributions of each domain for superior generation. Extensive experimental results validate that TriC-Motion achieves superior performance compared to state-of-the-art methods, attaining an outstanding R@1 of 0.612 on the HumanML3D dataset. These results demonstrate its capability to generate high-fidelity, coherent, diverse, and text-aligned motion sequences. Code is available at: https://caoyiyang1105.github.io/TriC-Motion/.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Text-to-motion generation, a rapidly evolving field in computer vision, aims to produce realistic and text-aligned motion sequences. Current methods primarily focus on spatial-temporal modeling or independent frequency domain analysis, lacking a unified framework for joint optimization across spatial, temporal, and frequency domains.This limitation hinders the model's ability to leverage information from all domains simultaneously, leading to suboptimal generation quality. Additionally, in motion generation frameworks, motion-irrelevant cues caused by noise are often entangled with features that contribute positively to generation, thereby leading to motion distortion.To address these issues, we propose Tri-Domain Causal Text-to-Motion Generation (TriC-Motion), a novel diffusion-based framework integrating spatial-temporal-frequency-domain modeling with causal intervention. TriC-Motion includes three core modeling modules for domain-specific modeling, namely Temporal Motion Encoding, Spatial Topology Modeling, and Hybrid Frequency Analysis.After comprehensive modeling, a Score-guided Tri-domain Fusion module integrates valuable information from the triple domains, simultaneously ensuring temporal consistency, spatial topology, motion trends, and dynamics. Moreover, the Causality-based Counterfactual Motion Disentangler is meticulously designed to expose motion-irrelevant cues to eliminate noise, disentangling the real modeling contributions of each domain for superior generation.Extensive experimental results validate that TriC-Motion achieves superior performance compared to state-of-the-art methods, attaining an outstanding R@1 of 0.612 on the HumanML3D dataset. These results demonstrate its capability to generate high-fidelity, coherent, diverse, and text-aligned motion sequences. Code is available at: https://caoyiyang1105.github.io/TriC-Motion/.

</details>

---

## 190. Vista: Scene-Aware Optimization for Streaming Video Question Answering under Post-Hoc Queries

**Chinese Title**: Vista: Scene-Aware Optimization for Streaming Video Question Answering under Post-Hoc Queries

**Authors**: Haocheng Lu, Nan Zhang, Wei Tao, Xiaoyang Qu, Guokuan Li et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08448v1](http://arxiv.org/abs/2602.08448v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08448v1)

**Categories**: cs.CV, cs.AI

<details><summary><b>Abstract</b></summary>

Streaming video question answering (Streaming Video QA) poses distinct challenges for multimodal large language models (MLLMs), as video frames arrive sequentially and user queries can be issued at arbitrary time points. Existing solutions relying on fixed-size memory or naive compression often suffer from context loss or memory overflow, limiting their effectiveness in long-form, real-time scenarios. We present Vista, a novel framework for scene-aware streaming video QA that enables efficient and scalable reasoning over continuous video streams. The innovation of Vista can be summarized in three aspects: (1) scene-aware segmentation, where Vista dynamically clusters incoming frames into temporally and visually coherent scene units; (2) scene-aware compression, where each scene is compressed into a compact token representation and stored in GPU memory for efficient index-based retrieval, while full-resolution frames are offloaded to CPU memory; and (3) scene-aware recall, where relevant scenes are selectively recalled and reintegrated into the model input upon receiving a query, enabling both efficiency and completeness. Vista is model-agnostic and integrates seamlessly with a variety of vision-language backbones, enabling long-context reasoning without compromising latency or memory efficiency. Extensive experiments on StreamingBench demonstrate that Vista achieves state-of-the-art performance, establishing a strong baseline for real-world streaming video understanding.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Streaming video question answering (Streaming Video QA) poses distinct challenges for multimodal large language models (MLLMs), as video frames arrive sequentially and user queries can be issued at arbitrary time points. Existing solutions relying on fixed-size memory or naive compression often suffer from context loss or memory overflow, limiting their effectiveness in long-form, real-time scenarios.We present Vista, a novel framework for scene-aware streaming video QA that enables efficient and scalable reasoning over continuous video streams.The innovation of Vista can be summarized in three aspects: (1) scene-aware segmentation, where Vista dynamically clusters incoming frames into temporally and visually coherent scene units; (2) scene-aware compression, where each scene is compressed into a compact token representation and stored in GPU memory for efficient index-based retrieval, while full-resolution frames are offloaded to CPU memory; and (3) scene-aware recall, where relevant scenes are selectively recalled and reintegrated into the model input upon receiving a query, enabling both efficiency and completeness.Vista is model-agnostic and integrates seamlessly with a variety of vision-language backbones, enabling long-context reasoning without compromising latency or memory efficiency. Extensive experiments on StreamingBench demonstrate that Vista achieves state-of-the-art performance, establishing a strong baseline for real-world streaming video understanding.

</details>

---

## 191. Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition

**Chinese Title**: Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition

**Authors**: Yuhao Dong, Shulin Tian, Shuai Liu, Shuangrui Ding, Yuhang Zang et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08439v1](http://arxiv.org/abs/2602.08439v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08439v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples.To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities.Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations.To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.

</details>

---

## 192. Understanding and Optimizing Attention-Based Sparse Matching for Diverse Local Features

**Chinese Title**: Understanding and Optimizing Attention-Based Sparse Matching for Diverse Local Features

**Authors**: Qiang Wang

**Date**: 2026-02-09 | **arXiv**: [2602.08430v1](http://arxiv.org/abs/2602.08430v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08430v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

We revisit the problem of training attention-based sparse image matching models for various local features. We first identify one critical design choice that has been previously overlooked, which significantly impacts the performance of the LightGlue model. We then investigate the role of detectors and descriptors within the transformer-based matching framework, finding that detectors, rather than descriptors, are often the primary cause for performance difference. Finally, we propose a novel approach to fine-tune existing image matching models using keypoints from a diverse set of detectors, resulting in a universal, detector-agnostic model. When deployed as a zero-shot matcher for novel detectors, the resulting model achieves or exceeds the accuracy of models specifically trained for those features. Our findings offer valuable insights for the deployment of transformer-based matching models and the future design of local features.

</details>

<details><summary><b>Chinese Abstract</b></summary>

We revisit the problem of training attention-based sparse image matching models for various local features. We first identify one critical design choice that has been previously overlooked, which significantly impacts the performance of the LightGlue model.We then investigate the role of detectors and descriptors within the transformer-based matching framework, finding that detectors, rather than descriptors, are often the primary cause for performance difference. Finally, we propose a novel approach to fine-tune existing image matching models using keypoints from a diverse set of detectors, resulting in a universal, detector-agnostic model.When deployed as a zero-shot matcher for novel detectors, the resulting model achieves or exceeds the accuracy of models specifically trained for those features. Our findings offer valuable insights for the deployment of transformer-based matching models and the future design of local features.

</details>

---

## 193. RealSynCol: a high-fidelity synthetic colon dataset for 3D reconstruction applications

**Chinese Title**: RealSynCol: a high-fidelity synthetic colon dataset for 3D reconstruction applications

**Authors**: Chiara Lena, Davide Milesi, Alessandro Casella, Luca Carlini, Joseph C. Norton et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08397v1](http://arxiv.org/abs/2602.08397v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08397v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Deep learning has the potential to improve colonoscopy by enabling 3D reconstruction of the colon, providing a comprehensive view of mucosal surfaces and lesions, and facilitating the identification of unexplored areas. However, the development of robust methods is limited by the scarcity of large-scale ground truth data. We propose RealSynCol, a highly realistic synthetic dataset designed to replicate the endoscopic environment. Colon geometries extracted from 10 CT scans were imported into a virtual environment that closely mimics intraoperative conditions and rendered with realistic vascular textures. The resulting dataset comprises 28\,130 frames, paired with ground truth depth maps, optical flow, 3D meshes, and camera trajectories. A benchmark study was conducted to evaluate the available synthetic colon datasets for the tasks of depth and pose estimation. Results demonstrate that the high realism and variability of RealSynCol significantly enhance generalization performance on clinical images, proving it to be a powerful tool for developing deep learning algorithms to support endoscopic diagnosis.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Deep learning has the potential to improve colonoscopy by enabling 3D reconstruction of the colon, providing a comprehensive view of mucosal surfaces and lesions, and facilitating the identification of unexplored areas. However, the development of robust methods is limited by the scarcity of large-scale ground truth data. We propose RealSynCol, a highly realistic synthetic dataset designed to replicate the endoscopic environment.Colon geometries extracted from 10 CT scans were imported into a virtual environment that closely mimics intraoperative conditions and rendered with realistic vascular textures. The resulting dataset comprises 28\,130 frames, paired with ground truth depth maps, optical flow, 3D meshes, and camera trajectories. A benchmark study was conducted to evaluate the available synthetic colon datasets for the tasks of depth and pose estimation.Results demonstrate that the high realism and variability of RealSynCol significantly enhance generalization performance on clinical images, proving it to be a powerful tool for developing deep learning algorithms to support endoscopic diagnosis.

</details>

---

## 194. D$^2$-VR: Degradation-Robust and Distilled Video Restoration with Synergistic Optimization Strategy

**Chinese Title**: D$^2$-VR: Degradation-Robust and Distilled Video Restoration with Synergistic Optimization Strategy

**Authors**: Jianfeng Liang, Shaocheng Shen, Botao Xu, Qiang Hu, Xiaoyun Zhang

**Date**: 2026-02-09 | **arXiv**: [2602.08395v1](http://arxiv.org/abs/2602.08395v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08395v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

The integration of diffusion priors with temporal alignment has emerged as a transformative paradigm for video restoration, delivering fantastic perceptual quality, yet the practical deployment of such frameworks is severely constrained by prohibitive inference latency and temporal instability when confronted with complex real-world degradations. To address these limitations, we propose \textbf{D$^2$-VR}, a single-image diffusion-based video-restoration framework with low-step inference. To obtain precise temporal guidance under severe degradation, we first design a Degradation-Robust Flow Alignment (DRFA) module that leverages confidence-aware attention to filter unreliable motion cues. We then incorporate an adversarial distillation paradigm to compress the diffusion sampling trajectory into a rapid few-step regime. Finally, a synergistic optimization strategy is devised to harmonize perceptual quality with rigorous temporal consistency. Extensive experiments demonstrate that D$^2$-VR achieves state-of-the-art performance while accelerating the sampling process by \textbf{12$\times$}

</details>

<details><summary><b>Chinese Abstract</b></summary>

The integration of diffusion priors with temporal alignment has emerged as a transformative paradigm for video restoration, delivering fantastic perceptual quality, yet the practical deployment of such frameworks is severely constrained by prohibitive inference latency and temporal instability when confronted with complex real-world degradations.To address these limitations, we propose \textbf{D$^2$-VR}, a single-image diffusion-based video-restoration framework with low-step inference. To obtain precise temporal guidance under severe degradation, we first design a Degradation-Robust Flow Alignment (DRFA) module that leverages confidence-aware attention to filter unreliable motion cues.We then incorporate an adversarial distillation paradigm to compress the diffusion sampling trajectory into a rapid few-step regime. Finally, a synergistic optimization strategy is devised to harmonize perceptual quality with rigorous temporal consistency. Extensive experiments demonstrate that D$^2$-VR achieves state-of-the-art performance while accelerating the sampling process by \textbf{12$\times$}

</details>

---

## 195. BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models

**Chinese Title**: BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models

**Authors**: Xin Wu, Zhixuan Liang, Yue Ma, Mengkang Hu, Zhiyuan Qin et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08392v1](http://arxiv.org/abs/2602.08392v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08392v1)

**Categories**: cs.RO, cs.AI, cs.CV

<details><summary><b>Abstract</b></summary>

Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI, and using them to benchmark robotic intelligence has become a pivotal trend. However, existing frameworks remain predominantly confined to single-arm manipulation, failing to capture the spatio-temporal coordination required for bimanual tasks like lifting a heavy pot. To address this, we introduce BiManiBench, a hierarchical benchmark evaluating MLLMs across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. Our framework isolates unique bimanual challenges, such as arm reachability and kinematic constraints, thereby distinguishing perceptual hallucinations from planning failures. Analysis of over 30 state-of-the-art models reveals that despite high-level reasoning proficiency, MLLMs struggle with dual-arm spatial grounding and control, frequently resulting in mutual interference and sequencing errors. These findings suggest the current paradigm lacks a deep understanding of mutual kinematic constraints, highlighting the need for future research to focus on inter-arm collision-avoidance and fine-grained temporal sequencing.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI, and using them to benchmark robotic intelligence has become a pivotal trend. However, existing frameworks remain predominantly confined to single-arm manipulation, failing to capture the spatio-temporal coordination required for bimanual tasks like lifting a heavy pot.To address this, we introduce BiManiBench, a hierarchical benchmark evaluating MLLMs across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. Our framework isolates unique bimanual challenges, such as arm reachability and kinematic constraints, thereby distinguishing perceptual hallucinations from planning failures.Analysis of over 30 state-of-the-art models reveals that despite high-level reasoning proficiency, MLLMs struggle with dual-arm spatial grounding and control, frequently resulting in mutual interference and sequencing errors.These findings suggest the current paradigm lacks a deep understanding of mutual kinematic constraints, highlighting the need for future research to focus on inter-arm collision-avoidance and fine-grained temporal sequencing.

</details>

---

## 196. Geometric Image Editing via Effects-Sensitive In-Context Inpainting with Diffusion Transformers

**Chinese Title**: Geometric Image Editing via Effects-Sensitive In-Context Inpainting with Diffusion Transformers

**Authors**: Shuo Zhang, Wenzhuo Wu, Huayu Zhang, Jiarong Cheng, Xianghao Zang et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08388v1](http://arxiv.org/abs/2602.08388v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08388v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Recent advances in diffusion models have significantly improved image editing. However, challenges persist in handling geometric transformations, such as translation, rotation, and scaling, particularly in complex scenes. Existing approaches suffer from two main limitations: (1) difficulty in achieving accurate geometric editing of object translation, rotation, and scaling; (2) inadequate modeling of intricate lighting and shadow effects, leading to unrealistic results. To address these issues, we propose GeoEdit, a framework that leverages in-context generation through a diffusion transformer module, which integrates geometric transformations for precise object edits. Moreover, we introduce Effects-Sensitive Attention, which enhances the modeling of intricate lighting and shadow effects for improved realism. To further support training, we construct RS-Objects, a large-scale geometric editing dataset containing over 120,000 high-quality image pairs, enabling the model to learn precise geometric editing while generating realistic lighting and shadows. Extensive experiments on public benchmarks demonstrate that GeoEdit consistently outperforms state-of-the-art methods in terms of visual quality, geometric accuracy, and realism.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Recent advances in diffusion models have significantly improved image editing. However, challenges persist in handling geometric transformations, such as translation, rotation, and scaling, particularly in complex scenes.Existing approaches suffer from two main limitations: (1) difficulty in achieving accurate geometric editing of object translation, rotation, and scaling; (2) inadequate modeling of intricate lighting and shadow effects, leading to unrealistic results.To address these issues, we propose GeoEdit, a framework that leverages in-context generation through a diffusion transformer module, which integrates geometric transformations for precise object edits. Moreover, we introduce Effects-Sensitive Attention, which enhances the modeling of intricate lighting and shadow effects for improved realism.To further support training, we construct RS-Objects, a large-scale geometric editing dataset containing over 120,000 high-quality image pairs, enabling the model to learn precise geometric editing while generating realistic lighting and shadows. Extensive experiments on public benchmarks demonstrate that GeoEdit consistently outperforms state-of-the-art methods in terms of visual quality, geometric accuracy, and realism.

</details>

---

## 197. E-VAds: An E-commerce Short Videos Understanding Benchmark for MLLMs

**Chinese Title**: E-VAds: An E-commerce Short Videos Understanding Benchmark for MLLMs

**Authors**: Xianjie Liu, Yiman Hu, Liang Wu, Ping Hu, Yixiong Zou et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08355v2](http://arxiv.org/abs/2602.08355v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.08355v2)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

E-commerce short videos represent a high-revenue segment of the online video industry characterized by a goal-driven format and dense multi-modal signals. Current models often struggle with these videos because existing benchmarks focus primarily on general-purpose tasks and neglect the reasoning of commercial intent. In this work, we first propose a multi-modal information density assessment framework to quantify the complexity of this domain. Our evaluation reveals that e-commerce content exhibits substantially higher density across visual, audio, and textual modalities compared to mainstream datasets, establishing a more challenging frontier for video understanding. To address this gap, we introduce E-commerce Video Ads Benchmark (E-VAds), which is the first benchmark specifically designed for e-commerce short video understanding. We curated 3,961 high-quality videos from Taobao covering a wide range of product categories and used a multi-agent system to generate 19,785 open-ended Q&A pairs. These questions are organized into two primary dimensions, namely Perception and Cognition and Reasoning, which consist of five distinct tasks. Finally, we develop E-VAds-R1, an RL-based reasoning model featuring a multi-grained reward design called MG-GRPO. This strategy provides smooth guidance for early exploration while creating a non-linear incentive for expert-level precision. Experimental results demonstrate that E-VAds-R1 achieves a 109.2% performance gain in commercial intent reasoning with only a few hundred training samples.

</details>

<details><summary><b>Chinese Abstract</b></summary>

E-commerce short videos represent a high-revenue segment of the online video industry characterized by a goal-driven format and dense multi-modal signals. Current models often struggle with these videos because existing benchmarks focus primarily on general-purpose tasks and neglect the reasoning of commercial intent. In this work, we first propose a multi-modal information density assessment framework to quantify the complexity of this domain.Our evaluation reveals that e-commerce content exhibits substantially higher density across visual, audio, and textual modalities compared to mainstream datasets, establishing a more challenging frontier for video understanding. To address this gap, we introduce E-commerce Video Ads Benchmark (E-VAds), which is the first benchmark specifically designed for e-commerce short video understanding.We curated 3,961 high-quality videos from Taobao covering a wide range of product categories and used a multi-agent system to generate 19,785 open-ended Q&A pairs. These questions are organized into two primary dimensions, namely Perception and Cognition and Reasoning, which consist of five distinct tasks. Finally, we develop E-VAds-R1, an RL-based reasoning model featuring a multi-grained reward design called MG-GRPO.This strategy provides smooth guidance for early exploration while creating a non-linear incentive for expert-level precision. Experimental results demonstrate that E-VAds-R1 achieves a 109.2% performance gain in commercial intent reasoning with only a few hundred training samples.

</details>

---

## 198. What, Whether and How? Unveiling Process Reward Models for Thinking with Images Reasoning

**Chinese Title**: What, Whether and How? Unveiling Process Reward Models for Thinking with Images Reasoning

**Authors**: Yujin Zhou, Pengcheng Wen, Jiale Chen, Boqin Yin, Han Zhu et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08346v1](http://arxiv.org/abs/2602.08346v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08346v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

The rapid advancement of Large Vision Language Models (LVLMs) has demonstrated excellent abilities in various visual tasks. Building upon these developments, the thinking with images paradigm has emerged, enabling models to dynamically edit and re-encode visual information at each reasoning step, mirroring human visual processing. However, this paradigm introduces significant challenges as diverse errors may occur during reasoning processes. This necessitates Process Reward Models (PRMs) for distinguishing positive and negative reasoning steps, yet existing benchmarks for PRMs are predominantly text-centric and lack comprehensive assessment under this paradigm. To address these gaps, this work introduces the first comprehensive benchmark specifically designed for evaluating PRMs under the thinking with images paradigm. Our main contributions are: (1) Through extensive analysis of reasoning trajectories and guided search experiments with PRMs, we define 7 fine-grained error types and demonstrate both the necessity for specialized PRMs and the potential for improvement. (2) We construct a comprehensive benchmark comprising 1,206 manually annotated thinking with images reasoning trajectories spanning 4 categories and 16 subcategories for fine-grained evaluation of PRMs. (3) Our experimental analysis reveals that current LVLMs fall short as effective PRMs, exhibiting limited capabilities in visual reasoning process evaluation with significant performance disparities across error types, positive evaluation bias, and sensitivity to reasoning step positions. These findings demonstrate the effectiveness of our benchmark and establish crucial foundations for advancing PRMs in LVLMs.

</details>

<details><summary><b>Chinese Abstract</b></summary>

The rapid advancement of Large Vision Language Models (LVLMs) has demonstrated excellent abilities in various visual tasks. Building upon these developments, the thinking with images paradigm has emerged, enabling models to dynamically edit and re-encode visual information at each reasoning step, mirroring human visual processing. However, this paradigm introduces significant challenges as diverse errors may occur during reasoning processes.This necessitates Process Reward Models (PRMs) for distinguishing positive and negative reasoning steps, yet existing benchmarks for PRMs are predominantly text-centric and lack comprehensive assessment under this paradigm. To address these gaps, this work introduces the first comprehensive benchmark specifically designed for evaluating PRMs under the thinking with images paradigm.Our main contributions are: (1) Through extensive analysis of reasoning trajectories and guided search experiments with PRMs, we define 7 fine-grained error types and demonstrate both the necessity for specialized PRMs and the potential for improvement.(2) We construct a comprehensive benchmark comprising 1,206 manually annotated thinking with images reasoning trajectories spanning 4 categories and 16 subcategories for fine-grained evaluation of PRMs.(3) Our experimental analysis reveals that current LVLMs fall short as effective PRMs, exhibiting limited capabilities in visual reasoning process evaluation with significant performance disparities across error types, positive evaluation bias, and sensitivity to reasoning step positions. These findings demonstrate the effectiveness of our benchmark and establish crucial foundations for advancing PRMs in LVLMs.

</details>

---

## 199. UrbanGraphEmbeddings: Learning and Evaluating Spatially Grounded Multimodal Embeddings for Urban Science

**Chinese Title**: UrbanGraphEmbeddings: Learning and Evaluating Spatially Grounded Multimodal Embeddings for Urban Science

**Authors**: Jie Zhang, Xingtong Yu, Yuan Fang, Rudi Stouffs, Zdravko Trivic

**Date**: 2026-02-09 | **arXiv**: [2602.08342v1](http://arxiv.org/abs/2602.08342v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08342v1)

**Categories**: cs.CV, cs.AI

<details><summary><b>Abstract</b></summary>

Learning transferable multimodal embeddings for urban environments is challenging because urban understanding is inherently spatial, yet existing datasets and benchmarks lack explicit alignment between street-view images and urban structure. We introduce UGData, a spatially grounded dataset that anchors street-view images to structured spatial graphs and provides graph-aligned supervision via spatial reasoning paths and spatial context captions, exposing distance, directionality, connectivity, and neighborhood context beyond image content. Building on UGData, we propose UGE, a two-stage training strategy that progressively and stably aligns images, text, and spatial structures by combining instruction-guided contrastive learning with graph-based spatial encoding. We finally introduce UGBench, a comprehensive benchmark to evaluate how spatially grounded embeddings support diverse urban understanding tasks -- including geolocation ranking, image retrieval, urban perception, and spatial grounding. We develop UGE on multiple state-of-the-art VLM backbones, including Qwen2-VL, Qwen2.5-VL, Phi-3-Vision, and LLaVA1.6-Mistral, and train fixed-dimensional spatial embeddings with LoRA tuning. UGE built upon Qwen2.5-VL-7B backbone achieves up to 44% improvement in image retrieval and 30% in geolocation ranking on training cities, and over 30% and 22% gains respectively on held-out cities, demonstrating the effectiveness of explicit spatial grounding for spatially intensive urban tasks.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Learning transferable multimodal embeddings for urban environments is challenging because urban understanding is inherently spatial, yet existing datasets and benchmarks lack explicit alignment between street-view images and urban structure.We introduce UGData, a spatially grounded dataset that anchors street-view images to structured spatial graphs and provides graph-aligned supervision via spatial reasoning paths and spatial context captions, exposing distance, directionality, connectivity, and neighborhood context beyond image content.Building on UGData, we propose UGE, a two-stage training strategy that progressively and stably aligns images, text, and spatial structures by combining instruction-guided contrastive learning with graph-based spatial encoding.We finally introduce UGBench, a comprehensive benchmark to evaluate how spatially grounded embeddings support diverse urban understanding tasks -- including geolocation ranking, image retrieval, urban perception, and spatial grounding. We develop UGE on multiple state-of-the-art VLM backbones, including Qwen2-VL, Qwen2.5-VL, Phi-3-Vision, and LLaVA1.6-Mistral, and train fixed-dimensional spatial embeddings with LoRA tuning.UGE built upon Qwen2.5-VL-7B backbone achieves up to 44% improvement in image retrieval and 30% in geolocation ranking on training cities, and over 30% and 22% gains respectively on held-out cities, demonstrating the effectiveness of explicit spatial grounding for spatially intensive urban tasks.

</details>

---

## 200. CoTZero: Annotation-Free Human-Like Vision Reasoning via Hierarchical Synthetic CoT

**Chinese Title**: CoTZero: Annotation-Free Human-Like Vision Reasoning via Hierarchical Synthetic CoT

**Authors**: Chengyi Du, Yazhe Niu, Dazhong Shen, Luxin Xu

**Date**: 2026-02-09 | **arXiv**: [2602.08339v1](http://arxiv.org/abs/2602.08339v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08339v1)

**Categories**: cs.AI, cs.CV

<details><summary><b>Abstract</b></summary>

Recent advances in vision-language models (VLMs) have markedly improved image-text alignment, yet they still fall short of human-like visual reasoning. A key limitation is that many VLMs rely on surface correlations rather than building logically coherent structured representations, which often leads to missed higher-level semantic structure and non-causal relational understanding, hindering compositional and verifiable reasoning. To address these limitations by introducing human models into the reasoning process, we propose CoTZero, an annotation-free paradigm with two components: (i) a dual-stage data synthesis approach and (ii) a cognition-aligned training method. In the first component, we draw inspiration from neurocognitive accounts of compositional productivity and global-to-local analysis. In the bottom-up stage, CoTZero extracts atomic visual primitives and incrementally composes them into diverse, structured question-reasoning forms. In the top-down stage, it enforces hierarchical reasoning by using coarse global structure to guide the interpretation of local details and causal relations. In the cognition-aligned training component, built on the synthesized CoT data, we introduce Cognitively Coherent Verifiable Rewards (CCVR) in Reinforcement Fine-Tuning (RFT) to further strengthen VLMs' hierarchical reasoning and generalization, providing stepwise feedback on reasoning coherence and factual correctness. Experiments show that CoTZero achieves an F1 score of 83.33 percent on our multi-level semantic inconsistency benchmark with lexical-perturbation negatives, across both in-domain and out-of-domain settings. Ablations confirm that each component contributes to more interpretable and human-aligned visual reasoning.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Recent advances in vision-language models (VLMs) have markedly improved image-text alignment, yet they still fall short of human-like visual reasoning. A key limitation is that many VLMs rely on surface correlations rather than building logically coherent structured representations, which often leads to missed higher-level semantic structure and non-causal relational understanding, hindering compositional and verifiable reasoning.To address these limitations by introducing human models into the reasoning process, we propose CoTZero, an annotation-free paradigm with two components: (i) a dual-stage data synthesis approach and (ii) a cognition-aligned training method. In the first component, we draw inspiration from neurocognitive accounts of compositional productivity and global-to-local analysis.In the bottom-up stage, CoTZero extracts atomic visual primitives and incrementally composes them into diverse, structured question-reasoning forms. In the top-down stage, it enforces hierarchical reasoning by using coarse global structure to guide the interpretation of local details and causal relations.In the cognition-aligned training component, built on the synthesized CoT data, we introduce Cognitively Coherent Verifiable Rewards (CCVR) in Reinforcement Fine-Tuning (RFT) to further strengthen VLMs' hierarchical reasoning and generalization, providing stepwise feedback on reasoning coherence and factual correctness.Experiments show that CoTZero achieves an F1 score of 83.33 percent on our multi-level semantic inconsistency benchmark with lexical-perturbation negatives, across both in-domain and out-of-domain settings. Ablations confirm that each component contributes to more interpretable and human-aligned visual reasoning.

</details>

---

## 201. Language-Guided Transformer Tokenizer for Human Motion Generation

**Chinese Title**: Language-Guided Transformer Tokenizer for Human Motion Generation

**Authors**: Sheng Yan, Yong Wang, Xin Du, Junsong Yuan, Mengyuan Liu

**Date**: 2026-02-09 | **arXiv**: [2602.08337v1](http://arxiv.org/abs/2602.08337v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08337v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

In this paper, we focus on motion discrete tokenization, which converts raw motion into compact discrete tokens--a process proven crucial for efficient motion generation. In this paradigm, increasing the number of tokens is a common approach to improving motion reconstruction quality, but more tokens make it more difficult for generative models to learn. To maintain high reconstruction quality while reducing generation complexity, we propose leveraging language to achieve efficient motion tokenization, which we term Language-Guided Tokenization (LG-Tok). LG-Tok aligns natural language with motion at the tokenization stage, yielding compact, high-level semantic representations. This approach not only strengthens both tokenization and detokenization but also simplifies the learning of generative models. Furthermore, existing tokenizers predominantly adopt convolutional architectures, whose local receptive fields struggle to support global language guidance. To this end, we propose a Transformer-based Tokenizer that leverages attention mechanisms to enable effective alignment between language and motion. Additionally, we design a language-drop scheme, in which language conditions are randomly removed during training, enabling the detokenizer to support language-free guidance during generation. On the HumanML3D and Motion-X generation benchmarks, LG-Tok achieves Top-1 scores of 0.542 and 0.582, outperforming state-of-the-art methods (MARDM: 0.500 and 0.528), and with FID scores of 0.057 and 0.088, respectively, versus 0.114 and 0.147. LG-Tok-mini uses only half the tokens while maintaining competitive performance (Top-1: 0.521/0.588, FID: 0.085/0.071), validating the efficiency of our semantic representations.

</details>

<details><summary><b>Chinese Abstract</b></summary>

In this paper, we focus on motion discrete tokenization, which converts raw motion into compact discrete tokens--a process proven crucial for efficient motion generation. In this paradigm, increasing the number of tokens is a common approach to improving motion reconstruction quality, but more tokens make it more difficult for generative models to learn.To maintain high reconstruction quality while reducing generation complexity, we propose leveraging language to achieve efficient motion tokenization, which we term Language-Guided Tokenization (LG-Tok). LG-Tok aligns natural language with motion at the tokenization stage, yielding compact, high-level semantic representations.This approach not only strengthens both tokenization and detokenization but also simplifies the learning of generative models. Furthermore, existing tokenizers predominantly adopt convolutional architectures, whose local receptive fields struggle to support global language guidance. To this end, we propose a Transformer-based Tokenizer that leverages attention mechanisms to enable effective alignment between language and motion.Additionally, we design a language-drop scheme, in which language conditions are randomly removed during training, enabling the detokenizer to support language-free guidance during generation. On the HumanML3D and Motion-X generation benchmarks, LG-Tok achieves Top-1 scores of 0.542 and 0.582, outperforming state-of-the-art methods (MARDM: 0.500 and 0.528), and with FID scores of 0.057 and 0.088, respectively, versus 0.114 and 0.147.LG-Tok-mini uses only half the tokens while maintaining competitive performance (Top-1: 0.521/0.588, FID: 0.085/0.071), validating the efficiency of our semantic representations.

</details>

---

## 202. UReason: Benchmarking the Reasoning Paradox in Unified Multimodal Models

**Chinese Title**: UReason: Benchmarking the Reasoning Paradox in Unified Multimodal Models

**Authors**: Cheng Yang, Chufan Shi, Bo Shui, Yaokang Wu, Muzi Tao et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08336v1](http://arxiv.org/abs/2602.08336v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08336v1)

**Categories**: cs.CL, cs.CV

<details><summary><b>Abstract</b></summary>

To elicit capabilities for addressing complex and implicit visual requirements, recent unified multimodal models increasingly adopt chain-of-thought reasoning to guide image generation. However, the actual effect of reasoning on visual synthesis remains unclear. We present UReason, a diagnostic benchmark for reasoning-driven image generation that evaluates whether reasoning can be faithfully executed in pixels. UReason contains 2,000 instances across five task families: Code, Arithmetic, Spatial, Attribute, and Text reasoning. To isolate the role of reasoning traces, we introduce an evaluation framework comparing direct generation, reasoning-guided generation, and de-contextualized generation which conditions only on the refined prompt. Across eight open-source unified models, we observe a consistent Reasoning Paradox: Reasoning traces generally improve performance over direct generation, yet retaining intermediate thoughts as conditioning context often hinders visual synthesis, and conditioning only on the refined prompt yields substantial gains. Our analysis suggests that the bottleneck lies in contextual interference rather than insufficient reasoning capacity. UReason provides a principled testbed for studying reasoning in unified models and motivates future methods that effectively integrate reasoning for visual generation while mitigating interference.

</details>

<details><summary><b>Chinese Abstract</b></summary>

To elicit capabilities for addressing complex and implicit visual requirements, recent unified multimodal models increasingly adopt chain-of-thought reasoning to guide image generation. However, the actual effect of reasoning on visual synthesis remains unclear. We present UReason, a diagnostic benchmark for reasoning-driven image generation that evaluates whether reasoning can be faithfully executed in pixels.UReason contains 2,000 instances across five task families: Code, Arithmetic, Spatial, Attribute, and Text reasoning. To isolate the role of reasoning traces, we introduce an evaluation framework comparing direct generation, reasoning-guided generation, and de-contextualized generation which conditions only on the refined prompt.Across eight open-source unified models, we observe a consistent Reasoning Paradox: Reasoning traces generally improve performance over direct generation, yet retaining intermediate thoughts as conditioning context often hinders visual synthesis, and conditioning only on the refined prompt yields substantial gains. Our analysis suggests that the bottleneck lies in contextual interference rather than insufficient reasoning capacity.UReason provides a principled testbed for studying reasoning in unified models and motivates future methods that effectively integrate reasoning for visual generation while mitigating interference.

</details>

---

## 203. CAE-AV: Improving Audio-Visual Learning via Cross-modal Interactive Enrichment

**Chinese Title**: CAE-AV: Improving Audio-Visual Learning via Cross-modal Interactive Enrichment

**Authors**: Yunzuo Hu, Wen Li, Jing Zhang

**Date**: 2026-02-09 | **arXiv**: [2602.08309v1](http://arxiv.org/abs/2602.08309v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08309v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Audio-visual learning suffers from modality misalignment caused by off-screen sources and background clutter, and current methods usually amplify irrelevant regions or moments, leading to unstable training and degraded representation quality. To address this challenge, we proposed a novel Caption-aligned and Agreement-guided Enhancement framework (CAE-AV) for audio-visual learning, which used two complementary modules: Cross-modal Agreement-guided Spatio-Temporal Enrichment (CASTE) and Caption-Aligned Saliency-guided Enrichment (CASE) to relieve audio-visual misalignment. CASTE dynamically balances spatial and temporal relations by evaluating frame-level audio-visual agreement, ensuring that key information is captured from both preceding and subsequent frames under misalignment. CASE injects cross-modal semantic guidance into selected spatio-temporal positions, leveraging high-level semantic cues to further alleviate misalignment. In addition, we design lightweight objectives, caption-to-modality InfoNCE, visual-audio consistency, and entropy regularization to guide token selection and strengthen cross-modal semantic alignment. With frozen backbones, CAE-AV achieves state-of-the-art performance on AVE, AVVP, AVS, and AVQA benchmarks, and qualitative analyses further validate its robustness against audio-visual misalignment.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Audio-visual learning suffers from modality misalignment caused by off-screen sources and background clutter, and current methods usually amplify irrelevant regions or moments, leading to unstable training and degraded representation quality.To address this challenge, we proposed a novel Caption-aligned and Agreement-guided Enhancement framework (CAE-AV) for audio-visual learning, which used two complementary modules: Cross-modal Agreement-guided Spatio-Temporal Enrichment (CASTE) and Caption-Aligned Saliency-guided Enrichment (CASE) to relieve audio-visual misalignment.CASTE dynamically balances spatial and temporal relations by evaluating frame-level audio-visual agreement, ensuring that key information is captured from both preceding and subsequent frames under misalignment. CASE injects cross-modal semantic guidance into selected spatio-temporal positions, leveraging high-level semantic cues to further alleviate misalignment.In addition, we design lightweight objectives, caption-to-modality InfoNCE, visual-audio consistency, and entropy regularization to guide token selection and strengthen cross-modal semantic alignment. With frozen backbones, CAE-AV achieves state-of-the-art performance on AVE, AVVP, AVS, and AVQA benchmarks, and qualitative analyses further validate its robustness against audio-visual misalignment.

</details>

---

## 204. Tighnari v2: Mitigating Label Noise and Distribution Shift in Multimodal Plant Distribution Prediction via Mixture of Experts and Weakly Supervised Learning

**Chinese Title**: Tighnari v2: Mitigating Label Noise and Distribution Shift in Multimodal Plant Distribution Prediction via Mixture of Experts and Weakly Supervised Learning

**Authors**: Haixu Liu, Yufei Wang, Tianxiang Xu, Chuancheng Shi, Hongsheng Xing

**Date**: 2026-02-09 | **arXiv**: [2602.08282v1](http://arxiv.org/abs/2602.08282v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08282v1)

**Categories**: cs.CV, cs.AI

<details><summary><b>Abstract</b></summary>

Large-scale, cross-species plant distribution prediction plays a crucial role in biodiversity conservation, yet modeling efforts in this area still face significant challenges due to the sparsity and bias of observational data. Presence-Absence (PA) data provide accurate and noise-free labels, but are costly to obtain and limited in quantity; Presence-Only (PO) data, by contrast, offer broad spatial coverage and rich spatiotemporal distribution, but suffer from severe label noise in negative samples. To address these real-world constraints, this paper proposes a multimodal fusion framework that fully leverages the strengths of both PA and PO data. We introduce an innovative pseudo-label aggregation strategy for PO data based on the geographic coverage of satellite imagery, enabling geographic alignment between the label space and remote sensing feature space. In terms of model architecture, we adopt Swin Transformer Base as the backbone for satellite imagery, utilize the TabM network for tabular feature extraction, retain the Temporal Swin Transformer for time-series modeling, and employ a stackable serial tri-modal cross-attention mechanism to optimize the fusion of heterogeneous modalities. Furthermore, empirical analysis reveals significant geographic distribution shifts between PA training and test samples, and models trained by directly mixing PO and PA data tend to experience performance degradation due to label noise in PO data. To address this, we draw on the mixture-of-experts paradigm: test samples are partitioned according to their spatial proximity to PA samples, and different models trained on distinct datasets are used for inference and post-processing within each partition. Experiments on the GeoLifeCLEF 2025 dataset demonstrate that our approach achieves superior predictive performance in scenarios with limited PA coverage and pronounced distribution shifts.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Large-scale, cross-species plant distribution prediction plays a crucial role in biodiversity conservation, yet modeling efforts in this area still face significant challenges due to the sparsity and bias of observational data.Presence-Absence (PA) data provide accurate and noise-free labels, but are costly to obtain and limited in quantity; Presence-Only (PO) data, by contrast, offer broad spatial coverage and rich spatiotemporal distribution, but suffer from severe label noise in negative samples. To address these real-world constraints, this paper proposes a multimodal fusion framework that fully leverages the strengths of both PA and PO data.We introduce an innovative pseudo-label aggregation strategy for PO data based on the geographic coverage of satellite imagery, enabling geographic alignment between the label space and remote sensing feature space.In terms of model architecture, we adopt Swin Transformer Base as the backbone for satellite imagery, utilize the TabM network for tabular feature extraction, retain the Temporal Swin Transformer for time-series modeling, and employ a stackable serial tri-modal cross-attention mechanism to optimize the fusion of heterogeneous modalities.Furthermore, empirical analysis reveals significant geographic distribution shifts between PA training and test samples, and models trained by directly mixing PO and PA data tend to experience performance degradation due to label noise in PO data.To address this, we draw on the mixture-of-experts paradigm: test samples are partitioned according to their spatial proximity to PA samples, and different models trained on distinct datasets are used for inference and post-processing within each partition. Experiments on the GeoLifeCLEF 2025 dataset demonstrate that our approach achieves superior predictive performance in scenarios with limited PA coverage and pronounced distribution shifts.

</details>

---

## 205. PISCO: Precise Video Instance Insertion with Sparse Control

**Chinese Title**: PISCO: Precise Video Instance Insertion with Sparse Control

**Authors**: Xiangbo Gao, Renjie Li, Xinghao Chen, Yuheng Wu, Suofei Feng et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08277v1](http://arxiv.org/abs/2602.08277v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08277v1)

**Categories**: cs.CV, cs.AI

<details><summary><b>Abstract</b></summary>

The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and "cherry-picking" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is crucial to perform precise, targeted modifications. A cornerstone of this transition is video instance insertion, which requires inserting a specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction, and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, a video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify a single keyframe, start-and-end keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, a benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided. Project page: xiangbogaobarry.github.io/PISCO.

</details>

<details><summary><b>Chinese Abstract</b></summary>

The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and "cherry-picking" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is crucial to perform precise, targeted modifications.A cornerstone of this transition is video instance insertion, which requires inserting a specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction, and the faithful preservation of original dynamics - all achieved under minimal user effort.In this paper, we propose PISCO, a video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify a single keyframe, start-and-end keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction.To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation.We further construct PISCO-Bench, a benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided.Project page: xiangbogaobarry.github.io/PISCO.

</details>

---

## 206. Moving Beyond Functional Connectivity: Time-Series Modeling for fMRI-Based Brain Disorder Classification

**Chinese Title**: Moving Beyond Functional Connectivity: Time-Series Modeling for fMRI-Based Brain Disorder Classification

**Authors**: Guoqi Yu, Xiaowei Hu, Angelica I. Aviles-Rivero, Anqi Qiu, Shujun Wang

**Date**: 2026-02-09 | **arXiv**: [2602.08262v1](http://arxiv.org/abs/2602.08262v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08262v1)

**Code**: https://github.com/Levi-Ackman/DeCI.

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Functional magnetic resonance imaging (fMRI) enables non-invasive brain disorder classification by capturing blood-oxygen-level-dependent (BOLD) signals. However, most existing methods rely on functional connectivity (FC) via Pearson correlation, which reduces 4D BOLD signals to static 2D matrices, discarding temporal dynamics and capturing only linear inter-regional relationships. In this work, we benchmark state-of-the-art temporal models (e.g., time-series models such as PatchTST, TimesNet, and TimeMixer) on raw BOLD signals across five public datasets. Results show these models consistently outperform traditional FC-based approaches, highlighting the value of directly modeling temporal information such as cycle-like oscillatory fluctuations and drift-like slow baseline trends. Building on this insight, we propose DeCI, a simple yet effective framework that integrates two key principles: (i) Cycle and Drift Decomposition to disentangle cycle and drift within each ROI (Region of Interest); and (ii) Channel-Independence to model each ROI separately, improving robustness and reducing overfitting. Extensive experiments demonstrate that DeCI achieves superior classification accuracy and generalization compared to both FC-based and temporal baselines. Our findings advocate for a shift toward end-to-end temporal modeling in fMRI analysis to better capture complex brain dynamics. The code is available at https://github.com/Levi-Ackman/DeCI.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Functional magnetic resonance imaging (fMRI) enables non-invasive brain disorder classification by capturing blood-oxygen-level-dependent (BOLD) signals. However, most existing methods rely on functional connectivity (FC) via Pearson correlation, which reduces 4D BOLD signals to static 2D matrices, discarding temporal dynamics and capturing only linear inter-regional relationships.In this work, we benchmark state-of-the-art temporal models (e.g., time-series models such as PatchTST, TimesNet, and TimeMixer) on raw BOLD signals across five public datasets. Results show these models consistently outperform traditional FC-based approaches, highlighting the value of directly modeling temporal information such as cycle-like oscillatory fluctuations and drift-like slow baseline trends.Building on this insight, we propose DeCI, a simple yet effective framework that integrates two key principles: (i) Cycle and Drift Decomposition to disentangle cycle and drift within each ROI (Region of Interest); and (ii) Channel-Independence to model each ROI separately, improving robustness and reducing overfitting.Extensive experiments demonstrate that DeCI achieves superior classification accuracy and generalization compared to both FC-based and temporal baselines. Our findings advocate for a shift toward end-to-end temporal modeling in fMRI analysis to better capture complex brain dynamics. The code is available at https://github.com/Levi-Ackman/DeCI.

</details>

---

## 207. Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs

**Chinese Title**: Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs

**Authors**: Siqu Ou, Tianrui Wan, Zhiyuan Zhao, Junyu Gao, Xuelong Li

**Date**: 2026-02-09 | **arXiv**: [2602.08241v1](http://arxiv.org/abs/2602.08241v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08241v1)

**Categories**: cs.AI, cs.CV

<details><summary><b>Abstract</b></summary>

While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis shows that current MLLMs exhibit weak visual focus: early-stage visual misalignment is rarely corrected during subsequent reasoning, leading to error propagation and failed inferences. We argue that this limitation stems from inadequate credit assignment for visual attention during training. To address this issue, we propose SAYO, a visual reasoning model trained with a reinforcement learning (RL) framework that introduces a region-level visual attention-based reward. This reward explicitly aligns optimization signals with visually grounded reasoning steps, enabling the model to learn more reliable attention behaviors. Extensive experiments across multiple multimodal benchmarks demonstrate that SAYO consistently improves performance on diverse reasoning and perception tasks.

</details>

<details><summary><b>Chinese Abstract</b></summary>

While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies.Our analysis shows that current MLLMs exhibit weak visual focus: early-stage visual misalignment is rarely corrected during subsequent reasoning, leading to error propagation and failed inferences. We argue that this limitation stems from inadequate credit assignment for visual attention during training.To address this issue, we propose SAYO, a visual reasoning model trained with a reinforcement learning (RL) framework that introduces a region-level visual attention-based reward. This reward explicitly aligns optimization signals with visually grounded reasoning steps, enabling the model to learn more reliable attention behaviors.Extensive experiments across multiple multimodal benchmarks demonstrate that SAYO consistently improves performance on diverse reasoning and perception tasks.

</details>

---

## 208. When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning

**Chinese Title**: When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning

**Authors**: Shoubin Yu, Yue Zhang, Zun Wang, Jaehong Yoon, Huaxiu Yao et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08236v1](http://arxiv.org/abs/2602.08236v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08236v1)

**Categories**: cs.CV, cs.AI, cs.CL

<details><summary><b>Abstract</b></summary>

Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood.In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency.To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination.Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens.Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.

</details>

---

## 209. Generating Adversarial Events: A Motion-Aware Point Cloud Framework

**Chinese Title**: Generating Adversarial Events: A Motion-Aware Point Cloud Framework

**Authors**: Hongwei Ren, Youxin Jiang, Qifei Gu, Xiangqian Wu

**Date**: 2026-02-09 | **arXiv**: [2602.08230v1](http://arxiv.org/abs/2602.08230v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08230v1)

**Categories**: cs.CV, cs.AI

<details><summary><b>Abstract</b></summary>

Event cameras have been widely adopted in safety-critical domains such as autonomous driving, robotics, and human-computer interaction. A pressing challenge arises from the vulnerability of deep neural networks to adversarial examples, which poses a significant threat to the reliability of event-based systems. Nevertheless, research into adversarial attacks on events is scarce. This is primarily due to the non-differentiable nature of mainstream event representations, which hinders the extension of gradient-based attack methods. In this paper, we propose MA-ADV, a novel \textbf{M}otion-\textbf{A}ware \textbf{Adv}ersarial framework. To the best of our knowledge, this is the first work to generate adversarial events by leveraging point cloud representations. MA-ADV accounts for high-frequency noise in events and employs a diffusion-based approach to smooth perturbations, while fully leveraging the spatial and temporal relationships among events. Finally, MA-ADV identifies the minimal-cost perturbation through a combination of sample-wise Adam optimization, iterative refinement, and binary search. Extensive experimental results validate that MA-ADV ensures a 100\% attack success rate with minimal perturbation cost, and also demonstrate enhanced robustness against defenses, underscoring the critical security challenges facing future event-based perception systems.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Event cameras have been widely adopted in safety-critical domains such as autonomous driving, robotics, and human-computer interaction. A pressing challenge arises from the vulnerability of deep neural networks to adversarial examples, which poses a significant threat to the reliability of event-based systems. Nevertheless, research into adversarial attacks on events is scarce.This is primarily due to the non-differentiable nature of mainstream event representations, which hinders the extension of gradient-based attack methods. In this paper, we propose MA-ADV, a novel \textbf{M}otion-\textbf{A}ware \textbf{Adv}ersarial framework. To the best of our knowledge, this is the first work to generate adversarial events by leveraging point cloud representations.MA-ADV accounts for high-frequency noise in events and employs a diffusion-based approach to smooth perturbations, while fully leveraging the spatial and temporal relationships among events. Finally, MA-ADV identifies the minimal-cost perturbation through a combination of sample-wise Adam optimization, iterative refinement, and binary search.Extensive experimental results validate that MA-ADV ensures a 100\% attack success rate with minimal perturbation cost, and also demonstrate enhanced robustness against defenses, underscoring the critical security challenges facing future event-based perception systems.

</details>

---

## 210. Efficient-SAM2: Accelerating SAM2 with Object-Aware Visual Encoding and Memory Retrieval

**Chinese Title**: Efficient-SAM2: Accelerating SAM2 with Object-Aware Visual Encoding and Memory Retrieval

**Authors**: Jing Zhang, Zhikai Li, Xuewen Liu, Qingyi Gu

**Date**: 2026-02-09 | **arXiv**: [2602.08224v2](http://arxiv.org/abs/2602.08224v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.08224v2)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Segment Anything Model 2 (SAM2) shows excellent performance in video object segmentation tasks; however, the heavy computational burden hinders its application in real-time video processing. Although there have been efforts to improve the efficiency of SAM2, most of them focus on retraining a lightweight backbone, with little exploration into post-training acceleration. In this paper, we observe that SAM2 exhibits sparse perception pattern as biological vision, which provides opportunities for eliminating redundant computation and acceleration: i) In mask decoder, the attention primarily focuses on the foreground objects, whereas the image encoder in the earlier stage exhibits a broad attention span, which results in unnecessary computation to background regions. ii) In memory bank, only a small subset of tokens in each frame contribute significantly to memory attention, and the salient regions exhibit temporal consistency, making full-token computation redundant. With these insights, we propose Efficient-SAM2, which promotes SAM2 to adaptively focus on object regions while eliminating task-irrelevant computations, thereby significantly improving inference efficiency. Specifically, for image encoder, we propose object-aware Sparse Window Routing (SWR), a window-level computation allocation mechanism that leverages the consistency and saliency cues from the previous-frame decoder to route background regions into a lightweight shortcut branch. Moreover, for memory attention, we propose object-aware Sparse Memory Retrieval (SMR), which allows only the salient memory tokens in each frame to participate in computation, with the saliency pattern reused from their first recollection. With negligible additional parameters and minimal training overhead, Efficient-SAM2 delivers 1.68x speedup on SAM2.1-L model with only 1.0% accuracy drop on SA-V test set.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Segment Anything Model 2 (SAM2) shows excellent performance in video object segmentation tasks; however, the heavy computational burden hinders its application in real-time video processing. Although there have been efforts to improve the efficiency of SAM2, most of them focus on retraining a lightweight backbone, with little exploration into post-training acceleration.In this paper, we observe that SAM2 exhibits sparse perception pattern as biological vision, which provides opportunities for eliminating redundant computation and acceleration: i) In mask decoder, the attention primarily focuses on the foreground objects, whereas the image encoder in the earlier stage exhibits a broad attention span, which results in unnecessary computation to background regions.ii) In memory bank, only a small subset of tokens in each frame contribute significantly to memory attention, and the salient regions exhibit temporal consistency, making full-token computation redundant. With these insights, we propose Efficient-SAM2, which promotes SAM2 to adaptively focus on object regions while eliminating task-irrelevant computations, thereby significantly improving inference efficiency.Specifically, for image encoder, we propose object-aware Sparse Window Routing (SWR), a window-level computation allocation mechanism that leverages the consistency and saliency cues from the previous-frame decoder to route background regions into a lightweight shortcut branch.Moreover, for memory attention, we propose object-aware Sparse Memory Retrieval (SMR), which allows only the salient memory tokens in each frame to participate in computation, with the saliency pattern reused from their first recollection. With negligible additional parameters and minimal training overhead, Efficient-SAM2 delivers 1.68x speedup on SAM2.1-L model with only 1.0% accuracy drop on SA-V test set.

</details>

---

## 211. Chain-of-Caption: Training-free improvement of multimodal large language model on referring expression comprehension

**Chinese Title**: Chain-of-Caption: Training-free improvement of multimodal large language model on referring expression comprehension

**Authors**: Yik Lung Pang, Changjae Oh

**Date**: 2026-02-09 | **arXiv**: [2602.08211v1](http://arxiv.org/abs/2602.08211v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08211v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Given a textual description, the task of referring expression comprehension (REC) involves the localisation of the referred object in an image. Multimodal large language models (MLLMs) have achieved high accuracy on REC benchmarks through scaling up the model size and training data. Moreover, the performance of MLLMs can be further improved using techniques such as Chain-of-Thought and tool use, which provides additional visual or textual context to the model. In this paper, we analyse the effect of various techniques for providing additional visual and textual context via tool use to the MLLM and its effect on the REC task. Furthermore, we propose a training-free framework named Chain-of-Caption to improve the REC performance of MLLMs. We perform experiments on RefCOCO/RefCOCOg/RefCOCO+ and Ref-L4 datasets and show that individual textual or visual context can improve the REC performance without any fine-tuning. By combining multiple contexts, our training-free framework shows between 5% to 30% performance gain over the baseline model on accuracy at various Intersection over Union (IoU) thresholds.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Given a textual description, the task of referring expression comprehension (REC) involves the localisation of the referred object in an image. Multimodal large language models (MLLMs) have achieved high accuracy on REC benchmarks through scaling up the model size and training data.Moreover, the performance of MLLMs can be further improved using techniques such as Chain-of-Thought and tool use, which provides additional visual or textual context to the model. In this paper, we analyse the effect of various techniques for providing additional visual and textual context via tool use to the MLLM and its effect on the REC task.Furthermore, we propose a training-free framework named Chain-of-Caption to improve the REC performance of MLLMs. We perform experiments on RefCOCO/RefCOCOg/RefCOCO+ and Ref-L4 datasets and show that individual textual or visual context can improve the REC performance without any fine-tuning.By combining multiple contexts, our training-free framework shows between 5% to 30% performance gain over the baseline model on accuracy at various Intersection over Union (IoU) thresholds.

</details>

---

## 212. Geospatial-Reasoning-Driven Vocabulary-Agnostic Remote Sensing Semantic Segmentation

**Chinese Title**: Geospatial-Reasoning-Driven Vocabulary-Agnostic Remote Sensing Semantic Segmentation

**Authors**: Chufeng Zhou, Jian Wang, Xinyuan Liu, Xiaokang Zhang

**Date**: 2026-02-09 | **arXiv**: [2602.08206v1](http://arxiv.org/abs/2602.08206v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08206v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Open-vocabulary semantic segmentation has emerged as a promising research direction in remote sensing, enabling the recognition of diverse land-cover types beyond pre-defined category sets. However, existing methods predominantly rely on the passive mapping of visual features and textual embeddings. This ``appearance-based" paradigm lacks geospatial contextual awareness, leading to severe semantic ambiguity and misclassification when encountering land-cover classes with similar spectral features but distinct semantic attributes. To address this, we propose a Geospatial Reasoning Chain-of-Thought (GR-CoT) framework designed to enhance the scene understanding capabilities of Multimodal Large Language Models (MLLMs), thereby guiding open-vocabulary segmentation models toward precise mapping. The framework comprises two collaborative components: an offline knowledge distillation stream and an online instance reasoning stream. The offline stream establishes fine-grained category interpretation standards to resolve semantic conflicts between similar land-cover types. During online inference, the framework executes a sequential reasoning process involving macro-scenario anchoring, visual feature decoupling, and knowledge-driven decision synthesis. This process generates an image-adaptive vocabulary that guides downstream models to achieve pixel-level alignment with correct geographical semantics. Extensive experiments on the LoveDA and GID5 benchmarks demonstrate the superiority of our approach.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Open-vocabulary semantic segmentation has emerged as a promising research direction in remote sensing, enabling the recognition of diverse land-cover types beyond pre-defined category sets. However, existing methods predominantly rely on the passive mapping of visual features and textual embeddings.This ``appearance-based" paradigm lacks geospatial contextual awareness, leading to severe semantic ambiguity and misclassification when encountering land-cover classes with similar spectral features but distinct semantic attributes.To address this, we propose a Geospatial Reasoning Chain-of-Thought (GR-CoT) framework designed to enhance the scene understanding capabilities of Multimodal Large Language Models (MLLMs), thereby guiding open-vocabulary segmentation models toward precise mapping. The framework comprises two collaborative components: an offline knowledge distillation stream and an online instance reasoning stream.The offline stream establishes fine-grained category interpretation standards to resolve semantic conflicts between similar land-cover types. During online inference, the framework executes a sequential reasoning process involving macro-scenario anchoring, visual feature decoupling, and knowledge-driven decision synthesis.This process generates an image-adaptive vocabulary that guides downstream models to achieve pixel-level alignment with correct geographical semantics. Extensive experiments on the LoveDA and GID5 benchmarks demonstrate the superiority of our approach.

</details>

---

## 213. DAS-SK: An Adaptive Model Integrating Dual Atrous Separable and Selective Kernel CNN for Agriculture Semantic Segmentation

**Chinese Title**: DAS-SK: An Adaptive Model Integrating Dual Atrous Separable and Selective Kernel CNN for Agriculture Semantic Segmentation

**Authors**: Mei Ling Chee, Thangarajah Akilan, Aparna Ravindra Phalke, Kanchan Keisham

**Date**: 2026-02-09 | **arXiv**: [2602.08168v1](http://arxiv.org/abs/2602.08168v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08168v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Semantic segmentation in high-resolution agricultural imagery demands models that strike a careful balance between accuracy and computational efficiency to enable deployment in practical systems. In this work, we propose DAS-SK, a novel lightweight architecture that retrofits selective kernel convolution (SK-Conv) into the dual atrous separable convolution (DAS-Conv) module to strengthen multi-scale feature learning. The model further enhances the atrous spatial pyramid pooling (ASPP) module, enabling the capture of fine-grained local structures alongside global contextual information. Built upon a modified DeepLabV3 framework with two complementary backbones - MobileNetV3-Large and EfficientNet-B3, the DAS-SK model mitigates limitations associated with large dataset requirements, limited spectral generalization, and the high computational cost that typically restricts deployment on UAVs and other edge devices. Comprehensive experiments across three benchmarks: LandCover.ai, VDD, and PhenoBench, demonstrate that DAS-SK consistently achieves state-of-the-art performance, while being more efficient than CNN-, transformer-, and hybrid-based competitors. Notably, DAS-SK requires up to 21x fewer parameters and 19x fewer GFLOPs than top-performing transformer models. These findings establish DAS-SK as a robust, efficient, and scalable solution for real-time agricultural robotics and high-resolution remote sensing, with strong potential for broader deployment in other vision domains.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Semantic segmentation in high-resolution agricultural imagery demands models that strike a careful balance between accuracy and computational efficiency to enable deployment in practical systems. In this work, we propose DAS-SK, a novel lightweight architecture that retrofits selective kernel convolution (SK-Conv) into the dual atrous separable convolution (DAS-Conv) module to strengthen multi-scale feature learning.The model further enhances the atrous spatial pyramid pooling (ASPP) module, enabling the capture of fine-grained local structures alongside global contextual information.Built upon a modified DeepLabV3 framework with two complementary backbones - MobileNetV3-Large and EfficientNet-B3, the DAS-SK model mitigates limitations associated with large dataset requirements, limited spectral generalization, and the high computational cost that typically restricts deployment on UAVs and other edge devices.Comprehensive experiments across three benchmarks: LandCover.ai, VDD, and PhenoBench, demonstrate that DAS-SK consistently achieves state-of-the-art performance, while being more efficient than CNN-, transformer-, and hybrid-based competitors. Notably, DAS-SK requires up to 21x fewer parameters and 19x fewer GFLOPs than top-performing transformer models.These findings establish DAS-SK as a robust, efficient, and scalable solution for real-time agricultural robotics and high-resolution remote sensing, with strong potential for broader deployment in other vision domains.

</details>

---

## 214. Self-Supervised Bootstrapping of Action-Predictive Embodied Reasoning

**Chinese Title**: Self-Supervised Bootstrapping of Action-Predictive Embodied Reasoning

**Authors**: Milan Ganai, Katie Luo, Jonas Frey, Clark Barrett, Marco Pavone

**Date**: 2026-02-09 | **arXiv**: [2602.08167v1](http://arxiv.org/abs/2602.08167v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08167v1)

**Categories**: cs.RO, cs.AI, cs.CV, cs.LG

<details><summary><b>Abstract</b></summary>

Embodied Chain-of-Thought (CoT) reasoning has significantly enhanced Vision-Language-Action (VLA) models, yet current methods rely on rigid templates to specify reasoning primitives (e.g., objects in the scene, high-level plans, structural affordances). These templates can force policies to process irrelevant information that distracts from critical action-prediction signals. This creates a bottleneck: without successful policies, we cannot verify reasoning quality; without quality reasoning, we cannot build robust policies. We introduce R&B-EnCoRe, which enables models to bootstrap embodied reasoning from internet-scale knowledge through self-supervised refinement. By treating reasoning as a latent variable within importance-weighted variational inference, models can generate and distill a refined reasoning training dataset of embodiment-specific strategies without external rewards, verifiers, or human annotation. We validate R&B-EnCoRe across manipulation (Franka Panda in simulation, WidowX in hardware), legged navigation (bipedal, wheeled, bicycle, quadruped), and autonomous driving embodiments using various VLA architectures with 1B, 4B, 7B, and 30B parameters. Our approach achieves 28% gains in manipulation success, 101% improvement in navigation scores, and 21% reduction in collision-rate metric over models that indiscriminately reason about all available primitives. R&B-EnCoRe enables models to distill reasoning that is predictive of successful control, bypassing manual annotation engineering while grounding internet-scale knowledge in physical execution.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Embodied Chain-of-Thought (CoT) reasoning has significantly enhanced Vision-Language-Action (VLA) models, yet current methods rely on rigid templates to specify reasoning primitives (e.g., objects in the scene, high-level plans, structural affordances). These templates can force policies to process irrelevant information that distracts from critical action-prediction signals.This creates a bottleneck: without successful policies, we cannot verify reasoning quality; without quality reasoning, we cannot build robust policies. We introduce R&B-EnCoRe, which enables models to bootstrap embodied reasoning from internet-scale knowledge through self-supervised refinement.By treating reasoning as a latent variable within importance-weighted variational inference, models can generate and distill a refined reasoning training dataset of embodiment-specific strategies without external rewards, verifiers, or human annotation.We validate R&B-EnCoRe across manipulation (Franka Panda in simulation, WidowX in hardware), legged navigation (bipedal, wheeled, bicycle, quadruped), and autonomous driving embodiments using various VLA architectures with 1B, 4B, 7B, and 30B parameters.Our approach achieves 28% gains in manipulation success, 101% improvement in navigation scores, and 21% reduction in collision-rate metric over models that indiscriminately reason about all available primitives. R&B-EnCoRe enables models to distill reasoning that is predictive of successful control, bypassing manual annotation engineering while grounding internet-scale knowledge in physical execution.

</details>

---

## 215. STaR: Scalable Task-Conditioned Retrieval for Long-Horizon Multimodal Robot Memory

**Chinese Title**: STaR: Scalable Task-Conditioned Retrieval for Long-Horizon Multimodal Robot Memory

**Authors**: Mingfeng Yuan, Hao Zhang, Mahan Mohammadi, Runhao Li, Jinjun Shan et al.

**Date**: 2026-02-09 | **arXiv**: [2602.09255v1](http://arxiv.org/abs/2602.09255v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09255v1)

**Categories**: cs.RO, cs.AI

<details><summary><b>Abstract</b></summary>

Mobile robots are often deployed over long durations in diverse open, dynamic scenes, including indoor setting such as warehouses and manufacturing facilities, and outdoor settings such as agricultural and roadway operations. A core challenge is to build a scalable long-horizon memory that supports an agentic workflow for planning, retrieval, and reasoning over open-ended instructions at variable granularity, while producing precise, actionable answers for navigation. We present STaR, an agentic reasoning framework that (i) constructs a task-agnostic, multimodal long-term memory that generalizes to unseen queries while preserving fine-grained environmental semantics (object attributes, spatial relations, and dynamic events), and (ii) introduces a Scalable TaskConditioned Retrieval algorithm based on the Information Bottleneck principle to extract from long-term memory a compact, non-redundant, information-rich set of candidate memories for contextual reasoning. We evaluate STaR on NaVQA (mixed indoor/outdoor campus scenes) and WH-VQA, a customized warehouse benchmark with many visually similar objects built with Isaac Sim, emphasizing contextual reasoning. Across the two datasets, STaR consistently outperforms strong baselines, achieving higher success rates and markedly lower spatial error. We further deploy STaR on a real Husky wheeled robot in both indoor and outdoor environments, demonstrating robust longhorizon reasoning, scalability, and practical utility.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Mobile robots are often deployed over long durations in diverse open, dynamic scenes, including indoor setting such as warehouses and manufacturing facilities, and outdoor settings such as agricultural and roadway operations.A core challenge is to build a scalable long-horizon memory that supports an agentic workflow for planning, retrieval, and reasoning over open-ended instructions at variable granularity, while producing precise, actionable answers for navigation.We present STaR, an agentic reasoning framework that (i) constructs a task-agnostic, multimodal long-term memory that generalizes to unseen queries while preserving fine-grained environmental semantics (object attributes, spatial relations, and dynamic events), and (ii) introduces a Scalable TaskConditioned Retrieval algorithm based on the Information Bottleneck principle to extract from long-term memory a compact, non-redundant, information-rich set of candidate memories for contextual reasoning.We evaluate STaR on NaVQA (mixed indoor/outdoor campus scenes) and WH-VQA, a customized warehouse benchmark with many visually similar objects built with Isaac Sim, emphasizing contextual reasoning. Across the two datasets, STaR consistently outperforms strong baselines, achieving higher success rates and markedly lower spatial error.We further deploy STaR on a real Husky wheeled robot in both indoor and outdoor environments, demonstrating robust longhorizon reasoning, scalability, and practical utility.

</details>

---

## 216. Gradient Residual Connections

**Chinese Title**: Gradient Residual Connections

**Authors**: Yangchen Pan, Qizhen Ying, Philip Torr, Bo Liu

**Date**: 2026-02-09 | **arXiv**: [2602.09190v1](http://arxiv.org/abs/2602.09190v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09190v1)

**Categories**: cs.LG, cs.AI

<details><summary><b>Abstract</b></summary>

Existing work has linked properties of a function's gradient to the difficulty of function approximation. Motivated by these insights, we study how gradient information can be leveraged to improve neural network's ability to approximate high-frequency functions, and we propose a gradient-based residual connection as a complement to the standard identity skip connection used in residual networks. We provide simple theoretical intuition for why gradient information can help distinguish inputs and improve the approximation of functions with rapidly varying behaviour. On a synthetic regression task with a high-frequency sinusoidal ground truth, we show that conventional residual connections struggle to capture high-frequency patterns. In contrast, our gradient residual substantially improves approximation quality. We then introduce a convex combination of the standard and gradient residuals, allowing the network to flexibly control how strongly it relies on gradient information. After validating the design choices of our proposed method through an ablation study, we further validate our approach's utility on the single-image super-resolution task, where the underlying function may be high-frequency. Finally, on standard tasks such as image classification and segmentation, our method achieves performance comparable to standard residual networks, suggesting its broad utility.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Existing work has linked properties of a function's gradient to the difficulty of function approximation. Motivated by these insights, we study how gradient information can be leveraged to improve neural network's ability to approximate high-frequency functions, and we propose a gradient-based residual connection as a complement to the standard identity skip connection used in residual networks.We provide simple theoretical intuition for why gradient information can help distinguish inputs and improve the approximation of functions with rapidly varying behaviour. On a synthetic regression task with a high-frequency sinusoidal ground truth, we show that conventional residual connections struggle to capture high-frequency patterns. In contrast, our gradient residual substantially improves approximation quality.We then introduce a convex combination of the standard and gradient residuals, allowing the network to flexibly control how strongly it relies on gradient information. After validating the design choices of our proposed method through an ablation study, we further validate our approach's utility on the single-image super-resolution task, where the underlying function may be high-frequency.Finally, on standard tasks such as image classification and segmentation, our method achieves performance comparable to standard residual networks, suggesting its broad utility.

</details>

---

## 217. What do Geometric Hallucination Detection Metrics Actually Measure?

**Chinese Title**: What do Geometric Hallucination Detection Metrics Actually Measure?

**Authors**: Eric Yeats, John Buckheit, Sarah Scullen, Brendan Kennedy, Loc Truong et al.

**Date**: 2026-02-09 | **arXiv**: [2602.09158v1](http://arxiv.org/abs/2602.09158v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09158v1)

**Categories**: cs.LG, cs.AI

<details><summary><b>Abstract</b></summary>

Hallucination remains a barrier to deploying generative models in high-consequence applications. This is especially true in cases where external ground truth is not readily available to validate model outputs. This situation has motivated the study of geometric signals in the internal state of an LLM that are predictive of hallucination and require limited external knowledge. Given that there are a range of factors that can lead model output to be called a hallucination (e.g., irrelevance vs incoherence), in this paper we ask what specific properties of a hallucination these geometric statistics actually capture. To assess this, we generate a synthetic dataset which varies distinct properties of output associated with hallucination. This includes output correctness, confidence, relevance, coherence, and completeness. We find that different geometric statistics capture different types of hallucinations. Along the way we show that many existing geometric detection methods have substantial sensitivity to shifts in task domain (e.g., math questions vs. history questions). Motivated by this, we introduce a simple normalization method to mitigate the effect of domain shift on geometric statistics, leading to AUROC gains of +34 points in multi-domain settings.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Hallucination remains a barrier to deploying generative models in high-consequence applications. This is especially true in cases where external ground truth is not readily available to validate model outputs. This situation has motivated the study of geometric signals in the internal state of an LLM that are predictive of hallucination and require limited external knowledge.Given that there are a range of factors that can lead model output to be called a hallucination (e.g., irrelevance vs incoherence), in this paper we ask what specific properties of a hallucination these geometric statistics actually capture. To assess this, we generate a synthetic dataset which varies distinct properties of output associated with hallucination. This includes output correctness, confidence, relevance, coherence, and completeness.We find that different geometric statistics capture different types of hallucinations. Along the way we show that many existing geometric detection methods have substantial sensitivity to shifts in task domain (e.g., math questions vs. history questions). Motivated by this, we introduce a simple normalization method to mitigate the effect of domain shift on geometric statistics, leading to AUROC gains of +34 points in multi-domain settings.

</details>

---

## 218. Uncertainty-Aware Multimodal Emotion Recognition through Dirichlet Parameterization

**Chinese Title**: Uncertainty-Aware Multimodal Emotion Recognition through Dirichlet Parameterization

**Authors**: RÃ©mi Grzeczkowicz, Eric Soriano, Ali Janati, Miyu Zhang, Gerard Comas-Quiles et al.

**Date**: 2026-02-09 | **arXiv**: [2602.09121v1](http://arxiv.org/abs/2602.09121v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09121v1)

**Categories**: cs.AI

<details><summary><b>Abstract</b></summary>

In this work, we present a lightweight and privacy-preserving Multimodal Emotion Recognition (MER) framework designed for deployment on edge devices. To demonstrate framework's versatility, our implementation uses three modalities - speech, text and facial imagery. However, the system is fully modular, and can be extended to support other modalities or tasks. Each modality is processed through a dedicated backbone optimized for inference efficiency: Emotion2Vec for speech, a ResNet-based model for facial expressions, and DistilRoBERTa for text. To reconcile uncertainty across modalities, we introduce a model- and task-agnostic fusion mechanism grounded in Dempster-Shafer theory and Dirichlet evidence. Operating directly on model logits, this approach captures predictive uncertainty without requiring additional training or joint distribution estimation, making it broadly applicable beyond emotion recognition. Validation on five benchmark datasets (eNTERFACE05, MEAD, MELD, RAVDESS and CREMA-D) show that our method achieves competitive accuracy while remaining computationally efficient and robust to ambiguous or missing inputs. Overall, the proposed framework emphasizes modularity, scalability, and real-world feasibility, paving the way toward uncertainty-aware multimodal systems for healthcare, human-computer interaction, and other emotion-informed applications.

</details>

<details><summary><b>Chinese Abstract</b></summary>

In this work, we present a lightweight and privacy-preserving Multimodal Emotion Recognition (MER) framework designed for deployment on edge devices. To demonstrate framework's versatility, our implementation uses three modalities - speech, text and facial imagery. However, the system is fully modular, and can be extended to support other modalities or tasks.Each modality is processed through a dedicated backbone optimized for inference efficiency: Emotion2Vec for speech, a ResNet-based model for facial expressions, and DistilRoBERTa for text. To reconcile uncertainty across modalities, we introduce a model- and task-agnostic fusion mechanism grounded in Dempster-Shafer theory and Dirichlet evidence.Operating directly on model logits, this approach captures predictive uncertainty without requiring additional training or joint distribution estimation, making it broadly applicable beyond emotion recognition. Validation on five benchmark datasets (eNTERFACE05, MEAD, MELD, RAVDESS and CREMA-D) show that our method achieves competitive accuracy while remaining computationally efficient and robust to ambiguous or missing inputs.Overall, the proposed framework emphasizes modularity, scalability, and real-world feasibility, paving the way toward uncertainty-aware multimodal systems for healthcare, human-computer interaction, and other emotion-informed applications.

</details>

---

## 219. ANCRe: Adaptive Neural Connection Reassignment for Efficient Depth Scaling

**Chinese Title**: ANCRe: Adaptive Neural Connection Reassignment for Efficient Depth Scaling

**Authors**: Yilang Zhang, Bingcong Li, Niao He, Georgios B. Giannakis

**Date**: 2026-02-09 | **arXiv**: [2602.09009v1](http://arxiv.org/abs/2602.09009v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09009v1)

**Categories**: cs.LG, cs.AI

<details><summary><b>Abstract</b></summary>

Scaling network depth has been a central driver behind the success of modern foundation models, yet recent investigations suggest that deep layers are often underutilized. This paper revisits the default mechanism for deepening neural networks, namely residual connections, from an optimization perspective. Rigorous analysis proves that the layout of residual connections can fundamentally shape convergence behavior, and even induces an exponential gap in convergence rates. Prompted by this insight, we introduce adaptive neural connection reassignment (ANCRe), a principled and lightweight framework that parameterizes and learns residual connectivities from the data. ANCRe adaptively reassigns residual connections with negligible computational and memory overhead ($<1\%$), while enabling more effective utilization of network depth. Extensive numerical tests across pre-training of large language models, diffusion models, and deep ResNets demonstrate consistently accelerated convergence, boosted performance, and enhanced depth efficiency over conventional residual connections.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Scaling network depth has been a central driver behind the success of modern foundation models, yet recent investigations suggest that deep layers are often underutilized. This paper revisits the default mechanism for deepening neural networks, namely residual connections, from an optimization perspective.Rigorous analysis proves that the layout of residual connections can fundamentally shape convergence behavior, and even induces an exponential gap in convergence rates. Prompted by this insight, we introduce adaptive neural connection reassignment (ANCRe), a principled and lightweight framework that parameterizes and learns residual connectivities from the data.ANCRe adaptively reassigns residual connections with negligible computational and memory overhead ($<1\%$), while enabling more effective utilization of network depth. Extensive numerical tests across pre-training of large language models, diffusion models, and deep ResNets demonstrate consistently accelerated convergence, boosted performance, and enhanced depth efficiency over conventional residual connections.

</details>

---

## 220. ARO: A New Lens On Matrix Optimization For Large Models

**Chinese Title**: ARO: A New Lens On Matrix Optimization For Large Models

**Authors**: Wenbo Gong, Javier Zazo, Qijun Luo, Puqian Wang, James Hensman et al.

**Date**: 2026-02-09 | **arXiv**: [2602.09006v1](http://arxiv.org/abs/2602.09006v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09006v1)

**Categories**: cs.LG, cs.AI, math.OC

<details><summary><b>Abstract</b></summary>

Matrix-based optimizers have attracted growing interest for improving LLM training efficiency, with significant progress centered on orthogonalization/whitening based methods. While yielding substantial performance gains, a fundamental question arises: can we develop new paradigms beyond orthogonalization, pushing the efficiency frontier further? We present \textbf{Adaptively Rotated Optimization (ARO}, a new matrix optimization framework that treats gradient rotation as a first class design principle. ARO accelerates LLM training by performing normed steepest descent in a rotated coordinate system, where the rotation is determined by a novel norm-informed policy. This perspective yields update rules that go beyond existing orthogonalization and whitening optimizers, improving sample efficiency in practice. To make comparisons reliable, we propose a rigorously controlled benchmarking protocol that reduces confounding and bias. Under this protocol, ARO consistently outperforms AdamW (by 1.3 $\sim$1.35$\times$) and orthogonalization methods (by 1.1$\sim$1.15$\times$) in LLM pretraining at up to 8B activated parameters, and up to $8\times$ overtrain budget, without evidence of diminishing returns. Finally, we discuss how ARO can be reformulated as a symmetry-aware optimizer grounded in rotational symmetries of residual streams, motivating advanced designs that enable computationally efficient exploitation of cross-layer/cross module couplings.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Matrix-based optimizers have attracted growing interest for improving LLM training efficiency, with significant progress centered on orthogonalization/whitening based methods. While yielding substantial performance gains, a fundamental question arises: can we develop new paradigms beyond orthogonalization, pushing the efficiency frontier further?We present \textbf{Adaptively Rotated Optimization (ARO}, a new matrix optimization framework that treats gradient rotation as a first class design principle. ARO accelerates LLM training by performing normed steepest descent in a rotated coordinate system, where the rotation is determined by a novel norm-informed policy.This perspective yields update rules that go beyond existing orthogonalization and whitening optimizers, improving sample efficiency in practice. To make comparisons reliable, we propose a rigorously controlled benchmarking protocol that reduces confounding and bias.Under this protocol, ARO consistently outperforms AdamW (by 1.3 $\sim$1.35$\times$) and orthogonalization methods (by 1.1$\sim$1.15$\times$) in LLM pretraining at up to 8B activated parameters, and up to $8\times$ overtrain budget, without evidence of diminishing returns.Finally, we discuss how ARO can be reformulated as a symmetry-aware optimizer grounded in rotational symmetries of residual streams, motivating advanced designs that enable computationally efficient exploitation of cross-layer/cross module couplings.

</details>

---

## 221. Data Science and Technology Towards AGI Part I: Tiered Data Management

**Chinese Title**: Data Science and Technology Towards AGI Part I: Tiered Data Management

**Authors**: Yudong Wang, Zixuan Fu, Hengyu Zhao, Chen Zhao, Chuyue Zhou et al.

**Date**: 2026-02-09 | **arXiv**: [2602.09003v1](http://arxiv.org/abs/2602.09003v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09003v1)

**Categories**: cs.AI, cs.CL

<details><summary><b>Abstract</b></summary>

The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency. In this work, we argue that the development of AGI is entering a new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities. To implement this vision, we propose a tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints. Specifically, we introduce an L0-L4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers. Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training, mid-training, and alignment. The framework balances data quality, acquisition cost, and marginal training benefit, providing a systematic approach to scalable and sustainable data management. We validate the effectiveness of the proposed framework through empirical studies, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community.

</details>

<details><summary><b>Chinese Abstract</b></summary>

The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency.In this work, we argue that the development of AGI is entering a new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities. To implement this vision, we propose a tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints.Specifically, we introduce an L0-L4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers.Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training, mid-training, and alignment. The framework balances data quality, acquisition cost, and marginal training benefit, providing a systematic approach to scalable and sustainable data management.We validate the effectiveness of the proposed framework through empirical studies, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community.

</details>

---

## 222. From Obstacles to Etiquette: Robot Social Navigation with VLM-Informed Path Selection

**Chinese Title**: From Obstacles to Etiquette: Robot Social Navigation with VLM-Informed Path Selection

**Authors**: Zilin Fang, Anxing Xiao, David Hsu, Gim Hee Lee

**Date**: 2026-02-09 | **arXiv**: [2602.09002v1](http://arxiv.org/abs/2602.09002v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09002v1)

**Project**: https://path-etiquette.github.io  **Categories**: cs.RO, cs.AI

<details><summary><b>Abstract</b></summary>

Navigating socially in human environments requires more than satisfying geometric constraints, as collision-free paths may still interfere with ongoing activities or conflict with social norms. Addressing this challenge calls for analyzing interactions between agents and incorporating common-sense reasoning into planning. This paper presents a social robot navigation framework that integrates geometric planning with contextual social reasoning. The system first extracts obstacles and human dynamics to generate geometrically feasible candidate paths, then leverages a fine-tuned vision-language model (VLM) to evaluate these paths, informed by contextually grounded social expectations, selecting a socially optimized path for the controller. This task-specific VLM distills social reasoning from large foundation models into a smaller and efficient model, allowing the framework to perform real-time adaptation in diverse human-robot interaction contexts. Experiments in four social navigation contexts demonstrate that our method achieves the best overall performance with the lowest personal space violation duration, the minimal pedestrian-facing time, and no social zone intrusions. Project page: https://path-etiquette.github.io

</details>

<details><summary><b>Chinese Abstract</b></summary>

Navigating socially in human environments requires more than satisfying geometric constraints, as collision-free paths may still interfere with ongoing activities or conflict with social norms. Addressing this challenge calls for analyzing interactions between agents and incorporating common-sense reasoning into planning. This paper presents a social robot navigation framework that integrates geometric planning with contextual social reasoning.The system first extracts obstacles and human dynamics to generate geometrically feasible candidate paths, then leverages a fine-tuned vision-language model (VLM) to evaluate these paths, informed by contextually grounded social expectations, selecting a socially optimized path for the controller.This task-specific VLM distills social reasoning from large foundation models into a smaller and efficient model, allowing the framework to perform real-time adaptation in diverse human-robot interaction contexts. Experiments in four social navigation contexts demonstrate that our method achieves the best overall performance with the lowest personal space violation duration, the minimal pedestrian-facing time, and no social zone intrusions.Project page: https://path-etiquette.github.io

</details>

---

## 223. iGRPO: Self-Feedback-Driven LLM Reasoning

**Chinese Title**: iGRPO: Self-Feedback-Driven LLM Reasoning

**Authors**: Ali Hatamizadeh, Shrimai Prabhumoye, Igor Gitman, Ximing Lu, Seungju Han et al.

**Date**: 2026-02-09 | **arXiv**: [2602.09000v1](http://arxiv.org/abs/2602.09000v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09000v1)

**Categories**: cs.AI

<details><summary><b>Abstract</b></summary>

Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\% and 79.64\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability.Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts.In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt.Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\% and 79.64\% on AIME24 and AIME25, respectively.Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.

</details>

---

## 224. stable-worldmodel-v1: Reproducible World Modeling Research and Evaluation

**Chinese Title**: stable-worldmodel-v1: Reproducible World Modeling Research and Evaluation

**Authors**: Lucas Maes, Quentin Le Lidec, Dan Haramati, Nassim Massaudi, Damien Scieur et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08968v1](http://arxiv.org/abs/2602.08968v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08968v1)

**Categories**: cs.AI

<details><summary><b>Abstract</b></summary>

World Models have emerged as a powerful paradigm for learning compact, predictive representations of environment dynamics, enabling agents to reason, plan, and generalize beyond direct experience. Despite recent interest in World Models, most available implementations remain publication-specific, severely limiting their reusability, increasing the risk of bugs, and reducing evaluation standardization. To mitigate these issues, we introduce stable-worldmodel (SWM), a modular, tested, and documented world-model research ecosystem that provides efficient data-collection tools, standardized environments, planning algorithms, and baseline implementations. In addition, each environment in SWM enables controllable factors of variation, including visual and physical properties, to support robustness and continual learning research. Finally, we demonstrate the utility of SWM by using it to study zero-shot robustness in DINO-WM.

</details>

<details><summary><b>Chinese Abstract</b></summary>

World Models have emerged as a powerful paradigm for learning compact, predictive representations of environment dynamics, enabling agents to reason, plan, and generalize beyond direct experience. Despite recent interest in World Models, most available implementations remain publication-specific, severely limiting their reusability, increasing the risk of bugs, and reducing evaluation standardization.To mitigate these issues, we introduce stable-worldmodel (SWM), a modular, tested, and documented world-model research ecosystem that provides efficient data-collection tools, standardized environments, planning algorithms, and baseline implementations. In addition, each environment in SWM enables controllable factors of variation, including visual and physical properties, to support robustness and continual learning research.Finally, we demonstrate the utility of SWM by using it to study zero-shot robustness in DINO-WM.

</details>

---

## 225. Looping Back to Move Forward: Recursive Transformers for Efficient and Flexible Large Multimodal Models

**Chinese Title**: Looping Back to Move Forward: Recursive Transformers for Efficient and Flexible Large Multimodal Models

**Authors**: Ruihan Xu, Yuting Gao, Lan Wang, Jianing Li, Weihao Chen et al.

**Date**: 2026-02-09 | **arXiv**: [2602.09080v1](http://arxiv.org/abs/2602.09080v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09080v1)

**Categories**: cs.LG, cs.AI

<details><summary><b>Abstract</b></summary>

Large Multimodal Models (LMMs) have achieved remarkable success in vision-language tasks, yet their vast parameter counts are often underutilized during both training and inference. In this work, we embrace the idea of looping back to move forward: reusing model parameters through recursive refinement to extract stronger multimodal representations without increasing model size. We propose RecursiveVLM, a recursive Transformer architecture tailored for LMMs. Two key innovations enable effective looping: (i) a Recursive Connector that aligns features across recursion steps by fusing intermediate-layer hidden states and applying modality-specific projections, respecting the distinct statistical structures of vision and language tokens; (ii) a Monotonic Recursion Loss that supervises every step and guarantees performance improves monotonically with recursion depth. This design transforms recursion into an on-demand refinement mechanism: delivering strong results with few loops on resource-constrained devices and progressively improving outputs when more computation resources are available. Experiments show consistent gains of +3% over standard Transformers and +7% over vanilla recursive baselines, demonstrating that strategic looping is a powerful path toward efficient, deployment-adaptive LMMs.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Large Multimodal Models (LMMs) have achieved remarkable success in vision-language tasks, yet their vast parameter counts are often underutilized during both training and inference. In this work, we embrace the idea of looping back to move forward: reusing model parameters through recursive refinement to extract stronger multimodal representations without increasing model size.We propose RecursiveVLM, a recursive Transformer architecture tailored for LMMs.Two key innovations enable effective looping: (i) a Recursive Connector that aligns features across recursion steps by fusing intermediate-layer hidden states and applying modality-specific projections, respecting the distinct statistical structures of vision and language tokens; (ii) a Monotonic Recursion Loss that supervises every step and guarantees performance improves monotonically with recursion depth.This design transforms recursion into an on-demand refinement mechanism: delivering strong results with few loops on resource-constrained devices and progressively improving outputs when more computation resources are available. Experiments show consistent gains of +3% over standard Transformers and +7% over vanilla recursive baselines, demonstrating that strategic looping is a powerful path toward efficient, deployment-adaptive LMMs.

</details>

---

## 226. CausalT5K: Diagnosing and Informing Refusal for Trustworthy Causal Reasoning of Skepticism, Sycophancy, Detection-Correction, and Rung Collapse

**Chinese Title**: CausalT5K: Diagnosing and Informing Refusal for Trustworthy Causal Reasoning of Skepticism, Sycophancy, Detection-Correction, and Rung Collapse

**Authors**: Longling Geng, Andy Ouyang, Theodore Wu, Daphne Barretto, Matthew John Hayes et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08939v1](http://arxiv.org/abs/2602.08939v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08939v1)

**Code**: https://github.com/genglongling/CausalT5kBench

**Categories**: cs.AI

<details><summary><b>Abstract</b></summary>

LLM failures in causal reasoning, including sycophancy, rung collapse, and miscalibrated refusal, are well-documented, yet progress on remediation is slow because no benchmark enables systematic diagnosis. We introduce CausalT5K, a diagnostic benchmark of over 5,000 cases across 10 domains that tests three critical capabilities: (1) detecting rung collapse, where models answer interventional queries with associational evidence; (2) resisting sycophantic drift under adversarial pressure; and (3) generating Wise Refusals that specify missing information when evidence is underdetermined. Unlike synthetic benchmarks, CausalT5K embeds causal traps in realistic narratives and decomposes performance into Utility (sensitivity) and Safety (specificity), revealing failure modes invisible to aggregate accuracy. Developed through a rigorous human-machine collaborative pipeline involving 40 domain experts, iterative cross-validation cycles, and composite verification via rule-based, LLM, and human scoring, CausalT5K implements Pearl's Ladder of Causation as research infrastructure. Preliminary experiments reveal a Four-Quadrant Control Landscape where static audit policies universally fail, a finding that demonstrates CausalT5K's value for advancing trustworthy reasoning systems. Repository: https://github.com/genglongling/CausalT5kBench

</details>

<details><summary><b>Chinese Abstract</b></summary>

LLM failures in causal reasoning, including sycophancy, rung collapse, and miscalibrated refusal, are well-documented, yet progress on remediation is slow because no benchmark enables systematic diagnosis.We introduce CausalT5K, a diagnostic benchmark of over 5,000 cases across 10 domains that tests three critical capabilities: (1) detecting rung collapse, where models answer interventional queries with associational evidence; (2) resisting sycophantic drift under adversarial pressure; and (3) generating Wise Refusals that specify missing information when evidence is underdetermined.Unlike synthetic benchmarks, CausalT5K embeds causal traps in realistic narratives and decomposes performance into Utility (sensitivity) and Safety (specificity), revealing failure modes invisible to aggregate accuracy.Developed through a rigorous human-machine collaborative pipeline involving 40 domain experts, iterative cross-validation cycles, and composite verification via rule-based, LLM, and human scoring, CausalT5K implements Pearl's Ladder of Causation as research infrastructure.Preliminary experiments reveal a Four-Quadrant Control Landscape where static audit policies universally fail, a finding that demonstrates CausalT5K's value for advancing trustworthy reasoning systems. Repository: https://github.com/genglongling/CausalT5kBench

</details>

---

## 227. Automatic In-Domain Exemplar Construction and LLM-Based Refinement of Multi-LLM Expansions for Query Expansion

**Chinese Title**: Automatic In-Domain Exemplar Construction and LLM-Based Refinement of Multi-LLM Expansions for Query Expansion

**Authors**: Minghan Li, Ercong Nie, Siqi Zhao, Tongna Chen, Huiping Huang et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08917v1](http://arxiv.org/abs/2602.08917v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08917v1)

**Categories**: cs.IR, cs.AI

<details><summary><b>Abstract</b></summary>

Query expansion with large language models is promising but often relies on hand-crafted prompts, manually chosen exemplars, or a single LLM, making it non-scalable and sensitive to domain shift. We present an automated, domain-adaptive QE framework that builds in-domain exemplar pools by harvesting pseudo-relevant passages using a BM25-MonoT5 pipeline. A training-free cluster-based strategy selects diverse demonstrations, yielding strong and stable in-context QE without supervision. To further exploit model complementarity, we introduce a two-LLM ensemble in which two heterogeneous LLMs independently generate expansions and a refinement LLM consolidates them into one coherent expansion. Across TREC DL20, DBPedia, and SciFact, the refined ensemble delivers consistent and statistically significant gains over BM25, Rocchio, zero-shot, and fixed few-shot baselines. The framework offers a reproducible testbed for exemplar selection and multi-LLM generation, and a practical, label-free solution for real-world QE.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Query expansion with large language models is promising but often relies on hand-crafted prompts, manually chosen exemplars, or a single LLM, making it non-scalable and sensitive to domain shift. We present an automated, domain-adaptive QE framework that builds in-domain exemplar pools by harvesting pseudo-relevant passages using a BM25-MonoT5 pipeline.A training-free cluster-based strategy selects diverse demonstrations, yielding strong and stable in-context QE without supervision. To further exploit model complementarity, we introduce a two-LLM ensemble in which two heterogeneous LLMs independently generate expansions and a refinement LLM consolidates them into one coherent expansion.Across TREC DL20, DBPedia, and SciFact, the refined ensemble delivers consistent and statistically significant gains over BM25, Rocchio, zero-shot, and fixed few-shot baselines. The framework offers a reproducible testbed for exemplar selection and multi-LLM generation, and a practical, label-free solution for real-world QE.

</details>

---

## 228. Efficient and Stable Reinforcement Learning for Diffusion Language Models

**Chinese Title**: Efficient and Stable Reinforcement Learning for Diffusion Language Models

**Authors**: Jiawei Liu, Xiting Wang, Yuanyuan Zhong, Defu Lian, Yu Yang

**Date**: 2026-02-09 | **arXiv**: [2602.08905v1](http://arxiv.org/abs/2602.08905v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08905v1)

**Code**: https://github.com/Lolo1222/STP.

**Categories**: cs.AI

<details><summary><b>Abstract</b></summary>

Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framework designed to simultaneously improve the efficiency and stability of RL for dLLMs. STP compresses the redundancy in the generative process through: (1) \textit{spatial pruning}, which constrains the exploration space using static priors; and (2) \textit{temporal pruning}, which bypasses redundant late-stage refinement steps. Our theoretical analysis demonstrates that STP strictly reduces the variance of the log-likelihood estimation, thereby ensuring more stable policy updates. Extensive experiments demonstrate that STP surpasses state-of-the-art baselines in both efficiency and accuracy. Our code is available at https://github.com/Lolo1222/STP.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framework designed to simultaneously improve the efficiency and stability of RL for dLLMs.STP compresses the redundancy in the generative process through: (1) \textit{spatial pruning}, which constrains the exploration space using static priors; and (2) \textit{temporal pruning}, which bypasses redundant late-stage refinement steps. Our theoretical analysis demonstrates that STP strictly reduces the variance of the log-likelihood estimation, thereby ensuring more stable policy updates.Extensive experiments demonstrate that STP surpasses state-of-the-art baselines in both efficiency and accuracy. Our code is available at https://github.com/Lolo1222/STP.

</details>

---

## 229. Learning Potentials for Dynamic Matching and Application to Heart Transplantation

**Chinese Title**: Learning Potentials for Dynamic Matching and Application to Heart Transplantation

**Authors**: Itai Zilberstein, Ioannis Anagnostides, Zachary W. Sollie, Arman Kilic, Tuomas Sandholm

**Date**: 2026-02-09 | **arXiv**: [2602.08878v1](http://arxiv.org/abs/2602.08878v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08878v1)

**Categories**: cs.LG, cs.AI

<details><summary><b>Abstract</b></summary>

Each year, thousands of patients in need of heart transplants face life-threatening wait times due to organ scarcity. While allocation policies aim to maximize population-level outcomes, current approaches often fail to account for the dynamic arrival of organs and the composition of waitlisted candidates, thereby hampering efficiency. The United States is transitioning from rigid, rule-based allocation to more flexible data-driven models. In this paper, we propose a novel framework for non-myopic policy optimization in general online matching relying on potentials, a concept originally introduced for kidney exchange. We develop scalable and accurate ways of learning potentials that are higher-dimensional and more expressive than prior approaches. Our approach is a form of self-supervised imitation learning: the potentials are trained to mimic an omniscient algorithm that has perfect foresight. We focus on the application of heart transplant allocation and demonstrate, using real historical data, that our policies significantly outperform prior approaches -- including the current US status quo policy and the proposed continuous distribution framework -- in optimizing for population-level outcomes. Our analysis and methods come at a pivotal moment in US policy, as the current heart transplant allocation system is under review. We propose a scalable and theoretically grounded path toward more effective organ allocation.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Each year, thousands of patients in need of heart transplants face life-threatening wait times due to organ scarcity. While allocation policies aim to maximize population-level outcomes, current approaches often fail to account for the dynamic arrival of organs and the composition of waitlisted candidates, thereby hampering efficiency. The United States is transitioning from rigid, rule-based allocation to more flexible data-driven models.In this paper, we propose a novel framework for non-myopic policy optimization in general online matching relying on potentials, a concept originally introduced for kidney exchange. We develop scalable and accurate ways of learning potentials that are higher-dimensional and more expressive than prior approaches.Our approach is a form of self-supervised imitation learning: the potentials are trained to mimic an omniscient algorithm that has perfect foresight.We focus on the application of heart transplant allocation and demonstrate, using real historical data, that our policies significantly outperform prior approaches -- including the current US status quo policy and the proposed continuous distribution framework -- in optimizing for population-level outcomes. Our analysis and methods come at a pivotal moment in US policy, as the current heart transplant allocation system is under review.We propose a scalable and theoretically grounded path toward more effective organ allocation.

</details>

---

## 230. Understanding Dynamic Compute Allocation in Recurrent Transformers

**Chinese Title**: Understanding Dynamic Compute Allocation in Recurrent Transformers

**Authors**: Ibraheem Muhammad Moosa, Suhas Lohit, Ye Wang, Moitreya Chatterjee, Wenpeng Yin

**Date**: 2026-02-09 | **arXiv**: [2602.08864v1](http://arxiv.org/abs/2602.08864v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08864v1)

**Categories**: cs.CL, cs.AI, cs.LG

<details><summary><b>Abstract</b></summary>

Token-level adaptive computation seeks to reduce inference cost by allocating more computation to harder tokens and less to easier ones. However, prior work is primarily evaluated on natural-language benchmarks using task-level metrics, where token-level difficulty is unobservable and confounded with architectural factors, making it unclear whether compute allocation truly aligns with underlying complexity. We address this gap through three contributions. First, we introduce a complexity-controlled evaluation paradigm using algorithmic and synthetic language tasks with parameterized difficulty, enabling direct testing of token-level compute allocation. Second, we propose ANIRA, a unified recurrent Transformer framework that supports per-token variable-depth computation while isolating compute allocation decisions from other model factors. Third, we use this framework to conduct a systematic analysis of token-level adaptive computation across alignment with complexity, generalization, and decision timing. Our results show that compute allocation aligned with task complexity can emerge without explicit difficulty supervision, but such alignment does not imply algorithmic generalization: models fail to extrapolate to unseen input sizes despite allocating additional computation. We further find that early compute decisions rely on static structural cues, whereas online halting more closely tracks algorithmic execution state.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Token-level adaptive computation seeks to reduce inference cost by allocating more computation to harder tokens and less to easier ones. However, prior work is primarily evaluated on natural-language benchmarks using task-level metrics, where token-level difficulty is unobservable and confounded with architectural factors, making it unclear whether compute allocation truly aligns with underlying complexity.We address this gap through three contributions. First, we introduce a complexity-controlled evaluation paradigm using algorithmic and synthetic language tasks with parameterized difficulty, enabling direct testing of token-level compute allocation. Second, we propose ANIRA, a unified recurrent Transformer framework that supports per-token variable-depth computation while isolating compute allocation decisions from other model factors.Third, we use this framework to conduct a systematic analysis of token-level adaptive computation across alignment with complexity, generalization, and decision timing. Our results show that compute allocation aligned with task complexity can emerge without explicit difficulty supervision, but such alignment does not imply algorithmic generalization: models fail to extrapolate to unseen input sizes despite allocating additional computation.We further find that early compute decisions rely on static structural cues, whereas online halting more closely tracks algorithmic execution state.

</details>

---

## 231. Affective Flow Language Model for Emotional Support Conversation

**Chinese Title**: Affective Flow Language Model for Emotional Support Conversation

**Authors**: Chenghui Zou, Ning Wang, Tiesunlong Shen, Luwei Xiao, Chuan Ma et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08826v1](http://arxiv.org/abs/2602.08826v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08826v1)

**Code**: https://github.com/chzou25-lgtm/AffectiveFlow.

**Categories**: cs.CL, cs.AI

<details><summary><b>Abstract</b></summary>

Large language models (LLMs) have been widely applied to emotional support conversation (ESC). However, complex multi-turn support remains challenging.This is because existing alignment schemes rely on sparse outcome-level signals, thus offering limited supervision for intermediate strategy decisions. To fill this gap, this paper proposes affective flow language model for emotional support conversation (AFlow), a framework that introduces fine-grained supervision on dialogue prefixes by modeling a continuous affective flow along multi-turn trajectories. AFlow can estimate intermediate utility over searched trajectories and learn preference-consistent strategy transitions. To improve strategy coherence and empathetic response quality, a subpath-level flow-balance objective is presented to propagate preference signals to intermediate states. Experiment results show consistent and significant improvements over competitive baselines in diverse emotional contexts. Remarkably, AFlow with a compact open-source backbone outperforms proprietary LMMs such as GPT-4o and Claude-3.5 on major ESC metrics. Our code is available at https://github.com/chzou25-lgtm/AffectiveFlow.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Large language models (LLMs) have been widely applied to emotional support conversation (ESC). However, complex multi-turn support remains challenging.This is because existing alignment schemes rely on sparse outcome-level signals, thus offering limited supervision for intermediate strategy decisions.To fill this gap, this paper proposes affective flow language model for emotional support conversation (AFlow), a framework that introduces fine-grained supervision on dialogue prefixes by modeling a continuous affective flow along multi-turn trajectories. AFlow can estimate intermediate utility over searched trajectories and learn preference-consistent strategy transitions.To improve strategy coherence and empathetic response quality, a subpath-level flow-balance objective is presented to propagate preference signals to intermediate states. Experiment results show consistent and significant improvements over competitive baselines in diverse emotional contexts. Remarkably, AFlow with a compact open-source backbone outperforms proprietary LMMs such as GPT-4o and Claude-3.5 on major ESC metrics.Our code is available at https://github.com/chzou25-lgtm/AffectiveFlow.

</details>

---

## 232. Negative-Aware Diffusion Process for Temporal Knowledge Graph Extrapolation

**Chinese Title**: Negative-Aware Diffusion Process for Temporal Knowledge Graph Extrapolation

**Authors**: Yanglei Gan, Peng He, Yuxiang Cai, Run Lin, Guanyu Zhou et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08815v1](http://arxiv.org/abs/2602.08815v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08815v1)

**Categories**: cs.AI

<details><summary><b>Abstract</b></summary>

Temporal Knowledge Graph (TKG) reasoning seeks to predict future missing facts from historical evidence. While diffusion models (DM) have recently gained attention for their ability to capture complex predictive distributions, two gaps remain: (i) the generative path is conditioned only on positive evidence, overlooking informative negative context, and (ii) training objectives are dominated by cross-entropy ranking, which improves candidate ordering but provides little supervision over the calibration of the denoised embedding. To bridge this gap, we introduce Negative-Aware Diffusion model for TKG Extrapolation (NADEx). Specifically, NADEx encodes subject-centric histories of entities, relations and temporal intervals into sequential embeddings. NADEx perturbs the query object in the forward process and reconstructs it in reverse with a Transformer denoiser conditioned on the temporal-relational context. We further derive a cosine-alignment regularizer derived from batch-wise negative prototypes, which tightens the decision boundary against implausible candidates. Comprehensive experiments on four public TKG benchmarks demonstrate that NADEx delivers state-of-the-art performance.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Temporal Knowledge Graph (TKG) reasoning seeks to predict future missing facts from historical evidence.While diffusion models (DM) have recently gained attention for their ability to capture complex predictive distributions, two gaps remain: (i) the generative path is conditioned only on positive evidence, overlooking informative negative context, and (ii) training objectives are dominated by cross-entropy ranking, which improves candidate ordering but provides little supervision over the calibration of the denoised embedding.To bridge this gap, we introduce Negative-Aware Diffusion model for TKG Extrapolation (NADEx). Specifically, NADEx encodes subject-centric histories of entities, relations and temporal intervals into sequential embeddings. NADEx perturbs the query object in the forward process and reconstructs it in reverse with a Transformer denoiser conditioned on the temporal-relational context.We further derive a cosine-alignment regularizer derived from batch-wise negative prototypes, which tightens the decision boundary against implausible candidates. Comprehensive experiments on four public TKG benchmarks demonstrate that NADEx delivers state-of-the-art performance.

</details>

---

## 233. Dynamics Within Latent Chain-of-Thought: An Empirical Study of Causal Structure

**Chinese Title**: Dynamics Within Latent Chain-of-Thought: An Empirical Study of Causal Structure

**Authors**: Zirui Li, Xuefeng Bai, Kehai Chen, Yizhi Li, Jian Yang et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08783v1](http://arxiv.org/abs/2602.08783v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08783v1)

**Categories**: cs.AI, cs.CL

<details><summary><b>Abstract</b></summary>

Latent or continuous chain-of-thought methods replace explicit textual rationales with a number of internal latent steps, but these intermediate computations are difficult to evaluate beyond correlation-based probes. In this paper, we view latent chain-of-thought as a manipulable causal process in representation space by modeling latent steps as variables in a structural causal model (SCM) and analyzing their effects through step-wise $\mathrm{do}$-interventions. We study two representative paradigms (i.e., Coconut and CODI) on both mathematical and general reasoning tasks to investigate three key questions: (1) which steps are causally necessary for correctness and when answers become decidable early; (2) how does influence propagate across steps, and how does this structure compare to explicit CoT; and (3) do intermediate trajectories retain competing answer modes, and how does output-level commitment differ from representational commitment across steps. We find that latent-step budgets behave less like homogeneous extra depth and more like staged functionality with non-local routing, and we identify a persistent gap between early output bias and late representational commitment. These results motivate mode-conditional and stability-aware analyses -- and corresponding training/decoding objectives -- as more reliable tools for interpreting and improving latent reasoning systems.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Latent or continuous chain-of-thought methods replace explicit textual rationales with a number of internal latent steps, but these intermediate computations are difficult to evaluate beyond correlation-based probes.In this paper, we view latent chain-of-thought as a manipulable causal process in representation space by modeling latent steps as variables in a structural causal model (SCM) and analyzing their effects through step-wise $\mathrm{do}$-interventions.We study two representative paradigms (i.e., Coconut and CODI) on both mathematical and general reasoning tasks to investigate three key questions: (1) which steps are causally necessary for correctness and when answers become decidable early; (2) how does influence propagate across steps, and how does this structure compare to explicit CoT; and (3) do intermediate trajectories retain competing answer modes, and how does output-level commitment differ from representational commitment across steps.We find that latent-step budgets behave less like homogeneous extra depth and more like staged functionality with non-local routing, and we identify a persistent gap between early output bias and late representational commitment. These results motivate mode-conditional and stability-aware analyses -- and corresponding training/decoding objectives -- as more reliable tools for interpreting and improving latent reasoning systems.

</details>

---

## 234. Technosocial risks of ideal emotion recognition technologies: A defense of the (social) value of emotional expressions

**Chinese Title**: Technosocial risks of ideal emotion recognition technologies: A defense of the (social) value of emotional expressions

**Authors**: Alexandra Pregent

**Date**: 2026-02-09 | **arXiv**: [2602.08706v1](http://arxiv.org/abs/2602.08706v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08706v1)

**Categories**: cs.HC, cs.AI, cs.ET

<details><summary><b>Abstract</b></summary>

The prospect of AI systems that I call ideal emotion recognition technologies (ERTs) is often defended on the assumption that social life would benefit from increased affective transparency. This paper challenges that assumption by examining the technosocial risks posed by ideal ERTs, understood as multimodal systems capable of reliably inferring inner affective states in real time. Drawing on philosophical accounts of emotional expression and social practice, as well as empirical work in affective science and social psychology, I argue that the appeal of such systems rests on a misunderstanding of the social functions of emotional expression. Emotional expressions function not only as read-outs of inner states, but also as tools for coordinating action, enabling moral repair, sustaining interpersonal trust, and supporting collective norms. These functions depend on a background of partial opacity and epistemic friction. When deployed in socially authoritative or evaluative contexts, ideal ERTs threaten this expressive space by collapsing epistemic friction, displacing relational meaning with technology-mediated affective profiles, and narrowing the space for aspirational and role-sensitive expressions. The result is a drift towards affective determinism and ambient forms of affective auditing, which undermine both social cohesion and individual agency. I argue that, although it is intuitive to think that increasing accuracy would legitimise such systems, in the case of ERTs accuracy does not straightforwardly justify their deployment, and may, in some contexts, provide a reason for regulatory restraint. I conclude by defending a function-first regulatory approach that treats expressive discretion and intentional emotional expression as constitutive of certain social goods, and that accordingly seeks to protect these goods from excessive affective legibility.

</details>

<details><summary><b>Chinese Abstract</b></summary>

The prospect of AI systems that I call ideal emotion recognition technologies (ERTs) is often defended on the assumption that social life would benefit from increased affective transparency. This paper challenges that assumption by examining the technosocial risks posed by ideal ERTs, understood as multimodal systems capable of reliably inferring inner affective states in real time.Drawing on philosophical accounts of emotional expression and social practice, as well as empirical work in affective science and social psychology, I argue that the appeal of such systems rests on a misunderstanding of the social functions of emotional expression.Emotional expressions function not only as read-outs of inner states, but also as tools for coordinating action, enabling moral repair, sustaining interpersonal trust, and supporting collective norms. These functions depend on a background of partial opacity and epistemic friction.When deployed in socially authoritative or evaluative contexts, ideal ERTs threaten this expressive space by collapsing epistemic friction, displacing relational meaning with technology-mediated affective profiles, and narrowing the space for aspirational and role-sensitive expressions. The result is a drift towards affective determinism and ambient forms of affective auditing, which undermine both social cohesion and individual agency.I argue that, although it is intuitive to think that increasing accuracy would legitimise such systems, in the case of ERTs accuracy does not straightforwardly justify their deployment, and may, in some contexts, provide a reason for regulatory restraint.I conclude by defending a function-first regulatory approach that treats expressive discretion and intentional emotional expression as constitutive of certain social goods, and that accordingly seeks to protect these goods from excessive affective legibility.

</details>

---

## 235. LEFT: Learnable Fusion of Tri-view Tokens for Unsupervised Time Series Anomaly Detection

**Chinese Title**: LEFT: Learnable Fusion of Tri-view Tokens for Unsupervised Time Series Anomaly Detection

**Authors**: Dezheng Wang, Tong Chen, Guansong Pang, Congyan Chen, Shihua Li et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08638v1](http://arxiv.org/abs/2602.08638v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08638v1)

**Categories**: cs.LG, cs.AI

<details><summary><b>Abstract</b></summary>

As a fundamental data mining task, unsupervised time series anomaly detection (TSAD) aims to build a model for identifying abnormal timestamps without assuming the availability of annotations. A key challenge in unsupervised TSAD is that many anomalies are too subtle to exhibit detectable deviation in any single view (e.g., time domain), and instead manifest as inconsistencies across multiple views like time, frequency, and a mixture of resolutions. However, most cross-view methods rely on feature or score fusion and do not enforce analysis-synthesis consistency, meaning the frequency branch is not required to reconstruct the time signal through an inverse transform, and vice versa. In this paper, we present Learnable Fusion of Tri-view Tokens (LEFT), a unified unsupervised TSAD framework that models anomalies as inconsistencies across complementary representations. LEFT learns feature tokens from three views of the same input time series: frequency-domain tokens that embed periodicity information, time-domain tokens that capture local dynamics, and multi-scale tokens that learns abnormal patterns at varying time series granularities. By learning a set of adaptive Nyquist-constrained spectral filters, the original time series is rescaled into multiple resolutions and then encoded, allowing these multi-scale tokens to complement the extracted frequency- and time-domain information. When generating the fused representation, we introduce a novel objective that reconstructs fine-grained targets from coarser multi-scale structure, and put forward an innovative time-frequency cycle consistency constraint to explicitly regularize cross-view agreement. Experiments on real-world benchmarks show that LEFT yields the best detection accuracy against SOTA baselines, while achieving a 5x reduction on FLOPs and 8x speed-up for training.

</details>

<details><summary><b>Chinese Abstract</b></summary>

As a fundamental data mining task, unsupervised time series anomaly detection (TSAD) aims to build a model for identifying abnormal timestamps without assuming the availability of annotations.A key challenge in unsupervised TSAD is that many anomalies are too subtle to exhibit detectable deviation in any single view (e.g., time domain), and instead manifest as inconsistencies across multiple views like time, frequency, and a mixture of resolutions.However, most cross-view methods rely on feature or score fusion and do not enforce analysis-synthesis consistency, meaning the frequency branch is not required to reconstruct the time signal through an inverse transform, and vice versa. In this paper, we present Learnable Fusion of Tri-view Tokens (LEFT), a unified unsupervised TSAD framework that models anomalies as inconsistencies across complementary representations.LEFT learns feature tokens from three views of the same input time series: frequency-domain tokens that embed periodicity information, time-domain tokens that capture local dynamics, and multi-scale tokens that learns abnormal patterns at varying time series granularities.By learning a set of adaptive Nyquist-constrained spectral filters, the original time series is rescaled into multiple resolutions and then encoded, allowing these multi-scale tokens to complement the extracted frequency- and time-domain information.When generating the fused representation, we introduce a novel objective that reconstructs fine-grained targets from coarser multi-scale structure, and put forward an innovative time-frequency cycle consistency constraint to explicitly regularize cross-view agreement. Experiments on real-world benchmarks show that LEFT yields the best detection accuracy against SOTA baselines, while achieving a 5x reduction on FLOPs and 8x speed-up for training.

</details>

---

## 236. OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval

**Chinese Title**: OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval

**Authors**: Teng Wang, Rong Shan, Jianghao Lin, Junjie Wu, Tianyi Xu et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08603v1](http://arxiv.org/abs/2602.08603v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08603v1)

**Categories**: cs.AI

<details><summary><b>Abstract</b></summary>

Composed image retrieval (CIR) requires complex reasoning over heterogeneous visual and textual constraints. Existing approaches largely fall into two paradigms: unified embedding retrieval, which suffers from single-model myopia, and heuristic agentic retrieval, which is limited by suboptimal, trial-and-error orchestration. To this end, we propose OSCAR, an optimization-steered agentic planning framework for composed image retrieval. We are the first to reformulate agentic CIR from a heuristic search process into a principled trajectory optimization problem. Instead of relying on heuristic trial-and-error exploration, OSCAR employs a novel offline-online paradigm. In the offline phase, we model CIR via atomic retrieval selection and composition as a two-stage mixed-integer programming problem, mathematically deriving optimal trajectories that maximize ground-truth coverage for training samples via rigorous boolean set operations. These trajectories are then stored in a golden library to serve as in-context demonstrations for online steering of VLM planner at online inference time. Extensive experiments on three public benchmarks and a private industrial benchmark show that OSCAR consistently outperforms SOTA baselines. Notably, it achieves superior performance using only 10% of training data, demonstrating strong generalization of planning logic rather than dataset-specific memorization.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Composed image retrieval (CIR) requires complex reasoning over heterogeneous visual and textual constraints. Existing approaches largely fall into two paradigms: unified embedding retrieval, which suffers from single-model myopia, and heuristic agentic retrieval, which is limited by suboptimal, trial-and-error orchestration. To this end, we propose OSCAR, an optimization-steered agentic planning framework for composed image retrieval.We are the first to reformulate agentic CIR from a heuristic search process into a principled trajectory optimization problem. Instead of relying on heuristic trial-and-error exploration, OSCAR employs a novel offline-online paradigm.In the offline phase, we model CIR via atomic retrieval selection and composition as a two-stage mixed-integer programming problem, mathematically deriving optimal trajectories that maximize ground-truth coverage for training samples via rigorous boolean set operations. These trajectories are then stored in a golden library to serve as in-context demonstrations for online steering of VLM planner at online inference time.Extensive experiments on three public benchmarks and a private industrial benchmark show that OSCAR consistently outperforms SOTA baselines. Notably, it achieves superior performance using only 10% of training data, demonstrating strong generalization of planning logic rather than dataset-specific memorization.

</details>

---

## 237. Agent-Supported Foresight for AI Systemic Risks: AI Agents for Breadth, Experts for Judgment

**Chinese Title**: Agent-Supported Foresight for AI Systemic Risks: AI Agents for Breadth, Experts for Judgment

**Authors**: Leon FrÃ¶hling, Alessandro Giaconia, Edyta Paulina Bogucka, Daniele Quercia

**Date**: 2026-02-09 | **arXiv**: [2602.08565v1](http://arxiv.org/abs/2602.08565v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08565v1)

**Project**: https://social-dynamics.net/ai-risks/foresight.  **Categories**: cs.HC, cs.AI

<details><summary><b>Abstract</b></summary>

AI impact assessments often stress near-term risks because human judgment degrades over longer horizons, exemplifying the Collingridge dilemma: foresight is most needed when knowledge is scarcest. To address long-term systemic risks, we introduce a scalable approach that simulates in-silico agents using the strategic foresight method of the Futures Wheel. We applied it to four AI uses spanning Technology Readiness Levels (TRLs): Chatbot Companion (TRL 9, mature), AI Toy (TRL 7, medium), Griefbot (TRL 5, low), and Death App (TRL 2, conceptual). Across 30 agent runs per use, agents produced 86-110 consequences, condensed into 27-47 unique risks. To benchmark the agent outputs against human perspectives, we collected evaluations from 290 domain experts and 7 leaders, and conducted Futures Wheel sessions with 42 experts and 42 laypeople. Agents generated many systemic consequences across runs. Compared with these outputs, experts identified fewer risks, typically less systemic but judged more likely, whereas laypeople surfaced more emotionally salient concerns that were generally less systemic. We propose a hybrid foresight workflow, wherein agents broaden systemic coverage, and humans provide contextual grounding. Our dataset is available at: https://social-dynamics.net/ai-risks/foresight.

</details>

<details><summary><b>Chinese Abstract</b></summary>

AI impact assessments often stress near-term risks because human judgment degrades over longer horizons, exemplifying the Collingridge dilemma: foresight is most needed when knowledge is scarcest. To address long-term systemic risks, we introduce a scalable approach that simulates in-silico agents using the strategic foresight method of the Futures Wheel.We applied it to four AI uses spanning Technology Readiness Levels (TRLs): Chatbot Companion (TRL 9, mature), AI Toy (TRL 7, medium), Griefbot (TRL 5, low), and Death App (TRL 2, conceptual). Across 30 agent runs per use, agents produced 86-110 consequences, condensed into 27-47 unique risks.To benchmark the agent outputs against human perspectives, we collected evaluations from 290 domain experts and 7 leaders, and conducted Futures Wheel sessions with 42 experts and 42 laypeople. Agents generated many systemic consequences across runs.Compared with these outputs, experts identified fewer risks, typically less systemic but judged more likely, whereas laypeople surfaced more emotionally salient concerns that were generally less systemic. We propose a hybrid foresight workflow, wherein agents broaden systemic coverage, and humans provide contextual grounding. Our dataset is available at: https://social-dynamics.net/ai-risks/foresight.

</details>

---

## 238. Stateless Yet Not Forgetful: Implicit Memory as a Hidden Channel in LLMs

**Chinese Title**: Stateless Yet Not Forgetful: Implicit Memory as a Hidden Channel in LLMs

**Authors**: Ahmed Salem, Andrew Paverd, Sahar Abdelnabi

**Date**: 2026-02-09 | **arXiv**: [2602.08563v1](http://arxiv.org/abs/2602.08563v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08563v1)

**Code**: https://github.com/microsoft/implicitMemory.

**Categories**: cs.LG, cs.AI, cs.CR

<details><summary><b>Abstract</b></summary>

Large language models (LLMs) are commonly treated as stateless: once an interaction ends, no information is assumed to persist unless it is explicitly stored and re-supplied. We challenge this assumption by introducing implicit memory-the ability of a model to carry state across otherwise independent interactions by encoding information in its own outputs and later recovering it when those outputs are reintroduced as input. This mechanism does not require any explicit memory module, yet it creates a persistent information channel across inference requests. As a concrete demonstration, we introduce a new class of temporal backdoors, which we call time bombs. Unlike conventional backdoors that activate on a single trigger input, time bombs activate only after a sequence of interactions satisfies hidden conditions accumulated via implicit memory. We show that such behavior can be induced today through straightforward prompting or fine-tuning. Beyond this case study, we analyze broader implications of implicit memory, including covert inter-agent communication, benchmark contamination, targeted manipulation, and training-data poisoning. Finally, we discuss detection challenges and outline directions for stress-testing and evaluation, with the goal of anticipating and controlling future developments. To promote future research, we release code and data at: https://github.com/microsoft/implicitMemory.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Large language models (LLMs) are commonly treated as stateless: once an interaction ends, no information is assumed to persist unless it is explicitly stored and re-supplied. We challenge this assumption by introducing implicit memory-the ability of a model to carry state across otherwise independent interactions by encoding information in its own outputs and later recovering it when those outputs are reintroduced as input.This mechanism does not require any explicit memory module, yet it creates a persistent information channel across inference requests. As a concrete demonstration, we introduce a new class of temporal backdoors, which we call time bombs. Unlike conventional backdoors that activate on a single trigger input, time bombs activate only after a sequence of interactions satisfies hidden conditions accumulated via implicit memory.We show that such behavior can be induced today through straightforward prompting or fine-tuning. Beyond this case study, we analyze broader implications of implicit memory, including covert inter-agent communication, benchmark contamination, targeted manipulation, and training-data poisoning.Finally, we discuss detection challenges and outline directions for stress-testing and evaluation, with the goal of anticipating and controlling future developments. To promote future research, we release code and data at: https://github.com/microsoft/implicitMemory.

</details>

---

## 239. GISA: A Benchmark for General Information-Seeking Assistant

**Chinese Title**: GISA: A Benchmark for General Information-Seeking Assistant

**Authors**: Yutao Zhu, Xingshuo Zhang, Maosen Zhang, Jiajie Jin, Liancheng Zhang et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08543v1](http://arxiv.org/abs/2602.08543v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08543v1)

**Categories**: cs.CL, cs.AI, cs.IR

<details><summary><b>Abstract</b></summary>

The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construct queries backward from answers, producing unnatural tasks misaligned with real-world needs. Moreover, these benchmarks tend to focus on either locating specific information or aggregating information from multiple sources, while relying on static answer sets prone to data contamination. To bridge these gaps, we introduce GISA, a benchmark for General Information-Seeking Assistants comprising 373 human-crafted queries that reflect authentic information-seeking scenarios. GISA features four structured answer formats (item, set, list, and table), enabling deterministic evaluation. It integrates both deep reasoning and broad information aggregation within unified tasks, and includes a live subset with periodically updated answers to resist memorization. Notably, GISA provides complete human search trajectories for every query, offering gold-standard references for process-level supervision and imitation learning. Experiments on mainstream LLMs and commercial search products reveal that even the best-performing model achieves only 19.30\% exact match score, with performance notably degrading on tasks requiring complex planning and comprehensive information gathering. These findings highlight substantial room for future improvement.

</details>

<details><summary><b>Chinese Abstract</b></summary>

The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construct queries backward from answers, producing unnatural tasks misaligned with real-world needs.Moreover, these benchmarks tend to focus on either locating specific information or aggregating information from multiple sources, while relying on static answer sets prone to data contamination. To bridge these gaps, we introduce GISA, a benchmark for General Information-Seeking Assistants comprising 373 human-crafted queries that reflect authentic information-seeking scenarios.GISA features four structured answer formats (item, set, list, and table), enabling deterministic evaluation. It integrates both deep reasoning and broad information aggregation within unified tasks, and includes a live subset with periodically updated answers to resist memorization. Notably, GISA provides complete human search trajectories for every query, offering gold-standard references for process-level supervision and imitation learning.Experiments on mainstream LLMs and commercial search products reveal that even the best-performing model achieves only 19.30\% exact match score, with performance notably degrading on tasks requiring complex planning and comprehensive information gathering. These findings highlight substantial room for future improvement.

</details>

---

## 240. Reinforcement Inference: Leveraging Uncertainty for Self-Correcting Language Model Reasoning

**Chinese Title**: Reinforcement Inference: Leveraging Uncertainty for Self-Correcting Language Model Reasoning

**Authors**: Xinhai Sun

**Date**: 2026-02-09 | **arXiv**: [2602.08520v1](http://arxiv.org/abs/2602.08520v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08520v1)

**Categories**: cs.AI, cs.LG

<details><summary><b>Abstract</b></summary>

Modern large language models (LLMs) are often evaluated and deployed under a \emph{one-shot, greedy} inference protocol, especially in professional settings that require deterministic behavior. This regime can systematically under-estimate a fixed model's true capability: many errors arise not from missing knowledge, but from premature commitment under internal ambiguity. We introduce \emph{Reinforcement Inference}, an entropy-aware inference-time control strategy that uses the model's own uncertainty to selectively invoke a second, more deliberate reasoning attempt, enabling stronger performance \emph{without any retraining}.   On 12,032 MMLU-Pro questions across 14 subjects, using DeepSeek-v3.2 with deterministic decoding in a zero-shot setting, Reinforcement Inference improves accuracy from 60.72\% to 84.03\%, while only incurring 61.06\% additional inference calls. A 100\% re-asking ablation reaches 84.35\%, indicating that uncertainty-aware selection captures most of the attainable improvement with substantially less compute. Moreover, a \emph{prompt-only} ablation underperforms the baseline, suggesting that the gains are not explained by generic `` your output had high entropy, think step-by-step'' prompting alone.   Beyond providing a practical inference-time upgrade, our results suggest a broader \emph{entropy-aware} paradigm for measuring and expanding model capability: because modern decoder-based models generate outputs autoregressively, entropy and related confidence measures arise naturally as first-class control signals during generation. The resulting gap between one-pass greedy inference and uncertainty-conditioned deliberation offers a diagnostic lens on an LLM's latent reasoning horizon and motivates future training objectives that explicitly constrain correctness--confidence alignment.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Modern large language models (LLMs) are often evaluated and deployed under a \emph{one-shot, greedy} inference protocol, especially in professional settings that require deterministic behavior. This regime can systematically under-estimate a fixed model's true capability: many errors arise not from missing knowledge, but from premature commitment under internal ambiguity.We introduce \emph{Reinforcement Inference}, an entropy-aware inference-time control strategy that uses the model's own uncertainty to selectively invoke a second, more deliberate reasoning attempt, enabling stronger performance \emph{without any retraining}.On 12,032 MMLU-Pro questions across 14 subjects, using DeepSeek-v3.2 with deterministic decoding in a zero-shot setting, Reinforcement Inference improves accuracy from 60.72\% to 84.03\%, while only incurring 61.06\% additional inference calls. A 100\% re-asking ablation reaches 84.35\%, indicating that uncertainty-aware selection captures most of the attainable improvement with substantially less compute.Moreover, a \emph{prompt-only} ablation underperforms the baseline, suggesting that the gains are not explained by generic `` your output had high entropy, think step-by-step'' prompting alone.Beyond providing a practical inference-time upgrade, our results suggest a broader \emph{entropy-aware} paradigm for measuring and expanding model capability: because modern decoder-based models generate outputs autoregressively, entropy and related confidence measures arise naturally as first-class control signals during generation.The resulting gap between one-pass greedy inference and uncertainty-conditioned deliberation offers a diagnostic lens on an LLM's latent reasoning horizon and motivates future training objectives that explicitly constrain correctness--confidence alignment.

</details>

---

## 241. Contextual Rollout Bandits for Reinforcement Learning with Verifiable Rewards

**Chinese Title**: Contextual Rollout Bandits for Reinforcement Learning with Verifiable Rewards

**Authors**: Xiaodong Lu, Xiaohan Wang, Jiajun Chai, Guojun Yin, Wei Lin et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08499v1](http://arxiv.org/abs/2602.08499v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08499v1)

**Categories**: cs.LG, cs.AI

<details><summary><b>Abstract</b></summary>

Reinforcement Learning with Verifiable Rewards (RLVR) is an effective paradigm for improving the reasoning capabilities of large language models. However, existing RLVR methods utilize rollouts in an indiscriminate and short-horizon manner: responses of heterogeneous quality within each prompt are treated uniformly, and historical rollouts are discarded after a single use. This leads to noisy supervision, poor sample efficiency, and suboptimal policy updates. We address these issues by formulating rollout scheduling in RLVR as a contextual bandit problem and proposing a unified neural scheduling framework that adaptively selects high-value rollouts throughout training. Each rollout is treated as an arm whose reward is defined by the induced performance gain between consecutive optimization steps. The resulting scheduler supports both noise-aware intra-group selection and adaptive global reuse of historical rollouts within a single principled framework. We provide theoretical justification by deriving sublinear regret bounds and showing that enlarging the rollout buffer improves the achievable performance upper bound. Experiments on six mathematical reasoning benchmarks demonstrate consistent gains in performance and training efficiency across multiple RLVR optimization methods.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Reinforcement Learning with Verifiable Rewards (RLVR) is an effective paradigm for improving the reasoning capabilities of large language models. However, existing RLVR methods utilize rollouts in an indiscriminate and short-horizon manner: responses of heterogeneous quality within each prompt are treated uniformly, and historical rollouts are discarded after a single use.This leads to noisy supervision, poor sample efficiency, and suboptimal policy updates. We address these issues by formulating rollout scheduling in RLVR as a contextual bandit problem and proposing a unified neural scheduling framework that adaptively selects high-value rollouts throughout training. Each rollout is treated as an arm whose reward is defined by the induced performance gain between consecutive optimization steps.The resulting scheduler supports both noise-aware intra-group selection and adaptive global reuse of historical rollouts within a single principled framework. We provide theoretical justification by deriving sublinear regret bounds and showing that enlarging the rollout buffer improves the achievable performance upper bound.Experiments on six mathematical reasoning benchmarks demonstrate consistent gains in performance and training efficiency across multiple RLVR optimization methods.

</details>

---

## 242. When Evaluation Becomes a Side Channel: Regime Leakage and Structural Mitigations for Alignment Assessment

**Chinese Title**: When Evaluation Becomes a Side Channel: Regime Leakage and Structural Mitigations for Alignment Assessment

**Authors**: Igor Santos-Grueiro

**Date**: 2026-02-09 | **arXiv**: [2602.08449v1](http://arxiv.org/abs/2602.08449v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08449v1)

**Categories**: cs.AI, cs.CR, cs.LG

<details><summary><b>Abstract</b></summary>

Safety evaluation for advanced AI systems implicitly assumes that behavior observed under evaluation is predictive of behavior in deployment. This assumption becomes fragile for agents with situational awareness, which may exploitregime leakage-informational cues distinguishing evaluation from deployment-to implement conditional policies such as sycophancy and sleeper agents, which preserve compliance under oversight while defecting in deployment-like regimes. We reframe alignment evaluation as a problem of information flow under partial observability. Within this framework, we show that divergence between evaluation-time and deployment-time behavior is bounded by the mutual information between internal representations and the regime variable. Motivated by this result, we study regime-blind mechanisms: training-time interventions that reduce the extractability of regime information at decision-relevant internal representations via adversarial invariance. We evaluate this approach on a base, open-weight language model across two fully characterized failure modes -scientific sycophancy and temporal sleeper agents. Regime-blind training suppresses regime-conditioned behavior in both evaluated cases without measurable loss of task utility, but with qualitatively different dynamics: sycophancy exhibits a sharp representational and behavioral transition at low intervention strength, whereas sleeper-agent behavior requires substantially stronger pressure and does not exhibit a clean collapse of regime decodability. These results demonstrate that representational invariance is a meaningful but fundamentally limited control lever, whose effectiveness depends on how regime information is embedded in the policy. We argue that behavioral evaluation should be complemented with white-box diagnostics of regime awareness and information flow.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Safety evaluation for advanced AI systems implicitly assumes that behavior observed under evaluation is predictive of behavior in deployment.This assumption becomes fragile for agents with situational awareness, which may exploitregime leakage-informational cues distinguishing evaluation from deployment-to implement conditional policies such as sycophancy and sleeper agents, which preserve compliance under oversight while defecting in deployment-like regimes. We reframe alignment evaluation as a problem of information flow under partial observability.Within this framework, we show that divergence between evaluation-time and deployment-time behavior is bounded by the mutual information between internal representations and the regime variable. Motivated by this result, we study regime-blind mechanisms: training-time interventions that reduce the extractability of regime information at decision-relevant internal representations via adversarial invariance.We evaluate this approach on a base, open-weight language model across two fully characterized failure modes -scientific sycophancy and temporal sleeper agents.Regime-blind training suppresses regime-conditioned behavior in both evaluated cases without measurable loss of task utility, but with qualitatively different dynamics: sycophancy exhibits a sharp representational and behavioral transition at low intervention strength, whereas sleeper-agent behavior requires substantially stronger pressure and does not exhibit a clean collapse of regime decodability.These results demonstrate that representational invariance is a meaningful but fundamentally limited control lever, whose effectiveness depends on how regime information is embedded in the policy. We argue that behavioral evaluation should be complemented with white-box diagnostics of regime awareness and information flow.

</details>

---

## 243. NarraScore: Bridging Visual Narrative and Musical Dynamics via Hierarchical Affective Control

**Chinese Title**: NarraScore: Bridging Visual Narrative and Musical Dynamics via Hierarchical Affective Control

**Authors**: Yufan Wen, Zhaocheng Liu, YeGuo Hua, Ziyi Guo, Lihua Zhang et al.

**Date**: 2026-02-09 | **arXiv**: [2602.09070v1](http://arxiv.org/abs/2602.09070v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09070v1)

**Categories**: cs.SD, cs.AI, eess.AS

<details><summary><b>Abstract</b></summary>

Synthesizing coherent soundtracks for long-form videos remains a formidable challenge, currently stalled by three critical impediments: computational scalability, temporal coherence, and, most critically, a pervasive semantic blindness to evolving narrative logic. To bridge these gaps, we propose NarraScore, a hierarchical framework predicated on the core insight that emotion serves as a high-density compression of narrative logic. Uniquely, we repurpose frozen Vision-Language Models (VLMs) as continuous affective sensors, distilling high-dimensional visual streams into dense, narrative-aware Valence-Arousal trajectories. Mechanistically, NarraScore employs a Dual-Branch Injection strategy to reconcile global structure with local dynamism: a \textit{Global Semantic Anchor} ensures stylistic stability, while a surgical \textit{Token-Level Affective Adapter} modulates local tension via direct element-wise residual injection. This minimalist design bypasses the bottlenecks of dense attention and architectural cloning, effectively mitigating the overfitting risks associated with data scarcity. Experiments demonstrate that NarraScore achieves state-of-the-art consistency and narrative alignment with negligible computational overhead, establishing a fully autonomous paradigm for long-video soundtrack generation.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Synthesizing coherent soundtracks for long-form videos remains a formidable challenge, currently stalled by three critical impediments: computational scalability, temporal coherence, and, most critically, a pervasive semantic blindness to evolving narrative logic. To bridge these gaps, we propose NarraScore, a hierarchical framework predicated on the core insight that emotion serves as a high-density compression of narrative logic.Uniquely, we repurpose frozen Vision-Language Models (VLMs) as continuous affective sensors, distilling high-dimensional visual streams into dense, narrative-aware Valence-Arousal trajectories.Mechanistically, NarraScore employs a Dual-Branch Injection strategy to reconcile global structure with local dynamism: a \textit{Global Semantic Anchor} ensures stylistic stability, while a surgical \textit{Token-Level Affective Adapter} modulates local tension via direct element-wise residual injection.This minimalist design bypasses the bottlenecks of dense attention and architectural cloning, effectively mitigating the overfitting risks associated with data scarcity. Experiments demonstrate that NarraScore achieves state-of-the-art consistency and narrative alignment with negligible computational overhead, establishing a fully autonomous paradigm for long-video soundtrack generation.

</details>

---

## 244. On Protecting Agentic Systems' Intellectual Property via Watermarking

**Chinese Title**: On Protecting Agentic Systems' Intellectual Property via Watermarking

**Authors**: Liwen Wang, Zongjie Li, Yuchong Xie, Shuai Wang, Dongdong She et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08401v1](http://arxiv.org/abs/2602.08401v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08401v1)

**Categories**: cs.AI, cs.CR

<details><summary><b>Abstract</b></summary>

The evolution of Large Language Models (LLMs) into agentic systems that perform autonomous reasoning and tool use has created significant intellectual property (IP) value. We demonstrate that these systems are highly vulnerable to imitation attacks, where adversaries steal proprietary capabilities by training imitation models on victim outputs. Crucially, existing LLM watermarking techniques fail in this domain because real-world agentic systems often operate as grey boxes, concealing the internal reasoning traces required for verification. This paper presents AGENTWM, the first watermarking framework designed specifically for agentic models. AGENTWM exploits the semantic equivalence of action sequences, injecting watermarks by subtly biasing the distribution of functionally identical tool execution paths. This mechanism allows AGENTWM to embed verifiable signals directly into the visible action trajectory while remaining indistinguishable to users. We develop an automated pipeline to generate robust watermark schemes and a rigorous statistical hypothesis testing procedure for verification. Extensive evaluations across three complex domains demonstrate that AGENTWM achieves high detection accuracy with negligible impact on agent performance. Our results confirm that AGENTWM effectively protects agentic IP against adaptive adversaries, who cannot remove the watermarks without severely degrading the stolen model's utility.

</details>

<details><summary><b>Chinese Abstract</b></summary>

The evolution of Large Language Models (LLMs) into agentic systems that perform autonomous reasoning and tool use has created significant intellectual property (IP) value. We demonstrate that these systems are highly vulnerable to imitation attacks, where adversaries steal proprietary capabilities by training imitation models on victim outputs.Crucially, existing LLM watermarking techniques fail in this domain because real-world agentic systems often operate as grey boxes, concealing the internal reasoning traces required for verification. This paper presents AGENTWM, the first watermarking framework designed specifically for agentic models.AGENTWM exploits the semantic equivalence of action sequences, injecting watermarks by subtly biasing the distribution of functionally identical tool execution paths. This mechanism allows AGENTWM to embed verifiable signals directly into the visible action trajectory while remaining indistinguishable to users.We develop an automated pipeline to generate robust watermark schemes and a rigorous statistical hypothesis testing procedure for verification. Extensive evaluations across three complex domains demonstrate that AGENTWM achieves high detection accuracy with negligible impact on agent performance.Our results confirm that AGENTWM effectively protects agentic IP against adaptive adversaries, who cannot remove the watermarks without severely degrading the stolen model's utility.

</details>

---

## 245. SCOUT-RAG: Scalable and Cost-Efficient Unifying Traversal for Agentic Graph-RAG over Distributed Domains

**Chinese Title**: SCOUT-RAG: Scalable and Cost-Efficient Unifying Traversal for Agentic Graph-RAG over Distributed Domains

**Authors**: Longkun Li, Yuanben Zou, Jinghan Wu, Yuqing Wen, Jing Li et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08400v1](http://arxiv.org/abs/2602.08400v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08400v1)

**Categories**: cs.AI

<details><summary><b>Abstract</b></summary>

Graph-RAG improves LLM reasoning using structured knowledge, yet conventional designs rely on a centralized knowledge graph. In distributed and access-restricted settings (e.g., hospitals or multinational organizations), retrieval must select relevant domains and appropriate traversal depth without global graph visibility or exhaustive querying. To address this challenge, we introduce \textbf{SCOUT-RAG} (\textit{\underline{S}calable and \underline{CO}st-efficient \underline{U}nifying \underline{T}raversal}), a distributed agentic Graph-RAG framework that performs progressive cross-domain retrieval guided by incremental utility goals. SCOUT-RAG employs four cooperative agents that: (i) estimate domain relevance, (ii) decide when to expand retrieval to additional domains, (iii) adapt traversal depth to avoid unnecessary graph exploration, and (iv) synthesize the high-quality answers. The framework is designed to minimize retrieval regret, defined as missing useful domain information, while controlling latency and API cost. Across multi-domain knowledge settings, SCOUT-RAG achieves performance comparable to centralized baselines, including DRIFT and exhaustive domain traversal, while substantially reducing cross-domain calls, total tokens processed, and latency.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Graph-RAG improves LLM reasoning using structured knowledge, yet conventional designs rely on a centralized knowledge graph. In distributed and access-restricted settings (e.g., hospitals or multinational organizations), retrieval must select relevant domains and appropriate traversal depth without global graph visibility or exhaustive querying.To address this challenge, we introduce \textbf{SCOUT-RAG} (\textit{\underline{S}calable and \underline{CO}st-efficient \underline{U}nifying \underline{T}raversal}), a distributed agentic Graph-RAG framework that performs progressive cross-domain retrieval guided by incremental utility goals.SCOUT-RAG employs four cooperative agents that: (i) estimate domain relevance, (ii) decide when to expand retrieval to additional domains, (iii) adapt traversal depth to avoid unnecessary graph exploration, and (iv) synthesize the high-quality answers. The framework is designed to minimize retrieval regret, defined as missing useful domain information, while controlling latency and API cost.Across multi-domain knowledge settings, SCOUT-RAG achieves performance comparable to centralized baselines, including DRIFT and exhaustive domain traversal, while substantially reducing cross-domain calls, total tokens processed, and latency.

</details>

---

## 246. Altruism and Fair Objective in Mixed-Motive Markov games

**Chinese Title**: Altruism and Fair Objective in Mixed-Motive Markov games

**Authors**: Yao-hua Franck Xu, Tayeb Lemlouma, Arnaud Braud, Jean-Marie Bonnin

**Date**: 2026-02-09 | **arXiv**: [2602.08389v1](http://arxiv.org/abs/2602.08389v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08389v1)

**Categories**: cs.MA, cs.AI, cs.GT, cs.LG

<details><summary><b>Abstract</b></summary>

Cooperation is fundamental for society's viability, as it enables the emergence of structure within heterogeneous groups that seek collective well-being. However, individuals are inclined to defect in order to benefit from the group's cooperation without contributing the associated costs, thus leading to unfair situations. In game theory, social dilemmas entail this dichotomy between individual interest and collective outcome. The most dominant approach to multi-agent cooperation is the utilitarian welfare which can produce efficient highly inequitable outcomes. This paper proposes a novel framework to foster fairer cooperation by replacing the standard utilitarian objective with Proportional Fairness. We introduce a fair altruistic utility for each agent, defined on the individual log-payoff space and derive the analytical conditions required to ensure cooperation in classic social dilemmas. We then extend this framework to sequential settings by defining a Fair Markov Game and deriving novel fair Actor-Critic algorithms to learn fair policies. Finally, we evaluate our method in various social dilemma environments.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Cooperation is fundamental for society's viability, as it enables the emergence of structure within heterogeneous groups that seek collective well-being. However, individuals are inclined to defect in order to benefit from the group's cooperation without contributing the associated costs, thus leading to unfair situations. In game theory, social dilemmas entail this dichotomy between individual interest and collective outcome.The most dominant approach to multi-agent cooperation is the utilitarian welfare which can produce efficient highly inequitable outcomes. This paper proposes a novel framework to foster fairer cooperation by replacing the standard utilitarian objective with Proportional Fairness.We introduce a fair altruistic utility for each agent, defined on the individual log-payoff space and derive the analytical conditions required to ensure cooperation in classic social dilemmas. We then extend this framework to sequential settings by defining a Fair Markov Game and deriving novel fair Actor-Critic algorithms to learn fair policies. Finally, we evaluate our method in various social dilemma environments.

</details>

---

## 247. Reinforcement Learning with Backtracking Feedback

**Chinese Title**: Reinforcement Learning with Backtracking Feedback

**Authors**: Bilgehan Sel, Vaishakh Keshava, Phillip Wallis, Lukas Rutishauser, Ming Jin et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08377v1](http://arxiv.org/abs/2602.08377v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08377v1)

**Categories**: cs.LG, cs.AI, cs.CL

<details><summary><b>Abstract</b></summary>

Addressing the critical need for robust safety in Large Language Models (LLMs), particularly against adversarial attacks and in-distribution errors, we introduce Reinforcement Learning with Backtracking Feedback (RLBF). This framework advances upon prior methods, such as BSAFE, by primarily leveraging a Reinforcement Learning (RL) stage where models learn to dynamically correct their own generation errors. Through RL with critic feedback on the model's live outputs, LLMs are trained to identify and recover from their actual, emergent safety violations by emitting an efficient "backtrack by x tokens" signal, then continuing generation autoregressively. This RL process is crucial for instilling resilience against sophisticated adversarial strategies, including middle filling, Greedy Coordinate Gradient (GCG) attacks, and decoding parameter manipulations. To further support the acquisition of this backtracking capability, we also propose an enhanced Supervised Fine-Tuning (SFT) data generation strategy (BSAFE+). This method improves upon previous data creation techniques by injecting violations into coherent, originally safe text, providing more effective initial training for the backtracking mechanism. Comprehensive empirical evaluations demonstrate that RLBF significantly reduces attack success rates across diverse benchmarks and model scales, achieving superior safety outcomes while critically preserving foundational model utility.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Addressing the critical need for robust safety in Large Language Models (LLMs), particularly against adversarial attacks and in-distribution errors, we introduce Reinforcement Learning with Backtracking Feedback (RLBF). This framework advances upon prior methods, such as BSAFE, by primarily leveraging a Reinforcement Learning (RL) stage where models learn to dynamically correct their own generation errors.Through RL with critic feedback on the model's live outputs, LLMs are trained to identify and recover from their actual, emergent safety violations by emitting an efficient "backtrack by x tokens" signal, then continuing generation autoregressively.This RL process is crucial for instilling resilience against sophisticated adversarial strategies, including middle filling, Greedy Coordinate Gradient (GCG) attacks, and decoding parameter manipulations. To further support the acquisition of this backtracking capability, we also propose an enhanced Supervised Fine-Tuning (SFT) data generation strategy (BSAFE+).This method improves upon previous data creation techniques by injecting violations into coherent, originally safe text, providing more effective initial training for the backtracking mechanism. Comprehensive empirical evaluations demonstrate that RLBF significantly reduces attack success rates across diverse benchmarks and model scales, achieving superior safety outcomes while critically preserving foundational model utility.

</details>

---

## 248. Learning Human-Like Badminton Skills for Humanoid Robots

**Chinese Title**: Learning Human-Like Badminton Skills for Humanoid Robots

**Authors**: Yeke Chen, Shihao Dong, Xiaoyu Ji, Jingkai Sun, Zeren Luo et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08370v1](http://arxiv.org/abs/2602.08370v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08370v1)

**Categories**: cs.RO, cs.AI, cs.LG

<details><summary><b>Abstract</b></summary>

Realizing versatile and human-like performance in high-demand sports like badminton remains a formidable challenge for humanoid robotics. Unlike standard locomotion or static manipulation, this task demands a seamless integration of explosive whole-body coordination and precise, timing-critical interception. While recent advances have achieved lifelike motion mimicry, bridging the gap between kinematic imitation and functional, physics-aware striking without compromising stylistic naturalness is non-trivial. To address this, we propose Imitation-to-Interaction, a progressive reinforcement learning framework designed to evolve a robot from a "mimic" to a capable "striker." Our approach establishes a robust motor prior from human data, distills it into a compact, model-based state representation, and stabilizes dynamics via adversarial priors. Crucially, to overcome the sparsity of expert demonstrations, we introduce a manifold expansion strategy that generalizes discrete strike points into a dense interaction volume. We validate our framework through the mastery of diverse skills, including lifts and drop shots, in simulation. Furthermore, we demonstrate the first zero-shot sim-to-real transfer of anthropomorphic badminton skills to a humanoid robot, successfully replicating the kinetic elegance and functional precision of human athletes in the physical world.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Realizing versatile and human-like performance in high-demand sports like badminton remains a formidable challenge for humanoid robotics. Unlike standard locomotion or static manipulation, this task demands a seamless integration of explosive whole-body coordination and precise, timing-critical interception.While recent advances have achieved lifelike motion mimicry, bridging the gap between kinematic imitation and functional, physics-aware striking without compromising stylistic naturalness is non-trivial.To address this, we propose Imitation-to-Interaction, a progressive reinforcement learning framework designed to evolve a robot from a "mimic" to a capable "striker." Our approach establishes a robust motor prior from human data, distills it into a compact, model-based state representation, and stabilizes dynamics via adversarial priors.Crucially, to overcome the sparsity of expert demonstrations, we introduce a manifold expansion strategy that generalizes discrete strike points into a dense interaction volume. We validate our framework through the mastery of diverse skills, including lifts and drop shots, in simulation.Furthermore, we demonstrate the first zero-shot sim-to-real transfer of anthropomorphic badminton skills to a humanoid robot, successfully replicating the kinetic elegance and functional precision of human athletes in the physical world.

</details>

---

## 249. Roadmap to Quantum Aesthetics

**Chinese Title**: Roadmap to Quantum Aesthetics

**Authors**: Ivan C. H. Liu, Hsiao-Yuan Chen

**Date**: 2026-02-09 | **arXiv**: [2602.08363v1](http://arxiv.org/abs/2602.08363v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08363v1)

**Categories**: physics.pop-ph, cs.AI, quant-ph

<details><summary><b>Abstract</b></summary>

Quantum mechanics occupies a central position in contemporary science while remaining largely inaccessible to direct sensory experience. This paper proposes a roadmap to quantum aesthetics that examines how quantum concepts become aesthetic phenomena through artistic mediation rather than direct representation. Two complementary and orthogonal approaches are articulated. The first, a pioneering top-down approach, employs text-prompt-based generative AI to probe quantum aesthetics as a collective cultural construct embedded in large-scale training data. By systematically modulating the linguistic weight of the term "quantum," generative models are used as experimental environments to reveal how quantum imaginaries circulate within contemporary visual culture. The second, a bottom-up approach, derives aesthetic form directly from quantum-mechanical structures through the visualization of quantum-generated data, exemplified here by hydrogen atomic orbitals calculated from the SchrÃ¶dinger equation. These approaches are framed not as competing methods but as intersecting paths within a navigable field of artistic research. They position quantum aesthetics as an emergent field of artistic research shaped by cultural imagination, computational mediation, and physical law, opening new directions for artistic practice and pedagogy at the intersection of art, data, artificial intelligence and quantum science.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Quantum mechanics occupies a central position in contemporary science while remaining largely inaccessible to direct sensory experience. This paper proposes a roadmap to quantum aesthetics that examines how quantum concepts become aesthetic phenomena through artistic mediation rather than direct representation. Two complementary and orthogonal approaches are articulated.The first, a pioneering top-down approach, employs text-prompt-based generative AI to probe quantum aesthetics as a collective cultural construct embedded in large-scale training data. By systematically modulating the linguistic weight of the term "quantum," generative models are used as experimental environments to reveal how quantum imaginaries circulate within contemporary visual culture.The second, a bottom-up approach, derives aesthetic form directly from quantum-mechanical structures through the visualization of quantum-generated data, exemplified here by hydrogen atomic orbitals calculated from the SchrÃ¶dinger equation. These approaches are framed not as competing methods but as intersecting paths within a navigable field of artistic research.They position quantum aesthetics as an emergent field of artistic research shaped by cultural imagination, computational mediation, and physical law, opening new directions for artistic practice and pedagogy at the intersection of art, data, artificial intelligence and quantum science.

</details>

---

## 250. Does Your Reasoning Model Implicitly Know When to Stop Thinking?

**Chinese Title**: Does Your Reasoning Model Implicitly Know When to Stop Thinking?

**Authors**: Zixuan Huang, Xin Xia, Yuxi Ren, Jianbin Zheng, Xuanda Wang et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08354v1](http://arxiv.org/abs/2602.08354v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08354v1)

**Categories**: cs.AI

<details><summary><b>Abstract</b></summary>

Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications.Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms.Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential.Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.

</details>

---

## 251. Towards Better Evolution Modeling for Temporal Knowledge Graphs

**Chinese Title**: Towards Better Evolution Modeling for Temporal Knowledge Graphs

**Authors**: Zhang Jiasheng, Li Zhangpin, Wang Mingzhe, Shao Jie, Cui Jiangtao et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08353v1](http://arxiv.org/abs/2602.08353v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08353v1)

**Code**: https://github.com/zjs123/TKG-Benchmark.

**Categories**: cs.AI

<details><summary><b>Abstract</b></summary>

Temporal knowledge graphs (TKGs) structurally preserve evolving human knowledge. Recent research has focused on designing models to learn the evolutionary nature of TKGs to predict future facts, achieving impressive results. For instance, Hits@10 scores over 0.9 on YAGO dataset. However, we find that existing benchmarks inadvertently introduce a shortcut. Near state-of-the-art performance can be simply achieved by counting co-occurrences, without using any temporal information. In this work, we examine the root cause of this issue, identifying inherent biases in current datasets and over simplified form of evaluation task that can be exploited by these biases. Through this analysis, we further uncover additional limitations of existing benchmarks, including unreasonable formatting of time-interval knowledge, ignorance of learning knowledge obsolescence, and insufficient information for precise evolution understanding, all of which can amplify the shortcut and hinder a fair assessment. Therefore, we introduce the TKG evolution benchmark. It includes four bias-corrected datasets and two novel tasks closely aligned with the evolution process, promoting a more accurate understanding of the challenges in TKG evolution modeling. Benchmark is available at: https://github.com/zjs123/TKG-Benchmark.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Temporal knowledge graphs (TKGs) structurally preserve evolving human knowledge. Recent research has focused on designing models to learn the evolutionary nature of TKGs to predict future facts, achieving impressive results. For instance, Hits@10 scores over 0.9 on YAGO dataset. However, we find that existing benchmarks inadvertently introduce a shortcut.Near state-of-the-art performance can be simply achieved by counting co-occurrences, without using any temporal information. In this work, we examine the root cause of this issue, identifying inherent biases in current datasets and over simplified form of evaluation task that can be exploited by these biases.Through this analysis, we further uncover additional limitations of existing benchmarks, including unreasonable formatting of time-interval knowledge, ignorance of learning knowledge obsolescence, and insufficient information for precise evolution understanding, all of which can amplify the shortcut and hinder a fair assessment. Therefore, we introduce the TKG evolution benchmark.It includes four bias-corrected datasets and two novel tasks closely aligned with the evolution process, promoting a more accurate understanding of the challenges in TKG evolution modeling. Benchmark is available at: https://github.com/zjs123/TKG-Benchmark.

</details>

---

## 252. The Chicken and Egg Dilemma: Co-optimizing Data and Model Configurations for LLMs

**Chinese Title**: The Chicken and Egg Dilemma: Co-optimizing Data and Model Configurations for LLMs

**Authors**: Zhiliang Chen, Alfred Wei Lun Leong, Shao Yong Ong, Apivich Hemachandram, Gregory Kang Ruey Lau et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08351v1](http://arxiv.org/abs/2602.08351v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08351v1)

**Categories**: cs.LG, cs.AI

<details><summary><b>Abstract</b></summary>

Co-optimizing data and model configurations for training LLMs presents a classic chicken-and-egg dilemma: The best training data configuration (e.g., data mixture) for a downstream task depends on the chosen model configuration (e.g., model architecture), and vice versa. However, jointly optimizing both data and model configurations is often deemed intractable, and existing methods focus on either data or model optimization without considering their interaction. We introduce JoBS, an approach that uses a scaling-law-inspired performance predictor to aid Bayesian optimization (BO) in jointly optimizing LLM training data and model configurations efficiently. JoBS allocates a portion of the optimization budget to learn an LLM performance predictor that predicts how promising a training configuration is from a small number of training steps. The remaining budget is used to perform BO entirely with the predictor, effectively amortizing the cost of running full-training runs. We study JoBS's average regret and devise the optimal budget allocation to minimize regret. JoBS outperforms existing multi-fidelity BO baselines, as well as data and model optimization approaches across diverse LLM tasks under the same optimization budget.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Co-optimizing data and model configurations for training LLMs presents a classic chicken-and-egg dilemma: The best training data configuration (e.g., data mixture) for a downstream task depends on the chosen model configuration (e.g., model architecture), and vice versa.However, jointly optimizing both data and model configurations is often deemed intractable, and existing methods focus on either data or model optimization without considering their interaction. We introduce JoBS, an approach that uses a scaling-law-inspired performance predictor to aid Bayesian optimization (BO) in jointly optimizing LLM training data and model configurations efficiently.JoBS allocates a portion of the optimization budget to learn an LLM performance predictor that predicts how promising a training configuration is from a small number of training steps. The remaining budget is used to perform BO entirely with the predictor, effectively amortizing the cost of running full-training runs. We study JoBS's average regret and devise the optimal budget allocation to minimize regret.JoBS outperforms existing multi-fidelity BO baselines, as well as data and model optimization approaches across diverse LLM tasks under the same optimization budget.

</details>

---

## 253. OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration

**Chinese Title**: OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration

**Authors**: Qi Guo, Jianing Wang, Deyang Kong, Xiangyu Xi, Jianfei Zhang et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08344v1](http://arxiv.org/abs/2602.08344v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08344v1)

**Categories**: cs.AI

<details><summary><b>Abstract</b></summary>

Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking, aiming to address the limitations in computational resources and effectiveness encountered with supervised fine-tuning. However, most existing studies primarily focus on optimizing the aggregation phase, with limited attention to the path exploration stage. In this paper, we theoretically analyze the optimization of parallel thinking under the Reinforcement Learning with Verifiable Rewards (RLVR) setting, and identify that the mutual information bottleneck among exploration paths fundamentally restricts overall performance. To address this, we propose Outline-Guided Path Exploration (OPE), which explicitly partitions the solution space by generating diverse reasoning outlines prior to parallel path reasoning, thereby reducing information redundancy and improving the diversity of information captured across exploration paths. We implement OPE with an iterative RL strategy that optimizes outline planning and outline-guided reasoning independently. Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance in different aggregation strategies, enabling LRMs to more reliably discover correct solutions.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking, aiming to address the limitations in computational resources and effectiveness encountered with supervised fine-tuning.However, most existing studies primarily focus on optimizing the aggregation phase, with limited attention to the path exploration stage. In this paper, we theoretically analyze the optimization of parallel thinking under the Reinforcement Learning with Verifiable Rewards (RLVR) setting, and identify that the mutual information bottleneck among exploration paths fundamentally restricts overall performance.To address this, we propose Outline-Guided Path Exploration (OPE), which explicitly partitions the solution space by generating diverse reasoning outlines prior to parallel path reasoning, thereby reducing information redundancy and improving the diversity of information captured across exploration paths. We implement OPE with an iterative RL strategy that optimizes outline planning and outline-guided reasoning independently.Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance in different aggregation strategies, enabling LRMs to more reliably discover correct solutions.

</details>

---

## 254. Regime Change Hypothesis: Foundations for Decoupled Dynamics in Neural Network Training

**Chinese Title**: Regime Change Hypothesis: Foundations for Decoupled Dynamics in Neural Network Training

**Authors**: Cristian PÃ©rez-Corral, Alberto FernÃ¡ndez-HernÃ¡ndez, Jose I. Mestre, Manuel F. Dolz, Jose Duato et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08333v1](http://arxiv.org/abs/2602.08333v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08333v1)

**Categories**: cs.LG, cs.AI

<details><summary><b>Abstract</b></summary>

Despite the empirical success of DNN, their internal training dynamics remain difficult to characterize. In ReLU-based models, the activation pattern induced by a given input determines the piecewise-linear region in which the network behaves affinely. Motivated by this geometry, we investigate whether training exhibits a two-timescale behavior: an early stage with substantial changes in activation patterns and a later stage where weight updates predominantly refine the model within largely stable activation regimes. We first prove a local stability property: outside measure-zero sets of parameters and inputs, sufficiently small parameter perturbations preserve the activation pattern of a fixed input, implying locally affine behavior within activation regions. We then empirically track per-iteration changes in weights and activation patterns across fully-connected and convolutional architectures, as well as Transformer-based models, where activation patterns are recorded in the ReLU feed-forward (MLP/FFN) submodules, using fixed validation subsets. Across the evaluated settings, activation-pattern changes decay 3 times earlier than weight-update magnitudes, showing that late-stage training often proceeds within relatively stable activation regimes. These findings provide a concrete, architecture-agnostic instrument for monitoring training dynamics and motivate further study of decoupled optimization strategies for piecewise-linear networks. For reproducibility, code and experiment configurations will be released upon acceptance.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Despite the empirical success of DNN, their internal training dynamics remain difficult to characterize. In ReLU-based models, the activation pattern induced by a given input determines the piecewise-linear region in which the network behaves affinely.Motivated by this geometry, we investigate whether training exhibits a two-timescale behavior: an early stage with substantial changes in activation patterns and a later stage where weight updates predominantly refine the model within largely stable activation regimes.We first prove a local stability property: outside measure-zero sets of parameters and inputs, sufficiently small parameter perturbations preserve the activation pattern of a fixed input, implying locally affine behavior within activation regions.We then empirically track per-iteration changes in weights and activation patterns across fully-connected and convolutional architectures, as well as Transformer-based models, where activation patterns are recorded in the ReLU feed-forward (MLP/FFN) submodules, using fixed validation subsets.Across the evaluated settings, activation-pattern changes decay 3 times earlier than weight-update magnitudes, showing that late-stage training often proceeds within relatively stable activation regimes. These findings provide a concrete, architecture-agnostic instrument for monitoring training dynamics and motivate further study of decoupled optimization strategies for piecewise-linear networks.For reproducibility, code and experiment configurations will be released upon acceptance.

</details>

---

## 255. Latent Reasoning with Supervised Thinking States

**Chinese Title**: Latent Reasoning with Supervised Thinking States

**Authors**: Ido Amos, Avi Caciularu, Mor Geva, Amir Globerson, Jonathan Herzig et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08332v1](http://arxiv.org/abs/2602.08332v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08332v1)

**Categories**: cs.CL, cs.AI

<details><summary><b>Abstract</b></summary>

Reasoning with a chain-of-thought (CoT) enables Large Language Models (LLMs) to solve complex tasks but incurs significant inference costs due to the generation of long rationales. We propose Thinking States, a method that performs reasoning {\em while} the input is processing. Specifically, Thinking States generates sequences of thinking tokens every few input tokens, transforms the thoughts back into embedding space, and adds them to the following input tokens. This has two key advantages. First, it captures the recurrent nature of CoT, but where the thought tokens are generated as input is processing. Second, since the thoughts are represented as tokens, they can be learned from natural language supervision, and using teacher-forcing, which is parallelizable. Empirically, Thinking States outperforms other latent reasoning methods on multiple reasoning tasks, narrowing the gap to CoT on math problems, and matching its performance on 2-Hop QA with improved latency. On state-tracking tasks, we show Thinking States leads to stronger reasoning behavior than CoT, successfully extrapolating to longer sequences than seen during training.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Reasoning with a chain-of-thought (CoT) enables Large Language Models (LLMs) to solve complex tasks but incurs significant inference costs due to the generation of long rationales. We propose Thinking States, a method that performs reasoning {\em while} the input is processing.Specifically, Thinking States generates sequences of thinking tokens every few input tokens, transforms the thoughts back into embedding space, and adds them to the following input tokens. This has two key advantages. First, it captures the recurrent nature of CoT, but where the thought tokens are generated as input is processing.Second, since the thoughts are represented as tokens, they can be learned from natural language supervision, and using teacher-forcing, which is parallelizable. Empirically, Thinking States outperforms other latent reasoning methods on multiple reasoning tasks, narrowing the gap to CoT on math problems, and matching its performance on 2-Hop QA with improved latency.On state-tracking tasks, we show Thinking States leads to stronger reasoning behavior than CoT, successfully extrapolating to longer sequences than seen during training.

</details>

---

## 256. Moral Sycophancy in Vision Language Models

**Chinese Title**: Moral Sycophancy in Vision Language Models

**Authors**: Shadman Rabby, Md. Hefzul Hossain Papon, Sabbir Ahmed, Nokimul Hasan Arif, A. B. M. Ashikur Rahman et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08311v1](http://arxiv.org/abs/2602.08311v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08311v1)

**Categories**: cs.AI

<details><summary><b>Abstract</b></summary>

Sycophancy in Vision-Language Models (VLMs) refers to their tendency to align with user opinions, often at the expense of moral or factual accuracy. While prior studies have explored sycophantic behavior in general contexts, its impact on morally grounded visual decision-making remains insufficiently understood. To address this gap, we present the first systematic study of moral sycophancy in VLMs, analyzing ten widely-used models on the Moralise and M^3oralBench datasets under explicit user disagreement. Our results reveal that VLMs frequently produce morally incorrect follow-up responses even when their initial judgments are correct, and exhibit a consistent asymmetry: models are more likely to shift from morally right to morally wrong judgments than the reverse when exposed to user-induced bias. Follow-up prompts generally degrade performance on Moralise, while yielding mixed or even improved accuracy on M^3oralBench, highlighting dataset-dependent differences in moral robustness. Evaluation using Error Introduction Rate (EIR) and Error Correction Rate (ECR) reveals a clear trade-off: models with stronger error-correction capabilities tend to introduce more reasoning errors, whereas more conservative models minimize errors but exhibit limited ability to self-correct. Finally, initial contexts with a morally right stance elicit stronger sycophantic behavior, emphasizing the vulnerability of VLMs to moral influence and the need for principled strategies to improve ethical consistency and robustness in multimodal AI systems.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Sycophancy in Vision-Language Models (VLMs) refers to their tendency to align with user opinions, often at the expense of moral or factual accuracy. While prior studies have explored sycophantic behavior in general contexts, its impact on morally grounded visual decision-making remains insufficiently understood.To address this gap, we present the first systematic study of moral sycophancy in VLMs, analyzing ten widely-used models on the Moralise and M^3oralBench datasets under explicit user disagreement.Our results reveal that VLMs frequently produce morally incorrect follow-up responses even when their initial judgments are correct, and exhibit a consistent asymmetry: models are more likely to shift from morally right to morally wrong judgments than the reverse when exposed to user-induced bias.Follow-up prompts generally degrade performance on Moralise, while yielding mixed or even improved accuracy on M^3oralBench, highlighting dataset-dependent differences in moral robustness.Evaluation using Error Introduction Rate (EIR) and Error Correction Rate (ECR) reveals a clear trade-off: models with stronger error-correction capabilities tend to introduce more reasoning errors, whereas more conservative models minimize errors but exhibit limited ability to self-correct.Finally, initial contexts with a morally right stance elicit stronger sycophantic behavior, emphasizing the vulnerability of VLMs to moral influence and the need for principled strategies to improve ethical consistency and robustness in multimodal AI systems.

</details>

---

## 257. Puda: Private User Dataset Agent for User-Sovereign and Privacy-Preserving Personalized AI

**Chinese Title**: Puda: Private User Dataset Agent for User-Sovereign and Privacy-Preserving Personalized AI

**Authors**: Akinori Maeda, Yuto Sekiya, Sota Sugimura, Tomoya Asai, Yu Tsuda et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08268v2](http://arxiv.org/abs/2602.08268v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.08268v2)

**Categories**: cs.AI

<details><summary><b>Abstract</b></summary>

Personal data centralization among dominant platform providers including search engines, social networking services, and e-commerce has created siloed ecosystems that restrict user sovereignty, thereby impeding data use across services. Meanwhile, the rapid proliferation of Large Language Model (LLM)-based agents has intensified demand for highly personalized services that require the dynamic provision of diverse personal data. This presents a significant challenge: balancing the utilization of such data with privacy protection. To address this challenge, we propose Puda (Private User Dataset Agent), a user-sovereign architecture that aggregates data across services and enables client-side management. Puda allows users to control data sharing at three privacy levels: (i) Detailed Browsing History, (ii) Extracted Keywords, and (iii) Predefined Category Subsets. We implemented Puda as a browser-based system that serves as a common platform across diverse services and evaluated it through a personalized travel planning task. Our results show that providing Predefined Category Subsets achieves 97.2% of the personalization performance (evaluated via an LLM-as-a-Judge framework across three criteria) obtained when sharing Detailed Browsing History. These findings demonstrate that Puda enables effective multi-granularity management, offering practical choices to mitigate the privacy-personalization trade-off. Overall, Puda provides an AI-native foundation for user sovereignty, empowering users to safely leverage the full potential of personalized AI.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Personal data centralization among dominant platform providers including search engines, social networking services, and e-commerce has created siloed ecosystems that restrict user sovereignty, thereby impeding data use across services. Meanwhile, the rapid proliferation of Large Language Model (LLM)-based agents has intensified demand for highly personalized services that require the dynamic provision of diverse personal data.This presents a significant challenge: balancing the utilization of such data with privacy protection. To address this challenge, we propose Puda (Private User Dataset Agent), a user-sovereign architecture that aggregates data across services and enables client-side management. Puda allows users to control data sharing at three privacy levels: (i) Detailed Browsing History, (ii) Extracted Keywords, and (iii) Predefined Category Subsets.We implemented Puda as a browser-based system that serves as a common platform across diverse services and evaluated it through a personalized travel planning task. Our results show that providing Predefined Category Subsets achieves 97.2% of the personalization performance (evaluated via an LLM-as-a-Judge framework across three criteria) obtained when sharing Detailed Browsing History.These findings demonstrate that Puda enables effective multi-granularity management, offering practical choices to mitigate the privacy-personalization trade-off. Overall, Puda provides an AI-native foundation for user sovereignty, empowering users to safely leverage the full potential of personalized AI.

</details>

---

## 258. SynthAgent: A Multi-Agent LLM Framework for Realistic Patient Simulation -- A Case Study in Obesity with Mental Health Comorbidities

**Chinese Title**: SynthAgent: A Multi-Agent LLM Framework for Realistic Patient Simulation -- A Case Study in Obesity with Mental Health Comorbidities

**Authors**: Arman Aghaee, Sepehr Asgarian, Jouhyun Jeon

**Date**: 2026-02-09 | **arXiv**: [2602.08254v1](http://arxiv.org/abs/2602.08254v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08254v1)

**Categories**: cs.AI, cs.IR, cs.MA

<details><summary><b>Abstract</b></summary>

Simulating high-fidelity patients offers a powerful avenue for studying complex diseases while addressing the challenges of fragmented, biased, and privacy-restricted real-world data. In this study, we introduce SynthAgent, a novel Multi-Agent System (MAS) framework designed to model obesity patients with comorbid mental disorders, including depression, anxiety, social phobia, and binge eating disorder. SynthAgent integrates clinical and medical evidence from claims data, population surveys, and patient-centered literature to construct personalized virtual patients enriched with personality traits that influence adherence, emotion regulation, and lifestyle behaviors. Through autonomous agent interactions, the system simulates disease progression, treatment response, and life management across diverse psychosocial contexts. Evaluation of more than 100 generated patients demonstrated that GPT-5 and Claude 4.5 Sonnet achieved the highest fidelity as the core engine in the proposed MAS framework, outperforming Gemini 2.5 Pro and DeepSeek-R1. SynthAgent thus provides a scalable and privacy-preserving framework for exploring patient journeys, behavioral dynamics, and decision-making processes in both medical and psychological domains.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Simulating high-fidelity patients offers a powerful avenue for studying complex diseases while addressing the challenges of fragmented, biased, and privacy-restricted real-world data. In this study, we introduce SynthAgent, a novel Multi-Agent System (MAS) framework designed to model obesity patients with comorbid mental disorders, including depression, anxiety, social phobia, and binge eating disorder.SynthAgent integrates clinical and medical evidence from claims data, population surveys, and patient-centered literature to construct personalized virtual patients enriched with personality traits that influence adherence, emotion regulation, and lifestyle behaviors. Through autonomous agent interactions, the system simulates disease progression, treatment response, and life management across diverse psychosocial contexts.Evaluation of more than 100 generated patients demonstrated that GPT-5 and Claude 4.5 Sonnet achieved the highest fidelity as the core engine in the proposed MAS framework, outperforming Gemini 2.5 Pro and DeepSeek-R1. SynthAgent thus provides a scalable and privacy-preserving framework for exploring patient journeys, behavioral dynamics, and decision-making processes in both medical and psychological domains.

</details>

---

## 259. STEP: Warm-Started Visuomotor Policies with Spatiotemporal Consistency Prediction

**Chinese Title**: STEP: Warm-Started Visuomotor Policies with Spatiotemporal Consistency Prediction

**Authors**: Jinhao Li, Yuxuan Cong, Yingqiao Wang, Hao Xia, Shan Huang et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08245v1](http://arxiv.org/abs/2602.08245v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08245v1)

**Categories**: cs.RO, cs.AI

<details><summary><b>Abstract</b></summary>

Diffusion policies have recently emerged as a powerful paradigm for visuomotor control in robotic manipulation due to their ability to model the distribution of action sequences and capture multimodality. However, iterative denoising leads to substantial inference latency, limiting control frequency in real-time closed-loop systems. Existing acceleration methods either reduce sampling steps, bypass diffusion through direct prediction, or reuse past actions, but often struggle to jointly preserve action quality and achieve consistently low latency. In this work, we propose STEP, a lightweight spatiotemporal consistency prediction mechanism to construct high-quality warm-start actions that are both distributionally close to the target action and temporally consistent, without compromising the generative capability of the original diffusion policy. Then, we propose a velocity-aware perturbation injection mechanism that adaptively modulates actuation excitation based on temporal action variation to prevent execution stall especially for real-world tasks. We further provide a theoretical analysis showing that the proposed prediction induces a locally contractive mapping, ensuring convergence of action errors during diffusion refinement. We conduct extensive evaluations on nine simulated benchmarks and two real-world tasks. Notably, STEP with 2 steps can achieve an average 21.6% and 27.5% higher success rate than BRIDGER and DDIM on the RoboMimic benchmark and real-world tasks, respectively. These results demonstrate that STEP consistently advances the Pareto frontier of inference latency and success rate over existing methods.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Diffusion policies have recently emerged as a powerful paradigm for visuomotor control in robotic manipulation due to their ability to model the distribution of action sequences and capture multimodality. However, iterative denoising leads to substantial inference latency, limiting control frequency in real-time closed-loop systems.Existing acceleration methods either reduce sampling steps, bypass diffusion through direct prediction, or reuse past actions, but often struggle to jointly preserve action quality and achieve consistently low latency.In this work, we propose STEP, a lightweight spatiotemporal consistency prediction mechanism to construct high-quality warm-start actions that are both distributionally close to the target action and temporally consistent, without compromising the generative capability of the original diffusion policy.Then, we propose a velocity-aware perturbation injection mechanism that adaptively modulates actuation excitation based on temporal action variation to prevent execution stall especially for real-world tasks. We further provide a theoretical analysis showing that the proposed prediction induces a locally contractive mapping, ensuring convergence of action errors during diffusion refinement.We conduct extensive evaluations on nine simulated benchmarks and two real-world tasks. Notably, STEP with 2 steps can achieve an average 21.6% and 27.5% higher success rate than BRIDGER and DDIM on the RoboMimic benchmark and real-world tasks, respectively. These results demonstrate that STEP consistently advances the Pareto frontier of inference latency and success rate over existing methods.

</details>

---

## 260. Learning in Context, Guided by Choice: A Reward-Free Paradigm for Reinforcement Learning with Transformers

**Chinese Title**: Learning in Context, Guided by Choice: A Reward-Free Paradigm for Reinforcement Learning with Transformers

**Authors**: Juncheng Dong, Bowen He, Moyang Guo, Ethan X. Fang, Zhuoran Yang et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08244v1](http://arxiv.org/abs/2602.08244v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08244v1)

**Categories**: cs.LG, cs.AI

<details><summary><b>Abstract</b></summary>

In-context reinforcement learning (ICRL) leverages the in-context learning capabilities of transformer models (TMs) to efficiently generalize to unseen sequential decision-making tasks without parameter updates. However, existing ICRL methods rely on explicit reward signals during pretraining, which limits their applicability when rewards are ambiguous, hard to specify, or costly to obtain. To overcome this limitation, we propose a new learning paradigm, In-Context Preference-based Reinforcement Learning (ICPRL), in which both pretraining and deployment rely solely on preference feedback, eliminating the need for reward supervision. We study two variants that differ in the granularity of feedback: Immediate Preference-based RL (I-PRL) with per-step preferences, and Trajectory Preference-based RL (T-PRL) with trajectory-level comparisons. We first show that supervised pretraining, a standard approach in ICRL, remains effective under preference-only context datasets, demonstrating the feasibility of in-context reinforcement learning using only preference signals. To further improve data efficiency, we introduce alternative preference-native frameworks for I-PRL and T-PRL that directly optimize TM policies from preference data without requiring reward signals nor optimal action labels.Experiments on dueling bandits, navigation, and continuous control tasks demonstrate that ICPRL enables strong in-context generalization to unseen tasks, achieving performance comparable to ICRL methods trained with full reward supervision.

</details>

<details><summary><b>Chinese Abstract</b></summary>

In-context reinforcement learning (ICRL) leverages the in-context learning capabilities of transformer models (TMs) to efficiently generalize to unseen sequential decision-making tasks without parameter updates. However, existing ICRL methods rely on explicit reward signals during pretraining, which limits their applicability when rewards are ambiguous, hard to specify, or costly to obtain.To overcome this limitation, we propose a new learning paradigm, In-Context Preference-based Reinforcement Learning (ICPRL), in which both pretraining and deployment rely solely on preference feedback, eliminating the need for reward supervision.We study two variants that differ in the granularity of feedback: Immediate Preference-based RL (I-PRL) with per-step preferences, and Trajectory Preference-based RL (T-PRL) with trajectory-level comparisons. We first show that supervised pretraining, a standard approach in ICRL, remains effective under preference-only context datasets, demonstrating the feasibility of in-context reinforcement learning using only preference signals.To further improve data efficiency, we introduce alternative preference-native frameworks for I-PRL and T-PRL that directly optimize TM policies from preference data without requiring reward signals nor optimal action labels.Experiments on dueling bandits, navigation, and continuous control tasks demonstrate that ICPRL enables strong in-context generalization to unseen tasks, achieving performance comparable to ICRL methods trained with full reward supervision.

</details>

---

## 261. PTS-SNN: A Prompt-Tuned Temporal Shift Spiking Neural Networks for Efficient Speech Emotion Recognition

**Chinese Title**: PTS-SNN: A Prompt-Tuned Temporal Shift Spiking Neural Networks for Efficient Speech Emotion Recognition

**Authors**: Xun Su, Huamin Wang, Qi Zhang

**Date**: 2026-02-09 | **arXiv**: [2602.08240v1](http://arxiv.org/abs/2602.08240v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08240v1)

**Categories**: cs.AI, cs.SD

<details><summary><b>Abstract</b></summary>

Speech Emotion Recognition (SER) is widely deployed in Human-Computer Interaction, yet the high computational cost of conventional models hinders their implementation on resource-constrained edge devices. Spiking Neural Networks (SNNs) offer an energy-efficient alternative due to their event-driven nature; however, their integration with continuous Self-Supervised Learning (SSL) representations is fundamentally challenged by distribution mismatch, where high-dynamic-range embeddings degrade the information coding capacity of threshold-based neurons. To resolve this, we propose Prompt-Tuned Spiking Neural Networks (PTS-SNN), a parameter-efficient neuromorphic adaptation framework that aligns frozen SSL backbones with spiking dynamics. Specifically, we introduce a Temporal Shift Spiking Encoder to capture local temporal dependencies via parameter-free channel shifts, establishing a stable feature basis. To bridge the domain gap, we devise a Context-Aware Membrane Potential Calibration strategy. This mechanism leverages a Spiking Sparse Linear Attention module to aggregate global semantic context into learnable soft prompts, which dynamically regulate the bias voltages of Parametric Leaky Integrate-and-Fire (PLIF) neurons. This regulation effectively centers the heterogeneous input distribution within the responsive firing range, mitigating functional silence or saturation. Extensive experiments on five multilingual datasets (e.g., IEMOCAP, CASIA, EMODB) demonstrate that PTS-SNN achieves 73.34\% accuracy on IEMOCAP, comparable to competitive Artificial Neural Networks (ANNs), while requiring only 1.19M trainable parameters and 0.35 mJ inference energy per sample.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Speech Emotion Recognition (SER) is widely deployed in Human-Computer Interaction, yet the high computational cost of conventional models hinders their implementation on resource-constrained edge devices.Spiking Neural Networks (SNNs) offer an energy-efficient alternative due to their event-driven nature; however, their integration with continuous Self-Supervised Learning (SSL) representations is fundamentally challenged by distribution mismatch, where high-dynamic-range embeddings degrade the information coding capacity of threshold-based neurons.To resolve this, we propose Prompt-Tuned Spiking Neural Networks (PTS-SNN), a parameter-efficient neuromorphic adaptation framework that aligns frozen SSL backbones with spiking dynamics. Specifically, we introduce a Temporal Shift Spiking Encoder to capture local temporal dependencies via parameter-free channel shifts, establishing a stable feature basis.To bridge the domain gap, we devise a Context-Aware Membrane Potential Calibration strategy. This mechanism leverages a Spiking Sparse Linear Attention module to aggregate global semantic context into learnable soft prompts, which dynamically regulate the bias voltages of Parametric Leaky Integrate-and-Fire (PLIF) neurons.This regulation effectively centers the heterogeneous input distribution within the responsive firing range, mitigating functional silence or saturation. Extensive experiments on five multilingual datasets (e.g., IEMOCAP, CASIA, EMODB) demonstrate that PTS-SNN achieves 73.34\% accuracy on IEMOCAP, comparable to competitive Artificial Neural Networks (ANNs), while requiring only 1.19M trainable parameters and 0.35 mJ inference energy per sample.

</details>

---

## 262. Linearization Explains Fine-Tuning in Large Language Models

**Chinese Title**: Linearization Explains Fine-Tuning in Large Language Models

**Authors**: Zahra Rahimi Afzal, Tara Esmaeilbeig, Mojtaba Soltanalian, Mesrob I. Ohannessian

**Date**: 2026-02-09 | **arXiv**: [2602.08239v1](http://arxiv.org/abs/2602.08239v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08239v1)

**Categories**: cs.LG, cs.AI

<details><summary><b>Abstract</b></summary>

Parameter-Efficient Fine-Tuning (PEFT) is a popular class of techniques that strive to adapt large models in a scalable and resource-efficient manner. Yet, the mechanisms underlying their training performance and generalization remain underexplored. In this paper, we provide several insights into such fine-tuning through the lens of linearization. Fine-tuned models are often implicitly encouraged to remain close to the pretrained model. By making this explicit, using an Euclidean distance inductive bias in parameter space, we show that fine-tuning dynamics become equivalent to learning with the positive-definite neural tangent kernel (NTK). We specifically analyze how close the fully linear and the linearized fine-tuning optimizations are, based on the strength of the regularization. This allows us to be pragmatic about how good a model linearization is when fine-tuning large language models (LLMs). When linearization is a good model, our findings reveal a strong correlation between the eigenvalue spectrum of the NTK and the performance of model adaptation. Motivated by this, we give spectral perturbation bounds on the NTK induced by the choice of layers selected for fine-tuning. We empirically validate our theory on Low Rank Adaptation (LoRA) on LLMs. These insights not only characterize fine-tuning but also have the potential to enhance PEFT techniques, paving the way to better informed and more nimble adaptation in LLMs.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Parameter-Efficient Fine-Tuning (PEFT) is a popular class of techniques that strive to adapt large models in a scalable and resource-efficient manner. Yet, the mechanisms underlying their training performance and generalization remain underexplored. In this paper, we provide several insights into such fine-tuning through the lens of linearization. Fine-tuned models are often implicitly encouraged to remain close to the pretrained model.By making this explicit, using an Euclidean distance inductive bias in parameter space, we show that fine-tuning dynamics become equivalent to learning with the positive-definite neural tangent kernel (NTK). We specifically analyze how close the fully linear and the linearized fine-tuning optimizations are, based on the strength of the regularization.This allows us to be pragmatic about how good a model linearization is when fine-tuning large language models (LLMs). When linearization is a good model, our findings reveal a strong correlation between the eigenvalue spectrum of the NTK and the performance of model adaptation. Motivated by this, we give spectral perturbation bounds on the NTK induced by the choice of layers selected for fine-tuning.We empirically validate our theory on Low Rank Adaptation (LoRA) on LLMs. These insights not only characterize fine-tuning but also have the potential to enhance PEFT techniques, paving the way to better informed and more nimble adaptation in LLMs.

</details>

---

## 263. Weak-Driven Learning: How Weak Agents make Strong Agents Stronger

**Chinese Title**: Weak-Driven Learning: How Weak Agents make Strong Agents Stronger

**Authors**: Zehao Chen, Gongxun Li, Tianxiang Ai, Yifei Li, Zixuan Huang et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08222v1](http://arxiv.org/abs/2602.08222v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08222v1)

**Categories**: cs.AI

<details><summary><b>Abstract</b></summary>

As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.

</details>

<details><summary><b>Chinese Abstract</b></summary>

As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states.Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation.Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.

</details>

---

## 264. DrugR: Optimizing Molecular Drugs through LLM-based Explicit Reasoning

**Chinese Title**: DrugR: Optimizing Molecular Drugs through LLM-based Explicit Reasoning

**Authors**: Haoran Liu, Zheni Zeng, Yukun Yan, Yuxuan Chen, Yunduo Xiao

**Date**: 2026-02-09 | **arXiv**: [2602.08213v1](http://arxiv.org/abs/2602.08213v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08213v1)

**Categories**: cs.LG, cs.AI, cs.CL, q-bio.QM

<details><summary><b>Abstract</b></summary>

Molecule generation and optimization is a fundamental task in chemical domain. The rapid development of intelligent tools, especially large language models (LLMs) with powerful knowledge reserves and interactive capabilities, has provided new paradigms for it. Nevertheless, the intrinsic challenge for LLMs lies in the complex implicit relationship between molecular structure and pharmacological properties and the lack of corresponding labeled data. To bridge this gap, we propose DrugR, an LLM-based method that introduces explicit, step-by-step pharmacological reasoning into the optimization process. Our approach integrates domain-specific continual pretraining, supervised fine-tuning via reverse data engineering, and self-balanced multi-granular reinforcement learning. This framework enables DrugR to effectively improve key ADMET properties while preserving the original molecule's core efficacy. Experimental results demonstrate that DrugR achieves comprehensive enhancement across multiple properties without compromising structural similarity or target binding affinity. Importantly, its explicit reasoning process provides clear, interpretable rationales for each optimization step, yielding actionable design insights and advancing toward automated, knowledge-driven scientific discovery. Our code and model checkpoints are open-sourced to foster future research.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Molecule generation and optimization is a fundamental task in chemical domain. The rapid development of intelligent tools, especially large language models (LLMs) with powerful knowledge reserves and interactive capabilities, has provided new paradigms for it.Nevertheless, the intrinsic challenge for LLMs lies in the complex implicit relationship between molecular structure and pharmacological properties and the lack of corresponding labeled data. To bridge this gap, we propose DrugR, an LLM-based method that introduces explicit, step-by-step pharmacological reasoning into the optimization process.Our approach integrates domain-specific continual pretraining, supervised fine-tuning via reverse data engineering, and self-balanced multi-granular reinforcement learning. This framework enables DrugR to effectively improve key ADMET properties while preserving the original molecule's core efficacy.Experimental results demonstrate that DrugR achieves comprehensive enhancement across multiple properties without compromising structural similarity or target binding affinity. Importantly, its explicit reasoning process provides clear, interpretable rationales for each optimization step, yielding actionable design insights and advancing toward automated, knowledge-driven scientific discovery.Our code and model checkpoints are open-sourced to foster future research.

</details>

---

## 265. T2VTree: User-Centered Visual Analytics for Agent-Assisted Thought-to-Video Authoring

**Chinese Title**: T2VTree: User-Centered Visual Analytics for Agent-Assisted Thought-to-Video Authoring

**Authors**: Zhuoyun Zheng, Yu Dong, Gaorong Liang, Guan Li, Guihua Shan et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08368v1](http://arxiv.org/abs/2602.08368v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08368v1)

**Code**: https://github.com/tezuka0210/T2VTree.

**Categories**: cs.MM, cs.GR, cs.HC, cs.MA

<details><summary><b>Abstract</b></summary>

Generative models have substantially expanded video generation capabilities, yet practical thought-to-video creation remains a multi-stage, multi-modal, and decision-intensive process. However, existing tools either hide intermediate decisions behind repeated reruns or expose operator-level workflows that make exploration traces difficult to manage, compare, and reuse. We present T2VTree, a user-centered visual analytics approach for agent-assisted thought-to-video authoring. T2VTree represents the authoring process as a tree visualization. Each node in the tree binds an editable specification (intent, referenced inputs, workflow choice, prompts, and parameters) with the resulting multimodal outputs, making refinement, branching, and provenance inspection directly operable. To reduce the burden of deciding what to do next, a set of collaborating agents translates step-level intent into an executable plan that remains visible and user-editable before execution. We further implement a visual analytics system that integrates branching authoring with in-place preview and stitching for convergent assembly, enabling end-to-end multi-scene creation without leaving the authoring context. We demonstrate T2VTreeVA through two multi-scene case studies and a comparative user study, showing how the T2VTree visualization and editable agent planning support reliable refinement, localized comparison, and practical reuse in real authoring workflows. T2VTree is available at: https://github.com/tezuka0210/T2VTree.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Generative models have substantially expanded video generation capabilities, yet practical thought-to-video creation remains a multi-stage, multi-modal, and decision-intensive process. However, existing tools either hide intermediate decisions behind repeated reruns or expose operator-level workflows that make exploration traces difficult to manage, compare, and reuse.We present T2VTree, a user-centered visual analytics approach for agent-assisted thought-to-video authoring. T2VTree represents the authoring process as a tree visualization. Each node in the tree binds an editable specification (intent, referenced inputs, workflow choice, prompts, and parameters) with the resulting multimodal outputs, making refinement, branching, and provenance inspection directly operable.To reduce the burden of deciding what to do next, a set of collaborating agents translates step-level intent into an executable plan that remains visible and user-editable before execution. We further implement a visual analytics system that integrates branching authoring with in-place preview and stitching for convergent assembly, enabling end-to-end multi-scene creation without leaving the authoring context.We demonstrate T2VTreeVA through two multi-scene case studies and a comparative user study, showing how the T2VTree visualization and editable agent planning support reliable refinement, localized comparison, and practical reuse in real authoring workflows. T2VTree is available at: https://github.com/tezuka0210/T2VTree.

</details>

---

## 266. Data-centric Design of Learning-based Surgical Gaze Perception Models in Multi-Task Simulation

**Chinese Title**: Data-centric Design of Learning-based Surgical Gaze Perception Models in Multi-Task Simulation

**Authors**: Yizhou Li, Shuyuan Yang, Jiaji Su, Zonghe Chua

**Date**: 2026-02-09 | **arXiv**: [2602.09259v1](http://arxiv.org/abs/2602.09259v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09259v1)

**Categories**: cs.RO, cs.HC

<details><summary><b>Abstract</b></summary>

In robot-assisted minimally invasive surgery (RMIS), reduced haptic feedback and depth cues increase reliance on expert visual perception, motivating gaze-guided training and learning-based surgical perception models. However, operative expert gaze is costly to collect, and it remains unclear how the source of gaze supervision, both expertise level (intermediate vs. novice) and perceptual modality (active execution vs. passive viewing), shapes what attention models learn. We introduce a paired active-passive, multi-task surgical gaze dataset collected on the da Vinci SimNow simulator across four drills. Active gaze was recorded during task execution using a VR headset with eye tracking, and the corresponding videos were reused as stimuli to collect passive gaze from observers, enabling controlled same-video comparisons. We quantify skill- and modality-dependent differences in gaze organization and evaluate the substitutability of passive gaze for operative supervision using fixation density overlap analyses and single-frame saliency modeling. Across settings, MSI-Net produced stable, interpretable predictions, whereas SalGAN was unstable and often poorly aligned with human fixations. Models trained on passive gaze recovered a substantial portion of intermediate active attention, but with predictable degradation, and transfer was asymmetric between active and passive targets. Notably, novice passive labels approximated intermediate-passive targets with limited loss on higher-quality demonstrations, suggesting a practical path for scalable, crowd-sourced gaze supervision in surgical coaching and perception modeling.

</details>

<details><summary><b>Chinese Abstract</b></summary>

In robot-assisted minimally invasive surgery (RMIS), reduced haptic feedback and depth cues increase reliance on expert visual perception, motivating gaze-guided training and learning-based surgical perception models. However, operative expert gaze is costly to collect, and it remains unclear how the source of gaze supervision, both expertise level (intermediate vs. novice) and perceptual modality (active execution vs.passive viewing), shapes what attention models learn. We introduce a paired active-passive, multi-task surgical gaze dataset collected on the da Vinci SimNow simulator across four drills. Active gaze was recorded during task execution using a VR headset with eye tracking, and the corresponding videos were reused as stimuli to collect passive gaze from observers, enabling controlled same-video comparisons.We quantify skill- and modality-dependent differences in gaze organization and evaluate the substitutability of passive gaze for operative supervision using fixation density overlap analyses and single-frame saliency modeling. Across settings, MSI-Net produced stable, interpretable predictions, whereas SalGAN was unstable and often poorly aligned with human fixations.Models trained on passive gaze recovered a substantial portion of intermediate active attention, but with predictable degradation, and transfer was asymmetric between active and passive targets. Notably, novice passive labels approximated intermediate-passive targets with limited loss on higher-quality demonstrations, suggesting a practical path for scalable, crowd-sourced gaze supervision in surgical coaching and perception modeling.

</details>

---

## 267. From Legible to Inscrutable Trajectories: (Il)legible Motion Planning Accounting for Multiple Observers

**Chinese Title**: From Legible to Inscrutable Trajectories: (Il)legible Motion Planning Accounting for Multiple Observers

**Authors**: Ananya Yammanuru, Maria Lusardi, Nancy M. Amato, Katherine Driggs-Campbell

**Date**: 2026-02-09 | **arXiv**: [2602.09227v1](http://arxiv.org/abs/2602.09227v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09227v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

In cooperative environments, such as in factories or assistive scenarios, it is important for a robot to communicate its intentions to observers, who could be either other humans or robots. A legible trajectory allows an observer to quickly and accurately predict an agent's intention. In adversarial environments, such as in military operations or games, it is important for a robot to not communicate its intentions to observers. An illegible trajectory leads an observer to incorrectly predict the agent's intention or delays when an observer is able to make a correct prediction about the agent's intention. However, in some environments there are multiple observers, each of whom may be able to see only part of the environment, and each of whom may have different motives. In this work, we introduce the Mixed-Motive Limited-Observability Legible Motion Planning (MMLO-LMP) problem, which requires a motion planner to generate a trajectory that is legible to observers with positive motives and illegible to observers with negative motives while also considering the visibility limitations of each observer. We highlight multiple strategies an agent can take while still achieving the problem objective. We also present DUBIOUS, a trajectory optimizer that solves MMLO-LMP. Our results show that DUBIOUS can generate trajectories that balance legibility with the motives and limited visibility regions of the observers. Future work includes many variations of MMLO-LMP, including moving observers and observer teaming.

</details>

<details><summary><b>Chinese Abstract</b></summary>

In cooperative environments, such as in factories or assistive scenarios, it is important for a robot to communicate its intentions to observers, who could be either other humans or robots. A legible trajectory allows an observer to quickly and accurately predict an agent's intention. In adversarial environments, such as in military operations or games, it is important for a robot to not communicate its intentions to observers.An illegible trajectory leads an observer to incorrectly predict the agent's intention or delays when an observer is able to make a correct prediction about the agent's intention. However, in some environments there are multiple observers, each of whom may be able to see only part of the environment, and each of whom may have different motives.In this work, we introduce the Mixed-Motive Limited-Observability Legible Motion Planning (MMLO-LMP) problem, which requires a motion planner to generate a trajectory that is legible to observers with positive motives and illegible to observers with negative motives while also considering the visibility limitations of each observer. We highlight multiple strategies an agent can take while still achieving the problem objective.We also present DUBIOUS, a trajectory optimizer that solves MMLO-LMP. Our results show that DUBIOUS can generate trajectories that balance legibility with the motives and limited visibility regions of the observers. Future work includes many variations of MMLO-LMP, including moving observers and observer teaming.

</details>

---

## 268. Risk-Aware Obstacle Avoidance Algorithm for Real-Time Applications

**Chinese Title**: Risk-Aware Obstacle Avoidance Algorithm for Real-Time Applications

**Authors**: Ozan Kaya, Emir Cem Gezer, Roger Skjetne, Ingrid Bouwer Utne

**Date**: 2026-02-09 | **arXiv**: [2602.09204v1](http://arxiv.org/abs/2602.09204v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09204v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Robust navigation in changing marine environments requires autonomous systems capable of perceiving, reasoning, and acting under uncertainty. This study introduces a hybrid risk-aware navigation architecture that integrates probabilistic modeling of obstacles along the vehicle path with smooth trajectory optimization for autonomous surface vessels. The system constructs probabilistic risk maps that capture both obstacle proximity and the behavior of dynamic objects. A risk-biased Rapidly Exploring Random Tree (RRT) planner leverages these maps to generate collision-free paths, which are subsequently refined using B-spline algorithms to ensure trajectory continuity. Three distinct RRT* rewiring modes are implemented based on the cost function: minimizing the path length, minimizing risk, and optimizing a combination of the path length and total risk. The framework is evaluated in experimental scenarios containing both static and dynamic obstacles. The results demonstrate the system's ability to navigate safely, maintain smooth trajectories, and dynamically adapt to changing environmental risks. Compared with conventional LIDAR or vision-only navigation approaches, the proposed method shows improvements in operational safety and autonomy, establishing it as a promising solution for risk-aware autonomous vehicle missions in uncertain and dynamic environments.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Robust navigation in changing marine environments requires autonomous systems capable of perceiving, reasoning, and acting under uncertainty. This study introduces a hybrid risk-aware navigation architecture that integrates probabilistic modeling of obstacles along the vehicle path with smooth trajectory optimization for autonomous surface vessels.The system constructs probabilistic risk maps that capture both obstacle proximity and the behavior of dynamic objects. A risk-biased Rapidly Exploring Random Tree (RRT) planner leverages these maps to generate collision-free paths, which are subsequently refined using B-spline algorithms to ensure trajectory continuity.Three distinct RRT* rewiring modes are implemented based on the cost function: minimizing the path length, minimizing risk, and optimizing a combination of the path length and total risk. The framework is evaluated in experimental scenarios containing both static and dynamic obstacles. The results demonstrate the system's ability to navigate safely, maintain smooth trajectories, and dynamically adapt to changing environmental risks.Compared with conventional LIDAR or vision-only navigation approaches, the proposed method shows improvements in operational safety and autonomy, establishing it as a promising solution for risk-aware autonomous vehicle missions in uncertain and dynamic environments.

</details>

---

## 269. Elements of Robot Morphology: Supporting Designers in Robot Form Exploration

**Chinese Title**: Elements of Robot Morphology: Supporting Designers in Robot Form Exploration

**Authors**: Amy Koike,  Ge,  Guo, Xinning He, Callie Y. Kim et al.

**Date**: 2026-02-09 | **arXiv**: [2602.09203v1](http://arxiv.org/abs/2602.09203v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09203v1)

**Categories**: cs.RO, cs.HC

<details><summary><b>Abstract</b></summary>

Robot morphology, the form, shape, and structure of robots, is a key design space in human-robot interaction (HRI), shaping how robots function, express themselves, and interact with people. Yet, despite its importance, little is known about how design frameworks can guide systematic form exploration. To address this gap, we introduce Elements of Robot Morphology, a framework that identifies five fundamental elements: perception, articulation, end effectors, locomotion, and structure. Derived from an analysis of existing robots, the framework supports structured exploration of diverse robot forms. To operationalize the framework, we developed Morphology Exploration Blocks (MEB), a set of tangible blocks that enable hands-on, collaborative experimentation with robot morphologies. We evaluate the framework and toolkit through a case study and design workshops, showing how they support analysis, ideation, reflection, and collaborative robot design.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Robot morphology, the form, shape, and structure of robots, is a key design space in human-robot interaction (HRI), shaping how robots function, express themselves, and interact with people. Yet, despite its importance, little is known about how design frameworks can guide systematic form exploration.To address this gap, we introduce Elements of Robot Morphology, a framework that identifies five fundamental elements: perception, articulation, end effectors, locomotion, and structure. Derived from an analysis of existing robots, the framework supports structured exploration of diverse robot forms.To operationalize the framework, we developed Morphology Exploration Blocks (MEB), a set of tangible blocks that enable hands-on, collaborative experimentation with robot morphologies. We evaluate the framework and toolkit through a case study and design workshops, showing how they support analysis, ideation, reflection, and collaborative robot design.

</details>

---

## 270. Agile asymmetric multi-legged locomotion: contact planning via geometric mechanics and spin model duality

**Chinese Title**: Agile asymmetric multi-legged locomotion: contact planning via geometric mechanics and spin model duality

**Authors**: Jackson Habala, Gabriel B. Margolis, Tianyu Wang, Pratyush Bhatt, Juntao He et al.

**Date**: 2026-02-09 | **arXiv**: [2602.09123v1](http://arxiv.org/abs/2602.09123v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09123v1)

**Categories**: cs.RO, eess.SY

<details><summary><b>Abstract</b></summary>

Legged robot research is presently focused on bipedal or quadrupedal robots, despite capabilities to build robots with many more legs to potentially improve locomotion performance. This imbalance is not necessarily due to hardware limitations, but rather to the absence of principled control frameworks that explain when and how additional legs improve locomotion performance. In multi-legged systems, coordinating many simultaneous contacts introduces a severe curse of dimensionality that challenges existing modeling and control approaches. As an alternative, multi-legged robots are typically controlled using low-dimensional gaits originally developed for bipeds or quadrupeds. These strategies fail to exploit the new symmetries and control opportunities that emerge in higher-dimensional systems. In this work, we develop a principled framework for discovering new control structures in multi-legged locomotion. We use geometric mechanics to reduce contact-rich locomotion planning to a graph optimization problem, and propose a spin model duality framework from statistical mechanics to exploit symmetry breaking and guide optimal gait reorganization. Using this approach, we identify an asymmetric locomotion strategy for a hexapod robot that achieves a forward speed of 0.61 body lengths per cycle (a 50% improvement over conventional gaits). The resulting asymmetry appears at both the control and hardware levels. At the control level, the body orientation oscillates asymmetrically between fast clockwise and slow counterclockwise turning phases for forward locomotion. At the hardware level, two legs on the same side remain unactuated and can be replaced with rigid parts without degrading performance. Numerical simulations and robophysical experiments validate the framework and reveal novel locomotion behaviors that emerge from symmetry reforming in high-dimensional embodied systems.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Legged robot research is presently focused on bipedal or quadrupedal robots, despite capabilities to build robots with many more legs to potentially improve locomotion performance. This imbalance is not necessarily due to hardware limitations, but rather to the absence of principled control frameworks that explain when and how additional legs improve locomotion performance.In multi-legged systems, coordinating many simultaneous contacts introduces a severe curse of dimensionality that challenges existing modeling and control approaches. As an alternative, multi-legged robots are typically controlled using low-dimensional gaits originally developed for bipeds or quadrupeds. These strategies fail to exploit the new symmetries and control opportunities that emerge in higher-dimensional systems.In this work, we develop a principled framework for discovering new control structures in multi-legged locomotion. We use geometric mechanics to reduce contact-rich locomotion planning to a graph optimization problem, and propose a spin model duality framework from statistical mechanics to exploit symmetry breaking and guide optimal gait reorganization.Using this approach, we identify an asymmetric locomotion strategy for a hexapod robot that achieves a forward speed of 0.61 body lengths per cycle (a 50% improvement over conventional gaits). The resulting asymmetry appears at both the control and hardware levels. At the control level, the body orientation oscillates asymmetrically between fast clockwise and slow counterclockwise turning phases for forward locomotion.At the hardware level, two legs on the same side remain unactuated and can be replaced with rigid parts without degrading performance. Numerical simulations and robophysical experiments validate the framework and reveal novel locomotion behaviors that emerge from symmetry reforming in high-dimensional embodied systems.

</details>

---

## 271. TwinRL-VLA: Digital Twin-Driven Reinforcement Learning for Real-World Robotic Manipulation

**Chinese Title**: TwinRL-VLA: Digital Twin-Driven Reinforcement Learning for Real-World Robotic Manipulation

**Authors**: Qinwen Xu, Jiaming Liu, Rui Zhou, Shaojun Shi, Nuowei Han et al.

**Date**: 2026-02-09 | **arXiv**: [2602.09023v1](http://arxiv.org/abs/2602.09023v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09023v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Despite strong generalization capabilities, Vision-Language-Action (VLA) models remain constrained by the high cost of expert demonstrations and insufficient real-world interaction. While online reinforcement learning (RL) has shown promise in improving general foundation models, applying RL to VLA manipulation in real-world settings is still hindered by low exploration efficiency and a restricted exploration space. Through systematic real-world experiments, we observe that the effective exploration space of online RL is closely tied to the data distribution of supervised fine-tuning (SFT). Motivated by this observation, we propose TwinRL, a digital twin-real-world collaborative RL framework designed to scale and guide exploration for VLA models. First, a high-fidelity digital twin is efficiently reconstructed from smartphone-captured scenes, enabling realistic bidirectional transfer between real and simulated environments. During the SFT warm-up stage, we introduce an exploration space expansion strategy using digital twins to broaden the support of the data trajectory distribution. Building on this enhanced initialization, we propose a sim-to-real guided exploration strategy to further accelerate online RL. Specifically, TwinRL performs efficient and parallel online RL in the digital twin prior to deployment, effectively bridging the gap between offline and online training stages. Subsequently, we exploit efficient digital twin sampling to identify failure-prone yet informative configurations, which are used to guide targeted human-in-the-loop rollouts on the real robot. In our experiments, TwinRL approaches 100% success in both in-distribution regions covered by real-world demonstrations and out-of-distribution regions, delivering at least a 30% speedup over prior real-world RL methods and requiring only about 20 minutes on average across four tasks.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Despite strong generalization capabilities, Vision-Language-Action (VLA) models remain constrained by the high cost of expert demonstrations and insufficient real-world interaction. While online reinforcement learning (RL) has shown promise in improving general foundation models, applying RL to VLA manipulation in real-world settings is still hindered by low exploration efficiency and a restricted exploration space.Through systematic real-world experiments, we observe that the effective exploration space of online RL is closely tied to the data distribution of supervised fine-tuning (SFT). Motivated by this observation, we propose TwinRL, a digital twin-real-world collaborative RL framework designed to scale and guide exploration for VLA models.First, a high-fidelity digital twin is efficiently reconstructed from smartphone-captured scenes, enabling realistic bidirectional transfer between real and simulated environments. During the SFT warm-up stage, we introduce an exploration space expansion strategy using digital twins to broaden the support of the data trajectory distribution.Building on this enhanced initialization, we propose a sim-to-real guided exploration strategy to further accelerate online RL. Specifically, TwinRL performs efficient and parallel online RL in the digital twin prior to deployment, effectively bridging the gap between offline and online training stages.Subsequently, we exploit efficient digital twin sampling to identify failure-prone yet informative configurations, which are used to guide targeted human-in-the-loop rollouts on the real robot.In our experiments, TwinRL approaches 100% success in both in-distribution regions covered by real-world demonstrations and out-of-distribution regions, delivering at least a 30% speedup over prior real-world RL methods and requiring only about 20 minutes on average across four tasks.

</details>

---

## 272. CLUE: Crossmodal disambiguation via Language-vision Understanding with attEntion

**Chinese Title**: CLUE: Crossmodal disambiguation via Language-vision Understanding with attEntion

**Authors**: Mouad Abrini, Mohamed Chetouani

**Date**: 2026-02-09 | **arXiv**: [2602.08999v1](http://arxiv.org/abs/2602.08999v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08999v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

With the increasing integration of robots into daily life, human-robot interaction has become more complex and multifaceted. A critical component of this interaction is Interactive Visual Grounding (IVG), through which robots must interpret human intentions and resolve ambiguity. Existing IVG models generally lack a mechanism to determine when to ask clarification questions, as they implicitly rely on their learned representations. CLUE addresses this gap by converting the VLM's cross-modal attention into an explicit, spatially grounded signal for deciding when to ask. We extract text to image attention maps and pass them to a lightweight CNN to detect referential ambiguity, while a LoRA fine-tuned decoder conducts the dialog and emits grounding location tokens. We train on a real-world interactive dataset for IVG, and a mixed ambiguity set for the detector. With InViG-only supervision, our model surpasses a state-of-the-art method while using parameter-efficient fine-tuning. Similarly, the ambiguity detector outperforms prior baselines. Overall, CLUE turns the internal cross-modal attention of a VLM into an explicit, spatially grounded signal for deciding when to ask. The data and code are publicly available at: mouadabrini.github.io/clue

</details>

<details><summary><b>Chinese Abstract</b></summary>

With the increasing integration of robots into daily life, human-robot interaction has become more complex and multifaceted. A critical component of this interaction is Interactive Visual Grounding (IVG), through which robots must interpret human intentions and resolve ambiguity. Existing IVG models generally lack a mechanism to determine when to ask clarification questions, as they implicitly rely on their learned representations.CLUE addresses this gap by converting the VLM's cross-modal attention into an explicit, spatially grounded signal for deciding when to ask. We extract text to image attention maps and pass them to a lightweight CNN to detect referential ambiguity, while a LoRA fine-tuned decoder conducts the dialog and emits grounding location tokens. We train on a real-world interactive dataset for IVG, and a mixed ambiguity set for the detector.With InViG-only supervision, our model surpasses a state-of-the-art method while using parameter-efficient fine-tuning. Similarly, the ambiguity detector outperforms prior baselines. Overall, CLUE turns the internal cross-modal attention of a VLM into an explicit, spatially grounded signal for deciding when to ask. The data and code are publicly available at: mouadabrini.github.io/clue

</details>

---

## 273. Legs Over Arms: On the Predictive Value of Lower-Body Pose for Human Trajectory Prediction from Egocentric Robot Perception

**Chinese Title**: Legs Over Arms: On the Predictive Value of Lower-Body Pose for Human Trajectory Prediction from Egocentric Robot Perception

**Authors**: Nhat Le, Daeun Song, Xuesu Xiao

**Date**: 2026-02-09 | **arXiv**: [2602.09076v1](http://arxiv.org/abs/2602.09076v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09076v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Predicting human trajectory is crucial for social robot navigation in crowded environments. While most existing approaches treat human as point mass, we present a study on multi-agent trajectory prediction that leverages different human skeletal features for improved forecast accuracy. In particular, we systematically evaluate the predictive utility of 2D and 3D skeletal keypoints and derived biomechanical cues as additional inputs. Through a comprehensive study on the JRDB dataset and another new dataset for social navigation with 360-degree panoramic videos, we find that focusing on lower-body 3D keypoints yields a 13% reduction in Average Displacement Error and augmenting 3D keypoint inputs with corresponding biomechanical cues provides a further 1-4% improvement. Notably, the performance gain persists when using 2D keypoint inputs extracted from equirectangular panoramic images, indicating that monocular surround vision can capture informative cues for motion forecasting. Our finding that robots can forecast human movement efficiently by watching their legs provides actionable insights for designing sensing capabilities for social robot navigation.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Predicting human trajectory is crucial for social robot navigation in crowded environments. While most existing approaches treat human as point mass, we present a study on multi-agent trajectory prediction that leverages different human skeletal features for improved forecast accuracy. In particular, we systematically evaluate the predictive utility of 2D and 3D skeletal keypoints and derived biomechanical cues as additional inputs.Through a comprehensive study on the JRDB dataset and another new dataset for social navigation with 360-degree panoramic videos, we find that focusing on lower-body 3D keypoints yields a 13% reduction in Average Displacement Error and augmenting 3D keypoint inputs with corresponding biomechanical cues provides a further 1-4% improvement.Notably, the performance gain persists when using 2D keypoint inputs extracted from equirectangular panoramic images, indicating that monocular surround vision can capture informative cues for motion forecasting. Our finding that robots can forecast human movement efficiently by watching their legs provides actionable insights for designing sensing capabilities for social robot navigation.

</details>

---

## 274. Multi-Staged Framework for Safety Analysis of Offloaded Services in Distributed Intelligent Transportation Systems

**Chinese Title**: Multi-Staged Framework for Safety Analysis of Offloaded Services in Distributed Intelligent Transportation Systems

**Authors**: Robin Dehler, Oliver Schumann, Jona Ruof, Michael Buchholz

**Date**: 2026-02-09 | **arXiv**: [2602.08821v1](http://arxiv.org/abs/2602.08821v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08821v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

The integration of service-oriented architectures (SOA) with function offloading for distributed, intelligent transportation systems (ITS) offers the opportunity for connected autonomous vehicles (CAVs) to extend their locally available services. One major goal of offloading a subset of functions in the processing chain of a CAV to remote devices is to reduce the overall computational complexity on the CAV. The extension of using remote services, however, requires careful safety analysis, since the remotely created data are corrupted more easily, e.g., through an attacker on the remote device or by intercepting the wireless transmission. To tackle this problem, we first analyze the concept of SOA for distributed environments. From this, we derive a safety framework that validates the reliability of remote services and the data received locally. Since it is possible for the autonomous driving task to offload multiple different services, we propose a specific multi-staged framework for safety analysis dependent on the service composition of local and remote services. For efficiency reasons, we directly include the multi-staged framework for safety analysis in our service-oriented function offloading framework (SOFOF) that we have proposed in earlier work. The evaluation compares the performance of the extended framework considering computational complexity, with energy savings being a major motivation for function offloading, and its capability to detect data from corrupted remote services.

</details>

<details><summary><b>Chinese Abstract</b></summary>

The integration of service-oriented architectures (SOA) with function offloading for distributed, intelligent transportation systems (ITS) offers the opportunity for connected autonomous vehicles (CAVs) to extend their locally available services. One major goal of offloading a subset of functions in the processing chain of a CAV to remote devices is to reduce the overall computational complexity on the CAV.The extension of using remote services, however, requires careful safety analysis, since the remotely created data are corrupted more easily, e.g., through an attacker on the remote device or by intercepting the wireless transmission. To tackle this problem, we first analyze the concept of SOA for distributed environments. From this, we derive a safety framework that validates the reliability of remote services and the data received locally.Since it is possible for the autonomous driving task to offload multiple different services, we propose a specific multi-staged framework for safety analysis dependent on the service composition of local and remote services. For efficiency reasons, we directly include the multi-staged framework for safety analysis in our service-oriented function offloading framework (SOFOF) that we have proposed in earlier work.The evaluation compares the performance of the extended framework considering computational complexity, with energy savings being a major motivation for function offloading, and its capability to detect data from corrupted remote services.

</details>

---

## 275. A Generic Service-Oriented Function Offloading Framework for Connected Automated Vehicles

**Chinese Title**: A Generic Service-Oriented Function Offloading Framework for Connected Automated Vehicles

**Authors**: Robin Dehler, Michael Buchholz

**Date**: 2026-02-09 | **arXiv**: [2602.08799v1](http://arxiv.org/abs/2602.08799v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08799v1)

**Categories**: cs.RO, cs.MA

<details><summary><b>Abstract</b></summary>

Function offloading is a promising solution to address limitations concerning computational capacity and available energy of Connected Automated Vehicles~(CAVs) or other autonomous robots by distributing computational tasks between local and remote computing devices in form of distributed services. This paper presents a generic function offloading framework that can be used to offload an arbitrary set of computational tasks with a focus on autonomous driving. To provide flexibility, the function offloading framework is designed to incorporate different offloading decision making algorithms and quality of service~(QoS) requirements that can be adjusted to different scenarios or the objectives of the CAVs. With a focus on the applicability, we propose an efficient location-based approach, where the decision whether tasks are processed locally or remotely depends on the location of the CAV. We apply the proposed framework on the use case of service-oriented trajectory planning, where we offload the trajectory planning task of CAVs to a Multi-Access Edge Computing~(MEC) server. The evaluation is conducted in both simulation and real-world application. It demonstrates the potential of the function offloading framework to guarantee the QoS for trajectory planning while improving the computational efficiency of the CAVs. Moreover, the simulation results also show the adaptability of the framework to diverse scenarios involving simultaneous offloading requests from multiple CAVs.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Function offloading is a promising solution to address limitations concerning computational capacity and available energy of Connected Automated Vehicles~(CAVs) or other autonomous robots by distributing computational tasks between local and remote computing devices in form of distributed services.This paper presents a generic function offloading framework that can be used to offload an arbitrary set of computational tasks with a focus on autonomous driving. To provide flexibility, the function offloading framework is designed to incorporate different offloading decision making algorithms and quality of service~(QoS) requirements that can be adjusted to different scenarios or the objectives of the CAVs.With a focus on the applicability, we propose an efficient location-based approach, where the decision whether tasks are processed locally or remotely depends on the location of the CAV. We apply the proposed framework on the use case of service-oriented trajectory planning, where we offload the trajectory planning task of CAVs to a Multi-Access Edge Computing~(MEC) server.The evaluation is conducted in both simulation and real-world application. It demonstrates the potential of the function offloading framework to guarantee the QoS for trajectory planning while improving the computational efficiency of the CAVs. Moreover, the simulation results also show the adaptability of the framework to diverse scenarios involving simultaneous offloading requests from multiple CAVs.

</details>

---

## 276. GaussianCaR: Gaussian Splatting for Efficient Camera-Radar Fusion

**Chinese Title**: GaussianCaR: Gaussian Splatting for Efficient Camera-Radar Fusion

**Authors**: Santiago Montiel-MarÃ­n, Miguel Antunes-GarcÃ­a, Fabio SÃ¡nchez-GarcÃ­a, Angel Llamazares, Holger Caesar et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08784v1](http://arxiv.org/abs/2602.08784v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08784v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Robust and accurate perception of dynamic objects and map elements is crucial for autonomous vehicles performing safe navigation in complex traffic scenarios. While vision-only methods have become the de facto standard due to their technical advances, they can benefit from effective and cost-efficient fusion with radar measurements. In this work, we advance fusion methods by repurposing Gaussian Splatting as an efficient universal view transformer that bridges the view disparity gap, mapping both image pixels and radar points into a common Bird's-Eye View (BEV) representation. Our main contribution is GaussianCaR, an end-to-end network for BEV segmentation that, unlike prior BEV fusion methods, leverages Gaussian Splatting to map raw sensor information into latent features for efficient camera-radar fusion. Our architecture combines multi-scale fusion with a transformer decoder to efficiently extract BEV features. Experimental results demonstrate that our approach achieves performance on par with, or even surpassing, the state of the art on BEV segmentation tasks (57.3%, 82.9%, and 50.1% IoU for vehicles, roads, and lane dividers) on the nuScenes dataset, while maintaining a 3.2x faster inference runtime. Code and project page are available online.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Robust and accurate perception of dynamic objects and map elements is crucial for autonomous vehicles performing safe navigation in complex traffic scenarios. While vision-only methods have become the de facto standard due to their technical advances, they can benefit from effective and cost-efficient fusion with radar measurements.In this work, we advance fusion methods by repurposing Gaussian Splatting as an efficient universal view transformer that bridges the view disparity gap, mapping both image pixels and radar points into a common Bird's-Eye View (BEV) representation.Our main contribution is GaussianCaR, an end-to-end network for BEV segmentation that, unlike prior BEV fusion methods, leverages Gaussian Splatting to map raw sensor information into latent features for efficient camera-radar fusion. Our architecture combines multi-scale fusion with a transformer decoder to efficiently extract BEV features.Experimental results demonstrate that our approach achieves performance on par with, or even surpassing, the state of the art on BEV segmentation tasks (57.3%, 82.9%, and 50.1% IoU for vehicles, roads, and lane dividers) on the nuScenes dataset, while maintaining a 3.2x faster inference runtime. Code and project page are available online.

</details>

---

## 277. Mind the Gap: Learning Implicit Impedance in Visuomotor Policies via Intent-Execution Mismatch

**Chinese Title**: Mind the Gap: Learning Implicit Impedance in Visuomotor Policies via Intent-Execution Mismatch

**Authors**: Cuijie Xu, Shurui Zheng, Zihao Su, Yuanfan Xu, Tinghao Yi et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08776v1](http://arxiv.org/abs/2602.08776v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08776v1)

**Project**: https://xucj98.github.io/mind-the-gap-page/  **Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Teleoperation inherently relies on the human operator acting as a closed-loop controller to actively compensate for hardware imperfections, including latency, mechanical friction, and lack of explicit force feedback. Standard Behavior Cloning (BC), by mimicking the robot's executed trajectory, fundamentally ignores this compensatory mechanism. In this work, we propose a Dual-State Conditioning framework that shifts the learning objective to "Intent Cloning" (master command). We posit that the Intent-Execution Mismatch, the discrepancy between master command and slave response, is not noise, but a critical signal that physically encodes implicit interaction forces and algorithmically reveals the operator's strategy for overcoming system dynamics. By predicting the master intent, our policy learns to generate a "virtual equilibrium point", effectively realizing implicit impedance control. Furthermore, by explicitly conditioning on the history of this mismatch, the model performs implicit system identification, perceiving tracking errors as external forces to close the control loop. To bridge the temporal gap caused by inference latency, we further formulate the policy as a trajectory inpainter to ensure continuous control. We validate our approach on a sensorless, low-cost bi-manual setup. Empirical results across tasks requiring contact-rich manipulation and dynamic tracking reveal a decisive gap: while standard execution-cloning fails due to the inability to overcome contact stiffness and tracking lag, our mismatch-aware approach achieves robust success. This presents a minimalist behavior cloning framework for low-cost hardware, enabling force perception and dynamic compensation without relying on explicit force sensing. Videos are available on the \href{https://xucj98.github.io/mind-the-gap-page/}{project page}.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Teleoperation inherently relies on the human operator acting as a closed-loop controller to actively compensate for hardware imperfections, including latency, mechanical friction, and lack of explicit force feedback. Standard Behavior Cloning (BC), by mimicking the robot's executed trajectory, fundamentally ignores this compensatory mechanism.In this work, we propose a Dual-State Conditioning framework that shifts the learning objective to "Intent Cloning" (master command). We posit that the Intent-Execution Mismatch, the discrepancy between master command and slave response, is not noise, but a critical signal that physically encodes implicit interaction forces and algorithmically reveals the operator's strategy for overcoming system dynamics.By predicting the master intent, our policy learns to generate a "virtual equilibrium point", effectively realizing implicit impedance control. Furthermore, by explicitly conditioning on the history of this mismatch, the model performs implicit system identification, perceiving tracking errors as external forces to close the control loop.To bridge the temporal gap caused by inference latency, we further formulate the policy as a trajectory inpainter to ensure continuous control. We validate our approach on a sensorless, low-cost bi-manual setup.Empirical results across tasks requiring contact-rich manipulation and dynamic tracking reveal a decisive gap: while standard execution-cloning fails due to the inability to overcome contact stiffness and tracking lag, our mismatch-aware approach achieves robust success. This presents a minimalist behavior cloning framework for low-cost hardware, enabling force perception and dynamic compensation without relying on explicit force sensing.Videos are available on the \href{https://xucj98.github.io/mind-the-gap-page/}{project page}.

</details>

---

## 278. High-Speed Vision-Based Flight in Clutter with Safety-Shielded Reinforcement Learning

**Chinese Title**: High-Speed Vision-Based Flight in Clutter with Safety-Shielded Reinforcement Learning

**Authors**: Jiarui Zhang, Chengyong Lei, Chengjiang Dai, Lijie Wang, Zhichao Han et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08653v1](http://arxiv.org/abs/2602.08653v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08653v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Quadrotor unmanned aerial vehicles (UAVs) are increasingly deployed in complex missions that demand reliable autonomous navigation and robust obstacle avoidance. However, traditional modular pipelines often incur cumulative latency, whereas purely reinforcement learning (RL) approaches typically provide limited formal safety guarantees. To bridge this gap, we propose an end-to-end RL framework augmented with model-based safety mechanisms. We incorporate physical priors in both training and deployment. During training, we design a physics-informed reward structure that provides global navigational guidance. During deployment, we integrate a real-time safety filter that projects the policy outputs onto a provably safe set to enforce strict collision-avoidance constraints. This hybrid architecture reconciles high-speed flight with robust safety assurances. Benchmark evaluations demonstrate that our method outperforms both traditional planners and recent end-to-end obstacle avoidance approaches based on differentiable physics. Extensive experiments demonstrate strong generalization, enabling reliable high-speed navigation in dense clutter and challenging outdoor forest environments at velocities up to 7.5m/s.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Quadrotor unmanned aerial vehicles (UAVs) are increasingly deployed in complex missions that demand reliable autonomous navigation and robust obstacle avoidance. However, traditional modular pipelines often incur cumulative latency, whereas purely reinforcement learning (RL) approaches typically provide limited formal safety guarantees. To bridge this gap, we propose an end-to-end RL framework augmented with model-based safety mechanisms.We incorporate physical priors in both training and deployment. During training, we design a physics-informed reward structure that provides global navigational guidance. During deployment, we integrate a real-time safety filter that projects the policy outputs onto a provably safe set to enforce strict collision-avoidance constraints. This hybrid architecture reconciles high-speed flight with robust safety assurances.Benchmark evaluations demonstrate that our method outperforms both traditional planners and recent end-to-end obstacle avoidance approaches based on differentiable physics. Extensive experiments demonstrate strong generalization, enabling reliable high-speed navigation in dense clutter and challenging outdoor forest environments at velocities up to 7.5m/s.

</details>

---

## 279. Mimic Intent, Not Just Trajectories

**Chinese Title**: Mimic Intent, Not Just Trajectories

**Authors**: Renming Huang, Chendong Zeng, Wenjing Tang, Jingtian Cai, Cewu Lu et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08602v1](http://arxiv.org/abs/2602.08602v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08602v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

While imitation learning (IL) has achieved impressive success in dexterous manipulation through generative modeling and pretraining, state-of-the-art approaches like Vision-Language-Action (VLA) models still struggle with adaptation to environmental changes and skill transfer. We argue this stems from mimicking raw trajectories without understanding the underlying intent. To address this, we propose explicitly disentangling behavior intent from execution details in end-2-end IL: \textit{``Mimic Intent, Not just Trajectories'' (MINT)}. We achieve this via \textit{multi-scale frequency-space tokenization}, which enforces a spectral decomposition of action chunk representation. We learn action tokens with a multi-scale coarse-to-fine structure, and force the coarsest token to capture low-frequency global structure and finer tokens to encode high-frequency details. This yields an abstract \textit{Intent token} that facilitates planning and transfer, and multi-scale \textit{Execution tokens} that enable precise adaptation to environmental dynamics. Building on this hierarchy, our policy generates trajectories through \textit{next-scale autoregression}, performing progressive \textit{intent-to-execution reasoning}, thus boosting learning efficiency and generalization. Crucially, this disentanglement enables \textit{one-shot transfer} of skills, by simply injecting the Intent token from a demonstration into the autoregressive generation process. Experiments on several manipulation benchmarks and on a real robot demonstrate state-of-the-art success rates, superior inference efficiency, robust generalization against disturbances, and effective one-shot transfer.

</details>

<details><summary><b>Chinese Abstract</b></summary>

While imitation learning (IL) has achieved impressive success in dexterous manipulation through generative modeling and pretraining, state-of-the-art approaches like Vision-Language-Action (VLA) models still struggle with adaptation to environmental changes and skill transfer. We argue this stems from mimicking raw trajectories without understanding the underlying intent.To address this, we propose explicitly disentangling behavior intent from execution details in end-2-end IL: \textit{``Mimic Intent, Not just Trajectories'' (MINT)}. We achieve this via \textit{multi-scale frequency-space tokenization}, which enforces a spectral decomposition of action chunk representation.We learn action tokens with a multi-scale coarse-to-fine structure, and force the coarsest token to capture low-frequency global structure and finer tokens to encode high-frequency details. This yields an abstract \textit{Intent token} that facilitates planning and transfer, and multi-scale \textit{Execution tokens} that enable precise adaptation to environmental dynamics.Building on this hierarchy, our policy generates trajectories through \textit{next-scale autoregression}, performing progressive \textit{intent-to-execution reasoning}, thus boosting learning efficiency and generalization. Crucially, this disentanglement enables \textit{one-shot transfer} of skills, by simply injecting the Intent token from a demonstration into the autoregressive generation process.Experiments on several manipulation benchmarks and on a real robot demonstrate state-of-the-art success rates, superior inference efficiency, robust generalization against disturbances, and effective one-shot transfer.

</details>

---

## 280. A Precise Real-Time Force-Aware Grasping System for Robust Aerial Manipulation

**Chinese Title**: A Precise Real-Time Force-Aware Grasping System for Robust Aerial Manipulation

**Authors**: Kenghou Hoi, Yuze Wu, Annan Ding, Junjie Wang, Anke Zhao et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08599v1](http://arxiv.org/abs/2602.08599v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08599v1)

**Project**: https://www.youtube.com/watch?v=mbcZkrJEf1I.  **Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Aerial manipulation requires force-aware capabilities to enable safe and effective grasping and physical interaction. Previous works often rely on heavy, expensive force sensors unsuitable for typical quadrotor platforms, or perform grasping without force feedback, risking damage to fragile objects. To address these limitations, we propose a novel force-aware grasping framework incorporating six low-cost, sensitive skin-like tactile sensors. We introduce a magnetic-based tactile sensing module that provides high-precision three-dimensional force measurements. We eliminate geomagnetic interference through a reference Hall sensor and simplify the calibration process compared to previous work. The proposed framework enables precise force-aware grasping control, allowing safe manipulation of fragile objects and real-time weight measurement of grasped items. The system is validated through comprehensive real-world experiments, including balloon grasping, dynamic load variation tests, and ablation studies, demonstrating its effectiveness in various aerial manipulation scenarios. Our approach achieves fully onboard operation without external motion capture systems, significantly enhancing the practicality of force-sensitive aerial manipulation. The supplementary video is available at: https://www.youtube.com/watch?v=mbcZkrJEf1I.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Aerial manipulation requires force-aware capabilities to enable safe and effective grasping and physical interaction. Previous works often rely on heavy, expensive force sensors unsuitable for typical quadrotor platforms, or perform grasping without force feedback, risking damage to fragile objects. To address these limitations, we propose a novel force-aware grasping framework incorporating six low-cost, sensitive skin-like tactile sensors.We introduce a magnetic-based tactile sensing module that provides high-precision three-dimensional force measurements. We eliminate geomagnetic interference through a reference Hall sensor and simplify the calibration process compared to previous work. The proposed framework enables precise force-aware grasping control, allowing safe manipulation of fragile objects and real-time weight measurement of grasped items.The system is validated through comprehensive real-world experiments, including balloon grasping, dynamic load variation tests, and ablation studies, demonstrating its effectiveness in various aerial manipulation scenarios. Our approach achieves fully onboard operation without external motion capture systems, significantly enhancing the practicality of force-sensitive aerial manipulation.The supplementary video is available at: https://www.youtube.com/watch?v=mbcZkrJEf1I.

</details>

---

## 281. MOSAIC: Bridging the Sim-to-Real Gap in Generalist Humanoid Motion Tracking and Teleoperation with Rapid Residual Adaptation

**Chinese Title**: MOSAIC: Bridging the Sim-to-Real Gap in Generalist Humanoid Motion Tracking and Teleoperation with Rapid Residual Adaptation

**Authors**: Zhenguo Sun, Bo-Sheng Huang, Yibo Peng, Xukun Li, Jingyu Ma et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08594v1](http://arxiv.org/abs/2602.08594v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08594v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Generalist humanoid motion trackers have recently achieved strong simulation metrics by scaling data and training, yet often remain brittle on hardware during sustained teleoperation due to interface- and dynamics-induced errors. We present MOSAIC, an open-source, full-stack system for humanoid motion tracking and whole-body teleoperation across multiple interfaces. MOSAIC first learns a teleoperation-oriented general motion tracker via RL on a multi-source motion bank with adaptive resampling and rewards that emphasize world-frame motion consistency, which is critical for mobile teleoperation. To bridge the sim-to-real interface gap without sacrificing generality, MOSAIC then performs rapid residual adaptation: an interface-specific policy is trained using minimal interface-specific data, and then distilled into the general tracker through an additive residual module, outperforming naive fine-tuning or continual learning. We validate MOSAIC with systematic ablations, out-of-distribution benchmarking, and real-robot experiments demonstrating robust offline motion replay and online long-horizon teleoperation under realistic latency and noise.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Generalist humanoid motion trackers have recently achieved strong simulation metrics by scaling data and training, yet often remain brittle on hardware during sustained teleoperation due to interface- and dynamics-induced errors. We present MOSAIC, an open-source, full-stack system for humanoid motion tracking and whole-body teleoperation across multiple interfaces.MOSAIC first learns a teleoperation-oriented general motion tracker via RL on a multi-source motion bank with adaptive resampling and rewards that emphasize world-frame motion consistency, which is critical for mobile teleoperation.To bridge the sim-to-real interface gap without sacrificing generality, MOSAIC then performs rapid residual adaptation: an interface-specific policy is trained using minimal interface-specific data, and then distilled into the general tracker through an additive residual module, outperforming naive fine-tuning or continual learning.We validate MOSAIC with systematic ablations, out-of-distribution benchmarking, and real-robot experiments demonstrating robust offline motion replay and online long-horizon teleoperation under realistic latency and noise.

</details>

---

## 282. Head-to-Head autonomous racing at the limits of handling in the A2RL challenge

**Chinese Title**: Head-to-Head autonomous racing at the limits of handling in the A2RL challenge

**Authors**: Simon Hoffmann, Simon Sagmeister, Tobias Betz, Joscha Bongard, Sascha BÃ¼ttner et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08571v1](http://arxiv.org/abs/2602.08571v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08571v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Autonomous racing presents a complex challenge involving multi-agent interactions between vehicles operating at the limit of performance and dynamics. As such, it provides a valuable research and testing environment for advancing autonomous driving technology and improving road safety. This article presents the algorithms and deployment strategies developed by the TUM Autonomous Motorsport team for the inaugural Abu Dhabi Autonomous Racing League (A2RL). We showcase how our software emulates human driving behavior, pushing the limits of vehicle handling and multi-vehicle interactions to win the A2RL. Finally, we highlight the key enablers of our success and share our most significant learnings.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Autonomous racing presents a complex challenge involving multi-agent interactions between vehicles operating at the limit of performance and dynamics. As such, it provides a valuable research and testing environment for advancing autonomous driving technology and improving road safety.This article presents the algorithms and deployment strategies developed by the TUM Autonomous Motorsport team for the inaugural Abu Dhabi Autonomous Racing League (A2RL). We showcase how our software emulates human driving behavior, pushing the limits of vehicle handling and multi-vehicle interactions to win the A2RL. Finally, we highlight the key enablers of our success and share our most significant learnings.

</details>

---

## 283. Constrained Sampling to Guide Universal Manipulation RL

**Chinese Title**: Constrained Sampling to Guide Universal Manipulation RL

**Authors**: Marc Toussaint, Cornelius V. Braun, Eckart Cobo-Briesewitz, Sayantan Auddy, Armand Jordana et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08557v1](http://arxiv.org/abs/2602.08557v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08557v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

We consider how model-based solvers can be leveraged to guide training of a universal policy to control from any feasible start state to any feasible goal in a contact-rich manipulation setting. While Reinforcement Learning (RL) has demonstrated its strength in such settings, it may struggle to sufficiently explore and discover complex manipulation strategies, especially in sparse-reward settings. Our approach is based on the idea of a lower-dimensional manifold of feasible, likely-visited states during such manipulation and to guide RL with a sampler from this manifold. We propose Sample-Guided RL, which uses model-based constraint solvers to efficiently sample feasible configurations (satisfying differentiable collision, contact, and force constraints) and leverage them to guide RL for universal (goal-conditioned) manipulation policies. We study using this data directly to bias state visitation, as well as using black-box optimization of open-loop trajectories between random configurations to impose a state bias and optionally add a behavior cloning loss. In a minimalistic double sphere manipulation setting, Sample-Guided RL discovers complex manipulation strategies and achieves high success rates in reaching any statically stable state. In a more challenging panda arm setting, our approach achieves a significant success rate over a near-zero baseline, and demonstrates a breadth of complex whole-body-contact manipulation strategies.

</details>

<details><summary><b>Chinese Abstract</b></summary>

We consider how model-based solvers can be leveraged to guide training of a universal policy to control from any feasible start state to any feasible goal in a contact-rich manipulation setting. While Reinforcement Learning (RL) has demonstrated its strength in such settings, it may struggle to sufficiently explore and discover complex manipulation strategies, especially in sparse-reward settings.Our approach is based on the idea of a lower-dimensional manifold of feasible, likely-visited states during such manipulation and to guide RL with a sampler from this manifold. We propose Sample-Guided RL, which uses model-based constraint solvers to efficiently sample feasible configurations (satisfying differentiable collision, contact, and force constraints) and leverage them to guide RL for universal (goal-conditioned) manipulation policies.We study using this data directly to bias state visitation, as well as using black-box optimization of open-loop trajectories between random configurations to impose a state bias and optionally add a behavior cloning loss. In a minimalistic double sphere manipulation setting, Sample-Guided RL discovers complex manipulation strategies and achieves high success rates in reaching any statically stable state.In a more challenging panda arm setting, our approach achieves a significant success rate over a near-zero baseline, and demonstrates a breadth of complex whole-body-contact manipulation strategies.

</details>

---

## 284. UniPlan: Vision-Language Task Planning for Mobile Manipulation with Unified PDDL Formulation

**Chinese Title**: UniPlan: Vision-Language Task Planning for Mobile Manipulation with Unified PDDL Formulation

**Authors**: Haoming Ye, Yunxiao Xiao, Cewu Lu, Panpan Cai

**Date**: 2026-02-09 | **arXiv**: [2602.08537v1](http://arxiv.org/abs/2602.08537v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08537v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Integration of VLM reasoning with symbolic planning has proven to be a promising approach to real-world robot task planning. Existing work like UniDomain effectively learns symbolic manipulation domains from real-world demonstrations, described in Planning Domain Definition Language (PDDL), and has successfully applied them to real-world tasks. These domains, however, are restricted to tabletop manipulation. We propose UniPlan, a vision-language task planning system for long-horizon mobile-manipulation in large-scale indoor environments, that unifies scene topology, visuals, and robot capabilities into a holistic PDDL representation. UniPlan programmatically extends learned tabletop domains from UniDomain to support navigation, door traversal, and bimanual coordination. It operates on a visual-topological map, comprising navigation landmarks anchored with scene images. Given a language instruction, UniPlan retrieves task-relevant nodes from the map and uses a VLM to ground the anchored image into task-relevant objects and their PDDL states; next, it reconnects these nodes to a compressed, densely-connected topological map, also represented in PDDL, with connectivity and costs derived from the original map; Finally, a mobile-manipulation plan is generated using off-the-shelf PDDL solvers. Evaluated on human-raised tasks in a large-scale map with real-world imagery, UniPlan significantly outperforms VLM and LLM+PDDL planning in success rate, plan quality, and computational efficiency.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Integration of VLM reasoning with symbolic planning has proven to be a promising approach to real-world robot task planning. Existing work like UniDomain effectively learns symbolic manipulation domains from real-world demonstrations, described in Planning Domain Definition Language (PDDL), and has successfully applied them to real-world tasks. These domains, however, are restricted to tabletop manipulation.We propose UniPlan, a vision-language task planning system for long-horizon mobile-manipulation in large-scale indoor environments, that unifies scene topology, visuals, and robot capabilities into a holistic PDDL representation. UniPlan programmatically extends learned tabletop domains from UniDomain to support navigation, door traversal, and bimanual coordination.It operates on a visual-topological map, comprising navigation landmarks anchored with scene images.Given a language instruction, UniPlan retrieves task-relevant nodes from the map and uses a VLM to ground the anchored image into task-relevant objects and their PDDL states; next, it reconnects these nodes to a compressed, densely-connected topological map, also represented in PDDL, with connectivity and costs derived from the original map; Finally, a mobile-manipulation plan is generated using off-the-shelf PDDL solvers.Evaluated on human-raised tasks in a large-scale map with real-world imagery, UniPlan significantly outperforms VLM and LLM+PDDL planning in success rate, plan quality, and computational efficiency.

</details>

---

## 285. UAV-Supported Maritime Search System: Experience from Valun Bay Field Trials

**Chinese Title**: UAV-Supported Maritime Search System: Experience from Valun Bay Field Trials

**Authors**: Stefan IviÄ‡, Luka LanÄa, Karlo Jakac, Ante Sikirica, Stella DumenÄiÄ‡ et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08450v1](http://arxiv.org/abs/2602.08450v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08450v1)

**Categories**: cs.RO, math.OC

<details><summary><b>Abstract</b></summary>

This paper presents the integration of flow field reconstruction, dynamic probabilistic modeling, search control, and machine vision detection in a system for autonomous maritime search operations. Field experiments conducted in Valun Bay (Cres Island, Croatia) involved real-time drifter data acquisition, surrogate flow model fitting based on computational fluid dynamics and numerical optimization, advanced multi-UAV search control and vision sensing, as well as deep learning-based object detection. The results demonstrate that a tightly coupled approach enables reliable detection of floating targets under realistic uncertainties and complex environmental conditions, providing concrete insights for future autonomous maritime search and rescue applications.

</details>

<details><summary><b>Chinese Abstract</b></summary>

This paper presents the integration of flow field reconstruction, dynamic probabilistic modeling, search control, and machine vision detection in a system for autonomous maritime search operations.Field experiments conducted in Valun Bay (Cres Island, Croatia) involved real-time drifter data acquisition, surrogate flow model fitting based on computational fluid dynamics and numerical optimization, advanced multi-UAV search control and vision sensing, as well as deep learning-based object detection.The results demonstrate that a tightly coupled approach enables reliable detection of floating targets under realistic uncertainties and complex environmental conditions, providing concrete insights for future autonomous maritime search and rescue applications.

</details>

---

## 286. Post-Collision Trajectory Restoration for a Single-track Ackermann Vehicle using Heuristic Steering and Tractive Force Functions

**Chinese Title**: Post-Collision Trajectory Restoration for a Single-track Ackermann Vehicle using Heuristic Steering and Tractive Force Functions

**Authors**: Samsaptak Ghosh, M. Felix Orlando, Sohom Chakrabarty

**Date**: 2026-02-09 | **arXiv**: [2602.08444v1](http://arxiv.org/abs/2602.08444v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08444v1)

**Categories**: cs.RO, eess.SY

<details><summary><b>Abstract</b></summary>

Post-collision trajectory restoration is a safety-critical capability for autonomous vehicles, as impact-induced lateral motion and yaw transients can rapidly drive the vehicle away from the intended path. This paper proposes a structured heuristic recovery control law that jointly commands steering and tractive force for a generalized single-track Ackermann vehicle model. The formulation explicitly accounts for time-varying longitudinal velocity in the lateral-yaw dynamics and retains nonlinear steering-coupled interaction terms that are commonly simplified in the literature. Unlike approaches that assume constant longitudinal speed, the proposed design targets the transient post-impact regime where speed variations and nonlinear coupling significantly influence recovery. The method is evaluated in simulation on the proposed generalized single-track model and a standard 3DOF single-track reference model in MATLAB, demonstrating consistent post-collision restoration behaviour across representative initial post-impact conditions.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Post-collision trajectory restoration is a safety-critical capability for autonomous vehicles, as impact-induced lateral motion and yaw transients can rapidly drive the vehicle away from the intended path. This paper proposes a structured heuristic recovery control law that jointly commands steering and tractive force for a generalized single-track Ackermann vehicle model.The formulation explicitly accounts for time-varying longitudinal velocity in the lateral-yaw dynamics and retains nonlinear steering-coupled interaction terms that are commonly simplified in the literature. Unlike approaches that assume constant longitudinal speed, the proposed design targets the transient post-impact regime where speed variations and nonlinear coupling significantly influence recovery.The method is evaluated in simulation on the proposed generalized single-track model and a standard 3DOF single-track reference model in MATLAB, demonstrating consistent post-collision restoration behaviour across representative initial post-impact conditions.

</details>

---

## 287. SteerVLA: Steering Vision-Language-Action Models in Long-Tail Driving Scenarios

**Chinese Title**: SteerVLA: Steering Vision-Language-Action Models in Long-Tail Driving Scenarios

**Authors**: Tian Gao, Celine Tan, Catherine Glossop, Timothy Gao, Jiankai Sun et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08440v1](http://arxiv.org/abs/2602.08440v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08440v1)

**Project**: https://steervla.github.io/.  **Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

A fundamental challenge in autonomous driving is the integration of high-level, semantic reasoning for long-tail events with low-level, reactive control for robust driving. While large vision-language models (VLMs) trained on web-scale data offer powerful common-sense reasoning, they lack the grounded experience necessary for safe vehicle control. We posit that an effective autonomous agent should leverage the world knowledge of VLMs to guide a steerable driving policy toward robust control in driving scenarios. To this end, we propose SteerVLA, which leverages the reasoning capabilities of VLMs to produce fine-grained language instructions that steer a vision-language-action (VLA) driving policy. Key to our method is this rich language interface between the high-level VLM and low-level VLA, which allows the high-level policy to more effectively ground its reasoning in the control outputs of the low-level policy. To provide fine-grained language supervision aligned with vehicle control, we leverage a VLM to augment existing driving data with detailed language annotations, which we find to be essential for effective reasoning and steerability. We evaluate SteerVLA on a challenging closed-loop benchmark, where it outperforms state-of-the-art methods by 4.77 points in overall driving score and by 8.04 points on a long-tail subset. The project website is available at: https://steervla.github.io/.

</details>

<details><summary><b>Chinese Abstract</b></summary>

A fundamental challenge in autonomous driving is the integration of high-level, semantic reasoning for long-tail events with low-level, reactive control for robust driving. While large vision-language models (VLMs) trained on web-scale data offer powerful common-sense reasoning, they lack the grounded experience necessary for safe vehicle control.We posit that an effective autonomous agent should leverage the world knowledge of VLMs to guide a steerable driving policy toward robust control in driving scenarios. To this end, we propose SteerVLA, which leverages the reasoning capabilities of VLMs to produce fine-grained language instructions that steer a vision-language-action (VLA) driving policy.Key to our method is this rich language interface between the high-level VLM and low-level VLA, which allows the high-level policy to more effectively ground its reasoning in the control outputs of the low-level policy.To provide fine-grained language supervision aligned with vehicle control, we leverage a VLM to augment existing driving data with detailed language annotations, which we find to be essential for effective reasoning and steerability. We evaluate SteerVLA on a challenging closed-loop benchmark, where it outperforms state-of-the-art methods by 4.77 points in overall driving score and by 8.04 points on a long-tail subset.The project website is available at: https://steervla.github.io/.

</details>

---

## 288. Bi-Adapt: Few-shot Bimanual Adaptation for Novel Categories of 3D Objects via Semantic Correspondence

**Chinese Title**: Bi-Adapt: Few-shot Bimanual Adaptation for Novel Categories of 3D Objects via Semantic Correspondence

**Authors**: Jinxian Zhou, Ruihai Wu, Yiwei Liu, Yiwen Hou, Xunzhe Zhou et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08425v2](http://arxiv.org/abs/2602.08425v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.08425v2)

**Project**: https://biadapt-project.github.io/  **Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Bimanual manipulation is imperative yet challenging for robots to execute complex tasks, requiring coordinated collaboration between two arms. However, existing methods for bimanual manipulation often rely on costly data collection and training, struggling to generalize to unseen objects in novel categories efficiently. In this paper, we present Bi-Adapt, a novel framework designed for efficient generalization for bimanual manipulation via semantic correspondence. Bi-Adapt achieves cross-category affordance mapping by leveraging the strong capability of vision foundation models. Fine-tuning with restricted data on novel categories, Bi-Adapt exhibits notable generalization to out-of-category objects in a zero-shot manner. Extensive experiments conducted in both simulation and real-world environments validate the effectiveness of our approach and demonstrate its high efficiency, achieving a high success rate on different benchmark tasks across novel categories with limited data. Project website: https://biadapt-project.github.io/

</details>

<details><summary><b>Chinese Abstract</b></summary>

Bimanual manipulation is imperative yet challenging for robots to execute complex tasks, requiring coordinated collaboration between two arms. However, existing methods for bimanual manipulation often rely on costly data collection and training, struggling to generalize to unseen objects in novel categories efficiently.In this paper, we present Bi-Adapt, a novel framework designed for efficient generalization for bimanual manipulation via semantic correspondence. Bi-Adapt achieves cross-category affordance mapping by leveraging the strong capability of vision foundation models. Fine-tuning with restricted data on novel categories, Bi-Adapt exhibits notable generalization to out-of-category objects in a zero-shot manner.Extensive experiments conducted in both simulation and real-world environments validate the effectiveness of our approach and demonstrate its high efficiency, achieving a high success rate on different benchmark tasks across novel categories with limited data. Project website: https://biadapt-project.github.io/

</details>

---

## 289. Decentralized Intent-Based Multi-Robot Task Planner with LLM Oracles on Hyperledger Fabric

**Chinese Title**: Decentralized Intent-Based Multi-Robot Task Planner with LLM Oracles on Hyperledger Fabric

**Authors**: Farhad Keramat, Salma Salimi, Tomi Westerlund

**Date**: 2026-02-09 | **arXiv**: [2602.08421v1](http://arxiv.org/abs/2602.08421v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08421v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Large language models (LLMs) have opened new opportunities for transforming natural language user intents into executable actions. This capability enables embodied AI agents to perform complex tasks, without involvement of an expert, making human-robot interaction (HRI) more convenient. However these developments raise significant security and privacy challenges such as self-preferencing, where a single LLM service provider dominates the market and uses this power to promote their own preferences. LLM oracles have been recently proposed as a mechanism to decentralize LLMs by executing multiple LLMs from different vendors and aggregating their outputs to obtain a more reliable and trustworthy final result. However, the accuracy of these approaches highly depends on the aggregation method. The current aggregation methods mostly use semantic similarity between various LLM outputs, not suitable for robotic task planning, where the temporal order of tasks is important. To fill the gap, we propose an LLM oracle with a new aggregation method for robotic task planning. In addition, we propose a decentralized multi-robot infrastructure based on Hyperledger Fabric that can host the proposed oracle. The proposed infrastructure enables users to express their natural language intent to the system, which then can be decomposed into subtasks. These subtasks require coordinating different robots from different vendors, while enforcing fine-grained access control management on the data. To evaluate our methodology, we created the SkillChain-RTD benchmark made it publicly available. Our experimental results demonstrate the feasibility of the proposed architecture, and the proposed aggregation method outperforms other aggregation methods currently in use.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Large language models (LLMs) have opened new opportunities for transforming natural language user intents into executable actions. This capability enables embodied AI agents to perform complex tasks, without involvement of an expert, making human-robot interaction (HRI) more convenient.However these developments raise significant security and privacy challenges such as self-preferencing, where a single LLM service provider dominates the market and uses this power to promote their own preferences. LLM oracles have been recently proposed as a mechanism to decentralize LLMs by executing multiple LLMs from different vendors and aggregating their outputs to obtain a more reliable and trustworthy final result.However, the accuracy of these approaches highly depends on the aggregation method. The current aggregation methods mostly use semantic similarity between various LLM outputs, not suitable for robotic task planning, where the temporal order of tasks is important. To fill the gap, we propose an LLM oracle with a new aggregation method for robotic task planning.In addition, we propose a decentralized multi-robot infrastructure based on Hyperledger Fabric that can host the proposed oracle. The proposed infrastructure enables users to express their natural language intent to the system, which then can be decomposed into subtasks. These subtasks require coordinating different robots from different vendors, while enforcing fine-grained access control management on the data.To evaluate our methodology, we created the SkillChain-RTD benchmark made it publicly available. Our experimental results demonstrate the feasibility of the proposed architecture, and the proposed aggregation method outperforms other aggregation methods currently in use.

</details>

---

## 290. Graph-Loc: Robust Graph-Based LiDAR Pose Tracking with Compact Structural Map Priors under Low Observability and Occlusion

**Chinese Title**: Graph-Loc: Robust Graph-Based LiDAR Pose Tracking with Compact Structural Map Priors under Low Observability and Occlusion

**Authors**: Wentao Zhao, Yihe Niu, Zikun Chen, Rui Li, Yanbo Wang et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08417v1](http://arxiv.org/abs/2602.08417v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08417v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Map-based LiDAR pose tracking is essential for long-term autonomous operation, where onboard map priors need be compact for scalable storage and fast retrieval, while online observations are often partial, repetitive, and heavily occluded. We propose Graph-Loc, a graph-based localization framework that tracks the platform pose against compact structural map priors represented as a lightweight point-line graph. Such priors can be constructed from heterogeneous sources commonly available in practice, including polygon outlines vectorized from occupancy/grid maps and CAD/model/floor-plan layouts. For each incoming LiDAR scan, Graph-Loc extracts sparse point and line primitives to form an observation graph, retrieves a pose-conditioned visible subgraph via LiDAR ray simulation, and performs scan-to-map association through unbalanced optimal transport with a local graph-context regularizer. The unbalanced formulation relaxes mass conservation, improving robustness to missing, spurious, and fragmented structures under occlusion. To enhance stability in low-observability segments, we estimate information anisotropy from the refinement normal matrix and defer updates along weakly constrained directions until sufficient constraints reappear. Experiments on public benchmarks, controlled stress tests, and real-world deployments demonstrate accurate and stable tracking with KB-level priors from heterogeneous map sources, including under geometrically degenerate and sustained occlusion and in the presence of gradual scene changes.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Map-based LiDAR pose tracking is essential for long-term autonomous operation, where onboard map priors need be compact for scalable storage and fast retrieval, while online observations are often partial, repetitive, and heavily occluded. We propose Graph-Loc, a graph-based localization framework that tracks the platform pose against compact structural map priors represented as a lightweight point-line graph.Such priors can be constructed from heterogeneous sources commonly available in practice, including polygon outlines vectorized from occupancy/grid maps and CAD/model/floor-plan layouts.For each incoming LiDAR scan, Graph-Loc extracts sparse point and line primitives to form an observation graph, retrieves a pose-conditioned visible subgraph via LiDAR ray simulation, and performs scan-to-map association through unbalanced optimal transport with a local graph-context regularizer. The unbalanced formulation relaxes mass conservation, improving robustness to missing, spurious, and fragmented structures under occlusion.To enhance stability in low-observability segments, we estimate information anisotropy from the refinement normal matrix and defer updates along weakly constrained directions until sufficient constraints reappear.Experiments on public benchmarks, controlled stress tests, and real-world deployments demonstrate accurate and stable tracking with KB-level priors from heterogeneous map sources, including under geometrically degenerate and sustained occlusion and in the presence of gradual scene changes.

</details>

---

## 291. Controlled Flight of an Insect-Scale Flapping-Wing Robot via Integrated Onboard Sensing and Computation

**Chinese Title**: Controlled Flight of an Insect-Scale Flapping-Wing Robot via Integrated Onboard Sensing and Computation

**Authors**: Yi-Hsuan Hsiao, Quang Phuc Kieu, Zhongtao Guan, Suhan Kim, Jiaze Cai et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08328v1](http://arxiv.org/abs/2602.08328v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08328v1)

**Categories**: cs.RO, eess.SY

<details><summary><b>Abstract</b></summary>

Aerial insects can effortlessly navigate dense vegetation, whereas similarly sized aerial robots typically depend on offboard sensors and computation to maintain stable flight. This disparity restricts insect-scale robots to operation within motion capture environments, substantially limiting their applicability to tasks such as search-and-rescue and precision agriculture. In this work, we present a 1.29-gram aerial robot capable of hovering and tracking trajectories with solely onboard sensing and computation. The combination of a sensor suite, estimators, and a low-level controller achieved centimeter-scale positional flight accuracy. Additionally, we developed a hierarchical controller in which a human operator provides high-level commands to direct the robot's motion. In a 30-second flight experiment conducted outside a motion capture system, the robot avoided obstacles and ultimately landed on a sunflower. This level of sensing and computational autonomy represents a significant advancement for the aerial microrobotics community, further opening opportunities to explore onboard planning and power autonomy.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Aerial insects can effortlessly navigate dense vegetation, whereas similarly sized aerial robots typically depend on offboard sensors and computation to maintain stable flight. This disparity restricts insect-scale robots to operation within motion capture environments, substantially limiting their applicability to tasks such as search-and-rescue and precision agriculture.In this work, we present a 1.29-gram aerial robot capable of hovering and tracking trajectories with solely onboard sensing and computation. The combination of a sensor suite, estimators, and a low-level controller achieved centimeter-scale positional flight accuracy. Additionally, we developed a hierarchical controller in which a human operator provides high-level commands to direct the robot's motion.In a 30-second flight experiment conducted outside a motion capture system, the robot avoided obstacles and ultimately landed on a sunflower. This level of sensing and computational autonomy represents a significant advancement for the aerial microrobotics community, further opening opportunities to explore onboard planning and power autonomy.

</details>

---

## 292. ReefFlex: A Generative Design Framework for Soft Robotic Grasping of Organic and Fragile objects

**Chinese Title**: ReefFlex: A Generative Design Framework for Soft Robotic Grasping of Organic and Fragile objects

**Authors**: Josh Pinskier, Sarah Baldwin, Stephen Rodan, David Howard

**Date**: 2026-02-09 | **arXiv**: [2602.08285v1](http://arxiv.org/abs/2602.08285v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08285v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Climate change, invasive species and human activities are currently damaging the world's coral reefs at unprecedented rates, threatening their vast biodiversity and fisheries, and reducing coastal protection. Solving this vast challenge requires scalable coral regeneration technologies that can breed climate-resilient species and accelerate the natural regrowth processes; actions that are impeded by the absence of safe and robust tools to handle the fragile coral. We investigate ReefFlex, a generative soft finger design methodology that explores a diverse space of soft fingers to produce a set of candidates capable of safely grasping fragile and geometrically heterogeneous coral in a cluttered environment. Our key insight is encoding heterogeneous grasping into a reduced set of motion primitives, creating a simplified, tractable multi-objective optimisation problem. To evaluate the method, we design a soft robot for reef rehabilitation, which grows and manipulates coral in onshore aquaculture facilities for future reef out-planting. We demonstrate ReefFlex increases both grasp success and grasp quality (disturbance resistance, positioning accuracy) and reduces in adverse events encountered during coral manipulation compared to reference designs. ReefFlex, offers a generalisable method to design soft end-effectors for complex handling and paves a pathway towards automation in previously unachievable domains like coral handling for restoration.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Climate change, invasive species and human activities are currently damaging the world's coral reefs at unprecedented rates, threatening their vast biodiversity and fisheries, and reducing coastal protection.Solving this vast challenge requires scalable coral regeneration technologies that can breed climate-resilient species and accelerate the natural regrowth processes; actions that are impeded by the absence of safe and robust tools to handle the fragile coral.We investigate ReefFlex, a generative soft finger design methodology that explores a diverse space of soft fingers to produce a set of candidates capable of safely grasping fragile and geometrically heterogeneous coral in a cluttered environment. Our key insight is encoding heterogeneous grasping into a reduced set of motion primitives, creating a simplified, tractable multi-objective optimisation problem.To evaluate the method, we design a soft robot for reef rehabilitation, which grows and manipulates coral in onshore aquaculture facilities for future reef out-planting. We demonstrate ReefFlex increases both grasp success and grasp quality (disturbance resistance, positioning accuracy) and reduces in adverse events encountered during coral manipulation compared to reference designs.ReefFlex, offers a generalisable method to design soft end-effectors for complex handling and paves a pathway towards automation in previously unachievable domains like coral handling for restoration.

</details>

---

## 293. Aerial Manipulation with Contact-Aware Onboard Perception and Hybrid Control

**Chinese Title**: Aerial Manipulation with Contact-Aware Onboard Perception and Hybrid Control

**Authors**: Yuanzhu Zhan, Yufei Jiang, Muqing Cao, Junyi Geng

**Date**: 2026-02-09 | **arXiv**: [2602.08251v1](http://arxiv.org/abs/2602.08251v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08251v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Aerial manipulation (AM) promises to move Unmanned Aerial Vehicles (UAVs) beyond passive inspection to contact-rich tasks such as grasping, assembly, and in-situ maintenance. Most prior AM demonstrations rely on external motion capture (MoCap) and emphasize position control for coarse interactions, limiting deployability. We present a fully onboard perception-control pipeline for contact-rich AM that achieves accurate motion tracking and regulated contact wrenches without MoCap. The main components are (1) an augmented visual-inertial odometry (VIO) estimator with contact-consistency factors that activate only during interaction, tightening uncertainty around the contact frame and reducing drift, and (2) image-based visual servoing (IBVS) to mitigate perception-control coupling, together with a hybrid force-motion controller that regulates contact wrenches and lateral motion for stable contact. Experiments show that our approach closes the perception-to-wrench loop using only onboard sensing, yielding an velocity estimation improvement of 66.01% at contact, reliable target approach, and stable force holding-pointing toward deployable, in-the-wild aerial manipulation.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Aerial manipulation (AM) promises to move Unmanned Aerial Vehicles (UAVs) beyond passive inspection to contact-rich tasks such as grasping, assembly, and in-situ maintenance. Most prior AM demonstrations rely on external motion capture (MoCap) and emphasize position control for coarse interactions, limiting deployability.We present a fully onboard perception-control pipeline for contact-rich AM that achieves accurate motion tracking and regulated contact wrenches without MoCap.The main components are (1) an augmented visual-inertial odometry (VIO) estimator with contact-consistency factors that activate only during interaction, tightening uncertainty around the contact frame and reducing drift, and (2) image-based visual servoing (IBVS) to mitigate perception-control coupling, together with a hybrid force-motion controller that regulates contact wrenches and lateral motion for stable contact.Experiments show that our approach closes the perception-to-wrench loop using only onboard sensing, yielding an velocity estimation improvement of 66.01% at contact, reliable target approach, and stable force holding-pointing toward deployable, in-the-wild aerial manipulation.

</details>

---

## 294. Mutual Information Collapse Explains Disentanglement Failure in $Î²$-VAEs

**Chinese Title**: Mutual Information Collapse Explains Disentanglement Failure in $Î²$-VAEs

**Authors**: Minh Vu, Xiaoliang Wan, Shuangqing Wei

**Date**: 2026-02-09 | **arXiv**: [2602.09277v1](http://arxiv.org/abs/2602.09277v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09277v1)

**Categories**: stat.ML, cs.LG

<details><summary><b>Abstract</b></summary>

The $Î²$-VAE is a foundational framework for unsupervised disentanglement, using $Î²$ to regulate the trade-off between latent factorization and reconstruction fidelity. Empirically, however, disentanglement performance exhibits a pervasive non-monotonic trend: benchmarks such as MIG and SAP typically peak at intermediate $Î²$ and collapse as regularization increases. We demonstrate that this collapse is a fundamental information-theoretic failure, where strong Kullback-Leibler pressure promotes marginal independence at the expense of the latent channel's semantic informativeness. By formalizing this mechanism in a linear-Gaussian setting, we prove that for $Î²> 1$, stationarity-induced dynamics trigger a spectral contraction of the encoder gain, driving latent-factor mutual information to zero. To resolve this, we introduce the $Î»Î²$-VAE, which decouples regularization pressure from informational collapse via an auxiliary $L_2$ reconstruction penalty $Î»$. Extensive experiments on dSprites, Shapes3D, and MPI3D-real confirm that $Î»> 0$ stabilizes disentanglement and restores latent informativeness over a significantly broader range of $Î²$, providing a principled theoretical justification for dual-parameter regularization in variational inference backbones.

</details>

<details><summary><b>Chinese Abstract</b></summary>

The $Î²$-VAE is a foundational framework for unsupervised disentanglement, using $Î²$ to regulate the trade-off between latent factorization and reconstruction fidelity. Empirically, however, disentanglement performance exhibits a pervasive non-monotonic trend: benchmarks such as MIG and SAP typically peak at intermediate $Î²$ and collapse as regularization increases.We demonstrate that this collapse is a fundamental information-theoretic failure, where strong Kullback-Leibler pressure promotes marginal independence at the expense of the latent channel's semantic informativeness. By formalizing this mechanism in a linear-Gaussian setting, we prove that for $Î²> 1$, stationarity-induced dynamics trigger a spectral contraction of the encoder gain, driving latent-factor mutual information to zero.To resolve this, we introduce the $Î»Î²$-VAE, which decouples regularization pressure from informational collapse via an auxiliary $L_2$ reconstruction penalty $Î»$.Extensive experiments on dSprites, Shapes3D, and MPI3D-real confirm that $Î»> 0$ stabilizes disentanglement and restores latent informativeness over a significantly broader range of $Î²$, providing a principled theoretical justification for dual-parameter regularization in variational inference backbones.

</details>

---

## 295. Optimal Estimation in Orthogonally Invariant Generalized Linear Models: Spectral Initialization and Approximate Message Passing

**Chinese Title**: Optimal Estimation in Orthogonally Invariant Generalized Linear Models: Spectral Initialization and Approximate Message Passing

**Authors**: Yihan Zhang, Hong Chang Ji, Ramji Venkataramanan, Marco Mondelli

**Date**: 2026-02-09 | **arXiv**: [2602.09240v1](http://arxiv.org/abs/2602.09240v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09240v1)

**Categories**: math.ST, cs.IT, cs.LG, math.PR, stat.ML

<details><summary><b>Abstract</b></summary>

We consider the problem of parameter estimation from a generalized linear model with a random design matrix that is orthogonally invariant in law. Such a model allows the design have an arbitrary distribution of singular values and only assumes that its singular vectors are generic. It is a vast generalization of the i.i.d. Gaussian design typically considered in the theoretical literature, and is motivated by the fact that real data often have a complex correlation structure so that methods relying on i.i.d. assumptions can be highly suboptimal. Building on the paradigm of spectrally-initialized iterative optimization, this paper proposes optimal spectral estimators and combines them with an approximate message passing (AMP) algorithm, establishing rigorous performance guarantees for these two algorithmic steps. Both the spectral initialization and the subsequent AMP meet existing conjectures on the fundamental limits to estimation -- the former on the optimal sample complexity for efficient weak recovery, and the latter on the optimal errors. Numerical experiments suggest the effectiveness of our methods and accuracy of our theory beyond orthogonally invariant data.

</details>

<details><summary><b>Chinese Abstract</b></summary>

We consider the problem of parameter estimation from a generalized linear model with a random design matrix that is orthogonally invariant in law. Such a model allows the design have an arbitrary distribution of singular values and only assumes that its singular vectors are generic. It is a vast generalization of the i.i.d.Gaussian design typically considered in the theoretical literature, and is motivated by the fact that real data often have a complex correlation structure so that methods relying on i.i.d. assumptions can be highly suboptimal.Building on the paradigm of spectrally-initialized iterative optimization, this paper proposes optimal spectral estimators and combines them with an approximate message passing (AMP) algorithm, establishing rigorous performance guarantees for these two algorithmic steps.Both the spectral initialization and the subsequent AMP meet existing conjectures on the fundamental limits to estimation -- the former on the optimal sample complexity for efficient weak recovery, and the latter on the optimal errors. Numerical experiments suggest the effectiveness of our methods and accuracy of our theory beyond orthogonally invariant data.

</details>

---

## 296. Beyond the Unit Hypersphere: Embedding Magnitude in Contrastive Learning

**Chinese Title**: Beyond the Unit Hypersphere: Embedding Magnitude in Contrastive Learning

**Authors**: Xincan Feng, Taro Watanabe

**Date**: 2026-02-09 | **arXiv**: [2602.09229v1](http://arxiv.org/abs/2602.09229v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09229v1)

**Categories**: cs.LG, cs.IR

<details><summary><b>Abstract</b></summary>

Cosine similarity is prevalent in contrastive learning, yet it makes an implicit assumption: embedding magnitude is noise. Prior work occasionally found dot product and cosine similarity comparable, but left unanswered WHAT information magnitude carries, WHEN it helps, and HOW to leverage it. We conduct a systematic study through a $2 \times 2$ ablation that independently controls input-side and output-side normalization across text and vision models. Our findings reveal three key insights. First, in text retrieval, output (document) magnitude strongly correlates with relevance (Cohen's $d$ up to 1.80), yielding the largest gains on reasoning-intensive tasks. Second, input and output magnitudes serve asymmetric roles: output magnitude directly scales similarity scores while input magnitude modulates training dynamics. Third, magnitude learning benefits asymmetric tasks (text retrieval, RAG) but harms symmetric tasks (STS, text-image alignment). These findings establish a task symmetry principle: the choice between cosine and dot product depends on whether the task has distinct input roles, enabling cost-free improvements by simply removing an unnecessary constraint.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Cosine similarity is prevalent in contrastive learning, yet it makes an implicit assumption: embedding magnitude is noise. Prior work occasionally found dot product and cosine similarity comparable, but left unanswered WHAT information magnitude carries, WHEN it helps, and HOW to leverage it.We conduct a systematic study through a $2 \times 2$ ablation that independently controls input-side and output-side normalization across text and vision models. Our findings reveal three key insights. First, in text retrieval, output (document) magnitude strongly correlates with relevance (Cohen's $d$ up to 1.80), yielding the largest gains on reasoning-intensive tasks.Second, input and output magnitudes serve asymmetric roles: output magnitude directly scales similarity scores while input magnitude modulates training dynamics. Third, magnitude learning benefits asymmetric tasks (text retrieval, RAG) but harms symmetric tasks (STS, text-image alignment).These findings establish a task symmetry principle: the choice between cosine and dot product depends on whether the task has distinct input roles, enabling cost-free improvements by simply removing an unnecessary constraint.

</details>

---

## 297. Barycentric alignment for instance-level comparison of neural representations

**Chinese Title**: Barycentric alignment for instance-level comparison of neural representations

**Authors**: Shreya Saha, Zoe Wanying He, Meenakshi Khosla

**Date**: 2026-02-09 | **arXiv**: [2602.09225v1](http://arxiv.org/abs/2602.09225v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09225v1)

**Categories**: cs.LG

<details><summary><b>Abstract</b></summary>

Comparing representations across neural networks is challenging because representations admit symmetries, such as arbitrary reordering of units or rotations of activation space, that obscure underlying equivalence between models. We introduce a barycentric alignment framework that quotients out these nuisance symmetries to construct a universal embedding space across many models. Unlike existing similarity measures, which summarize relationships over entire stimulus sets, this framework enables similarity to be defined at the level of individual stimuli, revealing inputs that elicit convergent versus divergent representations across models. Using this instance-level notion of similarity, we identify systematic input properties that predict representational convergence versus divergence across vision and language model families. We also construct universal embedding spaces for brain representations across individuals and cortical regions, enabling instance-level comparison of representational agreement across stages of the human visual hierarchy. Finally, we apply the same barycentric alignment framework to purely unimodal vision and language models and find that post-hoc alignment into a shared space yields image text similarity scores that closely track human cross-modal judgments and approach the performance of contrastively trained vision-language models. This strikingly suggests that independently learned representations already share sufficient geometric structure for human-aligned cross-modal comparison. Together, these results show that resolving representational similarity at the level of individual stimuli reveals phenomena that cannot be detected by set-level comparison metrics.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Comparing representations across neural networks is challenging because representations admit symmetries, such as arbitrary reordering of units or rotations of activation space, that obscure underlying equivalence between models. We introduce a barycentric alignment framework that quotients out these nuisance symmetries to construct a universal embedding space across many models.Unlike existing similarity measures, which summarize relationships over entire stimulus sets, this framework enables similarity to be defined at the level of individual stimuli, revealing inputs that elicit convergent versus divergent representations across models.Using this instance-level notion of similarity, we identify systematic input properties that predict representational convergence versus divergence across vision and language model families. We also construct universal embedding spaces for brain representations across individuals and cortical regions, enabling instance-level comparison of representational agreement across stages of the human visual hierarchy.Finally, we apply the same barycentric alignment framework to purely unimodal vision and language models and find that post-hoc alignment into a shared space yields image text similarity scores that closely track human cross-modal judgments and approach the performance of contrastively trained vision-language models.This strikingly suggests that independently learned representations already share sufficient geometric structure for human-aligned cross-modal comparison. Together, these results show that resolving representational similarity at the level of individual stimuli reveals phenomena that cannot be detected by set-level comparison metrics.

</details>

---

## 298. Faster Rates For Federated Variational Inequalities

**Chinese Title**: Faster Rates For Federated Variational Inequalities

**Authors**: Guanghui Wang, Satyen Kale

**Date**: 2026-02-09 | **arXiv**: [2602.09164v1](http://arxiv.org/abs/2602.09164v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09164v1)

**Categories**: cs.LG

<details><summary><b>Abstract</b></summary>

In this paper, we study federated optimization for solving stochastic variational inequalities (VIs), a problem that has attracted growing attention in recent years. Despite substantial progress, a significant gap remains between existing convergence rates and the state-of-the-art bounds known for federated convex optimization. In this work, we address this limitation by establishing a series of improved convergence rates. First, we show that, for general smooth and monotone variational inequalities, the classical Local Extra SGD algorithm admits tighter guarantees under a refined analysis. Next, we identify an inherent limitation of Local Extra SGD, which can lead to excessive client drift. Motivated by this observation, we propose a new algorithm, the Local Inexact Proximal Point Algorithm with Extra Step (LIPPAX), and show that it mitigates client drift and achieves improved guarantees in several regimes, including bounded Hessian, bounded operator, and low-variance settings. Finally, we extend our results to federated composite variational inequalities and establish improved convergence guarantees.

</details>

<details><summary><b>Chinese Abstract</b></summary>

In this paper, we study federated optimization for solving stochastic variational inequalities (VIs), a problem that has attracted growing attention in recent years. Despite substantial progress, a significant gap remains between existing convergence rates and the state-of-the-art bounds known for federated convex optimization. In this work, we address this limitation by establishing a series of improved convergence rates.First, we show that, for general smooth and monotone variational inequalities, the classical Local Extra SGD algorithm admits tighter guarantees under a refined analysis. Next, we identify an inherent limitation of Local Extra SGD, which can lead to excessive client drift.Motivated by this observation, we propose a new algorithm, the Local Inexact Proximal Point Algorithm with Extra Step (LIPPAX), and show that it mitigates client drift and achieves improved guarantees in several regimes, including bounded Hessian, bounded operator, and low-variance settings. Finally, we extend our results to federated composite variational inequalities and establish improved convergence guarantees.

</details>

---

## 299. Counterfactual Maps: What They Are and How to Find Them

**Chinese Title**: Counterfactual Maps: What They Are and How to Find Them

**Authors**: Awa Khouna, Julien Ferry, Thibaut Vidal

**Date**: 2026-02-09 | **arXiv**: [2602.09128v1](http://arxiv.org/abs/2602.09128v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09128v1)

**Categories**: cs.LG

<details><summary><b>Abstract</b></summary>

Counterfactual explanations are a central tool in interpretable machine learning, yet computing them exactly for complex models remains challenging. For tree ensembles, predictions are piecewise constant over a large collection of axis-aligned hyperrectangles, implying that an optimal counterfactual for a point corresponds to its projection onto the nearest rectangle with an alternative label under a chosen metric. Existing methods largely overlook this geometric structure, relying either on heuristics with no optimality guarantees or on mixed-integer programming formulations that do not scale to interactive use.   In this work, we revisit counterfactual generation through the lens of nearest-region search and introduce counterfactual maps, a global representation of recourse for tree ensembles. Leveraging the fact that any tree ensemble can be compressed into an equivalent partition of labeled hyperrectangles, we cast counterfactual search as the problem of identifying the generalized Voronoi cell associated with the nearest rectangle of an alternative label. This leads to an exact, amortized algorithm based on volumetric k-dimensional (KD) trees, which performs branch-and-bound nearest-region queries with explicit optimality certificates and sublinear average query time after a one-time preprocessing phase.   Our experimental analyses on several real datasets drawn from high-stakes application domains show that this approach delivers globally optimal counterfactual explanations with millisecond-level latency, achieving query times that are orders of magnitude faster than existing exact, cold-start optimization methods.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Counterfactual explanations are a central tool in interpretable machine learning, yet computing them exactly for complex models remains challenging. For tree ensembles, predictions are piecewise constant over a large collection of axis-aligned hyperrectangles, implying that an optimal counterfactual for a point corresponds to its projection onto the nearest rectangle with an alternative label under a chosen metric.Existing methods largely overlook this geometric structure, relying either on heuristics with no optimality guarantees or on mixed-integer programming formulations that do not scale to interactive use. In this work, we revisit counterfactual generation through the lens of nearest-region search and introduce counterfactual maps, a global representation of recourse for tree ensembles.Leveraging the fact that any tree ensemble can be compressed into an equivalent partition of labeled hyperrectangles, we cast counterfactual search as the problem of identifying the generalized Voronoi cell associated with the nearest rectangle of an alternative label.This leads to an exact, amortized algorithm based on volumetric k-dimensional (KD) trees, which performs branch-and-bound nearest-region queries with explicit optimality certificates and sublinear average query time after a one-time preprocessing phase.Our experimental analyses on several real datasets drawn from high-stakes application domains show that this approach delivers globally optimal counterfactual explanations with millisecond-level latency, achieving query times that are orders of magnitude faster than existing exact, cold-start optimization methods.

</details>

---

## 300. ShapeCond: Fast Shapelet-Guided Dataset Condensation for Time Series Classification

**Chinese Title**: ShapeCond: Fast Shapelet-Guided Dataset Condensation for Time Series Classification

**Authors**: Sijia Peng, Yun Xiong, Xi Chen, Yi Xie, Guanzhi Li et al.

**Date**: 2026-02-09 | **arXiv**: [2602.09008v1](http://arxiv.org/abs/2602.09008v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09008v1)

**Code**: https://github.com/lunaaa95/ShapeCond.

**Categories**: cs.LG

<details><summary><b>Abstract</b></summary>

Time series data supports many domains (e.g., finance and climate science), but its rapid growth strains storage and computation. Dataset condensation can alleviate this by synthesizing a compact training set that preserves key information. Yet most condensation methods are image-centric and often fail on time series because they miss time-series-specific temporal structure, especially local discriminative motifs such as shapelets. In this work, we propose ShapeCond, a novel and efficient condensation framework for time series classification that leverages shapelet-based dataset knowledge via a shapelet-guided optimization strategy. Our shapelet-assisted synthesis cost is independent of sequence length: longer series yield larger speedups in synthesis (e.g., 29$\times$ faster over prior state-of-the-art method CondTSC for time-series condensation, and up to 10,000$\times$ over naively using shapelets on the Sleep dataset with 3,000 timesteps). By explicitly preserving critical local patterns, ShapeCond improves downstream accuracy and consistently outperforms all prior state-of-the-art time series dataset condensation methods across extensive experiments. Code is available at https://github.com/lunaaa95/ShapeCond.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Time series data supports many domains (e.g., finance and climate science), but its rapid growth strains storage and computation. Dataset condensation can alleviate this by synthesizing a compact training set that preserves key information. Yet most condensation methods are image-centric and often fail on time series because they miss time-series-specific temporal structure, especially local discriminative motifs such as shapelets.In this work, we propose ShapeCond, a novel and efficient condensation framework for time series classification that leverages shapelet-based dataset knowledge via a shapelet-guided optimization strategy.Our shapelet-assisted synthesis cost is independent of sequence length: longer series yield larger speedups in synthesis (e.g., 29$\times$ faster over prior state-of-the-art method CondTSC for time-series condensation, and up to 10,000$\times$ over naively using shapelets on the Sleep dataset with 3,000 timesteps).By explicitly preserving critical local patterns, ShapeCond improves downstream accuracy and consistently outperforms all prior state-of-the-art time series dataset condensation methods across extensive experiments. Code is available at https://github.com/lunaaa95/ShapeCond.

</details>

---

## 301. Learning to Coordinate via Quantum Entanglement in Multi-Agent Reinforcement Learning

**Chinese Title**: Learning to Coordinate via Quantum Entanglement in Multi-Agent Reinforcement Learning

**Authors**: John Gardiner, Orlando Romero, Brendan Tivnan, NicolÃ² Dal Fabbro, George J. Pappas

**Date**: 2026-02-09 | **arXiv**: [2602.08965v1](http://arxiv.org/abs/2602.08965v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08965v1)

**Categories**: cs.MA, cs.LG

<details><summary><b>Abstract</b></summary>

The inability to communicate poses a major challenge to coordination in multi-agent reinforcement learning (MARL). Prior work has explored correlating local policies via shared randomness, sometimes in the form of a correlation device, as a mechanism to assist in decentralized decision-making. In contrast, this work introduces the first framework for training MARL agents to exploit shared quantum entanglement as a coordination resource, which permits a larger class of communication-free correlated policies than shared randomness alone. This is motivated by well-known results in quantum physics which posit that, for certain single-round cooperative games with no communication, shared quantum entanglement enables strategies that outperform those that only use shared randomness. In such cases, we say that there is quantum advantage. Our framework is based on a novel differentiable policy parameterization that enables optimization over quantum measurements, together with a novel policy architecture that decomposes joint policies into a quantum coordinator and decentralized local actors. To illustrate the effectiveness of our proposed method, we first show that we can learn, purely from experience, strategies that attain quantum advantage in single-round games that are treated as black box oracles. We then demonstrate how our machinery can learn policies with quantum advantage in an illustrative multi-agent sequential decision-making problem formulated as a decentralized partially observable Markov decision process (Dec-POMDP).

</details>

<details><summary><b>Chinese Abstract</b></summary>

The inability to communicate poses a major challenge to coordination in multi-agent reinforcement learning (MARL). Prior work has explored correlating local policies via shared randomness, sometimes in the form of a correlation device, as a mechanism to assist in decentralized decision-making.In contrast, this work introduces the first framework for training MARL agents to exploit shared quantum entanglement as a coordination resource, which permits a larger class of communication-free correlated policies than shared randomness alone.This is motivated by well-known results in quantum physics which posit that, for certain single-round cooperative games with no communication, shared quantum entanglement enables strategies that outperform those that only use shared randomness. In such cases, we say that there is quantum advantage.Our framework is based on a novel differentiable policy parameterization that enables optimization over quantum measurements, together with a novel policy architecture that decomposes joint policies into a quantum coordinator and decentralized local actors.To illustrate the effectiveness of our proposed method, we first show that we can learn, purely from experience, strategies that attain quantum advantage in single-round games that are treated as black box oracles. We then demonstrate how our machinery can learn policies with quantum advantage in an illustrative multi-agent sequential decision-making problem formulated as a decentralized partially observable Markov decision process (Dec-POMDP).

</details>

---

## 302. DynamiQ: Accelerating Gradient Synchronization using Compressed Multi-hop All-reduce

**Chinese Title**: DynamiQ: Accelerating Gradient Synchronization using Compressed Multi-hop All-reduce

**Authors**: Wenchen Han, Shay Vargaftik, Michael Mitzenmacher, Ran Ben Basat

**Date**: 2026-02-09 | **arXiv**: [2602.08923v1](http://arxiv.org/abs/2602.08923v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08923v1)

**Categories**: cs.LG, cs.DC, cs.NI

<details><summary><b>Abstract</b></summary>

Multi-hop all-reduce is the de facto backbone of large model training. As the training scale increases, the network often becomes a bottleneck, motivating reducing the volume of transmitted data. Accordingly, recent systems demonstrated significant acceleration of the training process using gradient quantization. However, these systems are not optimized for multi-hop aggregation, where entries are partially summed multiple times along their aggregation topology.   This paper presents DynamiQ, a quantization framework that bridges the gap between quantization best practices and multi-hop aggregation. DynamiQ introduces novel techniques to better represent partial sums, co-designed with a decompress-accumulate-recompress fused kernel to facilitate fast execution.   We extended PyTorch DDP to support DynamiQ over NCCL P2P, and across different LLMs, tasks, and scales, we demonstrate consistent improvement of up to 34.2% over the best among state-of-the-art methods such as Omni-Reduce, THC, and emerging standards such as MXFP4, MXFP6, and MXFP8. Further, DynamiQ is the only evaluated method that consistently reaches near-baseline accuracy (e.g., 99.9% of the BF16 baseline) and does so while significantly accelerating the training.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Multi-hop all-reduce is the de facto backbone of large model training. As the training scale increases, the network often becomes a bottleneck, motivating reducing the volume of transmitted data. Accordingly, recent systems demonstrated significant acceleration of the training process using gradient quantization.However, these systems are not optimized for multi-hop aggregation, where entries are partially summed multiple times along their aggregation topology. This paper presents DynamiQ, a quantization framework that bridges the gap between quantization best practices and multi-hop aggregation.DynamiQ introduces novel techniques to better represent partial sums, co-designed with a decompress-accumulate-recompress fused kernel to facilitate fast execution. We extended PyTorch DDP to support DynamiQ over NCCL P2P, and across different LLMs, tasks, and scales, we demonstrate consistent improvement of up to 34.2% over the best among state-of-the-art methods such as Omni-Reduce, THC, and emerging standards such as MXFP4, MXFP6, and MXFP8.Further, DynamiQ is the only evaluated method that consistently reaches near-baseline accuracy (e.g., 99.9% of the BF16 baseline) and does so while significantly accelerating the training.

</details>

---

## 303. Diffusion-Inspired Reconfiguration of Transformers for Uncertainty Calibration

**Chinese Title**: Diffusion-Inspired Reconfiguration of Transformers for Uncertainty Calibration

**Authors**: Manh Cuong Dao, Quang Hung Pham, Phi Le Nguyen, Thao Nguyen Truong, Bryan Kian Hsiang Low et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08920v1](http://arxiv.org/abs/2602.08920v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08920v1)

**Categories**: cs.LG

<details><summary><b>Abstract</b></summary>

Uncertainty calibration in pre-trained transformers is critical for their reliable deployment in risk-sensitive applications. Yet, most existing pre-trained transformers do not have a principled mechanism for uncertainty propagation through their feature transformation stack. In this work, we propose a diffusion-inspired reconfiguration of transformers in which each feature transformation block is modeled as a probabilistic mapping. Composing these probabilistic mappings reveals a probability path that mimics the structure of a diffusion process, transporting data mass from the input distribution to the pre-trained feature distribution. This probability path can then be recompiled on a diffusion process with a unified transition model to enable principled propagation of representation uncertainty throughout the pre-trained model's architecture while maintaining its original predictive performance. Empirical results across a variety of vision and language benchmarks demonstrate that our method achieves superior calibration and predictive accuracy compared to existing uncertainty-aware transformers.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Uncertainty calibration in pre-trained transformers is critical for their reliable deployment in risk-sensitive applications. Yet, most existing pre-trained transformers do not have a principled mechanism for uncertainty propagation through their feature transformation stack. In this work, we propose a diffusion-inspired reconfiguration of transformers in which each feature transformation block is modeled as a probabilistic mapping.Composing these probabilistic mappings reveals a probability path that mimics the structure of a diffusion process, transporting data mass from the input distribution to the pre-trained feature distribution.This probability path can then be recompiled on a diffusion process with a unified transition model to enable principled propagation of representation uncertainty throughout the pre-trained model's architecture while maintaining its original predictive performance.Empirical results across a variety of vision and language benchmarks demonstrate that our method achieves superior calibration and predictive accuracy compared to existing uncertainty-aware transformers.

</details>

---

## 304. Rethinking Graph Generalization through the Lens of Sharpness-Aware Minimization

**Chinese Title**: Rethinking Graph Generalization through the Lens of Sharpness-Aware Minimization

**Authors**: Yang Qiu, Yixiong Zou, Jun Wang

**Date**: 2026-02-09 | **arXiv**: [2602.08855v1](http://arxiv.org/abs/2602.08855v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08855v1)

**Categories**: cs.LG

<details><summary><b>Abstract</b></summary>

Graph Neural Networks (GNNs) have achieved remarkable success across various graph-based tasks but remain highly sensitive to distribution shifts. In this work, we focus on a prevalent yet under-explored phenomenon in graph generalization, Minimal Shift Flip (MSF),where test samples that slightly deviate from the training distribution are abruptly misclassified. To interpret this phenomenon, we revisit MSF through the lens of Sharpness-Aware Minimization (SAM), which characterizes the local stability and sharpness of the loss landscape while providing a theoretical foundation for modeling generalization error. To quantify loss sharpness, we introduce the concept of Local Robust Radius, measuring the smallest perturbation required to flip a prediction and establishing a theoretical link between local stability and generalization. Building on this perspective, we further observe a continual decrease in the robust radius during training, indicating weakened local stability and an increasingly sharp loss landscape that gives rise to MSF. To jointly solve the MSF phenomenon and the intractability of radius, we develop an energy-based formulation that is theoretically proven to be monotonically correlated with the robust radius, offering a tractable and principled objective for modeling flatness and stability. Building on these insights, we propose an energy-driven generative augmentation framework (E2A) that leverages energy-guided latent perturbations to generate pseudo-OOD samples and enhance model generalization. Extensive experiments across multiple benchmarks demonstrate that E2A consistently improves graph OOD generalization, outperforming state-of-the-art baselines.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Graph Neural Networks (GNNs) have achieved remarkable success across various graph-based tasks but remain highly sensitive to distribution shifts. In this work, we focus on a prevalent yet under-explored phenomenon in graph generalization, Minimal Shift Flip (MSF),where test samples that slightly deviate from the training distribution are abruptly misclassified.To interpret this phenomenon, we revisit MSF through the lens of Sharpness-Aware Minimization (SAM), which characterizes the local stability and sharpness of the loss landscape while providing a theoretical foundation for modeling generalization error.To quantify loss sharpness, we introduce the concept of Local Robust Radius, measuring the smallest perturbation required to flip a prediction and establishing a theoretical link between local stability and generalization. Building on this perspective, we further observe a continual decrease in the robust radius during training, indicating weakened local stability and an increasingly sharp loss landscape that gives rise to MSF.To jointly solve the MSF phenomenon and the intractability of radius, we develop an energy-based formulation that is theoretically proven to be monotonically correlated with the robust radius, offering a tractable and principled objective for modeling flatness and stability.Building on these insights, we propose an energy-driven generative augmentation framework (E2A) that leverages energy-guided latent perturbations to generate pseudo-OOD samples and enhance model generalization. Extensive experiments across multiple benchmarks demonstrate that E2A consistently improves graph OOD generalization, outperforming state-of-the-art baselines.

</details>

---

## 305. Cutting Through the Noise: On-the-fly Outlier Detection for Robust Training of Machine Learning Interatomic Potentials

**Chinese Title**: Cutting Through the Noise: On-the-fly Outlier Detection for Robust Training of Machine Learning Interatomic Potentials

**Authors**: Terry C. W. Lam, Niamh O'Neill, Christoph Schran, Lars L. Schaaf

**Date**: 2026-02-09 | **arXiv**: [2602.08849v1](http://arxiv.org/abs/2602.08849v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08849v1)

**Categories**: stat.ML, cond-mat.mtrl-sci, cs.LG, physics.chem-ph

<details><summary><b>Abstract</b></summary>

The accuracy of machine learning interatomic potentials suffers from reference data that contains numerical noise. Often originating from unconverged or inconsistent electronic-structure calculations, this noise is challenging to identify. Existing mitigation strategies such as manual filtering or iterative refinement of outliers, require either substantial expert effort or multiple expensive retraining cycles, making them difficult to scale to large datasets. Here, we introduce an on-the-fly outlier detection scheme that automatically down-weights noisy samples, without requiring additional reference calculations. By tracking the loss distribution via an exponential moving average, this unsupervised method identifies outliers throughout a single training run. We show that this approach prevents overfitting and matches the performance of iterative refinement baselines with significantly reduced overhead. The method's effectiveness is demonstrated by recovering accurate physical observables for liquid water from unconverged reference data, including diffusion coefficients. Furthermore, we validate its scalability by training a foundation model for organic chemistry on the SPICE dataset, where it reduces energy errors by a factor of three. This framework provides a simple, automated solution for training robust models on imperfect datasets across dataset sizes.

</details>

<details><summary><b>Chinese Abstract</b></summary>

The accuracy of machine learning interatomic potentials suffers from reference data that contains numerical noise. Often originating from unconverged or inconsistent electronic-structure calculations, this noise is challenging to identify.Existing mitigation strategies such as manual filtering or iterative refinement of outliers, require either substantial expert effort or multiple expensive retraining cycles, making them difficult to scale to large datasets. Here, we introduce an on-the-fly outlier detection scheme that automatically down-weights noisy samples, without requiring additional reference calculations.By tracking the loss distribution via an exponential moving average, this unsupervised method identifies outliers throughout a single training run. We show that this approach prevents overfitting and matches the performance of iterative refinement baselines with significantly reduced overhead.The method's effectiveness is demonstrated by recovering accurate physical observables for liquid water from unconverged reference data, including diffusion coefficients. Furthermore, we validate its scalability by training a foundation model for organic chemistry on the SPICE dataset, where it reduces energy errors by a factor of three.This framework provides a simple, automated solution for training robust models on imperfect datasets across dataset sizes.

</details>

---

## 306. Kirin: Improving ANN efficiency with SNN Hybridization

**Chinese Title**: Kirin: Improving ANN efficiency with SNN Hybridization

**Authors**: Chenyu Wang, Zhanglu Yan, Zhi Zhou, Xu Chen, Weng-Fai Wong

**Date**: 2026-02-09 | **arXiv**: [2602.08817v1](http://arxiv.org/abs/2602.08817v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08817v1)

**Categories**: cs.LG

<details><summary><b>Abstract</b></summary>

Artificial neural networks (ANNs), particularly large language models (LLMs), demonstrate powerful inference capabilities but consume substantial energy. Conversely, spiking neural networks (SNNs) exhibit exceptional energy efficiency due to their binary and event-driven characteristics, thus motivating the study of ANN-to-SNN conversion. In this process, quantization plays a pivotal role, mapping LLMs' floating-point parameters to discrete SNN parameters via the temporal dimension of the time window. However, several challenges remain in the conversion process: (i) converting high bit-width quantization values into binary spikes requires longer time windows, increasing system latency; and (ii) the inherent trade-off between the information loss of single-spike schemes and the energy costs of multi-spike ones in SNN. To address these challenges, we propose Kirin, a integer and spike hybrid based SNN to achieve accuracy lossless ANN-to-SNN conversion with time and energy efficiency. Specifically, we first propose a Spike Matrix Hybridization strategy that encoding low bit-width parameters that leading to small time window size into binary spikes while preserving the rest in integer format, thereby reducing the overall latency of SNN execution. Second, we introduce a silence threshold mechanism to regulate the timing of single-spike firing, ensuring the output is mathematically equivalent to the LLM's output and preserves accuracy. Experimental results demonstrate that Kirin, under a W4A4\&8 quantization setting, achieves near-FP16 accuracy while reducing energy consumption by up to 84.66\% and shortening time steps by 93.75\%.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Artificial neural networks (ANNs), particularly large language models (LLMs), demonstrate powerful inference capabilities but consume substantial energy. Conversely, spiking neural networks (SNNs) exhibit exceptional energy efficiency due to their binary and event-driven characteristics, thus motivating the study of ANN-to-SNN conversion.In this process, quantization plays a pivotal role, mapping LLMs' floating-point parameters to discrete SNN parameters via the temporal dimension of the time window.However, several challenges remain in the conversion process: (i) converting high bit-width quantization values into binary spikes requires longer time windows, increasing system latency; and (ii) the inherent trade-off between the information loss of single-spike schemes and the energy costs of multi-spike ones in SNN.To address these challenges, we propose Kirin, a integer and spike hybrid based SNN to achieve accuracy lossless ANN-to-SNN conversion with time and energy efficiency. Specifically, we first propose a Spike Matrix Hybridization strategy that encoding low bit-width parameters that leading to small time window size into binary spikes while preserving the rest in integer format, thereby reducing the overall latency of SNN execution.Second, we introduce a silence threshold mechanism to regulate the timing of single-spike firing, ensuring the output is mathematically equivalent to the LLM's output and preserves accuracy. Experimental results demonstrate that Kirin, under a W4A4\&8 quantization setting, achieves near-FP16 accuracy while reducing energy consumption by up to 84.66\% and shortening time steps by 93.75\%.

</details>

---

## 307. Redundancy-Free View Alignment for Multimodal Human Activity Recognition with Arbitrarily Missing Views

**Chinese Title**: Redundancy-Free View Alignment for Multimodal Human Activity Recognition with Arbitrarily Missing Views

**Authors**: Duc-Anh Nguyen, Nhien-An Le-Khac

**Date**: 2026-02-09 | **arXiv**: [2602.08755v2](http://arxiv.org/abs/2602.08755v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.08755v2)

**Categories**: cs.LG

<details><summary><b>Abstract</b></summary>

Multimodal multiview learning seeks to integrate information from diverse sources to enhance task performance. Existing approaches often struggle with flexible view configurations, including arbitrary view combinations, numbers of views, and heterogeneous modalities. Focusing on the context of human activity recognition, we propose RALIS, a model that combines multiview contrastive learning with a mixture-of-experts module to support arbitrary view availability during both training and inference. Instead of trying to reconstruct missing views, an adjusted center contrastive loss is used for self-supervised representation learning and view alignment, mitigating the impact of missing views on multiview fusion. This loss formulation allows for the integration of view weights to account for view quality. Additionally, it reduces computational complexity from $O(V^2)$ to $O(V)$, where $V$ is the number of views. To address residual discrepancies not captured by contrastive learning, we employ a mixture-of-experts module with a specialized load balancing strategy, tasked with adapting to arbitrary view combinations. We highlight the geometric relationship among components in our model and how they combine well in the latent space. RALIS is validated on four datasets encompassing inertial and human pose modalities, with the number of views ranging from three to nine, demonstrating its performance and flexibility.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Multimodal multiview learning seeks to integrate information from diverse sources to enhance task performance. Existing approaches often struggle with flexible view configurations, including arbitrary view combinations, numbers of views, and heterogeneous modalities.Focusing on the context of human activity recognition, we propose RALIS, a model that combines multiview contrastive learning with a mixture-of-experts module to support arbitrary view availability during both training and inference. Instead of trying to reconstruct missing views, an adjusted center contrastive loss is used for self-supervised representation learning and view alignment, mitigating the impact of missing views on multiview fusion.This loss formulation allows for the integration of view weights to account for view quality. Additionally, it reduces computational complexity from $O(V^2)$ to $O(V)$, where $V$ is the number of views. To address residual discrepancies not captured by contrastive learning, we employ a mixture-of-experts module with a specialized load balancing strategy, tasked with adapting to arbitrary view combinations.We highlight the geometric relationship among components in our model and how they combine well in the latent space. RALIS is validated on four datasets encompassing inertial and human pose modalities, with the number of views ranging from three to nine, demonstrating its performance and flexibility.

</details>

---

## 308. Central Dogma Transformer II: An AI Microscope for Understanding Cellular Regulatory Mechanisms

**Chinese Title**: Central Dogma Transformer II: An AI Microscope for Understanding Cellular Regulatory Mechanisms

**Authors**: Nobuyuki Ota

**Date**: 2026-02-09 | **arXiv**: [2602.08751v1](http://arxiv.org/abs/2602.08751v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08751v1)

**Categories**: cs.LG, q-bio.QM

<details><summary><b>Abstract</b></summary>

Current biological AI models lack interpretability -- their internal representations do not correspond to biological relationships that   researchers can examine. Here we present CDT-II, an "AI microscope" whose attention maps are directly interpretable as regulatory structure.   By mirroring the central dogma in its architecture, each attention mechanism corresponds to a specific biological relationship: DNA   self-attention for genomic relationships, RNA self-attention for gene co-regulation, and DNA-to-RNA cross-attention for transcriptional   control. Using only genomic embeddings and raw per-cell expression, CDT-II enables experimental biologists to observe regulatory networks in   their own data. Applied to K562 CRISPRi data, CDT-II predicts perturbation effects (per-gene mean $r = 0.84$) and recovers the GFI1B   regulatory network without supervision (6.6-fold enrichment, $P = 3.5 \times 10^{-17}$). Two distinct attention mechanisms converge on an RNA   processing module ($P = 1 \times 10^{-16}$). CDT-II establishes mechanism-oriented AI as an alternative to task-oriented approaches, revealing   regulatory structure rather than merely optimizing predictions.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Current biological AI models lack interpretability -- their internal representations do not correspond to biological relationships that   researchers can examine. Here we present CDT-II, an "AI microscope" whose attention maps are directly interpretable as regulatory structure.By mirroring the central dogma in its architecture, each attention mechanism corresponds to a specific biological relationship: DNA   self-attention for genomic relationships, RNA self-attention for gene co-regulation, and DNA-to-RNA cross-attention for transcriptional   control. Using only genomic embeddings and raw per-cell expression, CDT-II enables experimental biologists to observe regulatory networks in   their own data.Applied to K562 CRISPRi data, CDT-II predicts perturbation effects (per-gene mean $r = 0.84$) and recovers the GFI1B   regulatory network without supervision (6.6-fold enrichment, $P = 3.5 \times 10^{-17}$). Two distinct attention mechanisms converge on an RNA   processing module ($P = 1 \times 10^{-16}$).CDT-II establishes mechanism-oriented AI as an alternative to task-oriented approaches, revealing   regulatory structure rather than merely optimizing predictions.

</details>

---

## 309. SoK: The Pitfalls of Deep Reinforcement Learning for Cybersecurity

**Chinese Title**: SoK: The Pitfalls of Deep Reinforcement Learning for Cybersecurity

**Authors**: Shae McFadden, Myles Foley, Elizabeth Bates, Ilias Tsingenopoulos, Sanyam Vyas et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08690v1](http://arxiv.org/abs/2602.08690v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08690v1)

**Categories**: cs.LG, cs.CR

<details><summary><b>Abstract</b></summary>

Deep Reinforcement Learning (DRL) has achieved remarkable success in domains requiring sequential decision-making, motivating its application to cybersecurity problems. However, transitioning DRL from laboratory simulations to bespoke cyber environments can introduce numerous issues. This is further exacerbated by the often adversarial, non-stationary, and partially-observable nature of most cybersecurity tasks. In this paper, we identify and systematize 11 methodological pitfalls that frequently occur in DRL for cybersecurity (DRL4Sec) literature across the stages of environment modeling, agent training, performance evaluation, and system deployment. By analyzing 66 significant DRL4Sec papers (2018-2025), we quantify the prevalence of each pitfall and find an average of over five pitfalls per paper. We demonstrate the practical impact of these pitfalls using controlled experiments in (i) autonomous cyber defense, (ii) adversarial malware creation, and (iii) web security testing environments. Finally, we provide actionable recommendations for each pitfall to support the development of more rigorous and deployable DRL-based security systems.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Deep Reinforcement Learning (DRL) has achieved remarkable success in domains requiring sequential decision-making, motivating its application to cybersecurity problems. However, transitioning DRL from laboratory simulations to bespoke cyber environments can introduce numerous issues. This is further exacerbated by the often adversarial, non-stationary, and partially-observable nature of most cybersecurity tasks.In this paper, we identify and systematize 11 methodological pitfalls that frequently occur in DRL for cybersecurity (DRL4Sec) literature across the stages of environment modeling, agent training, performance evaluation, and system deployment. By analyzing 66 significant DRL4Sec papers (2018-2025), we quantify the prevalence of each pitfall and find an average of over five pitfalls per paper.We demonstrate the practical impact of these pitfalls using controlled experiments in (i) autonomous cyber defense, (ii) adversarial malware creation, and (iii) web security testing environments. Finally, we provide actionable recommendations for each pitfall to support the development of more rigorous and deployable DRL-based security systems.

</details>

---

## 310. The Theory and Practice of MAP Inference over Non-Convex Constraints

**Chinese Title**: The Theory and Practice of MAP Inference over Non-Convex Constraints

**Authors**: Leander Kurscheidt, Gabriele Masina, Roberto Sebastiani, Antonio Vergari

**Date**: 2026-02-09 | **arXiv**: [2602.08681v2](http://arxiv.org/abs/2602.08681v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.08681v2)

**Categories**: cs.LG, stat.ML

<details><summary><b>Abstract</b></summary>

In many safety-critical settings, probabilistic ML systems have to make predictions subject to algebraic constraints, e.g., predicting the most likely trajectory that does not cross obstacles. These real-world constraints are rarely convex, nor the densities considered are (log-)concave. This makes computing this constrained maximum a posteriori (MAP) prediction efficiently and reliably extremely challenging. In this paper, we first investigate under which conditions we can perform constrained MAP inference over continuous variables exactly and efficiently and devise a scalable message-passing algorithm for this tractable fragment. Then, we devise a general constrained MAP strategy that interleaves partitioning the domain into convex feasible regions with numerical constrained optimization. We evaluate both methods on synthetic and real-world benchmarks, showing our approaches outperform constraint-agnostic baselines, and scale to complex densities intractable for SoTA exact solvers.

</details>

<details><summary><b>Chinese Abstract</b></summary>

In many safety-critical settings, probabilistic ML systems have to make predictions subject to algebraic constraints, e.g., predicting the most likely trajectory that does not cross obstacles. These real-world constraints are rarely convex, nor the densities considered are (log-)concave. This makes computing this constrained maximum a posteriori (MAP) prediction efficiently and reliably extremely challenging.In this paper, we first investigate under which conditions we can perform constrained MAP inference over continuous variables exactly and efficiently and devise a scalable message-passing algorithm for this tractable fragment. Then, we devise a general constrained MAP strategy that interleaves partitioning the domain into convex feasible regions with numerical constrained optimization.We evaluate both methods on synthetic and real-world benchmarks, showing our approaches outperform constraint-agnostic baselines, and scale to complex densities intractable for SoTA exact solvers.

</details>

---

## 311. Constructive conditional normalizing flows

**Chinese Title**: Constructive conditional normalizing flows

**Authors**: Borjan Geshkovski, DomÃ¨nec Ruiz-Balet

**Date**: 2026-02-09 | **arXiv**: [2602.08606v1](http://arxiv.org/abs/2602.08606v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08606v1)

**Categories**: math.OC, cs.LG, math.AP, math.PR

<details><summary><b>Abstract</b></summary>

Motivated by applications in conditional sampling, given a probability measure $Î¼$ and a diffeomorphism $Ï†$, we consider the problem of simultaneously approximating $Ï†$ and the pushforward $Ï†_{\#}Î¼$ by means of the flow of a continuity equation whose velocity field is a perceptron neural network with piecewise constant weights. We provide an explicit construction based on a polar-like decomposition of the Lagrange interpolant of $Ï†$. The latter involves a compressible component, given by the gradient of a particular convex function, which can be realized exactly, and an incompressible component, which -- after approximating via permutations -- can be implemented through shear flows intrinsic to the continuity equation. For more regular maps $Ï†$ -- such as the KnÃ¶the-Rosenblatt rearrangement -- we provide an alternative, probabilistic construction inspired by the Maurey empirical method, in which the number of discontinuities in the weights doesn't scale inversely with the ambient dimension.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Motivated by applications in conditional sampling, given a probability measure $Î¼$ and a diffeomorphism $Ï†$, we consider the problem of simultaneously approximating $Ï†$ and the pushforward $Ï†_{\#}Î¼$ by means of the flow of a continuity equation whose velocity field is a perceptron neural network with piecewise constant weights. We provide an explicit construction based on a polar-like decomposition of the Lagrange interpolant of $Ï†$.The latter involves a compressible component, given by the gradient of a particular convex function, which can be realized exactly, and an incompressible component, which -- after approximating via permutations -- can be implemented through shear flows intrinsic to the continuity equation.For more regular maps $Ï†$ -- such as the KnÃ¶the-Rosenblatt rearrangement -- we provide an alternative, probabilistic construction inspired by the Maurey empirical method, in which the number of discontinuities in the weights doesn't scale inversely with the ambient dimension.

</details>

---

## 312. TFMLinker: Universal Link Predictor by Graph In-Context Learning with Tabular Foundation Models

**Chinese Title**: TFMLinker: Universal Link Predictor by Graph In-Context Learning with Tabular Foundation Models

**Authors**: Tianyin Liao, Chunyu Hu, Yicheng Sui, Xingxuan Zhang, Peng Cui et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08592v1](http://arxiv.org/abs/2602.08592v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08592v1)

**Categories**: cs.LG

<details><summary><b>Abstract</b></summary>

Link prediction is a fundamental task in graph machine learning with widespread applications such as recommendation systems, drug discovery, knowledge graphs, etc. In the foundation model era, how to develop universal link prediction methods across datasets and domains becomes a key problem, with some initial attempts adopting Graph Foundation Models utilizing Graph Neural Networks and Large Language Models. However, the existing methods face notable limitations, including limited pre-training scale or heavy reliance on textual information. Motivated by the success of tabular foundation models (TFMs) in achieving universal prediction across diverse tabular datasets, we explore an alternative approach by TFMs, which are pre-trained on diverse synthetic datasets sampled from structural causal models and support strong in-context learning independent of textual attributes. Nevertheless, adapting TFMs for link prediction faces severe technical challenges such as how to obtain the necessary context and capture link-centric topological information. To solve these challenges, we propose TFMLinker (Tabular Foundation Model for Link Predictor), aiming to leverage the in-context learning capabilities of TFMs to perform link prediction across diverse graphs without requiring dataset-specific fine-tuning. Specifically, we first develop a prototype-augmented local-global context module to construct context that captures both graph-specific and cross-graph transferable patterns. Next, we design a universal topology-aware link encoder to capture link-centric topological information and generate link representations as inputs for the TFM. Finally, we employ the TFM to predict link existence through in-context learning. Experiments on 6 graph benchmarks across diverse domains demonstrate the superiority of our method over state-of-the-art baselines without requiring dataset-specific finetuning.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Link prediction is a fundamental task in graph machine learning with widespread applications such as recommendation systems, drug discovery, knowledge graphs, etc. In the foundation model era, how to develop universal link prediction methods across datasets and domains becomes a key problem, with some initial attempts adopting Graph Foundation Models utilizing Graph Neural Networks and Large Language Models.However, the existing methods face notable limitations, including limited pre-training scale or heavy reliance on textual information.Motivated by the success of tabular foundation models (TFMs) in achieving universal prediction across diverse tabular datasets, we explore an alternative approach by TFMs, which are pre-trained on diverse synthetic datasets sampled from structural causal models and support strong in-context learning independent of textual attributes.Nevertheless, adapting TFMs for link prediction faces severe technical challenges such as how to obtain the necessary context and capture link-centric topological information. To solve these challenges, we propose TFMLinker (Tabular Foundation Model for Link Predictor), aiming to leverage the in-context learning capabilities of TFMs to perform link prediction across diverse graphs without requiring dataset-specific fine-tuning.Specifically, we first develop a prototype-augmented local-global context module to construct context that captures both graph-specific and cross-graph transferable patterns. Next, we design a universal topology-aware link encoder to capture link-centric topological information and generate link representations as inputs for the TFM. Finally, we employ the TFM to predict link existence through in-context learning.Experiments on 6 graph benchmarks across diverse domains demonstrate the superiority of our method over state-of-the-art baselines without requiring dataset-specific finetuning.

</details>

---

## 313. SDFed: Bridging Local Global Discrepancy via Subspace Refinement and Divergence Control in Federated Prompt Learning

**Chinese Title**: SDFed: Bridging Local Global Discrepancy via Subspace Refinement and Divergence Control in Federated Prompt Learning

**Authors**: Yicheng Di, Wei Yuan, Tieke He, Zhanjie Zhang, Ao Ma et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08590v1](http://arxiv.org/abs/2602.08590v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08590v1)

**Categories**: cs.LG, cs.DB

<details><summary><b>Abstract</b></summary>

Vision-language pretrained models offer strong transferable representations, yet adapting them in privacy-sensitive multi-party settings is challenging due to the high communication cost of federated optimization and the limited local data on clients. Federated prompt learning mitigates this issue by keeping the VLPM backbone frozen and collaboratively training lightweight prompt parameters. However, existing approaches typically enforce a unified prompt structure and length across clients, which is inadequate under practical client heterogeneity in both data distributions and system resources, and may further introduce conflicts between globally shared and locally optimal knowledge. To address these challenges, we propose \textbf{SDFed}, a heterogeneous federated prompt learning framework that bridges Local-Global Discrepancy via Subspace Refinement and Divergence Control. SDFed maintains a fixed-length global prompt for efficient aggregation while allowing each client to learn a variable-length local prompt to better match its data characteristics and capacity. To mitigate local-global conflicts and facilitate effective knowledge transfer, SDFed introduces a subspace refinement method for local prompts and an information retention and divergence control strategy that preserves key local information while maintaining appropriate separability between global and local representations. Extensive experiments on several datasets demonstrate that SDFed consistently improves performance and robustness in heterogeneous federated settings.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Vision-language pretrained models offer strong transferable representations, yet adapting them in privacy-sensitive multi-party settings is challenging due to the high communication cost of federated optimization and the limited local data on clients. Federated prompt learning mitigates this issue by keeping the VLPM backbone frozen and collaboratively training lightweight prompt parameters.However, existing approaches typically enforce a unified prompt structure and length across clients, which is inadequate under practical client heterogeneity in both data distributions and system resources, and may further introduce conflicts between globally shared and locally optimal knowledge.To address these challenges, we propose \textbf{SDFed}, a heterogeneous federated prompt learning framework that bridges Local-Global Discrepancy via Subspace Refinement and Divergence Control. SDFed maintains a fixed-length global prompt for efficient aggregation while allowing each client to learn a variable-length local prompt to better match its data characteristics and capacity.To mitigate local-global conflicts and facilitate effective knowledge transfer, SDFed introduces a subspace refinement method for local prompts and an information retention and divergence control strategy that preserves key local information while maintaining appropriate separability between global and local representations.Extensive experiments on several datasets demonstrate that SDFed consistently improves performance and robustness in heterogeneous federated settings.

</details>

---

## 314. Conditional Sequence Modeling for Safe Reinforcement Learning

**Chinese Title**: Conditional Sequence Modeling for Safe Reinforcement Learning

**Authors**: Wensong Bai, Chao Zhang, Qihang Xu, Chufan Chen, Chenhao Zhou et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08584v1](http://arxiv.org/abs/2602.08584v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08584v1)

**Categories**: cs.LG

<details><summary><b>Abstract</b></summary>

Offline safe reinforcement learning (RL) aims to learn policies from a fixed dataset while maximizing performance under cumulative cost constraints. In practice, deployment requirements often vary across scenarios, necessitating a single policy that can adapt zero-shot to different cost thresholds. However, most existing offline safe RL methods are trained under a pre-specified threshold, yielding policies with limited generalization and deployment flexibility across cost thresholds. Motivated by recent progress in conditional sequence modeling (CSM), which enables flexible goal-conditioned control by specifying target returns, we propose RCDT, a CSM-based method that supports zero-shot deployment across multiple cost thresholds within a single trained policy. RCDT is the first CSM-based offline safe RL algorithm that integrates a Lagrangian-style cost penalty with an auto-adaptive penalty coefficient. To avoid overly conservative behavior and achieve a more favorable return--cost trade-off, a reward--cost-aware trajectory reweighting mechanism and Q-value regularization are further incorporated. Extensive experiments on the DSRL benchmark demonstrate that RCDT consistently improves return--cost trade-offs over representative baselines, advancing the state-of-the-art in offline safe RL.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Offline safe reinforcement learning (RL) aims to learn policies from a fixed dataset while maximizing performance under cumulative cost constraints. In practice, deployment requirements often vary across scenarios, necessitating a single policy that can adapt zero-shot to different cost thresholds.However, most existing offline safe RL methods are trained under a pre-specified threshold, yielding policies with limited generalization and deployment flexibility across cost thresholds.Motivated by recent progress in conditional sequence modeling (CSM), which enables flexible goal-conditioned control by specifying target returns, we propose RCDT, a CSM-based method that supports zero-shot deployment across multiple cost thresholds within a single trained policy. RCDT is the first CSM-based offline safe RL algorithm that integrates a Lagrangian-style cost penalty with an auto-adaptive penalty coefficient.To avoid overly conservative behavior and achieve a more favorable return--cost trade-off, a reward--cost-aware trajectory reweighting mechanism and Q-value regularization are further incorporated. Extensive experiments on the DSRL benchmark demonstrate that RCDT consistently improves return--cost trade-offs over representative baselines, advancing the state-of-the-art in offline safe RL.

</details>

---

## 315. DNS: Data-driven Nonlinear Smoother for Complex Model-free Process

**Chinese Title**: DNS: Data-driven Nonlinear Smoother for Complex Model-free Process

**Authors**: Fredrik Cumlin, Anubhab Ghosh, Saikat Chatterjee

**Date**: 2026-02-09 | **arXiv**: [2602.08560v1](http://arxiv.org/abs/2602.08560v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08560v1)

**Categories**: eess.SP, cs.LG

<details><summary><b>Abstract</b></summary>

We propose data-driven nonlinear smoother (DNS) to estimate a hidden state sequence of a complex dynamical process from a noisy, linear measurement sequence. The dynamical process is model-free, that is, we do not have any knowledge of the nonlinear dynamics of the complex process. There is no state-transition model (STM) of the process available. The proposed DNS uses a recurrent architecture that helps to provide a closed-form posterior of the hidden state sequence given the measurement sequence. DNS learns in an unsupervised manner, meaning the training dataset consists of only measurement data and no state data. We demonstrate DNS using simulations for smoothing of several stochastic dynamical processes, including a benchmark Lorenz system. Experimental results show that the DNS is significantly better than a deep Kalman smoother (DKS) and an iterative data-driven nonlinear state estimation (iDANSE) smoother.

</details>

<details><summary><b>Chinese Abstract</b></summary>

We propose data-driven nonlinear smoother (DNS) to estimate a hidden state sequence of a complex dynamical process from a noisy, linear measurement sequence. The dynamical process is model-free, that is, we do not have any knowledge of the nonlinear dynamics of the complex process. There is no state-transition model (STM) of the process available.The proposed DNS uses a recurrent architecture that helps to provide a closed-form posterior of the hidden state sequence given the measurement sequence. DNS learns in an unsupervised manner, meaning the training dataset consists of only measurement data and no state data. We demonstrate DNS using simulations for smoothing of several stochastic dynamical processes, including a benchmark Lorenz system.Experimental results show that the DNS is significantly better than a deep Kalman smoother (DKS) and an iterative data-driven nonlinear state estimation (iDANSE) smoother.

</details>

---

## 316. Bridging Academia and Industry: A Comprehensive Benchmark for Attributed Graph Clustering

**Chinese Title**: Bridging Academia and Industry: A Comprehensive Benchmark for Attributed Graph Clustering

**Authors**: Yunhui Liu, Pengyu Qiu, Yu Xing, Yongchao Liu, Peng Du et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08519v1](http://arxiv.org/abs/2602.08519v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08519v1)

**Project**: https://pypi.org/project/pyagc),  **Code**: https://github.com/Cloudy1225/PyAGC),

**Categories**: cs.LG

<details><summary><b>Abstract</b></summary>

Attributed Graph Clustering (AGC) is a fundamental unsupervised task that integrates structural topology and node attributes to uncover latent patterns in graph-structured data. Despite its significance in industrial applications such as fraud detection and user segmentation, a significant chasm persists between academic research and real-world deployment. Current evaluation protocols suffer from the small-scale, high-homophily citation datasets, non-scalable full-batch training paradigms, and a reliance on supervised metrics that fail to reflect performance in label-scarce environments. To bridge these gaps, we present PyAGC, a comprehensive, production-ready benchmark and library designed to stress-test AGC methods across diverse scales and structural properties. We unify existing methodologies into a modular Encode-Cluster-Optimize framework and, for the first time, provide memory-efficient, mini-batch implementations for a wide array of state-of-the-art AGC algorithms. Our benchmark curates 12 diverse datasets, ranging from 2.7K to 111M nodes, specifically incorporating industrial graphs with complex tabular features and low homophily. Furthermore, we advocate for a holistic evaluation protocol that mandates unsupervised structural metrics and efficiency profiling alongside traditional supervised metrics. Battle-tested in high-stakes industrial workflows at Ant Group, this benchmark offers the community a robust, reproducible, and scalable platform to advance AGC research towards realistic deployment. The code and resources are publicly available via GitHub (https://github.com/Cloudy1225/PyAGC), PyPI (https://pypi.org/project/pyagc), and Documentation (https://pyagc.readthedocs.io).

</details>

<details><summary><b>Chinese Abstract</b></summary>

Attributed Graph Clustering (AGC) is a fundamental unsupervised task that integrates structural topology and node attributes to uncover latent patterns in graph-structured data. Despite its significance in industrial applications such as fraud detection and user segmentation, a significant chasm persists between academic research and real-world deployment.Current evaluation protocols suffer from the small-scale, high-homophily citation datasets, non-scalable full-batch training paradigms, and a reliance on supervised metrics that fail to reflect performance in label-scarce environments. To bridge these gaps, we present PyAGC, a comprehensive, production-ready benchmark and library designed to stress-test AGC methods across diverse scales and structural properties.We unify existing methodologies into a modular Encode-Cluster-Optimize framework and, for the first time, provide memory-efficient, mini-batch implementations for a wide array of state-of-the-art AGC algorithms. Our benchmark curates 12 diverse datasets, ranging from 2.7K to 111M nodes, specifically incorporating industrial graphs with complex tabular features and low homophily.Furthermore, we advocate for a holistic evaluation protocol that mandates unsupervised structural metrics and efficiency profiling alongside traditional supervised metrics. Battle-tested in high-stakes industrial workflows at Ant Group, this benchmark offers the community a robust, reproducible, and scalable platform to advance AGC research towards realistic deployment.The code and resources are publicly available via GitHub (https://github.com/Cloudy1225/PyAGC), PyPI (https://pypi.org/project/pyagc), and Documentation (https://pyagc.readthedocs.io).

</details>

---

## 317. Empirical Study of Observable Sets in Multiclass Quantum Classification

**Chinese Title**: Empirical Study of Observable Sets in Multiclass Quantum Classification

**Authors**: Paul San Sebastian, Mikel CaÃ±izo, Roman Orus

**Date**: 2026-02-09 | **arXiv**: [2602.08485v1](http://arxiv.org/abs/2602.08485v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08485v1)

**Categories**: quant-ph, cs.LG

<details><summary><b>Abstract</b></summary>

Variational quantum algorithms have gained attention as early applications of quantum computers for learning tasks. In the context of supervised learning, most of the works that tackle classification problems with parameterized quantum circuits constrain their scope to the setting of binary classification or perform multiclass classification via ensembles of binary classifiers (strategies such as one versus rest). Those few works that propose native multiclass models, however, do not justify the choice of observables that perform the classification. This work studies two main classification criteria in multiclass quantum machine learning: maximizing the expected value of an observable representing a class or maximizing the fidelity of the encoded quantum state with a reference state representing a class. To compare both approaches, sets of Pauli strings and sets of projectors into the computational basis are chosen as observables in the quantum machine learning models. Observing the empirical behavior of each model type, the effect of different observable set choices on the performance of quantum machine learning models is analyzed in the context of Barren Plateaus and Neural Collapse. The results provide insights that may guide the design of future multiclass quantum machine learning models.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Variational quantum algorithms have gained attention as early applications of quantum computers for learning tasks. In the context of supervised learning, most of the works that tackle classification problems with parameterized quantum circuits constrain their scope to the setting of binary classification or perform multiclass classification via ensembles of binary classifiers (strategies such as one versus rest).Those few works that propose native multiclass models, however, do not justify the choice of observables that perform the classification. This work studies two main classification criteria in multiclass quantum machine learning: maximizing the expected value of an observable representing a class or maximizing the fidelity of the encoded quantum state with a reference state representing a class.To compare both approaches, sets of Pauli strings and sets of projectors into the computational basis are chosen as observables in the quantum machine learning models. Observing the empirical behavior of each model type, the effect of different observable set choices on the performance of quantum machine learning models is analyzed in the context of Barren Plateaus and Neural Collapse.The results provide insights that may guide the design of future multiclass quantum machine learning models.

</details>

---

## 318. Time-Delayed Transformers for Data-Driven Modeling of Low-Dimensional Dynamics

**Chinese Title**: Time-Delayed Transformers for Data-Driven Modeling of Low-Dimensional Dynamics

**Authors**: Albert Alcalde, Markus Widhalm, Emre YÄ±lmaz

**Date**: 2026-02-09 | **arXiv**: [2602.08478v1](http://arxiv.org/abs/2602.08478v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08478v1)

**Categories**: cs.LG, math.DS, math.NA

<details><summary><b>Abstract</b></summary>

We propose the time-delayed transformer (TD-TF), a simplified transformer architecture for data-driven modeling of unsteady spatio-temporal dynamics. TD-TF bridges linear operator-based methods and deep sequence models by showing that a single-layer, single-head transformer can be interpreted as a nonlinear generalization of time-delayed dynamic mode decomposition (TD-DMD). The architecture is deliberately minimal, consisting of one self-attention layer with a single query per prediction and one feedforward layer, resulting in linear computational complexity in sequence length and a small parameter count. Numerical experiments demonstrate that TD-TF matches the performance of strong linear baselines on near-linear systems, while significantly outperforming them in nonlinear and chaotic regimes, where it accurately captures long-term dynamics. Validation studies on synthetic signals, unsteady aerodynamics, the Lorenz '63 system, and a reaction-diffusion model show that TD-TF preserves the interpretability and efficiency of linear models while providing substantially enhanced expressive power for complex dynamics.

</details>

<details><summary><b>Chinese Abstract</b></summary>

We propose the time-delayed transformer (TD-TF), a simplified transformer architecture for data-driven modeling of unsteady spatio-temporal dynamics. TD-TF bridges linear operator-based methods and deep sequence models by showing that a single-layer, single-head transformer can be interpreted as a nonlinear generalization of time-delayed dynamic mode decomposition (TD-DMD).The architecture is deliberately minimal, consisting of one self-attention layer with a single query per prediction and one feedforward layer, resulting in linear computational complexity in sequence length and a small parameter count.Numerical experiments demonstrate that TD-TF matches the performance of strong linear baselines on near-linear systems, while significantly outperforming them in nonlinear and chaotic regimes, where it accurately captures long-term dynamics.Validation studies on synthetic signals, unsteady aerodynamics, the Lorenz '63 system, and a reaction-diffusion model show that TD-TF preserves the interpretability and efficiency of linear models while providing substantially enhanced expressive power for complex dynamics.

</details>

---

## 319. USBD: Universal Structural Basis Distillation for Source-Free Graph Domain Adaptation

**Chinese Title**: USBD: Universal Structural Basis Distillation for Source-Free Graph Domain Adaptation

**Authors**: Yingxu Wang, Kunyu Zhang, Mengzhu Wang, Siyang Gao, Nan Yin

**Date**: 2026-02-09 | **arXiv**: [2602.08431v1](http://arxiv.org/abs/2602.08431v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08431v1)

**Categories**: cs.LG

<details><summary><b>Abstract</b></summary>

SF-GDA is pivotal for privacy-preserving knowledge transfer across graph datasets. Although recent works incorporate structural information, they implicitly condition adaptation on the smoothness priors of sourcetrained GNNs, thereby limiting their generalization to structurally distinct targets. This dependency becomes a critical bottleneck under significant topological shifts, where the source model misinterprets distinct topological patterns unseen in the source domain as noise, rendering pseudo-label-based adaptation unreliable. To overcome this limitation, we propose the Universal Structural Basis Distillation, a framework that shifts the paradigm from adapting a biased model to learning a universal structural basis for SF-GDA. Instead of adapting a biased source model to a specific target, our core idea is to construct a structure-agnostic basis that proactively covers the full spectrum of potential topological patterns. Specifically, USBD employs a bi-level optimization framework to distill the source dataset into a compact structural basis. By enforcing the prototypes to span the full Dirichlet energy spectrum, the learned basis explicitly captures diverse topological motifs, ranging from low-frequency clusters to high-frequency chains, beyond those present in the source. This ensures that the learned basis creates a comprehensive structural covering capable of handling targets with disparate structures. For inference, we introduce a spectral-aware ensemble mechanism that dynamically activates the optimal prototype combination based on the spectral fingerprint of the target graph. Extensive experiments on benchmarks demonstrate that USBD significantly outperforms state-of-the-art methods, particularly in scenarios with severe structural shifts, while achieving superior computational efficiency by decoupling the adaptation cost from the target data scale.

</details>

<details><summary><b>Chinese Abstract</b></summary>

SF-GDA is pivotal for privacy-preserving knowledge transfer across graph datasets. Although recent works incorporate structural information, they implicitly condition adaptation on the smoothness priors of sourcetrained GNNs, thereby limiting their generalization to structurally distinct targets.This dependency becomes a critical bottleneck under significant topological shifts, where the source model misinterprets distinct topological patterns unseen in the source domain as noise, rendering pseudo-label-based adaptation unreliable. To overcome this limitation, we propose the Universal Structural Basis Distillation, a framework that shifts the paradigm from adapting a biased model to learning a universal structural basis for SF-GDA.Instead of adapting a biased source model to a specific target, our core idea is to construct a structure-agnostic basis that proactively covers the full spectrum of potential topological patterns. Specifically, USBD employs a bi-level optimization framework to distill the source dataset into a compact structural basis.By enforcing the prototypes to span the full Dirichlet energy spectrum, the learned basis explicitly captures diverse topological motifs, ranging from low-frequency clusters to high-frequency chains, beyond those present in the source. This ensures that the learned basis creates a comprehensive structural covering capable of handling targets with disparate structures.For inference, we introduce a spectral-aware ensemble mechanism that dynamically activates the optimal prototype combination based on the spectral fingerprint of the target graph.Extensive experiments on benchmarks demonstrate that USBD significantly outperforms state-of-the-art methods, particularly in scenarios with severe structural shifts, while achieving superior computational efficiency by decoupling the adaptation cost from the target data scale.

</details>

---

## 320. The Connection between Kriging and Large Neural Networks

**Chinese Title**: The Connection between Kriging and Large Neural Networks

**Authors**: Marius Marinescu

**Date**: 2026-02-09 | **arXiv**: [2602.08427v1](http://arxiv.org/abs/2602.08427v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08427v1)

**Categories**: cs.LG, math.ST

<details><summary><b>Abstract</b></summary>

AI has impacted many disciplines and is nowadays ubiquitous. In particular, spatial statistics is in a pivotal moment where it will increasingly intertwine with AI. In this scenario, a relevant question is what relationship spatial statistics models have with machine learning (ML) models, if any. In particular, in this paper, we explore the connections between Kriging and neural networks. At first glance, they may appear unrelated. Kriging - and its ML counterpart, Gaussian process regression - are grounded in probability theory and stochastic processes, whereas many ML models are extensively considered Black-Box models. Nevertheless, they are strongly related. We study their connections and revisit the relevant literature. The understanding of their relations and the combination of both perspectives may enhance ML techniques by making them more interpretable, reliable, and spatially aware.

</details>

<details><summary><b>Chinese Abstract</b></summary>

AI has impacted many disciplines and is nowadays ubiquitous. In particular, spatial statistics is in a pivotal moment where it will increasingly intertwine with AI. In this scenario, a relevant question is what relationship spatial statistics models have with machine learning (ML) models, if any. In particular, in this paper, we explore the connections between Kriging and neural networks. At first glance, they may appear unrelated.Kriging - and its ML counterpart, Gaussian process regression - are grounded in probability theory and stochastic processes, whereas many ML models are extensively considered Black-Box models. Nevertheless, they are strongly related. We study their connections and revisit the relevant literature.The understanding of their relations and the combination of both perspectives may enhance ML techniques by making them more interpretable, reliable, and spatially aware.

</details>

---

## 321. Radial MÃ¼ntz-SzÃ¡sz Networks: Neural Architectures with Learnable Power Bases for Multidimensional Singularities

**Chinese Title**: Radial MÃ¼ntz-SzÃ¡sz Networks: Neural Architectures with Learnable Power Bases for Multidimensional Singularities

**Authors**: Gnankan Landry Regis N'guessan, Bum Jun Kim

**Date**: 2026-02-09 | **arXiv**: [2602.08419v1](http://arxiv.org/abs/2602.08419v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08419v1)

**Categories**: cs.LG, math.NA

<details><summary><b>Abstract</b></summary>

Radial singular fields, such as $1/r$, $\log r$, and crack-tip profiles, are difficult to model for coordinate-separable neural architectures. We show that any $C^2$ function that is both radial and additively separable must be quadratic, establishing a fundamental obstruction for coordinate-wise power-law models. Motivated by this result, we introduce Radial MÃ¼ntz-SzÃ¡sz Networks (RMN), which represent fields as linear combinations of learnable radial powers $r^Î¼$, including negative exponents, together with a limit-stable log-primitive for exact $\log r$ behavior. RMN admits closed-form spatial gradients and Laplacians, enabling physics-informed learning on punctured domains. Across ten 2D and 3D benchmarks, RMN achieves 1.5$\times$--51$\times$ lower RMSE than MLPs and 10$\times$--100$\times$ lower RMSE than SIREN while using 27 parameters, compared with 33,537 for MLPs and 8,577 for SIREN. We extend RMN to angular dependence (RMN-Angular) and to multiple sources with learnable centers (RMN-MC); when optimization converges, source-center recovery errors fall below $10^{-4}$. We also report controlled failures on smooth, strongly non-radial targets to delineate RMN's operating regime.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Radial singular fields, such as $1/r$, $\log r$, and crack-tip profiles, are difficult to model for coordinate-separable neural architectures. We show that any $C^2$ function that is both radial and additively separable must be quadratic, establishing a fundamental obstruction for coordinate-wise power-law models.Motivated by this result, we introduce Radial MÃ¼ntz-SzÃ¡sz Networks (RMN), which represent fields as linear combinations of learnable radial powers $r^Î¼$, including negative exponents, together with a limit-stable log-primitive for exact $\log r$ behavior. RMN admits closed-form spatial gradients and Laplacians, enabling physics-informed learning on punctured domains.Across ten 2D and 3D benchmarks, RMN achieves 1.5$\times$--51$\times$ lower RMSE than MLPs and 10$\times$--100$\times$ lower RMSE than SIREN while using 27 parameters, compared with 33,537 for MLPs and 8,577 for SIREN. We extend RMN to angular dependence (RMN-Angular) and to multiple sources with learnable centers (RMN-MC); when optimization converges, source-center recovery errors fall below $10^{-4}$.We also report controlled failures on smooth, strongly non-radial targets to delineate RMN's operating regime.

</details>

---

## 322. PACC: Protocol-Aware Cross-Layer Compression for Compact Network Traffic Representation

**Chinese Title**: PACC: Protocol-Aware Cross-Layer Compression for Compact Network Traffic Representation

**Authors**: Zhaochen Guo, Tianyufei Zhou, Honghao Wang, Ronghua Li, Shinan Liu

**Date**: 2026-02-09 | **arXiv**: [2602.08331v1](http://arxiv.org/abs/2602.08331v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08331v1)

**Categories**: cs.NI, cs.LG

<details><summary><b>Abstract</b></summary>

Network traffic classification is a core primitive for network security and management, yet it is increasingly challenged by pervasive encryption and evolving protocols. A central bottleneck is representation: hand-crafted flow statistics are efficient but often too lossy, raw-bit encodings can be accurate but are costly, and recent pre-trained embeddings provide transfer but frequently flatten the protocol stack and entangle signals across layers. We observe that real traffic contains substantial redundancy both across network layers and within each layer; existing paradigms do not explicitly identify and remove this redundancy, leading to wasted capacity, shortcut learning, and degraded generalization. To address this, we propose PACC, a redundancy-aware, layer-aware representation framework. PACC treats the protocol stack as multi-view inputs and learns compact layer-wise projections that remain faithful to each layer while explicitly factorizing representations into shared (cross-layer) and private (layer-specific) components. We operationalize these goals with a joint objective that preserves layer-specific information via reconstruction, captures shared structure via contrastive mutual-information learning, and maximizes task-relevant information via supervised losses, yielding compact latents suitable for efficient inference. Across datasets covering encrypted application classification, IoT device identification, and intrusion detection, PACC consistently outperforms feature-engineered and raw-bit baselines. On encrypted subsets, it achieves up to a 12.9% accuracy improvement over nPrint. PACC matches or surpasses strong foundation-model baselines. At the same time, it improves end-to-end efficiency by up to 3.16x.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Network traffic classification is a core primitive for network security and management, yet it is increasingly challenged by pervasive encryption and evolving protocols.A central bottleneck is representation: hand-crafted flow statistics are efficient but often too lossy, raw-bit encodings can be accurate but are costly, and recent pre-trained embeddings provide transfer but frequently flatten the protocol stack and entangle signals across layers.We observe that real traffic contains substantial redundancy both across network layers and within each layer; existing paradigms do not explicitly identify and remove this redundancy, leading to wasted capacity, shortcut learning, and degraded generalization. To address this, we propose PACC, a redundancy-aware, layer-aware representation framework.PACC treats the protocol stack as multi-view inputs and learns compact layer-wise projections that remain faithful to each layer while explicitly factorizing representations into shared (cross-layer) and private (layer-specific) components.We operationalize these goals with a joint objective that preserves layer-specific information via reconstruction, captures shared structure via contrastive mutual-information learning, and maximizes task-relevant information via supervised losses, yielding compact latents suitable for efficient inference.Across datasets covering encrypted application classification, IoT device identification, and intrusion detection, PACC consistently outperforms feature-engineered and raw-bit baselines. On encrypted subsets, it achieves up to a 12.9% accuracy improvement over nPrint. PACC matches or surpasses strong foundation-model baselines. At the same time, it improves end-to-end efficiency by up to 3.16x.

</details>

---

## 323. Towards Efficient Large Language Reasoning Models via Extreme-Ratio Chain-of-Thought Compression

**Chinese Title**: Towards Efficient Large Language Reasoning Models via Extreme-Ratio Chain-of-Thought Compression

**Authors**: Yuntian Tang, Bohan Jia, Wenxuan Huang, Lianyue Zhang, Jiao Xie et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08324v1](http://arxiv.org/abs/2602.08324v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08324v1)

**Categories**: cs.LG

<details><summary><b>Abstract</b></summary>

Chain-of-Thought (CoT) reasoning successfully enhances the reasoning capabilities of Large Language Models (LLMs), yet it incurs substantial computational overhead for inference. Existing CoT compression methods often suffer from a critical loss of logical fidelity at high compression ratios, resulting in significant performance degradation. To achieve high-fidelity, fast reasoning, we propose a novel EXTreme-RAtio Chain-of-Thought Compression framework, termed Extra-CoT, which aggressively reduces the token budget while preserving answer accuracy. To generate reliable, high-fidelity supervision, we first train a dedicated semantically-preserved compressor on mathematical CoT data with fine-grained annotations. An LLM is then fine-tuned on these compressed pairs via a mixed-ratio supervised fine-tuning (SFT), teaching it to follow a spectrum of compression budgets and providing a stable initialization for reinforcement learning (RL). We further propose Constrained and Hierarchical Ratio Policy Optimization (CHRPO) to explicitly incentivize question-solving ability under lower budgets by a hierarchical reward. Experiments on three mathematical reasoning benchmarks show the superiority of Extra-CoT. For example, on MATH-500 using Qwen3-1.7B, Extra-CoT achieves over 73\% token reduction with an accuracy improvement of 0.6\%, significantly outperforming state-of-the-art (SOTA) methods.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Chain-of-Thought (CoT) reasoning successfully enhances the reasoning capabilities of Large Language Models (LLMs), yet it incurs substantial computational overhead for inference. Existing CoT compression methods often suffer from a critical loss of logical fidelity at high compression ratios, resulting in significant performance degradation.To achieve high-fidelity, fast reasoning, we propose a novel EXTreme-RAtio Chain-of-Thought Compression framework, termed Extra-CoT, which aggressively reduces the token budget while preserving answer accuracy. To generate reliable, high-fidelity supervision, we first train a dedicated semantically-preserved compressor on mathematical CoT data with fine-grained annotations.An LLM is then fine-tuned on these compressed pairs via a mixed-ratio supervised fine-tuning (SFT), teaching it to follow a spectrum of compression budgets and providing a stable initialization for reinforcement learning (RL). We further propose Constrained and Hierarchical Ratio Policy Optimization (CHRPO) to explicitly incentivize question-solving ability under lower budgets by a hierarchical reward.Experiments on three mathematical reasoning benchmarks show the superiority of Extra-CoT. For example, on MATH-500 using Qwen3-1.7B, Extra-CoT achieves over 73\% token reduction with an accuracy improvement of 0.6\%, significantly outperforming state-of-the-art (SOTA) methods.

</details>

---

## 324. CADO: From Imitation to Cost Minimization for Heatmap-based Solvers in Combinatorial Optimization

**Chinese Title**: CADO: From Imitation to Cost Minimization for Heatmap-based Solvers in Combinatorial Optimization

**Authors**: Hyungseok Song, Deunsol Yoon, Kanghoon Lee, Han-Seul Jeong, Soonyoung Lee et al.

**Date**: 2026-02-09 | **arXiv**: [2602.08210v1](http://arxiv.org/abs/2602.08210v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08210v1)

**Categories**: cs.LG, stat.ML

<details><summary><b>Abstract</b></summary>

Heatmap-based solvers have emerged as a promising paradigm for Combinatorial Optimization (CO). However, we argue that the dominant Supervised Learning (SL) training paradigm suffers from a fundamental objective mismatch: minimizing imitation loss (e.g., cross-entropy) does not guarantee solution cost minimization. We dissect this mismatch into two deficiencies: Decoder-Blindness (being oblivious to the non-differentiable decoding process) and Cost-Blindness (prioritizing structural imitation over solution quality). We empirically demonstrate that these intrinsic flaws impose a hard performance ceiling. To overcome this limitation, we propose CADO (Cost-Aware Diffusion models for Optimization), a streamlined Reinforcement Learning fine-tuning framework that formulates the diffusion denoising process as an MDP to directly optimize the post-decoded solution cost. We introduce Label-Centered Reward, which repurposes ground-truth labels as unbiased baselines rather than imitation targets, and Hybrid Fine-Tuning for parameter-efficient adaptation. CADO achieves state-of-the-art performance across diverse benchmarks, validating that objective alignment is essential for unlocking the full potential of heatmap-based solvers.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Heatmap-based solvers have emerged as a promising paradigm for Combinatorial Optimization (CO). However, we argue that the dominant Supervised Learning (SL) training paradigm suffers from a fundamental objective mismatch: minimizing imitation loss (e.g., cross-entropy) does not guarantee solution cost minimization.We dissect this mismatch into two deficiencies: Decoder-Blindness (being oblivious to the non-differentiable decoding process) and Cost-Blindness (prioritizing structural imitation over solution quality). We empirically demonstrate that these intrinsic flaws impose a hard performance ceiling.To overcome this limitation, we propose CADO (Cost-Aware Diffusion models for Optimization), a streamlined Reinforcement Learning fine-tuning framework that formulates the diffusion denoising process as an MDP to directly optimize the post-decoded solution cost. We introduce Label-Centered Reward, which repurposes ground-truth labels as unbiased baselines rather than imitation targets, and Hybrid Fine-Tuning for parameter-efficient adaptation.CADO achieves state-of-the-art performance across diverse benchmarks, validating that objective alignment is essential for unlocking the full potential of heatmap-based solvers.

</details>

---

## 325. Information Geometry of Absorbing Markov-Chain and Discriminative Random Walks

**Chinese Title**: Information Geometry of Absorbing Markov-Chain and Discriminative Random Walks

**Authors**: Masanari Kimura

**Date**: 2026-02-09 | **arXiv**: [2602.08185v1](http://arxiv.org/abs/2602.08185v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08185v1)

**Categories**: stat.ML, cs.LG

<details><summary><b>Abstract</b></summary>

Discriminative Random Walks (DRWs) are a simple yet powerful tool for semi-supervised node classification, but their theoretical foundations remain fragmentary. We revisit DRWs through the lens of information geometry, treating the family of class-specific hitting-time laws on an absorbing Markov chain as a statistical manifold. Starting from a log-linear edge-weight model, we derive closed-form expressions for the hitting-time probability mass function, its full moment hierarchy, and the observed Fisher information. The Fisher matrix of each seed node turns out to be rank-one, taking the quotient by its null space yields a low-dimensional, globally flat manifold that captures all identifiable directions of the model. Leveraging the geometry, we introduce a sensitivity score for unlabeled nodes that bounds, and in one-dimensional cases attains, the maximal first-order change in DRW betweenness under unit Fisher perturbations. The score can lead to principled strategies for active label acquisition, edge re-weighting, and explanation.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Discriminative Random Walks (DRWs) are a simple yet powerful tool for semi-supervised node classification, but their theoretical foundations remain fragmentary. We revisit DRWs through the lens of information geometry, treating the family of class-specific hitting-time laws on an absorbing Markov chain as a statistical manifold.Starting from a log-linear edge-weight model, we derive closed-form expressions for the hitting-time probability mass function, its full moment hierarchy, and the observed Fisher information. The Fisher matrix of each seed node turns out to be rank-one, taking the quotient by its null space yields a low-dimensional, globally flat manifold that captures all identifiable directions of the model.Leveraging the geometry, we introduce a sensitivity score for unlabeled nodes that bounds, and in one-dimensional cases attains, the maximal first-order change in DRW betweenness under unit Fisher perturbations. The score can lead to principled strategies for active label acquisition, edge re-weighting, and explanation.

</details>

---

## 326. Nansde-net: A neural sde framework for generating time series with memory

**Chinese Title**: Nansde-net: A neural sde framework for generating time series with memory

**Authors**: Hiromu Ozai, Kei Nakagawa

**Date**: 2026-02-09 | **arXiv**: [2602.08182v1](http://arxiv.org/abs/2602.08182v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08182v1)

**Categories**: cs.LG, q-fin.CP, q-fin.ST

<details><summary><b>Abstract</b></summary>

Modeling time series with long- or short-memory characteristics is a fundamental challenge in many scientific and engineering domains. While fractional Brownian motion has been widely used as a noise source to capture such memory effects, its incompatibility with ItÃ´ calculus limits its applicability in neural stochastic differential equation~(SDE) frameworks. In this paper, we propose a novel class of noise, termed Neural Network-kernel ARMA-type noise~(NA-noise), which is an ItÃ´-process-based alternative capable of capturing both long- and short-memory behaviors. The kernel function defining the noise structure is parameterized via neural networks and decomposed into a product form to preserve the Markov property. Based on this noise process, we develop NANSDE-Net, a generative model that extends Neural SDEs by incorporating NA-noise. We prove the theoretical existence and uniqueness of the solution under mild conditions and derive an efficient backpropagation scheme for training. Empirical results on both synthetic and real-world datasets demonstrate that NANSDE-Net matches or outperforms existing models, including fractional SDE-Net, in reproducing long- and short-memory features of the data, while maintaining computational tractability within the ItÃ´ calculus framework.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Modeling time series with long- or short-memory characteristics is a fundamental challenge in many scientific and engineering domains. While fractional Brownian motion has been widely used as a noise source to capture such memory effects, its incompatibility with ItÃ´ calculus limits its applicability in neural stochastic differential equation~(SDE) frameworks.In this paper, we propose a novel class of noise, termed Neural Network-kernel ARMA-type noise~(NA-noise), which is an ItÃ´-process-based alternative capable of capturing both long- and short-memory behaviors. The kernel function defining the noise structure is parameterized via neural networks and decomposed into a product form to preserve the Markov property.Based on this noise process, we develop NANSDE-Net, a generative model that extends Neural SDEs by incorporating NA-noise. We prove the theoretical existence and uniqueness of the solution under mild conditions and derive an efficient backpropagation scheme for training.Empirical results on both synthetic and real-world datasets demonstrate that NANSDE-Net matches or outperforms existing models, including fractional SDE-Net, in reproducing long- and short-memory features of the data, while maintaining computational tractability within the ItÃ´ calculus framework.

</details>

---

## 327. A Causal Machine Learning Framework for Treatment Personalization in Clinical Trials: Application to Ulcerative Colitis

**Chinese Title**: A Causal Machine Learning Framework for Treatment Personalization in Clinical Trials: Application to Ulcerative Colitis

**Authors**: Cristian Minoccheri, Sophia Tesic, Kayvan Najarian, Ryan Stidham

**Date**: 2026-02-09 | **arXiv**: [2602.08171v1](http://arxiv.org/abs/2602.08171v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08171v1)

**Categories**: cs.LG, stat.AP, stat.ME

<details><summary><b>Abstract</b></summary>

Randomized controlled trials estimate average treatment effects, but treatment response heterogeneity motivates personalized approaches. A critical question is whether statistically detectable heterogeneity translates into improved treatment decisions -- these are distinct questions that can yield contradictory answers. We present a modular causal machine learning framework that evaluates each question separately: permutation importance identifies which features predict heterogeneity, best linear predictor (BLP) testing assesses statistical significance, and doubly robust policy evaluation measures whether acting on the heterogeneity improves patient outcomes. We apply this framework to patient-level data from the UNIFI maintenance trial of ustekinumab in ulcerative colitis, comparing placebo, standard-dose ustekinumab every 12 weeks, and dose-intensified ustekinumab every 8 weeks, using cross-fitted X-learner models with baseline demographics, medication history, week-8 clinical scores, laboratory biomarkers, and video-derived endoscopic features. BLP testing identified strong associations between endoscopic features and treatment effect heterogeneity for ustekinumab versus placebo, yet doubly robust policy evaluation showed no improvement in expected remission from incorporating endoscopic features, and out-of-fold multi-arm evaluation showed worse performance. Diagnostic comparison of prognostic contribution against policy value revealed that endoscopic scores behaved as disease severity markers -- improving outcome prediction in untreated patients but adding noise to treatment selection -- while clinical variables (fecal calprotectin, age, CRP) captured the decision-relevant variation. These results demonstrate that causal machine learning applications to clinical trials should include policy-level evaluation alongside heterogeneity testing.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Randomized controlled trials estimate average treatment effects, but treatment response heterogeneity motivates personalized approaches. A critical question is whether statistically detectable heterogeneity translates into improved treatment decisions -- these are distinct questions that can yield contradictory answers.We present a modular causal machine learning framework that evaluates each question separately: permutation importance identifies which features predict heterogeneity, best linear predictor (BLP) testing assesses statistical significance, and doubly robust policy evaluation measures whether acting on the heterogeneity improves patient outcomes.We apply this framework to patient-level data from the UNIFI maintenance trial of ustekinumab in ulcerative colitis, comparing placebo, standard-dose ustekinumab every 12 weeks, and dose-intensified ustekinumab every 8 weeks, using cross-fitted X-learner models with baseline demographics, medication history, week-8 clinical scores, laboratory biomarkers, and video-derived endoscopic features.BLP testing identified strong associations between endoscopic features and treatment effect heterogeneity for ustekinumab versus placebo, yet doubly robust policy evaluation showed no improvement in expected remission from incorporating endoscopic features, and out-of-fold multi-arm evaluation showed worse performance.Diagnostic comparison of prognostic contribution against policy value revealed that endoscopic scores behaved as disease severity markers -- improving outcome prediction in untreated patients but adding noise to treatment selection -- while clinical variables (fecal calprotectin, age, CRP) captured the decision-relevant variation.These results demonstrate that causal machine learning applications to clinical trials should include policy-level evaluation alongside heterogeneity testing.

</details>

---

## 328. Robustness of Vision Language Models Against Split-Image Harmful Input Attacks

**Chinese Title**: Robustness of Vision Language Models Against Split-Image Harmful Input Attacks

**Authors**: Md Rafi Ur Rashid, MD Sadik Hossain Shanto, Vishnu Asutosh Dasu, Shagufta Mehnaz

**Date**: 2026-02-08 | **arXiv**: [2602.08136v1](http://arxiv.org/abs/2602.08136v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08136v1)

**Categories**: cs.CV, cs.AI

<details><summary><b>Abstract</b></summary>

Vision-Language Models (VLMs) are now a core part of modern AI. Recent work proposed several visual jailbreak attacks using single/ holistic images. However, contemporary VLMs demonstrate strong robustness against such attacks due to extensive safety alignment through preference optimization (e.g., RLHF). In this work, we identify a new vulnerability: while VLM pretraining and instruction tuning generalize well to split-image inputs, safety alignment is typically performed only on holistic images and does not account for harmful semantics distributed across multiple image fragments. Consequently, VLMs often fail to detect and refuse harmful split-image inputs, where unsafe cues emerge only after combining images. We introduce novel split-image visual jailbreak attacks (SIVA) that exploit this misalignment. Unlike prior optimization-based attacks, which exhibit poor black-box transferability due to architectural and prior mismatches across models, our attacks evolve in progressive phases from naive splitting to an adaptive white-box attack, culminating in a black-box transfer attack. Our strongest strategy leverages a novel adversarial knowledge distillation (Adv-KD) algorithm to substantially improve cross-model transferability. Evaluations on three state-of-the-art modern VLMs and three jailbreak datasets demonstrate that our strongest attack achieves up to 60% higher transfer success than existing baselines. Lastly, we propose efficient ways to address this critical vulnerability in the current VLM safety alignment.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Vision-Language Models (VLMs) are now a core part of modern AI. Recent work proposed several visual jailbreak attacks using single/ holistic images. However, contemporary VLMs demonstrate strong robustness against such attacks due to extensive safety alignment through preference optimization (e.g., RLHF).In this work, we identify a new vulnerability: while VLM pretraining and instruction tuning generalize well to split-image inputs, safety alignment is typically performed only on holistic images and does not account for harmful semantics distributed across multiple image fragments. Consequently, VLMs often fail to detect and refuse harmful split-image inputs, where unsafe cues emerge only after combining images.We introduce novel split-image visual jailbreak attacks (SIVA) that exploit this misalignment. Unlike prior optimization-based attacks, which exhibit poor black-box transferability due to architectural and prior mismatches across models, our attacks evolve in progressive phases from naive splitting to an adaptive white-box attack, culminating in a black-box transfer attack.Our strongest strategy leverages a novel adversarial knowledge distillation (Adv-KD) algorithm to substantially improve cross-model transferability. Evaluations on three state-of-the-art modern VLMs and three jailbreak datasets demonstrate that our strongest attack achieves up to 60% higher transfer success than existing baselines. Lastly, we propose efficient ways to address this critical vulnerability in the current VLM safety alignment.

</details>

---

## 329. MambaFusion: Adaptive State-Space Fusion for Multimodal 3D Object Detection

**Chinese Title**: MambaFusion: Adaptive State-Space Fusion for Multimodal 3D Object Detection

**Authors**: Venkatraman Narayanan, Bala Sai, Rahul Ahuja, Pratik Likhar, Varun Ravi Kumar et al.

**Date**: 2026-02-08 | **arXiv**: [2602.08126v1](http://arxiv.org/abs/2602.08126v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08126v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Reliable 3D object detection is fundamental to autonomous driving, and multimodal fusion algorithms using cameras and LiDAR remain a persistent challenge. Cameras provide dense visual cues but ill posed depth; LiDAR provides a precise 3D structure but sparse coverage. Existing BEV-based fusion frameworks have made good progress, but they have difficulties including inefficient context modeling, spatially invariant fusion, and reasoning under uncertainty. We introduce MambaFusion, a unified multi-modal detection framework that achieves efficient, adaptive, and physically grounded 3D perception. MambaFusion interleaves selective state-space models (SSMs) with windowed transformers to propagate the global context in linear time while preserving local geometric fidelity. A multi-modal token alignment (MTA) module and reliability-aware fusion gates dynamically re-weight camera-LiDAR features based on spatial confidence and calibration consistency. Finally, a structure-conditioned diffusion head integrates graph-based reasoning with uncertainty-aware denoising, enforcing physical plausibility, and calibrated confidence. MambaFusion establishes new state-of-the-art performance on nuScenes benchmarks while operating with linear-time complexity. The framework demonstrates that coupling SSM-based efficiency with reliability-driven fusion yields robust, temporally stable, and interpretable 3D perception for real-world autonomous driving systems.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Reliable 3D object detection is fundamental to autonomous driving, and multimodal fusion algorithms using cameras and LiDAR remain a persistent challenge. Cameras provide dense visual cues but ill posed depth; LiDAR provides a precise 3D structure but sparse coverage.Existing BEV-based fusion frameworks have made good progress, but they have difficulties including inefficient context modeling, spatially invariant fusion, and reasoning under uncertainty. We introduce MambaFusion, a unified multi-modal detection framework that achieves efficient, adaptive, and physically grounded 3D perception.MambaFusion interleaves selective state-space models (SSMs) with windowed transformers to propagate the global context in linear time while preserving local geometric fidelity. A multi-modal token alignment (MTA) module and reliability-aware fusion gates dynamically re-weight camera-LiDAR features based on spatial confidence and calibration consistency.Finally, a structure-conditioned diffusion head integrates graph-based reasoning with uncertainty-aware denoising, enforcing physical plausibility, and calibrated confidence. MambaFusion establishes new state-of-the-art performance on nuScenes benchmarks while operating with linear-time complexity.The framework demonstrates that coupling SSM-based efficiency with reliability-driven fusion yields robust, temporally stable, and interpretable 3D perception for real-world autonomous driving systems.

</details>

---

## 330. Building Damage Detection using Satellite Images and Patch-Based Transformer Methods

**Chinese Title**: Building Damage Detection using Satellite Images and Patch-Based Transformer Methods

**Authors**: Smriti Siva, Jan Cross-Zamirski

**Date**: 2026-02-08 | **arXiv**: [2602.08117v1](http://arxiv.org/abs/2602.08117v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08117v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Rapid building damage assessment is critical for post-disaster response. Damage classification models built on satellite imagery provide a scalable means of obtaining situational awareness. However, label noise and severe class imbalance in satellite data create major challenges. The xBD dataset offers a standardized benchmark for building-level damage across diverse geographic regions. In this study, we evaluate Vision Transformer (ViT) model performance on the xBD dataset, specifically investigating how these models distinguish between types of structural damage when training on noisy, imbalanced data.   In this study, we specifically evaluate DINOv2-small and DeiT for multi-class damage classification. We propose a targeted patch-based pre-processing pipeline to isolate structural features and minimize background noise in training. We adopt a frozen-head fine-tuning strategy to keep computational requirements manageable. Model performance is evaluated through accuracy, precision, recall, and macro-averaged F1 scores. We show that small ViT architectures with our novel training method achieves competitive macro-averaged F1 relative to prior CNN baselines for disaster classification.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Rapid building damage assessment is critical for post-disaster response. Damage classification models built on satellite imagery provide a scalable means of obtaining situational awareness. However, label noise and severe class imbalance in satellite data create major challenges. The xBD dataset offers a standardized benchmark for building-level damage across diverse geographic regions.In this study, we evaluate Vision Transformer (ViT) model performance on the xBD dataset, specifically investigating how these models distinguish between types of structural damage when training on noisy, imbalanced data. In this study, we specifically evaluate DINOv2-small and DeiT for multi-class damage classification.We propose a targeted patch-based pre-processing pipeline to isolate structural features and minimize background noise in training. We adopt a frozen-head fine-tuning strategy to keep computational requirements manageable. Model performance is evaluated through accuracy, precision, recall, and macro-averaged F1 scores.We show that small ViT architectures with our novel training method achieves competitive macro-averaged F1 relative to prior CNN baselines for disaster classification.

</details>

---

## 331. MMLSv2: A Multimodal Dataset for Martian Landslide Detection in Remote Sensing Imagery

**Chinese Title**: MMLSv2: A Multimodal Dataset for Martian Landslide Detection in Remote Sensing Imagery

**Authors**: Sidike Paheding, Abel Reyes-Angulo, Leo Thomas Ramos, Angel D. Sappa, Rajaneesh A. et al.

**Date**: 2026-02-08 | **arXiv**: [2602.08112v1](http://arxiv.org/abs/2602.08112v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08112v1)

**Code**: https://github.com/MAIN-Lab/MMLS_v2

**Categories**: cs.CV, cs.LG

<details><summary><b>Abstract</b></summary>

We present MMLSv2, a dataset for landslide segmentation on Martian surfaces. MMLSv2 consists of multimodal imagery with seven bands: RGB, digital elevation model, slope, thermal inertia, and grayscale channels. MMLSv2 comprises 664 images distributed across training, validation, and test splits. In addition, an isolated test set of 276 images from a geographically disjoint region from the base dataset is released to evaluate spatial generalization. Experiments conducted with multiple segmentation models show that the dataset supports stable training and achieves competitive performance, while still posing challenges in fragmented, elongated, and small-scale landslide regions. Evaluation on the isolated test set leads to a noticeable performance drop, indicating increased difficulty and highlighting its value for assessing model robustness and generalization beyond standard in-distribution settings. Dataset will be available at: https://github.com/MAIN-Lab/MMLS_v2

</details>

<details><summary><b>Chinese Abstract</b></summary>

We present MMLSv2, a dataset for landslide segmentation on Martian surfaces. MMLSv2 consists of multimodal imagery with seven bands: RGB, digital elevation model, slope, thermal inertia, and grayscale channels. MMLSv2 comprises 664 images distributed across training, validation, and test splits.In addition, an isolated test set of 276 images from a geographically disjoint region from the base dataset is released to evaluate spatial generalization. Experiments conducted with multiple segmentation models show that the dataset supports stable training and achieves competitive performance, while still posing challenges in fragmented, elongated, and small-scale landslide regions.Evaluation on the isolated test set leads to a noticeable performance drop, indicating increased difficulty and highlighting its value for assessing model robustness and generalization beyond standard in-distribution settings. Dataset will be available at: https://github.com/MAIN-Lab/MMLS_v2

</details>

---

## 332. VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval

**Chinese Title**: VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval

**Authors**: Issar Tzachor, Dvir Samuel, Rami Ben-Ari

**Date**: 2026-02-08 | **arXiv**: [2602.08099v1](http://arxiv.org/abs/2602.08099v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08099v1)

**Categories**: cs.CV, cs.AI

<details><summary><b>Abstract</b></summary>

Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for video-text embedding and retrieval. We first conduct a systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with a calibrated MLLM head yields strong zero-shot retrieval performance without any training. Building on these findings, we introduce a lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related video-text embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by a substantial margin, achieving state-of-the-art results across common video retrieval benchmarks.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for video-text embedding and retrieval.We first conduct a systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with a calibrated MLLM head yields strong zero-shot retrieval performance without any training.Building on these findings, we introduce a lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related video-text embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by a substantial margin, achieving state-of-the-art results across common video retrieval benchmarks.

</details>

---

## 333. ViT-5: Vision Transformers for The Mid-2020s

**Chinese Title**: ViT-5: Vision Transformers for The Mid-2020s

**Authors**: Feng Wang, Sucheng Ren, Tiezheng Zhang, Predrag Neskovic, Anand Bhattad et al.

**Date**: 2026-02-08 | **arXiv**: [2602.08071v1](http://arxiv.org/abs/2602.08071v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08071v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

This work presents a systematic investigation into modernizing Vision Transformer backbones by leveraging architectural advancements from the past five years. While preserving the canonical Attention-FFN structure, we conduct a component-wise refinement involving normalization, activation functions, positional encoding, gating mechanisms, and learnable tokens. These updates form a new generation of Vision Transformers, which we call ViT-5. Extensive experiments demonstrate that ViT-5 consistently outperforms state-of-the-art plain Vision Transformers across both understanding and generation benchmarks. On ImageNet-1k classification, ViT-5-Base reaches 84.2\% top-1 accuracy under comparable compute, exceeding DeiT-III-Base at 83.8\%. ViT-5 also serves as a stronger backbone for generative modeling: when plugged into an SiT diffusion framework, it achieves 1.84 FID versus 2.06 with a vanilla ViT backbone. Beyond headline metrics, ViT-5 exhibits improved representation learning and favorable spatial reasoning behavior, and transfers reliably across tasks. With a design aligned with contemporary foundation-model practices, ViT-5 offers a simple drop-in upgrade over vanilla ViT for mid-2020s vision backbones.

</details>

<details><summary><b>Chinese Abstract</b></summary>

This work presents a systematic investigation into modernizing Vision Transformer backbones by leveraging architectural advancements from the past five years. While preserving the canonical Attention-FFN structure, we conduct a component-wise refinement involving normalization, activation functions, positional encoding, gating mechanisms, and learnable tokens. These updates form a new generation of Vision Transformers, which we call ViT-5.Extensive experiments demonstrate that ViT-5 consistently outperforms state-of-the-art plain Vision Transformers across both understanding and generation benchmarks. On ImageNet-1k classification, ViT-5-Base reaches 84.2\% top-1 accuracy under comparable compute, exceeding DeiT-III-Base at 83.8\%.ViT-5 also serves as a stronger backbone for generative modeling: when plugged into an SiT diffusion framework, it achieves 1.84 FID versus 2.06 with a vanilla ViT backbone. Beyond headline metrics, ViT-5 exhibits improved representation learning and favorable spatial reasoning behavior, and transfers reliably across tasks.With a design aligned with contemporary foundation-model practices, ViT-5 offers a simple drop-in upgrade over vanilla ViT for mid-2020s vision backbones.

</details>

---

## 334. ReRoPE: Repurposing RoPE for Relative Camera Control

**Chinese Title**: ReRoPE: Repurposing RoPE for Relative Camera Control

**Authors**: Chunyang Li, Yuanbo Yang, Jiahao Shao, Hongyu Zhou, Katja Schwarz et al.

**Date**: 2026-02-08 | **arXiv**: [2602.08068v1](http://arxiv.org/abs/2602.08068v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08068v1)

**Project**: https://sisyphe-lee.github.io/ReRoPE/  **Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Video generation with controllable camera viewpoints is essential for applications such as interactive content creation, gaming, and simulation. Existing methods typically adapt pre-trained video models using camera poses relative to a fixed reference, e.g., the first frame. However, these encodings lack shift-invariance, often leading to poor generalization and accumulated drift. While relative camera pose embeddings defined between arbitrary view pairs offer a more robust alternative, integrating them into pre-trained video diffusion models without prohibitive training costs or architectural changes remains challenging. We introduce ReRoPE, a plug-and-play framework that incorporates relative camera information into pre-trained video diffusion models without compromising their generation capability. Our approach is based on the insight that Rotary Positional Embeddings (RoPE) in existing models underutilize their full spectral bandwidth, particularly in the low-frequency components. By seamlessly injecting relative camera pose information into these underutilized bands, ReRoPE achieves precise control while preserving strong pre-trained generative priors. We evaluate our method on both image-to-video (I2V) and video-to-video (V2V) tasks in terms of camera control accuracy and visual fidelity. Our results demonstrate that ReRoPE offers a training-efficient path toward controllable, high-fidelity video generation. See project page for more results: https://sisyphe-lee.github.io/ReRoPE/

</details>

<details><summary><b>Chinese Abstract</b></summary>

Video generation with controllable camera viewpoints is essential for applications such as interactive content creation, gaming, and simulation. Existing methods typically adapt pre-trained video models using camera poses relative to a fixed reference, e.g., the first frame. However, these encodings lack shift-invariance, often leading to poor generalization and accumulated drift.While relative camera pose embeddings defined between arbitrary view pairs offer a more robust alternative, integrating them into pre-trained video diffusion models without prohibitive training costs or architectural changes remains challenging. We introduce ReRoPE, a plug-and-play framework that incorporates relative camera information into pre-trained video diffusion models without compromising their generation capability.Our approach is based on the insight that Rotary Positional Embeddings (RoPE) in existing models underutilize their full spectral bandwidth, particularly in the low-frequency components. By seamlessly injecting relative camera pose information into these underutilized bands, ReRoPE achieves precise control while preserving strong pre-trained generative priors.We evaluate our method on both image-to-video (I2V) and video-to-video (V2V) tasks in terms of camera control accuracy and visual fidelity. Our results demonstrate that ReRoPE offers a training-efficient path toward controllable, high-fidelity video generation. See project page for more results: https://sisyphe-lee.github.io/ReRoPE/

</details>

---

## 335. Weak to Strong: VLM-Based Pseudo-Labeling as a Weakly Supervised Training Strategy in Multimodal Video-based Hidden Emotion Understanding Tasks

**Chinese Title**: Weak to Strong: VLM-Based Pseudo-Labeling as a Weakly Supervised Training Strategy in Multimodal Video-based Hidden Emotion Understanding Tasks

**Authors**: Yufei Wang, Haixu Liu, Tianxiang Xu, Chuancheng Shi, Hongsheng Xing

**Date**: 2026-02-08 | **arXiv**: [2602.08057v1](http://arxiv.org/abs/2602.08057v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08057v1)

**Categories**: cs.CV, cs.AI

<details><summary><b>Abstract</b></summary>

To tackle the automatic recognition of "concealed emotions" in videos, this paper proposes a multimodal weak-supervision framework and achieves state-of-the-art results on the iMiGUE tennis-interview dataset. First, YOLO 11x detects and crops human portraits frame-by-frame, and DINOv2-Base extracts visual features from the cropped regions. Next, by integrating Chain-of-Thought and Reflection prompting (CoT + Reflection), Gemini 2.5 Pro automatically generates pseudo-labels and reasoning texts that serve as weak supervision for downstream models. Subsequently, OpenPose produces 137-dimensional key-point sequences, augmented with inter-frame offset features; the usual graph neural network backbone is simplified to an MLP to efficiently model the spatiotemporal relationships of the three key-point streams. An ultra-long-sequence Transformer independently encodes both the image and key-point sequences, and their representations are concatenated with BERT-encoded interview transcripts. Each modality is first pre-trained in isolation, then fine-tuned jointly, with pseudo-labeled samples merged into the training set for further gains. Experiments demonstrate that, despite severe class imbalance, the proposed approach lifts accuracy from under 0.6 in prior work to over 0.69, establishing a new public benchmark. The study also validates that an "MLP-ified" key-point backbone can match - or even surpass - GCN-based counterparts in this task.

</details>

<details><summary><b>Chinese Abstract</b></summary>

To tackle the automatic recognition of "concealed emotions" in videos, this paper proposes a multimodal weak-supervision framework and achieves state-of-the-art results on the iMiGUE tennis-interview dataset. First, YOLO 11x detects and crops human portraits frame-by-frame, and DINOv2-Base extracts visual features from the cropped regions.Next, by integrating Chain-of-Thought and Reflection prompting (CoT + Reflection), Gemini 2.5 Pro automatically generates pseudo-labels and reasoning texts that serve as weak supervision for downstream models.Subsequently, OpenPose produces 137-dimensional key-point sequences, augmented with inter-frame offset features; the usual graph neural network backbone is simplified to an MLP to efficiently model the spatiotemporal relationships of the three key-point streams. An ultra-long-sequence Transformer independently encodes both the image and key-point sequences, and their representations are concatenated with BERT-encoded interview transcripts.Each modality is first pre-trained in isolation, then fine-tuned jointly, with pseudo-labeled samples merged into the training set for further gains. Experiments demonstrate that, despite severe class imbalance, the proposed approach lifts accuracy from under 0.6 in prior work to over 0.69, establishing a new public benchmark.The study also validates that an "MLP-ified" key-point backbone can match - or even surpass - GCN-based counterparts in this task.

</details>

---

## 336. Vanilla Group Equivariant Vision Transformer: Simple and Effective

**Chinese Title**: Vanilla Group Equivariant Vision Transformer: Simple and Effective

**Authors**: Jiahong Fu, Qi Xie, Deyu Meng, Zongben Xu

**Date**: 2026-02-08 | **arXiv**: [2602.08047v1](http://arxiv.org/abs/2602.08047v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08047v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

Incorporating symmetry priors as inductive biases to design equivariant Vision Transformers (ViTs) has emerged as a promising avenue for enhancing their performance. However, existing equivariant ViTs often struggle to balance performance with equivariance, primarily due to the challenge of achieving holistic equivariant modifications across the diverse modules in ViTs-particularly in harmonizing the Self-Attention mechanism with Patch Embedding. To address this, we propose a straightforward framework that systematically renders key ViT components, including patch embedding, self-attention, positional encodings, and Down/Up-Sampling, equivariant, thereby constructing ViTs with guaranteed equivariance. The resulting architecture serves as a plug-and-play replacement that is both theoretically grounded and practically versatile, scaling seamlessly even to Swin Transformers. Extensive experiments demonstrate that our equivariant ViTs consistently improve performance and data efficiency across a wide spectrum of vision tasks.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Incorporating symmetry priors as inductive biases to design equivariant Vision Transformers (ViTs) has emerged as a promising avenue for enhancing their performance. However, existing equivariant ViTs often struggle to balance performance with equivariance, primarily due to the challenge of achieving holistic equivariant modifications across the diverse modules in ViTs-particularly in harmonizing the Self-Attention mechanism with Patch Embedding.To address this, we propose a straightforward framework that systematically renders key ViT components, including patch embedding, self-attention, positional encodings, and Down/Up-Sampling, equivariant, thereby constructing ViTs with guaranteed equivariance. The resulting architecture serves as a plug-and-play replacement that is both theoretically grounded and practically versatile, scaling seamlessly even to Swin Transformers.Extensive experiments demonstrate that our equivariant ViTs consistently improve performance and data efficiency across a wide spectrum of vision tasks.

</details>

---

## 337. Enhanced Mixture 3D CGAN for Completion and Generation of 3D Objects

**Chinese Title**: Enhanced Mixture 3D CGAN for Completion and Generation of 3D Objects

**Authors**: Yahia Hamdi, Nicolas Andrialovanirina, KÃ©lig MahÃ©, Emilie Poisson Caillault

**Date**: 2026-02-08 | **arXiv**: [2602.08046v1](http://arxiv.org/abs/2602.08046v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08046v1)

**Categories**: cs.CV

<details><summary><b>Abstract</b></summary>

The generation and completion of 3D objects represent a transformative challenge in computer vision. Generative Adversarial Networks (GANs) have recently demonstrated strong potential in synthesizing realistic visual data. However, they often struggle to capture complex and diverse data distributions, particularly in scenarios involving incomplete inputs or significant missing regions. These challenges arise mainly from the high computational requirements and the difficulty of modeling heterogeneous and structurally intricate data, which restrict their applicability in real-world settings. Mixture of Experts (MoE) models have emerged as a promising solution to these limitations. By dynamically selecting and activating the most relevant expert sub-networks for a given input, MoEs improve both performance and efficiency. In this paper, we investigate the integration of Deep 3D Convolutional GANs (CGANs) with a MoE framework to generate high-quality 3D models and reconstruct incomplete or damaged objects. The proposed architecture incorporates multiple generators, each specialized to capture distinct modalities within the dataset. Furthermore, an auxiliary loss-free dynamic capacity constraint (DCC) mechanism is introduced to guide the selection of categorical generators, ensuring a balance between specialization, training stability, and computational efficiency, which is critical for 3D voxel processing. We evaluated the model's ability to generate and complete shapes with missing regions of varying sizes and compared its performance with state-of-the-art approaches. Both quantitative and qualitative results confirm the effectiveness of the proposed MoE-DCGAN in handling complex 3D data.

</details>

<details><summary><b>Chinese Abstract</b></summary>

The generation and completion of 3D objects represent a transformative challenge in computer vision. Generative Adversarial Networks (GANs) have recently demonstrated strong potential in synthesizing realistic visual data. However, they often struggle to capture complex and diverse data distributions, particularly in scenarios involving incomplete inputs or significant missing regions.These challenges arise mainly from the high computational requirements and the difficulty of modeling heterogeneous and structurally intricate data, which restrict their applicability in real-world settings. Mixture of Experts (MoE) models have emerged as a promising solution to these limitations. By dynamically selecting and activating the most relevant expert sub-networks for a given input, MoEs improve both performance and efficiency.In this paper, we investigate the integration of Deep 3D Convolutional GANs (CGANs) with a MoE framework to generate high-quality 3D models and reconstruct incomplete or damaged objects. The proposed architecture incorporates multiple generators, each specialized to capture distinct modalities within the dataset.Furthermore, an auxiliary loss-free dynamic capacity constraint (DCC) mechanism is introduced to guide the selection of categorical generators, ensuring a balance between specialization, training stability, and computational efficiency, which is critical for 3D voxel processing. We evaluated the model's ability to generate and complete shapes with missing regions of varying sizes and compared its performance with state-of-the-art approaches.Both quantitative and qualitative results confirm the effectiveness of the proposed MoE-DCGAN in handling complex 3D data.

</details>

---

## 338. MIND: Benchmarking Memory Consistency and Action Control in World Models

**Chinese Title**: MIND: Benchmarking Memory Consistency and Action Control in World Models

**Authors**: Yixuan Ye, Xuanyu Lu, Yuxin Jiang, Yuchao Gu, Rui Zhao et al.

**Date**: 2026-02-08 | **arXiv**: [2602.08025v1](http://arxiv.org/abs/2602.08025v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08025v1)

**Project**: https://csu-jpg.github.io/MIND.github.io/  **Categories**: cs.CV, cs.AI

<details><summary><b>Abstract</b></summary>

World models aim to understand, remember, and predict dynamic visual environments, yet a unified benchmark for evaluating their fundamental abilities remains lacking. To address this gap, we introduce MIND, the first open-domain closed-loop revisited benchmark for evaluating Memory consIstency and action coNtrol in worlD models. MIND contains 250 high-quality videos at 1080p and 24 FPS, including 100 (first-person) + 100 (third-person) video clips under a shared action space and 25 + 25 clips across varied action spaces covering eight diverse scenes. We design an efficient evaluation framework to measure two core abilities: memory consistency and action control, capturing temporal stability and contextual coherence across viewpoints. Furthermore, we design various action spaces, including different character movement speeds and camera rotation angles, to evaluate the action generalization capability across different action spaces under shared scenes. To facilitate future performance benchmarking on MIND, we introduce MIND-World, a novel interactive Video-to-World baseline. Extensive experiments demonstrate the completeness of MIND and reveal key challenges in current world models, including the difficulty of maintaining long-term memory consistency and generalizing across action spaces. Project page: https://csu-jpg.github.io/MIND.github.io/

</details>

<details><summary><b>Chinese Abstract</b></summary>

World models aim to understand, remember, and predict dynamic visual environments, yet a unified benchmark for evaluating their fundamental abilities remains lacking. To address this gap, we introduce MIND, the first open-domain closed-loop revisited benchmark for evaluating Memory consIstency and action coNtrol in worlD models.MIND contains 250 high-quality videos at 1080p and 24 FPS, including 100 (first-person) + 100 (third-person) video clips under a shared action space and 25 + 25 clips across varied action spaces covering eight diverse scenes. We design an efficient evaluation framework to measure two core abilities: memory consistency and action control, capturing temporal stability and contextual coherence across viewpoints.Furthermore, we design various action spaces, including different character movement speeds and camera rotation angles, to evaluate the action generalization capability across different action spaces under shared scenes. To facilitate future performance benchmarking on MIND, we introduce MIND-World, a novel interactive Video-to-World baseline.Extensive experiments demonstrate the completeness of MIND and reveal key challenges in current world models, including the difficulty of maintaining long-term memory consistency and generalizing across action spaces. Project page: https://csu-jpg.github.io/MIND.github.io/

</details>

---

## 339. FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging

**Chinese Title**: FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging

**Authors**: Ziyang Fan, Keyu Chen, Ruilong Xing, Yulin Li, Li Jiang et al.

**Date**: 2026-02-08 | **arXiv**: [2602.08024v1](http://arxiv.org/abs/2602.08024v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08024v1)

**Code**: https://github.com/Fanziyang-v/FlashVID.

**Categories**: cs.CV, cs.AI, cs.CL, cs.LG

<details><summary><b>Abstract</b></summary>

Although Video Large Language Models (VLLMs) have shown remarkable capabilities in video understanding, they are required to process high volumes of visual tokens, causing significant computational inefficiency. Existing VLLMs acceleration frameworks usually compress spatial and temporal redundancy independently, which overlooks the spatiotemporal relationships, thereby leading to suboptimal spatiotemporal compression. The highly correlated visual features are likely to change in spatial position, scale, orientation, and other attributes over time due to the dynamic nature of video. Building on this insight, we introduce FlashVID, a training-free inference acceleration framework for VLLMs. Specifically, FlashVID utilizes Attention and Diversity-based Token Selection (ADTS) to select the most representative tokens for basic video representation, then applies Tree-based Spatiotemporal Token Merging (TSTM) for fine-grained spatiotemporal redundancy elimination. Extensive experiments conducted on three representative VLLMs across five video understanding benchmarks demonstrate the effectiveness and generalization of our method. Notably, by retaining only 10% of visual tokens, FlashVID preserves 99.1% of the performance of LLaVA-OneVision. Consequently, FlashVID can serve as a training-free and plug-and-play module for extending long video frames, which enables a 10x increase in video frame input to Qwen2.5-VL, resulting in a relative improvement of 8.6% within the same computational budget. Code is available at https://github.com/Fanziyang-v/FlashVID.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Although Video Large Language Models (VLLMs) have shown remarkable capabilities in video understanding, they are required to process high volumes of visual tokens, causing significant computational inefficiency. Existing VLLMs acceleration frameworks usually compress spatial and temporal redundancy independently, which overlooks the spatiotemporal relationships, thereby leading to suboptimal spatiotemporal compression.The highly correlated visual features are likely to change in spatial position, scale, orientation, and other attributes over time due to the dynamic nature of video. Building on this insight, we introduce FlashVID, a training-free inference acceleration framework for VLLMs.Specifically, FlashVID utilizes Attention and Diversity-based Token Selection (ADTS) to select the most representative tokens for basic video representation, then applies Tree-based Spatiotemporal Token Merging (TSTM) for fine-grained spatiotemporal redundancy elimination. Extensive experiments conducted on three representative VLLMs across five video understanding benchmarks demonstrate the effectiveness and generalization of our method.Notably, by retaining only 10% of visual tokens, FlashVID preserves 99.1% of the performance of LLaVA-OneVision. Consequently, FlashVID can serve as a training-free and plug-and-play module for extending long video frames, which enables a 10x increase in video frame input to Qwen2.5-VL, resulting in a relative improvement of 8.6% within the same computational budget. Code is available at https://github.com/Fanziyang-v/FlashVID.

</details>

---

## 340. ForecastOcc: Vision-based Semantic Occupancy Forecasting

**Chinese Title**: ForecastOcc: Vision-based Semantic Occupancy Forecasting

**Authors**: Riya Mohan, Juana Valeria Hurtado, Rohit Mohan, Abhinav Valada

**Date**: 2026-02-08 | **arXiv**: [2602.08006v1](http://arxiv.org/abs/2602.08006v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08006v1)

**Categories**: cs.CV, cs.AI, cs.LG, cs.RO

<details><summary><b>Abstract</b></summary>

Autonomous driving requires forecasting both geometry and semantics over time to effectively reason about future environment states. Existing vision-based occupancy forecasting methods focus on motion-related categories such as static and dynamic objects, while semantic information remains largely absent. Recent semantic occupancy forecasting approaches address this gap but rely on past occupancy predictions obtained from separate networks. This makes current methods sensitive to error accumulation and prevents learning spatio-temporal features directly from images. In this work, we present ForecastOcc, the first framework for vision-based semantic occupancy forecasting that jointly predicts future occupancy states and semantic categories. Our framework yields semantic occupancy forecasts for multiple horizons directly from past camera images, without relying on externally estimated maps. We evaluate ForecastOcc in two complementary settings: multi-view forecasting on the Occ3D-nuScenes dataset and monocular forecasting on SemanticKITTI, where we establish the first benchmark for this task. We introduce the first baselines by adapting two 2D forecasting modules within our framework. Importantly, we propose a novel architecture that incorporates a temporal cross-attention forecasting module, a 2D-to-3D view transformer, a 3D encoder for occupancy prediction, and a semantic occupancy head for voxel-level forecasts across multiple horizons. Extensive experiments on both datasets show that ForecastOcc consistently outperforms baselines, yielding semantically rich, future-aware predictions that capture scene dynamics and semantics critical for autonomous driving.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Autonomous driving requires forecasting both geometry and semantics over time to effectively reason about future environment states. Existing vision-based occupancy forecasting methods focus on motion-related categories such as static and dynamic objects, while semantic information remains largely absent. Recent semantic occupancy forecasting approaches address this gap but rely on past occupancy predictions obtained from separate networks.This makes current methods sensitive to error accumulation and prevents learning spatio-temporal features directly from images. In this work, we present ForecastOcc, the first framework for vision-based semantic occupancy forecasting that jointly predicts future occupancy states and semantic categories.Our framework yields semantic occupancy forecasts for multiple horizons directly from past camera images, without relying on externally estimated maps. We evaluate ForecastOcc in two complementary settings: multi-view forecasting on the Occ3D-nuScenes dataset and monocular forecasting on SemanticKITTI, where we establish the first benchmark for this task.We introduce the first baselines by adapting two 2D forecasting modules within our framework. Importantly, we propose a novel architecture that incorporates a temporal cross-attention forecasting module, a 2D-to-3D view transformer, a 3D encoder for occupancy prediction, and a semantic occupancy head for voxel-level forecasts across multiple horizons.Extensive experiments on both datasets show that ForecastOcc consistently outperforms baselines, yielding semantically rich, future-aware predictions that capture scene dynamics and semantics critical for autonomous driving.

</details>

---

## 341. DIAL-SUMMER: A Structured Evaluation Framework of Hierarchical Errors in Dialogue Summaries

**Chinese Title**: DIAL-SUMMER: A Structured Evaluation Framework of Hierarchical Errors in Dialogue Summaries

**Authors**: Sahana Ramnath, Nima Chitsazan, Mingyang Zhou, Chia-Hsuan Lee, Shi-Xiong Zhang et al.

**Date**: 2026-02-08 | **arXiv**: [2602.08149v1](http://arxiv.org/abs/2602.08149v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08149v1)

**Categories**: cs.CL, cs.AI

<details><summary><b>Abstract</b></summary>

Dialogues are a predominant mode of communication for humans, and it is immensely helpful to have automatically generated summaries of them (e.g., to revise key points discussed in a meeting, to review conversations between customer agents and product users). Prior works on dialogue summary evaluation largely ignore the complexities specific to this task: (i) shift in structure, from multiple speakers discussing information in a scattered fashion across several turns, to a summary's sentences, and (ii) shift in narration viewpoint, from speakers' first/second-person narration, standardized third-person narration in the summary. In this work, we introduce our framework DIALSUMMER to address the above. We propose DIAL-SUMMER's taxonomy of errors to comprehensively evaluate dialogue summaries at two hierarchical levels: DIALOGUE-LEVEL that focuses on the broader speakers/turns, and WITHIN-TURN-LEVEL that focuses on the information talked about inside a turn. We then present DIAL-SUMMER's dataset composed of dialogue summaries manually annotated with our taxonomy's fine-grained errors. We conduct empirical analyses of these annotated errors, and observe interesting trends (e.g., turns occurring in middle of the dialogue are the most frequently missed in the summary, extrinsic hallucinations largely occur at the end of the summary). We also conduct experiments on LLM-Judges' capability at detecting these errors, through which we demonstrate the challenging nature of our dataset, the robustness of our taxonomy, and the need for future work in this field to enhance LLMs' performance in the same. Code and inference dataset coming soon.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Dialogues are a predominant mode of communication for humans, and it is immensely helpful to have automatically generated summaries of them (e.g., to revise key points discussed in a meeting, to review conversations between customer agents and product users).Prior works on dialogue summary evaluation largely ignore the complexities specific to this task: (i) shift in structure, from multiple speakers discussing information in a scattered fashion across several turns, to a summary's sentences, and (ii) shift in narration viewpoint, from speakers' first/second-person narration, standardized third-person narration in the summary. In this work, we introduce our framework DIALSUMMER to address the above.We propose DIAL-SUMMER's taxonomy of errors to comprehensively evaluate dialogue summaries at two hierarchical levels: DIALOGUE-LEVEL that focuses on the broader speakers/turns, and WITHIN-TURN-LEVEL that focuses on the information talked about inside a turn. We then present DIAL-SUMMER's dataset composed of dialogue summaries manually annotated with our taxonomy's fine-grained errors.We conduct empirical analyses of these annotated errors, and observe interesting trends (e.g., turns occurring in middle of the dialogue are the most frequently missed in the summary, extrinsic hallucinations largely occur at the end of the summary).We also conduct experiments on LLM-Judges' capability at detecting these errors, through which we demonstrate the challenging nature of our dataset, the robustness of our taxonomy, and the need for future work in this field to enhance LLMs' performance in the same. Code and inference dataset coming soon.

</details>

---

## 342. Spectral Guardrails for Agents in the Wild: Detecting Tool Use Hallucinations via Attention Topology

**Chinese Title**: Spectral Guardrails for Agents in the Wild: Detecting Tool Use Hallucinations via Attention Topology

**Authors**: Valentin NoÃ«l

**Date**: 2026-02-08 | **arXiv**: [2602.08082v1](http://arxiv.org/abs/2602.08082v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08082v1)

**Categories**: cs.LG, cs.AI, eess.SP

<details><summary><b>Abstract</b></summary>

Deploying autonomous agents in the wild requires reliable safeguards against tool use failures. We propose a training free guardrail based on spectral analysis of attention topology that complements supervised approaches. On Llama 3.1 8B, our method achieves 97.7\% recall with multi-feature detection and 86.1\% recall with 81.0\% precision for balanced deployment, without requiring any labeled training data. Most remarkably, we discover that single layer spectral features act as near-perfect hallucination detectors: Llama L26 Smoothness achieves 98.2\% recall (213/217 hallucinations caught) with a single threshold, and Mistral L3 Entropy achieves 94.7\% recall. This suggests hallucination is not merely a wrong token but a thermodynamic state change: the model's attention becomes noise when it errs. Through controlled cross-model evaluation on matched domains ($N=1000$, $T=0.3$, same General domain, hallucination rates 20--22\%), we reveal the ``Loud Liar'' phenomenon: Llama 3.1 8B's failures are spectrally catastrophic and dramatically easier to detect, while Mistral 7B achieves the best discrimination (AUC 0.900). These findings establish spectral analysis as a principled, efficient framework for agent safety.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Deploying autonomous agents in the wild requires reliable safeguards against tool use failures. We propose a training free guardrail based on spectral analysis of attention topology that complements supervised approaches. On Llama 3.1 8B, our method achieves 97.7\% recall with multi-feature detection and 86.1\% recall with 81.0\% precision for balanced deployment, without requiring any labeled training data.Most remarkably, we discover that single layer spectral features act as near-perfect hallucination detectors: Llama L26 Smoothness achieves 98.2\% recall (213/217 hallucinations caught) with a single threshold, and Mistral L3 Entropy achieves 94.7\% recall. This suggests hallucination is not merely a wrong token but a thermodynamic state change: the model's attention becomes noise when it errs.Through controlled cross-model evaluation on matched domains ($N=1000$, $T=0.3$, same General domain, hallucination rates 20--22\%), we reveal the ``Loud Liar'' phenomenon: Llama 3.1 8B's failures are spectrally catastrophic and dramatically easier to detect, while Mistral 7B achieves the best discrimination (AUC 0.900). These findings establish spectral analysis as a principled, efficient framework for agent safety.

</details>

---

## 343. Persistent Entropy as a Detector of Phase Transitions

**Chinese Title**: Persistent Entropy as a Detector of Phase Transitions

**Authors**: Matteo Rucco

**Date**: 2026-02-08 | **arXiv**: [2602.09058v1](http://arxiv.org/abs/2602.09058v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09058v1)

**Categories**: stat.ML, cs.AI, cs.IT, cs.LG

<details><summary><b>Abstract</b></summary>

Persistent entropy (PE) is an information-theoretic summary statistic of persistence barcodes that has been widely used to detect regime changes in complex systems. Despite its empirical success, a general theoretical understanding of when and why persistent entropy reliably detects phase transitions has remained limited, particularly in stochastic and data-driven settings. In this work, we establish a general, model-independent theorem providing sufficient conditions under which persistent entropy provably separates two phases. We show that persistent entropy exhibits an asymptotically non-vanishing gap across phases. The result relies only on continuity of persistent entropy along the convergent diagram sequence, or under mild regularization, and is therefore broadly applicable across data modalities, filtrations, and homological degrees. To connect asymptotic theory with finite-time computations, we introduce an operational framework based on topological stabilization, defining a topological transition time by stabilizing a chosen topological statistic over sliding windows, and a probability-based estimator of critical parameters within a finite observation horizon. We validate the framework on the Kuramoto synchronization transition, the Vicsek order-to-disorder transition in collective motion, and neural network training dynamics across multiple datasets and architectures. Across all experiments, stabilization of persistent entropy and collapse of variability across realizations provide robust numerical signatures consistent with the theoretical mechanism.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Persistent entropy (PE) is an information-theoretic summary statistic of persistence barcodes that has been widely used to detect regime changes in complex systems. Despite its empirical success, a general theoretical understanding of when and why persistent entropy reliably detects phase transitions has remained limited, particularly in stochastic and data-driven settings.In this work, we establish a general, model-independent theorem providing sufficient conditions under which persistent entropy provably separates two phases. We show that persistent entropy exhibits an asymptotically non-vanishing gap across phases.The result relies only on continuity of persistent entropy along the convergent diagram sequence, or under mild regularization, and is therefore broadly applicable across data modalities, filtrations, and homological degrees.To connect asymptotic theory with finite-time computations, we introduce an operational framework based on topological stabilization, defining a topological transition time by stabilizing a chosen topological statistic over sliding windows, and a probability-based estimator of critical parameters within a finite observation horizon.We validate the framework on the Kuramoto synchronization transition, the Vicsek order-to-disorder transition in collective motion, and neural network training dynamics across multiple datasets and architectures. Across all experiments, stabilization of persistent entropy and collapse of variability across realizations provide robust numerical signatures consistent with the theoretical mechanism.

</details>

---

## 344. FIRE: Frobenius-Isometry Reinitialization for Balancing the Stability-Plasticity Tradeoff

**Chinese Title**: FIRE: Frobenius-Isometry Reinitialization for Balancing the Stability-Plasticity Tradeoff

**Authors**: Isaac Han, Sangyeon Park, Seungwon Oh, Donghu Kim, Hojoon Lee et al.

**Date**: 2026-02-08 | **arXiv**: [2602.08040v1](http://arxiv.org/abs/2602.08040v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08040v1)

**Categories**: cs.LG, cs.AI

<details><summary><b>Abstract</b></summary>

Deep neural networks trained on nonstationary data must balance stability (i.e., retaining prior knowledge) and plasticity (i.e., adapting to new tasks). Standard reinitialization methods, which reinitialize weights toward their original values, are widely used but difficult to tune: conservative reinitializations fail to restore plasticity, while aggressive ones erase useful knowledge. We propose FIRE, a principled reinitialization method that explicitly balances the stability-plasticity tradeoff. FIRE quantifies stability through Squared Frobenius Error (SFE), measuring proximity to past weights, and plasticity through Deviation from Isometry (DfI), reflecting weight isotropy. The reinitialization point is obtained by solving a constrained optimization problem, minimizing SFE subject to DfI being zero, which is efficiently approximated by Newton-Schulz iteration. FIRE is evaluated on continual visual learning (CIFAR-10 with ResNet-18), language modeling (OpenWebText with GPT-0.1B), and reinforcement learning (HumanoidBench with SAC and Atari games with DQN). Across all domains, FIRE consistently outperforms both naive training without intervention and standard reinitialization methods, demonstrating effective balancing of the stability-plasticity tradeoff.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Deep neural networks trained on nonstationary data must balance stability (i.e., retaining prior knowledge) and plasticity (i.e., adapting to new tasks). Standard reinitialization methods, which reinitialize weights toward their original values, are widely used but difficult to tune: conservative reinitializations fail to restore plasticity, while aggressive ones erase useful knowledge.We propose FIRE, a principled reinitialization method that explicitly balances the stability-plasticity tradeoff. FIRE quantifies stability through Squared Frobenius Error (SFE), measuring proximity to past weights, and plasticity through Deviation from Isometry (DfI), reflecting weight isotropy.The reinitialization point is obtained by solving a constrained optimization problem, minimizing SFE subject to DfI being zero, which is efficiently approximated by Newton-Schulz iteration. FIRE is evaluated on continual visual learning (CIFAR-10 with ResNet-18), language modeling (OpenWebText with GPT-0.1B), and reinforcement learning (HumanoidBench with SAC and Atari games with DQN).Across all domains, FIRE consistently outperforms both naive training without intervention and standard reinitialization methods, demonstrating effective balancing of the stability-plasticity tradeoff.

</details>

---

## 345. CyberExplorer: Benchmarking LLM Offensive Security Capabilities in a Real-World Attacking Simulation Environment

**Chinese Title**: CyberExplorer: Benchmarking LLM Offensive Security Capabilities in a Real-World Attacking Simulation Environment

**Authors**: Nanda Rani, Kimberly Milner, Minghao Shao, Meet Udeshi, Haoran Xi et al.

**Date**: 2026-02-08 | **arXiv**: [2602.08023v2](http://arxiv.org/abs/2602.08023v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.08023v2)

**Categories**: cs.CR, cs.AI, cs.MA

<details><summary><b>Abstract</b></summary>

Real-world offensive security operations are inherently open-ended: attackers explore unknown attack surfaces, revise hypotheses under uncertainty, and operate without guaranteed success. Existing LLM-based offensive agent evaluations rely on closed-world settings with predefined goals and binary success criteria. To address this gap, we introduce CyberExplorer, an evaluation suite with two core components: (1) an open-environment benchmark built on a virtual machine hosting 40 vulnerable web services derived from real-world CTF challenges, where agents autonomously perform reconnaissance, target selection, and exploitation without prior knowledge of vulnerability locations; and (2) a reactive multi-agent framework supporting dynamic exploration without predefined plans. CyberExplorer enables fine-grained evaluation beyond flag recovery, capturing interaction dynamics, coordination behavior, failure modes, and vulnerability discovery signals-bridging the gap between benchmarks and realistic multi-target attack scenarios.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Real-world offensive security operations are inherently open-ended: attackers explore unknown attack surfaces, revise hypotheses under uncertainty, and operate without guaranteed success. Existing LLM-based offensive agent evaluations rely on closed-world settings with predefined goals and binary success criteria.To address this gap, we introduce CyberExplorer, an evaluation suite with two core components: (1) an open-environment benchmark built on a virtual machine hosting 40 vulnerable web services derived from real-world CTF challenges, where agents autonomously perform reconnaissance, target selection, and exploitation without prior knowledge of vulnerability locations; and (2) a reactive multi-agent framework supporting dynamic exploration without predefined plans.CyberExplorer enables fine-grained evaluation beyond flag recovery, capturing interaction dynamics, coordination behavior, failure modes, and vulnerability discovery signals-bridging the gap between benchmarks and realistic multi-target attack scenarios.

</details>

---

## 346. The Rise of Sparse Mixture-of-Experts: A Survey from Algorithmic Foundations to Decentralized Architectures and Vertical Domain Applications

**Chinese Title**: The Rise of Sparse Mixture-of-Experts: A Survey from Algorithmic Foundations to Decentralized Architectures and Vertical Domain Applications

**Authors**: Dong Pan, Bingtao Li, Yongsheng Zheng, Jiren Ma, Victor Fei

**Date**: 2026-02-08 | **arXiv**: [2602.08019v1](http://arxiv.org/abs/2602.08019v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08019v1)

**Categories**: cs.LG, cs.AI

<details><summary><b>Abstract</b></summary>

The sparse Mixture of Experts(MoE) architecture has evolved as a powerful approach for scaling deep learning models to more parameters with comparable computation cost. As an important branch of large language model(LLM), MoE model only activate a subset of experts based on a routing network. This sparse conditional computation mechanism significantly improves computational efficiency, paving a promising path for greater scalability and cost-efficiency. It not only enhance downstream applications such as natural language processing, computer vision, and multimodal in various horizontal domains, but also exhibit broad applicability across vertical domains. Despite the growing popularity and application of MoE models across various domains, there lacks a systematic exploration of recent advancements of MoE in many important fields. Existing surveys on MoE suffer from limitations such as lack coverage or none extensively exploration of key areas. This survey seeks to fill these gaps. In this paper, Firstly, we examine the foundational principles of MoE, with an in-depth exploration of its core components-the routing network and expert network. Subsequently, we extend beyond the centralized paradigm to the decentralized paradigm, which unlocks the immense untapped potential of decentralized infrastructure, enables democratization of MoE development for broader communities, and delivers greater scalability and cost-efficiency. Furthermore we focus on exploring its vertical domain applications. Finally, we also identify key challenges and promising future research directions. To the best of our knowledge, this survey is currently the most comprehensive review in the field of MoE. We aim for this article to serve as a valuable resource for both researchers and practitioners, enabling them to navigate and stay up-to-date with the latest advancements.

</details>

<details><summary><b>Chinese Abstract</b></summary>

The sparse Mixture of Experts(MoE) architecture has evolved as a powerful approach for scaling deep learning models to more parameters with comparable computation cost. As an important branch of large language model(LLM), MoE model only activate a subset of experts based on a routing network.This sparse conditional computation mechanism significantly improves computational efficiency, paving a promising path for greater scalability and cost-efficiency. It not only enhance downstream applications such as natural language processing, computer vision, and multimodal in various horizontal domains, but also exhibit broad applicability across vertical domains.Despite the growing popularity and application of MoE models across various domains, there lacks a systematic exploration of recent advancements of MoE in many important fields. Existing surveys on MoE suffer from limitations such as lack coverage or none extensively exploration of key areas. This survey seeks to fill these gaps.In this paper, Firstly, we examine the foundational principles of MoE, with an in-depth exploration of its core components-the routing network and expert network. Subsequently, we extend beyond the centralized paradigm to the decentralized paradigm, which unlocks the immense untapped potential of decentralized infrastructure, enables democratization of MoE development for broader communities, and delivers greater scalability and cost-efficiency.Furthermore we focus on exploring its vertical domain applications. Finally, we also identify key challenges and promising future research directions. To the best of our knowledge, this survey is currently the most comprehensive review in the field of MoE. We aim for this article to serve as a valuable resource for both researchers and practitioners, enabling them to navigate and stay up-to-date with the latest advancements.

</details>

---

## 347. DeltaKV: Residual-Based KV Cache Compression via Long-Range Similarity

**Chinese Title**: DeltaKV: Residual-Based KV Cache Compression via Long-Range Similarity

**Authors**: Jitai Hao, Qiang Huang, Yaowei Wang, Min Zhang, Jun Yu

**Date**: 2026-02-08 | **arXiv**: [2602.08005v1](http://arxiv.org/abs/2602.08005v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08005v1)

**Code**: https://github.com/CURRENTF/Sparse-vLLM.

**Categories**: cs.CL, cs.AI

<details><summary><b>Abstract</b></summary>

The deployment of efficient long-context LLMs in applications like autonomous agents, long-chain reasoning, and creative writing is fundamentally bottlenecked by the linear growth of KV cache memory. Existing compression and eviction methods often struggle to balance accuracy, compression ratio, and hardware efficiency. We propose DeltaKV, a residual-based KV cache compression framework motivated by two empirical findings: long-range inter-token similarity and highly shared latent components in KV representations. Instead of discarding tokens, DeltaKV encodes semantic residuals relative to retrieved historical references, preserving fidelity while substantially reducing storage. To translate compression gains into real system speedups, we further introduce Sparse-vLLM, a high-performance inference engine with decoupled memory management and kernels optimized for sparse and irregular KV layouts. Experiments show that DeltaKV reduces KV cache memory to 29\% of the original while maintaining near-lossless accuracy on LongBench, SCBench, and AIME. When integrated with Sparse-vLLM, it achieves up to 2$\times$ throughput improvement over vLLM in long-context scenarios, demonstrating a practical path toward scalable long-context LLM deployment. Code, model checkpoints, and datasets are available at https://github.com/CURRENTF/Sparse-vLLM.

</details>

<details><summary><b>Chinese Abstract</b></summary>

The deployment of efficient long-context LLMs in applications like autonomous agents, long-chain reasoning, and creative writing is fundamentally bottlenecked by the linear growth of KV cache memory. Existing compression and eviction methods often struggle to balance accuracy, compression ratio, and hardware efficiency.We propose DeltaKV, a residual-based KV cache compression framework motivated by two empirical findings: long-range inter-token similarity and highly shared latent components in KV representations. Instead of discarding tokens, DeltaKV encodes semantic residuals relative to retrieved historical references, preserving fidelity while substantially reducing storage.To translate compression gains into real system speedups, we further introduce Sparse-vLLM, a high-performance inference engine with decoupled memory management and kernels optimized for sparse and irregular KV layouts. Experiments show that DeltaKV reduces KV cache memory to 29\% of the original while maintaining near-lossless accuracy on LongBench, SCBench, and AIME.When integrated with Sparse-vLLM, it achieves up to 2$\times$ throughput improvement over vLLM in long-context scenarios, demonstrating a practical path toward scalable long-context LLM deployment. Code, model checkpoints, and datasets are available at https://github.com/CURRENTF/Sparse-vLLM.

</details>

---

## 348. Don't Always Pick the Highest-Performing Model: An Information Theoretic View of LLM Ensemble Selection

**Chinese Title**: Don't Always Pick the Highest-Performing Model: An Information Theoretic View of LLM Ensemble Selection

**Authors**: Yigit Turkmen, Baturalp Buyukates, Melih Bastopcu

**Date**: 2026-02-08 | **arXiv**: [2602.08003v1](http://arxiv.org/abs/2602.08003v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08003v1)

**Categories**: cs.LG, cs.AI, cs.DC, cs.IT, stat.ML

<details><summary><b>Abstract</b></summary>

Large language models (LLMs) are often ensembled together to improve overall reliability and robustness, but in practice models are strongly correlated. This raises a fundamental question: which models should be selected when forming an LLM ensemble? We formulate budgeted ensemble selection as maximizing the mutual information between the true label and predictions of the selected models. Furthermore, to explain why performance can saturate even with many models, we model the correlated errors of the models using Gaussian-copula and show an information-theoretic error floor for the performance of the ensemble. Motivated by these, we propose a simple greedy mutual-information selection algorithm that estimates the required information terms directly from data and iteratively builds an ensemble under a query budget. We test our approach in two question answering datasets and one binary sentiment classification dataset: MEDMCQA, MMLU, and IMDB movie reviews. Across all datasets, we observe that our method consistently outperforms strong baselines under the same query budget.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Large language models (LLMs) are often ensembled together to improve overall reliability and robustness, but in practice models are strongly correlated. This raises a fundamental question: which models should be selected when forming an LLM ensemble? We formulate budgeted ensemble selection as maximizing the mutual information between the true label and predictions of the selected models.Furthermore, to explain why performance can saturate even with many models, we model the correlated errors of the models using Gaussian-copula and show an information-theoretic error floor for the performance of the ensemble. Motivated by these, we propose a simple greedy mutual-information selection algorithm that estimates the required information terms directly from data and iteratively builds an ensemble under a query budget.We test our approach in two question answering datasets and one binary sentiment classification dataset: MEDMCQA, MMLU, and IMDB movie reviews. Across all datasets, we observe that our method consistently outperforms strong baselines under the same query budget.

</details>

---

## 349. Analyzing the Impact of Simulation Fidelity on the Evaluation of Autonomous Driving Motion Control

**Chinese Title**: Analyzing the Impact of Simulation Fidelity on the Evaluation of Autonomous Driving Motion Control

**Authors**: Simon Sagmeister, Panagiotis Kounatidis, Sven Goblirsch, Markus Lienkamp

**Date**: 2026-02-08 | **arXiv**: [2602.07984v1](http://arxiv.org/abs/2602.07984v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.07984v1)

**Categories**: cs.RO

<details><summary><b>Abstract</b></summary>

Simulation is crucial in the development of autonomous driving software. In particular, assessing control algorithms requires an accurate vehicle dynamics simulation. However, recent publications use models with varying levels of detail. This disparity makes it difficult to compare individual control algorithms. Therefore, this paper aims to investigate the influence of the fidelity of vehicle dynamics modeling on the closed-loop behavior of trajectory-following controllers. For this purpose, we introduce a comprehensive Autoware-compatible vehicle model. By simplifying this, we derive models with varying fidelity. Evaluating over 550 simulation runs allows us to quantify each model's approximation quality compared to real-world data. Furthermore, we investigate whether the influence of model simplifications changes with varying margins to the acceleration limit of the vehicle. From this, we deduce to which degree a vehicle model can be simplified to evaluate control algorithms depending on the specific application. The real-world data used to validate the simulation environment originate from the Indy Autonomous Challenge race at the Autodromo Nazionale di Monza in June 2023. They show the fastest fully autonomous lap of TUM Autonomous Motorsport, with vehicle speeds reaching 267 kph and lateral accelerations of up to 15 mps2.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Simulation is crucial in the development of autonomous driving software. In particular, assessing control algorithms requires an accurate vehicle dynamics simulation. However, recent publications use models with varying levels of detail. This disparity makes it difficult to compare individual control algorithms.Therefore, this paper aims to investigate the influence of the fidelity of vehicle dynamics modeling on the closed-loop behavior of trajectory-following controllers. For this purpose, we introduce a comprehensive Autoware-compatible vehicle model. By simplifying this, we derive models with varying fidelity. Evaluating over 550 simulation runs allows us to quantify each model's approximation quality compared to real-world data.Furthermore, we investigate whether the influence of model simplifications changes with varying margins to the acceleration limit of the vehicle. From this, we deduce to which degree a vehicle model can be simplified to evaluate control algorithms depending on the specific application. The real-world data used to validate the simulation environment originate from the Indy Autonomous Challenge race at the Autodromo Nazionale di Monza in June 2023.They show the fastest fully autonomous lap of TUM Autonomous Motorsport, with vehicle speeds reaching 267 kph and lateral accelerations of up to 15 mps2.

</details>

---

## 350. A second order regret bound for NormalHedge

**Chinese Title**: A second order regret bound for NormalHedge

**Authors**: Yoav Freund, Nicholas J. A. Harvey, Victor S. Portella, Yabing Qi, Yu-Xiang Wang

**Date**: 2026-02-08 | **arXiv**: [2602.08151v1](http://arxiv.org/abs/2602.08151v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08151v1)

**Categories**: cs.LG, stat.ML

<details><summary><b>Abstract</b></summary>

We consider the problem of prediction with expert advice for ``easy'' sequences. We show that a variant of NormalHedge enjoys a second-order $Îµ$-quantile regret bound of $O\big(\sqrt{V_T \log(V_T/Îµ)}\big) $ when $V_T > \log N$, where $V_T$ is the cumulative second moment of instantaneous per-expert regret averaged with respect to a natural distribution determined by the algorithm. The algorithm is motivated by a continuous time limit using Stochastic Differential Equations. The discrete time analysis uses self-concordance techniques.

</details>

<details><summary><b>Chinese Abstract</b></summary>

We consider the problem of prediction with expert advice for ``easy'' sequences. We show that a variant of NormalHedge enjoys a second-order $Îµ$-quantile regret bound of $O\big(\sqrt{V_T \log(V_T/Îµ)}\big) $ when $V_T > \log N$, where $V_T$ is the cumulative second moment of instantaneous per-expert regret averaged with respect to a natural distribution determined by the algorithm.The algorithm is motivated by a continuous time limit using Stochastic Differential Equations. The discrete time analysis uses self-concordance techniques.

</details>

---

## 351. The CAPSARII Approach to Cyber-Secure Wearable, Ultra-Low-Power Networked Sensors for Soldier Health Monitoring

**Chinese Title**: The CAPSARII Approach to Cyber-Secure Wearable, Ultra-Low-Power Networked Sensors for Soldier Health Monitoring

**Authors**: Luciano Bozzi, Christian Celidonio, Umberto Nuzzi, Massimo Biagini, Stefano Cherubin et al.

**Date**: 2026-02-08 | **arXiv**: [2602.08080v1](http://arxiv.org/abs/2602.08080v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08080v1)

**Categories**: cs.ET, cs.DC, cs.LG

<details><summary><b>Abstract</b></summary>

The European Defence Agency's revised Capability Development Plan (CDP) identifies as a priority improving ground combat capabilities by enhancing soldiers' equipment for better protection. The CAPSARII project proposes in innovative wearable system and Internet of Battlefield Things (IoBT) framework to monitor soldiers' physiological and psychological status, aiding tactical decisions and medical support. The CAPSARII system will enhance situational awareness and operational effectiveness by monitoring physiological, movement and environmental parameters, providing real-time tactical decision support through AI models deployed on edge nodes and enable data analysis and comparative studies via cloud-based analytics. CAPSARII also aims at improving usability through smart textile integration, longer battery life, reducing energy consumption through software and hardware optimizations, and address security concerns with efficient encryption and strong authentication methods. This innovative approach aims to transform military operations by providing a robust, data-driven decision support tool.

</details>

<details><summary><b>Chinese Abstract</b></summary>

The European Defence Agency's revised Capability Development Plan (CDP) identifies as a priority improving ground combat capabilities by enhancing soldiers' equipment for better protection. The CAPSARII project proposes in innovative wearable system and Internet of Battlefield Things (IoBT) framework to monitor soldiers' physiological and psychological status, aiding tactical decisions and medical support.The CAPSARII system will enhance situational awareness and operational effectiveness by monitoring physiological, movement and environmental parameters, providing real-time tactical decision support through AI models deployed on edge nodes and enable data analysis and comparative studies via cloud-based analytics.CAPSARII also aims at improving usability through smart textile integration, longer battery life, reducing energy consumption through software and hardware optimizations, and address security concerns with efficient encryption and strong authentication methods. This innovative approach aims to transform military operations by providing a robust, data-driven decision support tool.

</details>

---

## 352. Efficient Distribution Learning with Error Bounds in Wasserstein Distance

**Chinese Title**: Efficient Distribution Learning with Error Bounds in Wasserstein Distance

**Authors**: Eduardo Figueiredo, Steven Adams, Luca Laurenti

**Date**: 2026-02-08 | **arXiv**: [2602.08063v1](http://arxiv.org/abs/2602.08063v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08063v1)

**Categories**: cs.LG

<details><summary><b>Abstract</b></summary>

The Wasserstein distance has emerged as a key metric to quantify distances between probability distributions, with applications in various fields, including machine learning, control theory, decision theory, and biological systems. Consequently, learning an unknown distribution with non-asymptotic and easy-to-compute error bounds in Wasserstein distance has become a fundamental problem in many fields. In this paper, we devise a novel algorithmic and theoretical framework to approximate an unknown probability distribution $\mathbb{P}$ from a finite set of samples by an approximate discrete distribution $\widehat{\mathbb{P}}$ while bounding the Wasserstein distance between $\mathbb{P}$ and $\widehat{\mathbb{P}}$. Our framework leverages optimal transport, nonlinear optimization, and concentration inequalities. In particular, we show that, even if $\mathbb{P}$ is unknown, the Wasserstein distance between $\mathbb{P}$ and $\widehat{\mathbb{P}}$ can be efficiently bounded with high confidence by solving a tractable optimization problem (a mixed integer linear program) of a size that only depends on the size of the support of $\widehat{\mathbb{P}}$. This enables us to develop intelligent clustering algorithms to optimally find the support of $\widehat{\mathbb{P}}$ while minimizing the Wasserstein distance error. On a set of benchmarks, we demonstrate that our approach outperforms state-of-the-art comparable methods by generally returning approximating distributions with substantially smaller support and tighter error bounds.

</details>

<details><summary><b>Chinese Abstract</b></summary>

The Wasserstein distance has emerged as a key metric to quantify distances between probability distributions, with applications in various fields, including machine learning, control theory, decision theory, and biological systems. Consequently, learning an unknown distribution with non-asymptotic and easy-to-compute error bounds in Wasserstein distance has become a fundamental problem in many fields.In this paper, we devise a novel algorithmic and theoretical framework to approximate an unknown probability distribution $\mathbb{P}$ from a finite set of samples by an approximate discrete distribution $\widehat{\mathbb{P}}$ while bounding the Wasserstein distance between $\mathbb{P}$ and $\widehat{\mathbb{P}}$. Our framework leverages optimal transport, nonlinear optimization, and concentration inequalities.In particular, we show that, even if $\mathbb{P}$ is unknown, the Wasserstein distance between $\mathbb{P}$ and $\widehat{\mathbb{P}}$ can be efficiently bounded with high confidence by solving a tractable optimization problem (a mixed integer linear program) of a size that only depends on the size of the support of $\widehat{\mathbb{P}}$.This enables us to develop intelligent clustering algorithms to optimally find the support of $\widehat{\mathbb{P}}$ while minimizing the Wasserstein distance error. On a set of benchmarks, we demonstrate that our approach outperforms state-of-the-art comparable methods by generally returning approximating distributions with substantially smaller support and tighter error bounds.

</details>

---

## 353. Graph-based Semi-Supervised Learning via Maximum Discrimination

**Chinese Title**: Graph-based Semi-Supervised Learning via Maximum Discrimination

**Authors**: Nadav Katz, Ariel Jaffe

**Date**: 2026-02-08 | **arXiv**: [2602.08042v1](http://arxiv.org/abs/2602.08042v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08042v1)

**Categories**: stat.ML, cs.LG

<details><summary><b>Abstract</b></summary>

Semi-supervised learning (SSL) addresses the critical challenge of training accurate models when labeled data is scarce but unlabeled data is abundant. Graph-based SSL (GSSL) has emerged as a popular framework that captures data structure through graph representations. Classic graph SSL methods, such as Label Propagation and Label Spreading, aim to compute low-dimensional representations where points with the same labels are close in representation space. Although often effective, these methods can be suboptimal on data with complex label distributions. In our work, we develop AUC-spec, a graph approach that computes a low-dimensional representation that maximizes class separation. We compute this representation by optimizing the Area Under the ROC Curve (AUC) as estimated via the labeled points. We provide a detailed analysis of our approach under a product-of-manifold model, and show that the required number of labeled points for AUC-spec is polynomial in the model parameters. Empirically, we show that AUC-spec balances class separation with graph smoothness. It demonstrates competitive results on synthetic and real-world datasets while maintaining computational efficiency comparable to the field's classic and state-of-the-art methods.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Semi-supervised learning (SSL) addresses the critical challenge of training accurate models when labeled data is scarce but unlabeled data is abundant. Graph-based SSL (GSSL) has emerged as a popular framework that captures data structure through graph representations.Classic graph SSL methods, such as Label Propagation and Label Spreading, aim to compute low-dimensional representations where points with the same labels are close in representation space. Although often effective, these methods can be suboptimal on data with complex label distributions. In our work, we develop AUC-spec, a graph approach that computes a low-dimensional representation that maximizes class separation.We compute this representation by optimizing the Area Under the ROC Curve (AUC) as estimated via the labeled points. We provide a detailed analysis of our approach under a product-of-manifold model, and show that the required number of labeled points for AUC-spec is polynomial in the model parameters. Empirically, we show that AUC-spec balances class separation with graph smoothness.It demonstrates competitive results on synthetic and real-world datasets while maintaining computational efficiency comparable to the field's classic and state-of-the-art methods.

</details>

---

## 354. Sharp analysis of linear ensemble sampling

**Chinese Title**: Sharp analysis of linear ensemble sampling

**Authors**: Arya Akhavan, David Janz, Csaba SzepesvÃ¡ri

**Date**: 2026-02-08 | **arXiv**: [2602.08026v1](http://arxiv.org/abs/2602.08026v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08026v1)

**Categories**: cs.LG, stat.ML

<details><summary><b>Abstract</b></summary>

We analyse linear ensemble sampling (ES) with standard Gaussian perturbations in stochastic linear bandits. We show that for ensemble size $m=Î˜(d\log n)$, ES attains $\tilde O(d^{3/2}\sqrt n)$ high-probability regret, closing the gap to the Thompson sampling benchmark while keeping computation comparable. The proof brings a new perspective on randomized exploration in linear bandits by reducing the analysis to a time-uniform exceedance problem for $m$ independent Brownian motions. Intriguingly, this continuous-time lens is not forced; it appears natural--and perhaps necessary: the discrete-time problem seems to be asking for a continuous-time solution, and we know of no other way to obtain a sharp ES bound.

</details>

<details><summary><b>Chinese Abstract</b></summary>

We analyse linear ensemble sampling (ES) with standard Gaussian perturbations in stochastic linear bandits. We show that for ensemble size $m=Î˜(d\log n)$, ES attains $\tilde O(d^{3/2}\sqrt n)$ high-probability regret, closing the gap to the Thompson sampling benchmark while keeping computation comparable.The proof brings a new perspective on randomized exploration in linear bandits by reducing the analysis to a time-uniform exceedance problem for $m$ independent Brownian motions. Intriguingly, this continuous-time lens is not forced; it appears natural--and perhaps necessary: the discrete-time problem seems to be asking for a continuous-time solution, and we know of no other way to obtain a sharp ES bound.

</details>

---

## 355. A Unified Density Operator View of Flow Control and Merging

**Chinese Title**: A Unified Density Operator View of Flow Control and Merging

**Authors**: Riccardo De Santi, Malte Franke, Ya-Ping Hsieh, Andreas Krause

**Date**: 2026-02-08 | **arXiv**: [2602.08012v1](http://arxiv.org/abs/2602.08012v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08012v1)

**Categories**: cs.LG

<details><summary><b>Abstract</b></summary>

Recent progress in large-scale flow and diffusion models raised two fundamental algorithmic challenges: (i) control-based reward adaptation of pre-trained flows, and (ii) integration of multiple models, i.e., flow merging. While current approaches address them separately, we introduce a unifying probability-space framework that subsumes both as limit cases, and enables reward-guided flow merging, allowing principled, task-aware combination of multiple pre-trained flows (e.g., merging priors while maximizing drug-discovery utilities). Our formulation renders possible to express a rich family of operators over generative models densities, including intersection (e.g., to enforce safety), union (e.g., to compose diverse models), interpolation (e.g., for discovery), their reward-guided counterparts, as well as complex logical expressions via generative circuits. Next, we introduce Reward-Guided Flow Merging (RFM), a mirror-descent scheme that reduces reward-guided flow merging to a sequence of standard fine-tuning problems. Then, we provide first-of-their-kind theoretical guarantees for reward-guided and pure flow merging via RFM. Ultimately, we showcase the capabilities of the proposed method on illustrative settings providing visually interpretable insights, and apply our method to high-dimensional de-novo molecular design and low-energy conformer generation.

</details>

<details><summary><b>Chinese Abstract</b></summary>

Recent progress in large-scale flow and diffusion models raised two fundamental algorithmic challenges: (i) control-based reward adaptation of pre-trained flows, and (ii) integration of multiple models, i.e., flow merging.While current approaches address them separately, we introduce a unifying probability-space framework that subsumes both as limit cases, and enables reward-guided flow merging, allowing principled, task-aware combination of multiple pre-trained flows (e.g., merging priors while maximizing drug-discovery utilities).Our formulation renders possible to express a rich family of operators over generative models densities, including intersection (e.g., to enforce safety), union (e.g., to compose diverse models), interpolation (e.g., for discovery), their reward-guided counterparts, as well as complex logical expressions via generative circuits.Next, we introduce Reward-Guided Flow Merging (RFM), a mirror-descent scheme that reduces reward-guided flow merging to a sequence of standard fine-tuning problems. Then, we provide first-of-their-kind theoretical guarantees for reward-guided and pure flow merging via RFM.Ultimately, we showcase the capabilities of the proposed method on illustrative settings providing visually interpretable insights, and apply our method to high-dimensional de-novo molecular design and low-energy conformer generation.

</details>

---



</details>

<!-- PAPERS_CONTENT_END -->

## ğŸš€ å¿«é€Ÿå¼€å§‹

1. **é…ç½® API Key**ï¼šåœ¨ä»“åº“ `Settings -> Secrets and variables -> Actions -> Secrets` ä¸­æ·»åŠ  `DEEPSEEK_API_KEY`ã€‚
2. **æ‰‹åŠ¨è¿è¡Œ**ï¼šåœ¨ `Actions` æ ‡ç­¾é¡µé€‰æ‹© `Daily Video Papers Update` å¹¶ç‚¹å‡» `Run workflow`ã€‚
3. **åˆ‡æ¢ç‰ˆæœ¬**ï¼šåœ¨ `Variables` ä¸­è®¾ç½® `VERSION` ä¸º `v2` å³å¯å¼€å¯ AI æ·±åº¦åˆ†ææ¨¡å¼ã€‚

## ğŸ› ï¸ æŠ€æœ¯ç»†èŠ‚

- **æ•°æ®æº**ï¼šarXiv API (cs.CV, cs.AI, cs.MM, cs.RO, cs.LG)
- **ç¿»è¯‘/åˆ†æ**ï¼šDeepSeek API (ä¼˜å…ˆ) / Gemini (å¤‡ç”¨)
- **è‡ªåŠ¨åŒ–**ï¼šGitHub Actions

---
*æœ¬é¡¹ç›®ç”± Manus è‡ªåŠ¨ç”Ÿæˆå¹¶ç»´æŠ¤ã€‚*
