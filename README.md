# Daily Video Papers ğŸ¥

?? **Website**: https://greasebig.github.io/daily-video-papers/

![Actions](https://github.com/greasebig/daily-video-papers/actions/workflows/daily-update.yml/badge.svg) ![Pages](https://img.shields.io/badge/pages-online-brightgreen) ![Visitors](https://visitor-badge.laobi.icu/badge?page_id=greasebig.daily-video-papers)

## ğŸ“š è®ºæ–‡ç´¢å¼•

<!-- PAPERS_INDEX_START -->
- [2026-02-13](papers/2026-02-13.md) - 19 papers
<!-- PAPERS_INDEX_END -->

## Other Topics

- [World Model Papers](world-model/README.md)
- [Agent Papers](agent/README.md)

## Daily Papers

<!-- PAPERS_CONTENT_START -->
<details><summary><b>2026-02-13 (19 papers)</b></summary>

# arXiv Video Papers - 2026-02-13

**Paper Count**: 19

---

## 1. MonarchRT: Efficient Attention for Real-Time Video Generation / MonarchRTï¼šå®æ—¶è§†é¢‘ç”Ÿæˆçš„é«˜æ•ˆå…³æ³¨

**Date**: 2026-02-12 | **arXiv**: [2602.12271v1](http://arxiv.org/abs/2602.12271v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.12271v1)

**Categories**: cs.CV, cs.LG

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Real-time video generation with Diffusion Transformers is bottlenecked by the quadratic cost of 3D self-attention, especially in real-time regimes that are both few-step and autoregressive, where errors compound across time and each denoising step must carry substantially more information. In this setting, we find that prior sparse-attention approximations break down, despite showing strong results for bidirectional, many-step diffusion. Specifically, we observe that video attention is not reliably sparse, but instead combines pronounced periodic structure driven by spatiotemporal position with dynamic, sparse semantic correspondences and dense mixing, exceeding the representational capacity of even oracle top-k attention. Building on this insight, we propose Monarch-RT, a structured attention parameterization for video diffusion models that factorizes attention using Monarch matrices. Through appropriately aligned block structure and our extended tiled Monarch parameterization, we achieve high expressivity while preserving computational efficiency. We further overcome the overhead of parameterization through finetuning, with custom Triton kernels. We first validate the high efficacy of Monarch-RT over existing sparse baselines designed only for bidirectional models. We further observe that Monarch-RT attains up to 95% attention sparsity with no loss in quality when applied to the state-of-the-art model Self-Forcing, making Monarch-RT a pioneering work on highly-capable sparse attention parameterization for real-time video generation. Our optimized implementation outperforms FlashAttention-2, FlashAttention-3, and FlashAttention-4 kernels on Nvidia RTX 5090, H100, and B200 GPUs respectively, providing kernel speedups in the range of 1.4-11.8X. This enables us, for the first time, to achieve true real-time video generation with Self-Forcing at 16 FPS on a single RTX 5090.

ä½¿ç”¨æ‰©æ•£å˜å‹å™¨çš„å®æ—¶è§†é¢‘ç”Ÿæˆå—åˆ° 3D è‡ªæ³¨æ„åŠ›äºŒæ¬¡æˆæœ¬çš„ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯åœ¨å°‘æ­¥å’Œè‡ªå›å½’çš„å®æ—¶æœºåˆ¶ä¸­ï¼Œå…¶ä¸­è¯¯å·®éšæ—¶é—´å¤åˆï¼Œæ¯ä¸ªå»å™ªæ­¥éª¤å¿…é¡»æºå¸¦æ›´å¤šä¿¡æ¯ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å‘ç°å°½ç®¡åŒå‘ã€å¤šæ­¥æ‰©æ•£æ˜¾ç¤ºå‡ºå¾ˆå¼ºçš„ç»“æœï¼Œä½†å…ˆå‰çš„ç¨€ç–æ³¨æ„åŠ›è¿‘ä¼¼æ–¹æ³•è¿˜æ˜¯å¤±æ•ˆäº†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è§†é¢‘æ³¨æ„åŠ›å¹¶ä¸æ˜¯å¯é çš„ç¨€ç–ï¼Œè€Œæ˜¯å°†ç”±æ—¶ç©ºä½ç½®é©±åŠ¨çš„æ˜¾ç€å‘¨æœŸç»“æ„ä¸åŠ¨æ€ã€ç¨€ç–è¯­ä¹‰å¯¹åº”å’Œå¯†é›†æ··åˆç›¸ç»“åˆï¼Œç”šè‡³è¶…è¿‡äº† oracle top-k æ³¨æ„åŠ›çš„è¡¨ç¤ºèƒ½åŠ›ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº† Monarch-RTï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè§†é¢‘æ‰©æ•£æ¨¡å‹çš„ç»“æ„åŒ–æ³¨æ„åŠ›å‚æ•°åŒ–ï¼Œå®ƒä½¿ç”¨ Monarch çŸ©é˜µåˆ†è§£æ³¨æ„åŠ›ã€‚é€šè¿‡é€‚å½“å¯¹é½çš„å—ç»“æ„å’Œæ‰©å±•çš„å¹³é“º Monarch å‚æ•°åŒ–ï¼Œæˆ‘ä»¬åœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶å®ç°äº†é«˜è¡¨è¾¾åŠ›ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨è‡ªå®šä¹‰ Triton å†…æ ¸è¿›è¡Œå¾®è°ƒï¼Œè¿›ä¸€æ­¥å…‹æœäº†å‚æ•°åŒ–çš„å¼€é”€ã€‚æˆ‘ä»¬é¦–å…ˆéªŒè¯ Monarch-RT ç›¸å¯¹äºä»…ä¸ºåŒå‘æ¨¡å‹è®¾è®¡çš„ç°æœ‰ç¨€ç–åŸºçº¿çš„é«˜æ•ˆæ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è§‚å¯Ÿåˆ°ï¼Œå½“åº”ç”¨äºæœ€å…ˆè¿›çš„æ¨¡å‹ Self-Forcing æ—¶ï¼ŒMonarch-RT è·å¾—äº†é«˜è¾¾ 95% çš„æ³¨æ„åŠ›ç¨€ç–åº¦ï¼Œä¸”è´¨é‡æ²¡æœ‰æŸå¤±ï¼Œè¿™ä½¿å¾— Monarch-RT æˆä¸ºå®æ—¶è§†é¢‘ç”Ÿæˆçš„é«˜æ€§èƒ½ç¨€ç–æ³¨æ„åŠ›å‚æ•°åŒ–çš„å¼€åˆ›æ€§å·¥ä½œã€‚æˆ‘ä»¬çš„ä¼˜åŒ–å®ç°åœ¨ Nvidia RTX 5090ã€H100 å’Œ B200 GPU ä¸Šçš„æ€§èƒ½åˆ†åˆ«ä¼˜äº FlashAttention-2ã€FlashAttention-3 å’Œ FlashAttention-4 å†…æ ¸ï¼Œæä¾› 1.4-11.8 å€çš„å†…æ ¸åŠ é€Ÿã€‚è¿™ä½¿æˆ‘ä»¬ç¬¬ä¸€æ¬¡èƒ½å¤Ÿåœ¨å•ä¸ª RTX 5090 ä¸Šä»¥ 16 FPS çš„é€Ÿåº¦å®ç°çœŸæ­£çš„å®æ—¶è§†é¢‘ç”Ÿæˆã€‚

</details>

---

## 2. DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation / DreamID-Omniï¼šå¯æ§çš„ä»¥äººä¸ºæœ¬çš„éŸ³é¢‘è§†é¢‘ç”Ÿæˆçš„ç»Ÿä¸€æ¡†æ¶

**Date**: 2026-02-12 | **arXiv**: [2602.12160v1](http://arxiv.org/abs/2602.12160v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.12160v1)

**Categories**: cs.CV

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audio-video generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives. Furthermore, achieving precise, disentangled control over multiple character identities and voice timbres within a single framework remains an open challenge. In this paper, we propose DreamID-Omni, a unified framework for controllable human-centric audio-video generation. Specifically, we design a Symmetric Conditional Diffusion Transformer that integrates heterogeneous conditioning signals via a symmetric conditional injection scheme. To resolve the pervasive identity-timbre binding failures and speaker confusion in multi-person scenarios, we introduce a Dual-Level Disentanglement strategy: Synchronized RoPE at the signal level to ensure rigid attention-space binding, and Structured Captions at the semantic level to establish explicit attribute-subject mappings. Furthermore, we devise a Multi-Task Progressive Training scheme that leverages weakly-constrained generative priors to regularize strongly-constrained tasks, preventing overfitting and harmonizing disparate objectives. Extensive experiments demonstrate that DreamID-Omni achieves comprehensive state-of-the-art performance across video, audio, and audio-visual consistency, even outperforming leading proprietary commercial models. We will release our code to bridge the gap between academic research and commercial-grade applications.

åŸºç¡€æ¨¡å‹çš„æœ€æ–°è¿›å±•å½»åº•æ”¹å˜äº†è”åˆéŸ³é¢‘è§†é¢‘ç”Ÿæˆã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸å°†ä»¥äººä¸ºä¸­å¿ƒçš„ä»»åŠ¡è§†ä¸ºå­¤ç«‹çš„ç›®æ ‡ï¼ŒåŒ…æ‹¬åŸºäºå‚è€ƒçš„éŸ³é¢‘è§†é¢‘ç”Ÿæˆï¼ˆR2AVï¼‰ã€è§†é¢‘ç¼–è¾‘ï¼ˆRV2AVï¼‰å’ŒéŸ³é¢‘é©±åŠ¨è§†é¢‘åŠ¨ç”»ï¼ˆRA2Vï¼‰ã€‚æ­¤å¤–ï¼Œåœ¨å•ä¸€æ¡†æ¶å†…å®ç°å¯¹å¤šä¸ªè§’è‰²èº«ä»½å’ŒéŸ³è‰²çš„ç²¾ç¡®ã€åˆ†ç¦»çš„æ§åˆ¶ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† DreamID-Omniï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¯æ§çš„ä»¥äººä¸ºä¸­å¿ƒçš„éŸ³é¢‘è§†é¢‘ç”Ÿæˆçš„ç»Ÿä¸€æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¯¹ç§°æ¡ä»¶æ‰©æ•£å˜å‹å™¨ï¼Œå®ƒé€šè¿‡å¯¹ç§°æ¡ä»¶æ³¨å…¥æ–¹æ¡ˆé›†æˆå¼‚æ„è°ƒèŠ‚ä¿¡å·ã€‚ä¸ºäº†è§£å†³å¤šäººåœºæ™¯ä¸­æ™®éå­˜åœ¨çš„èº«ä»½-éŸ³è‰²ç»‘å®šå¤±è´¥å’Œè¯´è¯è€…æ··ä¹±çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŒå±‚è§£ç¼ ç­–ç•¥ï¼šä¿¡å·çº§åˆ«çš„åŒæ­¥ RoPE ä»¥ç¡®ä¿ä¸¥æ ¼çš„æ³¨æ„åŠ›ç©ºé—´ç»‘å®šï¼Œè¯­ä¹‰çº§åˆ«çš„ç»“æ„åŒ–å­—å¹•ä»¥å»ºç«‹æ˜ç¡®çš„å±æ€§-ä¸»é¢˜æ˜ å°„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å¤šä»»åŠ¡æ¸è¿›è®­ç»ƒæ–¹æ¡ˆï¼Œåˆ©ç”¨å¼±çº¦æŸçš„ç”Ÿæˆå…ˆéªŒæ¥è§„èŒƒå¼ºçº¦æŸçš„ä»»åŠ¡ï¼Œé˜²æ­¢è¿‡åº¦æ‹Ÿåˆå¹¶åè°ƒä¸åŒçš„ç›®æ ‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDreamID-Omni åœ¨è§†é¢‘ã€éŸ³é¢‘å’Œè§†å¬ä¸€è‡´æ€§æ–¹é¢å®ç°äº†å…¨é¢çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œç”šè‡³è¶…è¶Šäº†é¢†å…ˆçš„ä¸“æœ‰å•†ä¸šæ¨¡å‹ã€‚æˆ‘ä»¬å°†å‘å¸ƒæˆ‘ä»¬çš„ä»£ç ï¼Œä»¥å¼¥åˆå­¦æœ¯ç ”ç©¶å’Œå•†ä¸šçº§åº”ç”¨ç¨‹åºä¹‹é—´çš„å·®è·ã€‚

</details>

---

## 3. FAIL: Flow Matching Adversarial Imitation Learning for Image Generation / å¤±è´¥ï¼šç”¨äºå›¾åƒç”Ÿæˆçš„æµåŒ¹é…å¯¹æŠ—æ€§æ¨¡ä»¿å­¦ä¹ 

**Date**: 2026-02-12 | **arXiv**: [2602.12155v1](http://arxiv.org/abs/2602.12155v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.12155v1)

**Categories**: cs.CV

**Code**: https://github.com/HansPolo113/FAIL.

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Post-training of flow matching models-aligning the output distribution with a high-quality target-is mathematically equivalent to imitation learning. While Supervised Fine-Tuning mimics expert demonstrations effectively, it cannot correct policy drift in unseen states. Preference optimization methods address this but require costly preference pairs or reward modeling. We propose Flow Matching Adversarial Imitation Learning (FAIL), which minimizes policy-expert divergence through adversarial training without explicit rewards or pairwise comparisons. We derive two algorithms: FAIL-PD exploits differentiable ODE solvers for low-variance pathwise gradients, while FAIL-PG provides a black-box alternative for discrete or computationally constrained settings. Fine-tuning FLUX with only 13,000 demonstrations from Nano Banana pro, FAIL achieves competitive performance on prompt following and aesthetic benchmarks. Furthermore, the framework generalizes effectively to discrete image and video generation, and functions as a robust regularizer to mitigate reward hacking in reward-based optimization. Code and data are available at https://github.com/HansPolo113/FAIL.

æµåŒ¹é…æ¨¡å‹çš„åè®­ç»ƒâ€”â€”å°†è¾“å‡ºåˆ†å¸ƒä¸é«˜è´¨é‡ç›®æ ‡å¯¹é½â€”â€”åœ¨æ•°å­¦ä¸Šç­‰åŒäºæ¨¡ä»¿å­¦ä¹ ã€‚è™½ç„¶ç›‘ç£å¾®è°ƒæœ‰æ•ˆåœ°æ¨¡ä»¿äº†ä¸“å®¶çš„æ¼”ç¤ºï¼Œä½†å®ƒæ— æ³•çº æ­£çœ‹ä¸è§çš„çŠ¶æ€ä¸­çš„æ”¿ç­–æ¼‚ç§»ã€‚åå¥½ä¼˜åŒ–æ–¹æ³•å¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†éœ€è¦æ˜‚è´µçš„åå¥½å¯¹æˆ–å¥–åŠ±å»ºæ¨¡ã€‚æˆ‘ä»¬æå‡ºäº†æµåŒ¹é…å¯¹æŠ—æ€§æ¨¡ä»¿å­¦ä¹ ï¼ˆFAILï¼‰ï¼Œå®ƒé€šè¿‡å¯¹æŠ—æ€§è®­ç»ƒæ¥æœ€å°åŒ–æ”¿ç­–ä¸“å®¶åˆ†æ­§ï¼Œè€Œæ— éœ€æ˜ç¡®çš„å¥–åŠ±æˆ–æˆå¯¹æ¯”è¾ƒã€‚æˆ‘ä»¬æ¨å¯¼å‡ºä¸¤ç§ç®—æ³•ï¼šFAIL-PD åˆ©ç”¨å¯å¾®åˆ† ODE æ±‚è§£å™¨æ¥å®ç°ä½æ–¹å·®è·¯å¾„æ¢¯åº¦ï¼Œè€Œ FAIL-PG åˆ™ä¸ºç¦»æ•£æˆ–è®¡ç®—çº¦æŸè®¾ç½®æä¾›é»‘ç›’æ›¿ä»£æ–¹æ¡ˆã€‚ä»…é€šè¿‡ Nano Banana pro çš„ 13,000 æ¬¡æ¼”ç¤ºå¯¹ FLUX è¿›è¡Œå¾®è°ƒï¼ŒFAIL åœ¨å¿«é€Ÿè·Ÿéšå’Œç¾å­¦åŸºå‡†æ–¹é¢å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆåœ°æ¨å¹¿åˆ°ç¦»æ•£å›¾åƒå’Œè§†é¢‘ç”Ÿæˆï¼Œå¹¶ä½œä¸ºå¼ºå¤§çš„æ­£åˆ™åŒ–å™¨æ¥å‡è½»åŸºäºå¥–åŠ±çš„ä¼˜åŒ–ä¸­çš„å¥–åŠ±é»‘å®¢è¡Œä¸ºã€‚ä»£ç å’Œæ•°æ®å¯åœ¨ https://github.com/HansPolo113/FAIL è·å–ã€‚

</details>

---

## 4. How to Sample High Quality 3D Fractals for Action Recognition Pre-Training? / å¦‚ä½•å¯¹é«˜è´¨é‡ 3D åˆ†å½¢è¿›è¡Œé‡‡æ ·ä»¥è¿›è¡ŒåŠ¨ä½œè¯†åˆ«é¢„è®­ç»ƒï¼Ÿ

**Date**: 2026-02-12 | **arXiv**: [2602.11810v1](http://arxiv.org/abs/2602.11810v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11810v1)

**Categories**: cs.CV, cs.LG

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Synthetic datasets are being recognized in the deep learning realm as a valuable alternative to exhaustively labeled real data. One such synthetic data generation method is Formula Driven Supervised Learning (FDSL), which can provide an infinite number of perfectly labeled data through a formula driven approach, such as fractals or contours. FDSL does not have common drawbacks like manual labor, privacy and other ethical concerns. In this work we generate 3D fractals using 3D Iterated Function Systems (IFS) for pre-training an action recognition model. The fractals are temporally transformed to form a video that is used as a pre-training dataset for downstream task of action recognition. We find that standard methods of generating fractals are slow and produce degenerate 3D fractals. Therefore, we systematically explore alternative ways of generating fractals and finds that overly-restrictive approaches, while generating aesthetically pleasing fractals, are detrimental for downstream task performance. We propose a novel method, Targeted Smart Filtering, to address both the generation speed and fractal diversity issue. The method reports roughly 100 times faster sampling speed and achieves superior downstream performance against other 3D fractal filtering methods.

åˆæˆæ•°æ®é›†åœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸè¢«è®¤ä¸ºæ˜¯è¯¦å°½æ ‡è®°çš„çœŸå®æ•°æ®çš„æœ‰ä»·å€¼çš„æ›¿ä»£æ–¹æ¡ˆã€‚å…¶ä¸­ä¸€ç§åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•æ˜¯å…¬å¼é©±åŠ¨ç›‘ç£å­¦ä¹ ï¼ˆFDSLï¼‰ï¼Œå®ƒå¯ä»¥é€šè¿‡å…¬å¼é©±åŠ¨æ–¹æ³•ï¼ˆä¾‹å¦‚åˆ†å½¢æˆ–è½®å»“ï¼‰æä¾›æ— é™æ•°é‡çš„å®Œç¾æ ‡è®°æ•°æ®ã€‚ FDSL æ²¡æœ‰ä½“åŠ›åŠ³åŠ¨ã€éšç§å’Œå…¶ä»–é“å¾·é—®é¢˜ç­‰å¸¸è§ç¼ºç‚¹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ 3D è¿­ä»£å‡½æ•°ç³»ç»Ÿ (IFS) ç”Ÿæˆ 3D åˆ†å½¢ï¼Œä»¥é¢„è®­ç»ƒåŠ¨ä½œè¯†åˆ«æ¨¡å‹ã€‚åˆ†å½¢åœ¨æ—¶é—´ä¸Šè¿›è¡Œå˜æ¢ä»¥å½¢æˆè§†é¢‘ï¼Œè¯¥è§†é¢‘ç”¨ä½œåŠ¨ä½œè¯†åˆ«ä¸‹æ¸¸ä»»åŠ¡çš„é¢„è®­ç»ƒæ•°æ®é›†ã€‚æˆ‘ä»¬å‘ç°ç”Ÿæˆåˆ†å½¢çš„æ ‡å‡†æ–¹æ³•å¾ˆæ…¢å¹¶ä¸”ä¼šäº§ç”Ÿç®€å¹¶çš„ 3D åˆ†å½¢ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ¢ç´¢äº†ç”Ÿæˆåˆ†å½¢çš„æ›¿ä»£æ–¹æ³•ï¼Œå¹¶å‘ç°è¿‡åº¦é™åˆ¶çš„æ–¹æ³•è™½ç„¶ç”Ÿæˆç¾è§‚çš„åˆ†å½¢ï¼Œä½†ä¸åˆ©äºä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå³ç›®æ ‡æ™ºèƒ½è¿‡æ»¤ï¼Œæ¥è§£å†³ç”Ÿæˆé€Ÿåº¦å’Œåˆ†å½¢å¤šæ ·æ€§é—®é¢˜ã€‚ä¸å…¶ä»– 3D åˆ†å½¢è¿‡æ»¤æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•çš„é‡‡æ ·é€Ÿåº¦å¿«äº†å¤§çº¦ 100 å€ï¼Œå¹¶å®ç°äº†å“è¶Šçš„ä¸‹æ¸¸æ€§èƒ½ã€‚

</details>

---

## 5. STVG-R1: Incentivizing Instance-Level Reasoning and Grounding in Videos via Reinforcement Learning / STVG-R1ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¿€åŠ±è§†é¢‘ä¸­çš„å®ä¾‹çº§æ¨ç†å’ŒåŸºç¡€

**Date**: 2026-02-12 | **arXiv**: [2602.11730v1](http://arxiv.org/abs/2602.11730v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11730v1)

**Categories**: cs.CV

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

In vision-language models (VLMs), misalignment between textual descriptions and visual coordinates often induces hallucinations. This issue becomes particularly severe in dense prediction tasks such as spatial-temporal video grounding (STVG). Prior approaches typically focus on enhancing visual-textual alignment or attaching auxiliary decoders. However, these strategies inevitably introduce additional trainable modules, leading to significant annotation costs and computational overhead. In this work, we propose a novel visual prompting paradigm that avoids the difficult problem of aligning coordinates across modalities. Specifically, we reformulate per-frame coordinate prediction as a compact instance-level identification problem by assigning each object a unique, temporally consistent ID. These IDs are embedded into the video as visual prompts, providing explicit and interpretable inputs to the VLMs. Furthermore, we introduce STVG-R1, the first reinforcement learning framework for STVG, which employs a task-driven reward to jointly optimize temporal accuracy, spatial consistency, and structural format regularization. Extensive experiments on six benchmarks demonstrate the effectiveness of our approach. STVG-R1 surpasses the baseline Qwen2.5-VL-7B by a remarkable margin of 20.9% on m_IoU on the HCSTVG-v2 benchmark, establishing a new state of the art (SOTA). Surprisingly, STVG-R1 also exhibits strong zero-shot generalization to multi-object referring video object segmentation tasks, achieving a SOTA 47.3% J&F on MeViS.

åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸­ï¼Œæ–‡æœ¬æè¿°å’Œè§†è§‰åæ ‡ä¹‹é—´çš„é”™ä½é€šå¸¸ä¼šå¼•èµ·å¹»è§‰ã€‚è¿™ä¸ªé—®é¢˜åœ¨æ—¶ç©ºè§†é¢‘æ¥åœ°ï¼ˆSTVGï¼‰ç­‰å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­å˜å¾—å°¤ä¸ºä¸¥é‡ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾§é‡äºå¢å¼ºè§†è§‰æ–‡æœ¬å¯¹é½æˆ–é™„åŠ è¾…åŠ©è§£ç å™¨ã€‚ç„¶è€Œï¼Œè¿™äº›ç­–ç•¥ä¸å¯é¿å…åœ°å¼•å…¥é¢å¤–çš„å¯è®­ç»ƒæ¨¡å—ï¼Œå¯¼è‡´æ˜¾ç€çš„æ³¨é‡Šæˆæœ¬å’Œè®¡ç®—å¼€é”€ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†è§‰æç¤ºèŒƒä¾‹ï¼Œé¿å…äº†è·¨æ¨¡æ€å¯¹é½åæ ‡çš„éš¾é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡ä¸ºæ¯ä¸ªå¯¹è±¡åˆ†é…ä¸€ä¸ªå”¯ä¸€çš„ã€æ—¶é—´ä¸€è‡´çš„ IDï¼Œå°†æ¯å¸§åæ ‡é¢„æµ‹é‡æ–°è¡¨è¿°ä¸ºä¸€ä¸ªç´§å‡‘çš„å®ä¾‹çº§è¯†åˆ«é—®é¢˜ã€‚è¿™äº› ID ä½œä¸ºè§†è§‰æç¤ºåµŒå…¥åˆ°è§†é¢‘ä¸­ï¼Œä¸º VLM æä¾›æ˜ç¡®ä¸”å¯è§£é‡Šçš„è¾“å…¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº† STVG-R1ï¼Œè¿™æ˜¯ STVG çš„ç¬¬ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒé‡‡ç”¨ä»»åŠ¡é©±åŠ¨çš„å¥–åŠ±æ¥è”åˆä¼˜åŒ–æ—¶é—´å‡†ç¡®æ€§ã€ç©ºé—´ä¸€è‡´æ€§å’Œç»“æ„æ ¼å¼æ­£åˆ™åŒ–ã€‚å¯¹å…­ä¸ªåŸºå‡†çš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ STVG-R1 åœ¨ HCSTVG-v2 åŸºå‡†ä¸Šçš„ m_IoU ä¸Šè¶…è¶Šäº†åŸºçº¿ Qwen2.5-VL-7Bï¼Œæ˜¾ç€æé«˜äº† 20.9%ï¼Œå»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›æŠ€æœ¯ (SOTA)ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼ŒSTVG-R1 è¿˜å¯¹å¤šå¯¹è±¡å‚è€ƒè§†é¢‘å¯¹è±¡åˆ†å‰²ä»»åŠ¡è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨ MeViS ä¸Šå®ç°äº† SOTA 47.3% J&Fã€‚

</details>

---

## 6. LUVE : Latent-Cascaded Ultra-High-Resolution Video Generation with Dual Frequency Experts / LUVEï¼šåŒé¢‘ä¸“å®¶çš„æ½œåœ¨çº§è”è¶…é«˜åˆ†è¾¨ç‡è§†é¢‘ç”Ÿæˆ

**Date**: 2026-02-12 | **arXiv**: [2602.11564v1](http://arxiv.org/abs/2602.11564v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11564v1)

**Categories**: cs.CV

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Recent advances in video diffusion models have significantly improved visual quality, yet ultra-high-resolution (UHR) video generation remains a formidable challenge due to the compounded difficulties of motion modeling, semantic planning, and detail synthesis. To address these limitations, we propose \textbf{LUVE}, a \textbf{L}atent-cascaded \textbf{U}HR \textbf{V}ideo generation framework built upon dual frequency \textbf{E}xperts. LUVE employs a three-stage architecture comprising low-resolution motion generation for motion-consistent latent synthesis, video latent upsampling that performs resolution upsampling directly in the latent space to mitigate memory and computational overhead, and high-resolution content refinement that integrates low-frequency and high-frequency experts to jointly enhance semantic coherence and fine-grained detail generation. Extensive experiments demonstrate that our LUVE achieves superior photorealism and content fidelity in UHR video generation, and comprehensive ablation studies further validate the effectiveness of each component. The project is available at \href{https://unicornanrocinu.github.io/LUVE_web/}{https://github.io/LUVE/}.

è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•æ˜¾ç€æé«˜äº†è§†è§‰è´¨é‡ï¼Œä½†ç”±äºè¿åŠ¨å»ºæ¨¡ã€è¯­ä¹‰è§„åˆ’å’Œç»†èŠ‚åˆæˆçš„å¤æ‚å›°éš¾ï¼Œè¶…é«˜åˆ†è¾¨ç‡ (UHR) è§†é¢‘ç”Ÿæˆä»ç„¶æ˜¯ä¸€ä¸ªè‰°å·¨çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº† \textbf{LUVE}ï¼Œä¸€ä¸ªåŸºäºåŒé¢‘ \textbf{E}xperts æ„å»ºçš„ \textbf{L}atent çº§è” \textbf{U}HR \textbf{V}ideo ç”Ÿæˆæ¡†æ¶ã€‚ LUVE é‡‡ç”¨ä¸‰é˜¶æ®µæ¶æ„ï¼ŒåŒ…æ‹¬ç”¨äºè¿åŠ¨ä¸€è‡´æ½œåœ¨åˆæˆçš„ä½åˆ†è¾¨ç‡è¿åŠ¨ç”Ÿæˆã€ç›´æ¥åœ¨æ½œåœ¨ç©ºé—´ä¸­æ‰§è¡Œåˆ†è¾¨ç‡ä¸Šé‡‡æ ·ä»¥å‡è½»å†…å­˜å’Œè®¡ç®—å¼€é”€çš„è§†é¢‘æ½œåœ¨ä¸Šé‡‡æ ·ï¼Œä»¥åŠé›†æˆä½é¢‘å’Œé«˜é¢‘ä¸“å®¶ä»¥å…±åŒå¢å¼ºè¯­ä¹‰ä¸€è‡´æ€§å’Œç»†ç²’åº¦ç»†èŠ‚ç”Ÿæˆçš„é«˜åˆ†è¾¨ç‡å†…å®¹ç»†åŒ–ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ LUVE åœ¨ UHR è§†é¢‘ç”Ÿæˆä¸­å®ç°äº†å“è¶Šçš„ç…§ç‰‡çœŸå®æ„Ÿå’Œå†…å®¹ä¿çœŸåº¦ï¼Œå…¨é¢çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†æ¯ä¸ªç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚è¯¥é¡¹ç›®ä½äº \href{https://unicornanrocinu.github.io/LUVE_web/}{https://github.io/LUVE/}ã€‚

</details>

---

## 7. SAM3-LiteText: An Anatomical Study of the SAM3 Text Encoder for Efficient Vision-Language Segmentation / SAM3-LiteTextï¼šç”¨äºé«˜æ•ˆè§†è§‰è¯­è¨€åˆ†å‰²çš„ SAM3 æ–‡æœ¬ç¼–ç å™¨çš„è§£å‰–ç ”ç©¶

**Date**: 2026-02-12 | **arXiv**: [2602.12173v1](http://arxiv.org/abs/2602.12173v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.12173v1)

**Categories**: cs.AI

**Code**: https://github.com/SimonZeng7108/efficientsam3/tree/sam3_litetext.

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Vision-language segmentation models such as SAM3 enable flexible, prompt-driven visual grounding, but inherit large, general-purpose text encoders originally designed for open-ended language understanding. In practice, segmentation prompts are short, structured, and semantically constrained, leading to substantial over-provisioning in text encoder capacity and persistent computational and memory overhead. In this paper, we perform a large-scale anatomical analysis of text prompting in vision-language segmentation, covering 404,796 real prompts across multiple benchmarks. Our analysis reveals severe redundancy: most context windows are underutilized, vocabulary usage is highly sparse, and text embeddings lie on low-dimensional manifold despite high-dimensional representations. Motivated by these findings, we propose SAM3-LiteText, a lightweight text encoding framework that replaces the original SAM3 text encoder with a compact MobileCLIP student that is optimized by knowledge distillation. Extensive experiments on image and video segmentation benchmarks show that SAM3-LiteText reduces text encoder parameters by up to 88%, substantially reducing static memory footprint, while maintaining segmentation performance comparable to the original model. Code: https://github.com/SimonZeng7108/efficientsam3/tree/sam3_litetext.

SAM3 ç­‰è§†è§‰è¯­è¨€åˆ†å‰²æ¨¡å‹å¯å®ç°çµæ´»ã€æç¤ºé©±åŠ¨çš„è§†è§‰åŸºç¡€ï¼Œä½†ç»§æ‰¿äº†æœ€åˆä¸ºå¼€æ”¾å¼è¯­è¨€ç†è§£è€Œè®¾è®¡çš„å¤§å‹é€šç”¨æ–‡æœ¬ç¼–ç å™¨ã€‚åœ¨å®è·µä¸­ï¼Œåˆ†æ®µæç¤ºå¾ˆçŸ­ã€ç»“æ„åŒ–ä¸”è¯­ä¹‰å—é™ï¼Œå¯¼è‡´æ–‡æœ¬ç¼–ç å™¨å®¹é‡çš„å¤§é‡è¿‡åº¦é…ç½®ä»¥åŠæŒç»­çš„è®¡ç®—å’Œå†…å­˜å¼€é”€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹è§†è§‰è¯­è¨€åˆ†å‰²ä¸­çš„æ–‡æœ¬æç¤ºè¿›è¡Œäº†å¤§è§„æ¨¡çš„è§£å‰–åˆ†æï¼Œæ¶µç›–äº†è·¨å¤šä¸ªåŸºå‡†çš„ 404,796 ä¸ªçœŸå®æç¤ºã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ä¸¥é‡çš„å†—ä½™ï¼šå¤§å¤šæ•°ä¸Šä¸‹æ–‡çª—å£æœªå¾—åˆ°å……åˆ†åˆ©ç”¨ï¼Œè¯æ±‡ä½¿ç”¨é«˜åº¦ç¨€ç–ï¼Œå°½ç®¡å…·æœ‰é«˜ç»´è¡¨ç¤ºï¼Œä½†æ–‡æœ¬åµŒå…¥ä½äºä½ç»´æµå½¢ä¸Šã€‚å—è¿™äº›å‘ç°çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº† SAM3-LiteTextï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§æ–‡æœ¬ç¼–ç æ¡†æ¶ï¼Œç”¨é€šè¿‡çŸ¥è¯†è’¸é¦ä¼˜åŒ–çš„ç´§å‡‘å‹ MobileCLIP Student å–ä»£äº†åŸå§‹çš„ SAM3 æ–‡æœ¬ç¼–ç å™¨ã€‚å¯¹å›¾åƒå’Œè§†é¢‘åˆ†å‰²åŸºå‡†çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSAM3-LiteText å°†æ–‡æœ¬ç¼–ç å™¨å‚æ•°å‡å°‘äº†é«˜è¾¾ 88%ï¼Œå¤§å¤§å‡å°‘äº†é™æ€å†…å­˜å ç”¨ï¼ŒåŒæ—¶ä¿æŒäº†ä¸åŸå§‹æ¨¡å‹ç›¸å½“çš„åˆ†å‰²æ€§èƒ½ã€‚ä»£ç ï¼šhttps://github.com/SimonZeng7108/efficientsam3/tree/sam3_litetextã€‚

</details>

---

## 8. HLA: Hadamard Linear Attention / HLAï¼šHadamard çº¿æ€§æ³¨æ„åŠ›

**Date**: 2026-02-12 | **arXiv**: [2602.12128v1](http://arxiv.org/abs/2602.12128v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.12128v1)

**Categories**: cs.AI

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

The attention mechanism is an important reason for the success of transformers. It relies on computing pairwise relations between tokens. To reduce the high computational cost of standard quadratic attention, linear attention has been proposed as an efficient approximation. It employs kernel functions that are applied independently to the inputs before the pairwise similarities are calculated. That allows for an efficient computational procedure which, however, amounts to a low-degree rational function approximating softmax.   We propose Hadamard Linear Attention (HLA). Unlike previous works on linear attention, the nonlinearity in HLA is not applied separately to queries and keys, but, analogously to standard softmax attention, after the pairwise similarities have been computed. It will be shown that the proposed nonlinearity amounts to a higher-degree rational function to approximate softmax. An efficient computational scheme for the proposed method is derived that is similar to that of standard linear attention. In contrast to other approaches, no time-consuming tensor reshaping is necessary to apply the proposed algorithm. The effectiveness of the approach is demonstrated by applying it to a large diffusion transformer model for video generation, an application that involves very large amounts of tokens.

æ³¨æ„åŠ›æœºåˆ¶æ˜¯ Transformer æˆåŠŸçš„é‡è¦åŸå› ã€‚å®ƒä¾èµ–äºè®¡ç®—æ ‡è®°ä¹‹é—´çš„æˆå¯¹å…³ç³»ã€‚ä¸ºäº†é™ä½æ ‡å‡†äºŒæ¬¡æ³¨æ„åŠ›çš„é«˜è®¡ç®—æˆæœ¬ï¼Œçº¿æ€§æ³¨æ„åŠ›è¢«æå‡ºä½œä¸ºä¸€ç§æœ‰æ•ˆçš„è¿‘ä¼¼ã€‚å®ƒé‡‡ç”¨åœ¨è®¡ç®—æˆå¯¹ç›¸ä¼¼åº¦ä¹‹å‰ç‹¬ç«‹åº”ç”¨äºè¾“å…¥çš„æ ¸å‡½æ•°ã€‚è¿™å…è®¸é«˜æ•ˆçš„è®¡ç®—è¿‡ç¨‹ï¼Œç„¶è€Œï¼Œè¿™ç›¸å½“äºä¸€ä¸ªé€¼è¿‘ softmax çš„ä½æ¬¡æœ‰ç†å‡½æ•°ã€‚   æˆ‘ä»¬æå‡ºå“ˆè¾¾ç›çº¿æ€§æ³¨æ„åŠ›ï¼ˆHLAï¼‰ã€‚ä¸ä¹‹å‰çš„çº¿æ€§æ³¨æ„åŠ›å·¥ä½œä¸åŒï¼ŒHLA ä¸­çš„éçº¿æ€§ä¸æ˜¯å•ç‹¬åº”ç”¨äºæŸ¥è¯¢å’Œé”®ï¼Œè€Œæ˜¯ç±»ä¼¼äºæ ‡å‡†çš„ softmax æ³¨æ„åŠ›ï¼Œåœ¨è®¡ç®—æˆå¯¹ç›¸ä¼¼æ€§ä¹‹ååº”ç”¨ã€‚å°†ä¼šè¡¨æ˜ï¼Œæ‰€æå‡ºçš„éçº¿æ€§ç›¸å½“äºä¸€ä¸ªæ›´é«˜é˜¶çš„æœ‰ç†å‡½æ•°æ¥è¿‘ä¼¼ softmaxã€‚æ¨å¯¼äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆè®¡ç®—æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆç±»ä¼¼äºæ ‡å‡†çº¿æ€§æ³¨æ„çš„æ–¹æ¡ˆã€‚ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œåº”ç”¨æ‰€æå‡ºçš„ç®—æ³•ä¸éœ€è¦è€—æ—¶çš„å¼ é‡æ•´å½¢ã€‚è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§é€šè¿‡å°†å…¶åº”ç”¨äºç”¨äºè§†é¢‘ç”Ÿæˆçš„å¤§å‹æ‰©æ•£å˜å‹å™¨æ¨¡å‹ï¼ˆæ¶‰åŠå¤§é‡ä»¤ç‰Œçš„åº”ç”¨ç¨‹åºï¼‰æ¥è¯æ˜ã€‚

</details>

---

## 9. Beyond End-to-End Video Models: An LLM-Based Multi-Agent System for Educational Video Generation / è¶…è¶Šç«¯åˆ°ç«¯è§†é¢‘æ¨¡å‹ï¼šç”¨äºæ•™è‚²è§†é¢‘ç”Ÿæˆçš„åŸºäº LLM çš„å¤šä»£ç†ç³»ç»Ÿ

**Date**: 2026-02-12 | **arXiv**: [2602.11790v1](http://arxiv.org/abs/2602.11790v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11790v1)

**Categories**: cs.AI, cs.CL

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Although recent end-to-end video generation models demonstrate impressive performance in visually oriented content creation, they remain limited in scenarios that require strict logical rigor and precise knowledge representation, such as instructional and educational media. To address this problem, we propose LAVES, a hierarchical LLM-based multi-agent system for generating high-quality instructional videos from educational problems. The LAVES formulates educational video generation as a multi-objective task that simultaneously demands correct step-by-step reasoning, pedagogically coherent narration, semantically faithful visual demonstrations, and precise audio--visual alignment. To address the limitations of prior approaches--including low procedural fidelity, high production cost, and limited controllability--LAVES decomposes the generation workflow into specialized agents coordinated by a central Orchestrating Agent with explicit quality gates and iterative critique mechanisms. Specifically, the Orchestrating Agent supervises a Solution Agent for rigorous problem solving, an Illustration Agent that produces executable visualization codes, and a Narration Agent for learner-oriented instructional scripts. In addition, all outputs from the working agents are subject to semantic critique, rule-based constraints, and tool-based compilation checks. Rather than directly synthesizing pixels, the system constructs a structured executable video script that is deterministically compiled into synchronized visuals and narration using template-driven assembly rules, enabling fully automated end-to-end production without manual editing. In large-scale deployments, LAVES achieves a throughput exceeding one million videos per day, delivering over a 95% reduction in cost compared to current industry-standard approaches while maintaining a high acceptance rate.

å°½ç®¡æœ€è¿‘çš„ç«¯åˆ°ç«¯è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨é¢å‘è§†è§‰çš„å†…å®¹åˆ›å»ºæ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åœ¨éœ€è¦ä¸¥æ ¼é€»è¾‘ä¸¥è°¨æ€§å’Œç²¾ç¡®çŸ¥è¯†è¡¨ç¤ºçš„åœºæ™¯ä¸­ä»ç„¶å—åˆ°é™åˆ¶ï¼Œä¾‹å¦‚æ•™å­¦å’Œæ•™è‚²åª’ä½“ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº† LAVESï¼Œä¸€ç§åŸºäº LLM çš„åˆ†å±‚å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œç”¨äºæ ¹æ®æ•™è‚²é—®é¢˜ç”Ÿæˆé«˜è´¨é‡çš„æ•™å­¦è§†é¢‘ã€‚ LAVES å°†æ•™è‚²è§†é¢‘ç”Ÿæˆåˆ¶å®šä¸ºä¸€é¡¹å¤šç›®æ ‡ä»»åŠ¡ï¼ŒåŒæ—¶è¦æ±‚æ­£ç¡®çš„é€æ­¥æ¨ç†ã€æ•™å­¦ä¸Šè¿è´¯çš„å™è¿°ã€è¯­ä¹‰ä¸Šå¿ å®çš„è§†è§‰æ¼”ç¤ºä»¥åŠç²¾ç¡®çš„è§†å¬å¯¹é½ã€‚ä¸ºäº†è§£å†³å…ˆå‰æ–¹æ³•çš„å±€é™æ€§ï¼ˆåŒ…æ‹¬ç¨‹åºä¿çœŸåº¦ä½ã€ç”Ÿäº§æˆæœ¬é«˜å’Œå¯æ§æ€§æœ‰é™ï¼‰ï¼ŒLAVES å°†ç”Ÿæˆå·¥ä½œæµç¨‹åˆ†è§£ä¸ºç”±å…·æœ‰æ˜ç¡®è´¨é‡é—¨å’Œè¿­ä»£æ‰¹è¯„æœºåˆ¶çš„ä¸­å¤®ç¼–æ’ä»£ç†åè°ƒçš„ä¸“é—¨ä»£ç†ã€‚å…·ä½“æ¥è¯´ï¼Œç¼–æ’ä»£ç†ç›‘ç£è§£å†³æ–¹æ¡ˆä»£ç†ä»¥ä¸¥æ ¼è§£å†³é—®é¢˜ï¼Œæ’å›¾ä»£ç†ç”Ÿæˆå¯æ‰§è¡Œçš„å¯è§†åŒ–ä»£ç ï¼Œä»¥åŠå™è¿°ä»£ç†ä»¥ç”¨äºé¢å‘å­¦ä¹ è€…çš„æ•™å­¦è„šæœ¬ã€‚æ­¤å¤–ï¼Œå·¥ä½œä»£ç†çš„æ‰€æœ‰è¾“å‡ºéƒ½å—åˆ°è¯­ä¹‰æ‰¹è¯„ã€åŸºäºè§„åˆ™çš„çº¦æŸå’ŒåŸºäºå·¥å…·çš„ç¼–è¯‘æ£€æŸ¥ã€‚è¯¥ç³»ç»Ÿä¸æ˜¯ç›´æ¥åˆæˆåƒç´ ï¼Œè€Œæ˜¯æ„å»ºä¸€ä¸ªç»“æ„åŒ–çš„å¯æ‰§è¡Œè§†é¢‘è„šæœ¬ï¼Œè¯¥è„šæœ¬ä½¿ç”¨æ¨¡æ¿é©±åŠ¨çš„ç»„è£…è§„åˆ™ç¡®å®šæ€§åœ°ç¼–è¯‘æˆåŒæ­¥çš„è§†è§‰æ•ˆæœå’Œæ—ç™½ï¼Œä»è€Œå®ç°å®Œå…¨è‡ªåŠ¨åŒ–çš„ç«¯åˆ°ç«¯åˆ¶ä½œï¼Œæ— éœ€æ‰‹åŠ¨ç¼–è¾‘ã€‚åœ¨å¤§è§„æ¨¡éƒ¨ç½²ä¸­ï¼ŒLAVES çš„ååé‡æ¯å¤©è¶…è¿‡ 100 ä¸‡ä¸ªè§†é¢‘ï¼Œä¸å½“å‰è¡Œä¸šæ ‡å‡†æ–¹æ³•ç›¸æ¯”ï¼Œæˆæœ¬é™ä½äº† 95% ä»¥ä¸Šï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„æ¥å—ç‡ã€‚

</details>

---

## 10. VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model / VLAWï¼šæ„¿æ™¯-è¯­è¨€-è¡ŒåŠ¨æ”¿ç­–å’Œä¸–ç•Œæ¨¡å‹çš„è¿­ä»£å…±åŒæ”¹è¿›

**Date**: 2026-02-12 | **arXiv**: [2602.12063v1](http://arxiv.org/abs/2602.12063v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.12063v1)

**Categories**: cs.RO

**Project**: https://sites.google.com/view/vla-w  <details><summary><b>Abstract / æ‘˜è¦</b></summary>

The goal of this paper is to improve the performance and reliability of vision-language-action (VLA) models through iterative online interaction. Since collecting policy rollouts in the real world is expensive, we investigate whether a learned simulator-specifically, an action-conditioned video generation model-can be used to generate additional rollout data. Unfortunately, existing world models lack the physical fidelity necessary for policy improvement: they are predominantly trained on demonstration datasets that lack coverage of many different physical interactions (particularly failure cases) and struggle to accurately model small yet critical physical details in contact-rich object manipulation. We propose a simple iterative improvement algorithm that uses real-world roll-out data to improve the fidelity of the world model, which can then, in turn, be used to generate supplemental synthetic data for improving the VLA model. In our experiments on a real robot, we use this approach to improve the performance of a state-of-the-art VLA model on multiple downstream tasks. We achieve a 39.2% absolute success rate improvement over the base policy and 11.6% improvement from training with the generated synthetic rollouts. Videos can be found at this anonymous website: https://sites.google.com/view/vla-w

æœ¬æ–‡çš„ç›®æ ‡æ˜¯é€šè¿‡è¿­ä»£åœ¨çº¿äº¤äº’æ¥æé«˜è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹çš„æ€§èƒ½å’Œå¯é æ€§ã€‚ç”±äºåœ¨ç°å®ä¸–ç•Œä¸­æ”¶é›†ç­–ç•¥æ¨å‡ºçš„æˆæœ¬å¾ˆé«˜ï¼Œå› æ­¤æˆ‘ä»¬ç ”ç©¶äº†æ˜¯å¦å¯ä»¥ä½¿ç”¨å­¦ä¹ çš„æ¨¡æ‹Ÿå™¨ï¼ˆå…·ä½“è€Œè¨€ï¼ŒåŠ¨ä½œæ¡ä»¶è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼‰æ¥ç”Ÿæˆé¢å¤–çš„æ¨å‡ºæ•°æ®ã€‚ä¸å¹¸çš„æ˜¯ï¼Œç°æœ‰çš„ä¸–ç•Œæ¨¡å‹ç¼ºä¹æ”¿ç­–æ”¹è¿›æ‰€éœ€çš„ç‰©ç†ä¿çœŸåº¦ï¼šå®ƒä»¬ä¸»è¦æ˜¯åœ¨æ¼”ç¤ºæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œè¿™äº›æ•°æ®é›†ç¼ºä¹å¯¹è®¸å¤šä¸åŒç‰©ç†äº¤äº’ï¼ˆç‰¹åˆ«æ˜¯å¤±è´¥æ¡ˆä¾‹ï¼‰çš„è¦†ç›–ï¼Œå¹¶ä¸”å¾ˆéš¾åœ¨æ¥è§¦ä¸°å¯Œçš„å¯¹è±¡æ“ä½œä¸­å‡†ç¡®åœ°æ¨¡æ‹Ÿå¾®å°ä½†å…³é”®çš„ç‰©ç†ç»†èŠ‚ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„è¿­ä»£æ”¹è¿›ç®—æ³•ï¼Œè¯¥ç®—æ³•ä½¿ç”¨ç°å®ä¸–ç•Œçš„è½¬å‡ºæ•°æ®æ¥æé«˜ä¸–ç•Œæ¨¡å‹çš„ä¿çœŸåº¦ï¼Œç„¶åå¯ä»¥ä½¿ç”¨è¯¥ç®—æ³•ç”Ÿæˆè¡¥å……åˆæˆæ•°æ®ä»¥æ”¹è¿› VLA æ¨¡å‹ã€‚åœ¨æˆ‘ä»¬å¯¹çœŸå®æœºå™¨äººçš„å®éªŒä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™ç§æ–¹æ³•æ¥æé«˜æœ€å…ˆè¿›çš„ VLA æ¨¡å‹åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ä¸åŸºæœ¬ç­–ç•¥ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ç»å¯¹æˆåŠŸç‡æé«˜äº† 39.2%ï¼Œé€šè¿‡ç”Ÿæˆçš„ç»¼åˆéƒ¨ç½²è¿›è¡Œè®­ç»ƒï¼Œç»å¯¹æˆåŠŸç‡æé«˜äº† 11.6%ã€‚è§†é¢‘å¯ä»¥åœ¨è¿™ä¸ªåŒ¿åç½‘ç«™ä¸Šæ‰¾åˆ°ï¼šhttps://sites.google.com/view/vla-w

</details>

---

## 11. SurfPhase: 3D Interfacial Dynamics in Two-Phase Flows from Sparse Videos / SurfPhaseï¼šç¨€ç–è§†é¢‘ä¸­ä¸¤ç›¸æµçš„ 3D ç•Œé¢åŠ¨åŠ›å­¦

**Date**: 2026-02-11 | **arXiv**: [2602.11154v1](http://arxiv.org/abs/2602.11154v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11154v1)

**Categories**: cs.CV

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Interfacial dynamics in two-phase flows govern momentum, heat, and mass transfer, yet remain difficult to measure experimentally. Classical techniques face intrinsic limitations near moving interfaces, while existing neural rendering methods target single-phase flows with diffuse boundaries and cannot handle sharp, deformable liquid-vapor interfaces. We propose SurfPhase, a novel model for reconstructing 3D interfacial dynamics from sparse camera views. Our approach integrates dynamic Gaussian surfels with a signed distance function formulation for geometric consistency, and leverages a video diffusion model to synthesize novel-view videos to refine reconstruction from sparse observations. We evaluate on a new dataset of high-speed pool boiling videos, demonstrating high-quality view synthesis and velocity estimation from only two camera views. Project website: https://yuegao.me/SurfPhase.

ä¸¤ç›¸æµä¸­çš„ç•Œé¢åŠ¨åŠ›å­¦æ§åˆ¶åŠ¨é‡ã€çƒ­é‡å’Œè´¨é‡ä¼ é€’ï¼Œä½†ä»ç„¶éš¾ä»¥é€šè¿‡å®éªŒæµ‹é‡ã€‚ç»å…¸æŠ€æœ¯åœ¨ç§»åŠ¨ç•Œé¢é™„è¿‘é¢ä¸´å›ºæœ‰çš„å±€é™æ€§ï¼Œè€Œç°æœ‰çš„ç¥ç»æ¸²æŸ“æ–¹æ³•é’ˆå¯¹å…·æœ‰æ‰©æ•£è¾¹ç•Œçš„å•ç›¸æµï¼Œæ— æ³•å¤„ç†å°–é”ã€å¯å˜å½¢çš„æ¶²-æ°”ç•Œé¢ã€‚æˆ‘ä»¬æå‡ºäº† SurfPhaseï¼Œä¸€ç§ä»ç¨€ç–ç›¸æœºè§†å›¾é‡å»º 3D ç•Œé¢åŠ¨åŠ›å­¦çš„æ–°é¢–æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†åŠ¨æ€é«˜æ–¯é¢å…ƒä¸å¸¦ç¬¦å·è·ç¦»å‡½æ•°å…¬å¼ç›¸ç»“åˆä»¥å®ç°å‡ ä½•ä¸€è‡´æ€§ï¼Œå¹¶åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹æ¥åˆæˆæ–°é¢–çš„è§†å›¾è§†é¢‘ï¼Œä»¥æ”¹è¿›ç¨€ç–è§‚æµ‹çš„é‡å»ºã€‚æˆ‘ä»¬å¯¹é«˜é€Ÿæ± æ²¸è…¾è§†é¢‘çš„æ–°æ•°æ®é›†è¿›è¡Œäº†è¯„ä¼°ï¼Œä»…é€šè¿‡ä¸¤ä¸ªæ‘„åƒæœºè§†å›¾æ¼”ç¤ºäº†é«˜è´¨é‡çš„è§†å›¾åˆæˆå’Œé€Ÿåº¦ä¼°è®¡ã€‚é¡¹ç›®ç½‘ç«™ï¼šhttps://yuegao.me/SurfPhaseã€‚

</details>

---

## 12. HairWeaver: Few-Shot Photorealistic Hair Motion Synthesis with Sim-to-Real Guided Video Diffusion / HairWeaverï¼šé€šè¿‡æ¨¡æ‹Ÿåˆ°çœŸå®å¼•å¯¼è§†é¢‘æ‰©æ•£è¿›è¡Œå°‘é•œå¤´çœŸå®æ„Ÿå¤´å‘è¿åŠ¨åˆæˆ

**Date**: 2026-02-11 | **arXiv**: [2602.11117v1](http://arxiv.org/abs/2602.11117v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11117v1)

**Categories**: cs.CV

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

We present HairWeaver, a diffusion-based pipeline that animates a single human image with realistic and expressive hair dynamics. While existing methods successfully control body pose, they lack specific control over hair, and as a result, fail to capture the intricate hair motions, resulting in stiff and unrealistic animations. HairWeaver overcomes this limitation using two specialized modules: a Motion-Context-LoRA to integrate motion conditions and a Sim2Real-Domain-LoRA to preserve the subject's photoreal appearance across different data domains. These lightweight components are designed to guide a video diffusion backbone while maintaining its core generative capabilities. By training on a specialized dataset of dynamic human motion generated from a CG simulator, HairWeaver affords fine control over hair motion and ultimately learns to produce highly realistic hair that responds naturally to movement. Comprehensive evaluations demonstrate that our approach sets a new state of the art, producing lifelike human hair animations with dynamic details.

æˆ‘ä»¬æ¨å‡ºäº† HairWeaverï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„ç®¡é“ï¼Œå¯ä»¥é€šè¿‡é€¼çœŸä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„å¤´å‘åŠ¨æ€æ¥å¯¹å•ä¸ªäººç±»å›¾åƒè¿›è¡ŒåŠ¨ç”»å¤„ç†ã€‚è™½ç„¶ç°æœ‰æ–¹æ³•æˆåŠŸåœ°æ§åˆ¶äº†èº«ä½“å§¿åŠ¿ï¼Œä½†å®ƒä»¬ç¼ºä¹å¯¹å¤´å‘çš„å…·ä½“æ§åˆ¶ï¼Œå› æ­¤æ— æ³•æ•æ‰å¤æ‚çš„å¤´å‘è¿åŠ¨ï¼Œå¯¼è‡´åŠ¨ç”»åƒµç¡¬ä¸”ä¸åˆ‡å®é™…ã€‚ HairWeaver ä½¿ç”¨ä¸¤ä¸ªä¸“ç”¨æ¨¡å—å…‹æœäº†è¿™ä¸€é™åˆ¶ï¼šä¸€ä¸ªç”¨äºé›†æˆè¿åŠ¨æ¡ä»¶çš„ Motion-Context-LoRAï¼Œå¦ä¸€ä¸ªæ˜¯ Sim2Real-Domain-LoRAï¼Œç”¨äºåœ¨ä¸åŒæ•°æ®åŸŸä¸­ä¿ç•™ä¸»ä½“çš„çœŸå®å¤–è§‚ã€‚è¿™äº›è½»é‡çº§ç»„ä»¶æ—¨åœ¨æŒ‡å¯¼è§†é¢‘ä¼ æ’­ä¸»å¹²ï¼ŒåŒæ—¶ä¿æŒå…¶æ ¸å¿ƒç”ŸæˆåŠŸèƒ½ã€‚é€šè¿‡å¯¹ CG æ¨¡æ‹Ÿå™¨ç”Ÿæˆçš„åŠ¨æ€äººä½“è¿åŠ¨çš„ä¸“é—¨æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼ŒHairWeaver å¯ä»¥å¯¹å¤´å‘è¿åŠ¨è¿›è¡Œç²¾ç»†æ§åˆ¶ï¼Œå¹¶æœ€ç»ˆå­¦ä¼šç”Ÿæˆå¯¹è¿åŠ¨åšå‡ºè‡ªç„¶å“åº”çš„é«˜åº¦é€¼çœŸçš„å¤´å‘ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ ‘ç«‹äº†æ–°çš„æŠ€æœ¯æ°´å¹³ï¼Œå¯ä»¥åˆ¶ä½œå…·æœ‰åŠ¨æ€ç»†èŠ‚çš„é€¼çœŸçš„äººå‘åŠ¨ç”»ã€‚

</details>

---

## 13. FastFlow: Accelerating The Generative Flow Matching Models with Bandit Inference / FastFlowï¼šé€šè¿‡å¼ºç›—æ¨ç†åŠ é€Ÿç”ŸæˆæµåŒ¹é…æ¨¡å‹

**Date**: 2026-02-11 | **arXiv**: [2602.11105v1](http://arxiv.org/abs/2602.11105v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11105v1)

**Categories**: cs.CV

**Code**: https://github.com/Div290/FastFlow.

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Flow-matching models deliver state-of-the-art fidelity in image and video generation, but the inherent sequential denoising process renders them slower. Existing acceleration methods like distillation, trajectory truncation, and consistency approaches are static, require retraining, and often fail to generalize across tasks. We propose FastFlow, a plug-and-play adaptive inference framework that accelerates generation in flow matching models. FastFlow identifies denoising steps that produce only minor adjustments to the denoising path and approximates them without using the full neural network models used for velocity predictions. The approximation utilizes finite-difference velocity estimates from prior predictions to efficiently extrapolate future states, enabling faster advancements along the denoising path at zero compute cost. This enables skipping computation at intermediary steps. We model the decision of how many steps to safely skip before requiring a full model computation as a multi-armed bandit problem. The bandit learns the optimal skips to balance speed with performance. FastFlow integrates seamlessly with existing pipelines and generalizes across image generation, video generation, and editing tasks. Experiments demonstrate a speedup of over 2.6x while maintaining high-quality outputs. The source code for this work can be found at https://github.com/Div290/FastFlow.

æµåŒ¹é…æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä¸­æä¾›æœ€å…ˆè¿›çš„ä¿çœŸåº¦ï¼Œä½†å›ºæœ‰çš„é¡ºåºå»å™ªè¿‡ç¨‹ä½¿å®ƒä»¬é€Ÿåº¦å˜æ…¢ã€‚ç°æœ‰çš„åŠ é€Ÿæ–¹æ³•ï¼ˆä¾‹å¦‚è’¸é¦ã€è½¨è¿¹æˆªæ–­å’Œä¸€è‡´æ€§æ–¹æ³•ï¼‰æ˜¯é™æ€çš„ï¼Œéœ€è¦é‡æ–°è®­ç»ƒï¼Œå¹¶ä¸”é€šå¸¸æ— æ³•è·¨ä»»åŠ¡æ³›åŒ–ã€‚æˆ‘ä»¬æå‡ºäº† FastFlowï¼Œä¸€ç§å³æ’å³ç”¨çš„è‡ªé€‚åº”æ¨ç†æ¡†æ¶ï¼Œå¯åŠ é€ŸæµåŒ¹é…æ¨¡å‹çš„ç”Ÿæˆã€‚ FastFlow è¯†åˆ«ä»…å¯¹å»å™ªè·¯å¾„äº§ç”Ÿå¾®å°è°ƒæ•´çš„å»å™ªæ­¥éª¤ï¼Œå¹¶åœ¨ä¸ä½¿ç”¨ç”¨äºé€Ÿåº¦é¢„æµ‹çš„å®Œæ•´ç¥ç»ç½‘ç»œæ¨¡å‹çš„æƒ…å†µä¸‹å¯¹å…¶è¿›è¡Œè¿‘ä¼¼ã€‚è¯¥è¿‘ä¼¼åˆ©ç”¨å…ˆå‰é¢„æµ‹çš„æœ‰é™å·®åˆ†é€Ÿåº¦ä¼°è®¡æ¥æœ‰æ•ˆåœ°æ¨æ–­æœªæ¥çŠ¶æ€ï¼Œä»è€Œä»¥é›¶è®¡ç®—æˆæœ¬æ²¿ç€å»å™ªè·¯å¾„å®ç°æ›´å¿«çš„è¿›å±•ã€‚è¿™ä½¿å¾—èƒ½å¤Ÿè·³è¿‡ä¸­é—´æ­¥éª¤çš„è®¡ç®—ã€‚æˆ‘ä»¬å°†åœ¨éœ€è¦å®Œæ•´æ¨¡å‹è®¡ç®—ä¹‹å‰å®‰å…¨è·³è¿‡å¤šå°‘æ­¥éª¤çš„å†³ç­–å»ºæ¨¡ä¸ºå¤šè‡‚è€è™æœºé—®é¢˜ã€‚è€è™æœºå­¦ä¹ æœ€ä½³è·³è·ƒä»¥å¹³è¡¡é€Ÿåº¦ä¸æ€§èƒ½ã€‚ FastFlow ä¸ç°æœ‰ç®¡é“æ— ç¼é›†æˆï¼Œå¹¶å¯æ³›åŒ–å›¾åƒç”Ÿæˆã€è§†é¢‘ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ä¿æŒé«˜è´¨é‡è¾“å‡ºçš„åŒæ—¶ï¼Œé€Ÿåº¦æé«˜äº† 2.6 å€ä»¥ä¸Šã€‚è¿™é¡¹å·¥ä½œçš„æºä»£ç å¯ä»¥åœ¨ https://github.com/Div290/FastFlow æ‰¾åˆ°ã€‚

</details>

---

## 14. ReTracing: An Archaeological Approach Through Body, Machine, and Generative Systems / è¿½æº¯ï¼šé€šè¿‡èº«ä½“ã€æœºå™¨å’Œç”Ÿæˆç³»ç»Ÿçš„è€ƒå¤æ–¹æ³•

**Date**: 2026-02-11 | **arXiv**: [2602.11242v1](http://arxiv.org/abs/2602.11242v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11242v1)

**Categories**: cs.CV

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

We present ReTracing, a multi-agent embodied performance art that adopts an archaeological approach to examine how artificial intelligence shapes, constrains, and produces bodily movement. Drawing from science-fiction novels, the project extracts sentences that describe human-machine interaction. We use large language models (LLMs) to generate paired prompts "what to do" and "what not to do" for each excerpt. A diffusion-based text-to-video model transforms these prompts into choreographic guides for a human performer and motor commands for a quadruped robot. Both agents enact the actions on a mirrored floor, captured by multi-camera motion tracking and reconstructed into 3D point clouds and motion trails, forming a digital archive of motion traces. Through this process, ReTracing serves as a novel approach to reveal how generative systems encode socio-cultural biases through choreographed movements. Through an immersive interplay of AI, human, and robot, ReTracing confronts a critical question of our time: What does it mean to be human among AIs that also move, think, and leave traces behind?

æˆ‘ä»¬å±•ç¤ºäº† ReTracingï¼Œè¿™æ˜¯ä¸€ç§å¤šæ™ºèƒ½ä½“ä½“ç°çš„è¡¨æ¼”è‰ºæœ¯ï¼Œå®ƒé‡‡ç”¨è€ƒå¤å­¦çš„æ–¹æ³•æ¥ç ”ç©¶äººå·¥æ™ºèƒ½å¦‚ä½•å¡‘é€ ã€çº¦æŸå’Œäº§ç”Ÿèº«ä½“è¿åŠ¨ã€‚è¯¥é¡¹ç›®å€Ÿé‰´ç§‘å¹»å°è¯´ï¼Œæå–æè¿°äººæœºäº¤äº’çš„å¥å­ã€‚æˆ‘ä»¬ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) ä¸ºæ¯ä¸ªæ‘˜å½•ç”Ÿæˆé…å¯¹æç¤ºâ€œè¯¥åšä»€ä¹ˆâ€å’Œâ€œä¸è¯¥åšä»€ä¹ˆâ€ã€‚åŸºäºæ‰©æ•£çš„æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹å°†è¿™äº›æç¤ºè½¬æ¢ä¸ºäººç±»è¡¨æ¼”è€…çš„ç¼–èˆæŒ‡å—å’Œå››è¶³æœºå™¨äººçš„è¿åŠ¨å‘½ä»¤ã€‚ä¸¤ä¸ªä»£ç†éƒ½åœ¨é•œåƒåœ°æ¿ä¸Šæ‰§è¡ŒåŠ¨ä½œï¼Œé€šè¿‡å¤šæ‘„åƒå¤´è¿åŠ¨è·Ÿè¸ªæ•è·å¹¶é‡å»ºä¸º 3D ç‚¹äº‘å’Œè¿åŠ¨è½¨è¿¹ï¼Œå½¢æˆè¿åŠ¨è½¨è¿¹çš„æ•°å­—æ¡£æ¡ˆã€‚é€šè¿‡è¿™ä¸ªè¿‡ç¨‹ï¼ŒReTracing ä½œä¸ºä¸€ç§æ–°é¢–çš„æ–¹æ³•æ¥æ­ç¤ºç”Ÿæˆç³»ç»Ÿå¦‚ä½•é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„åŠ¨ä½œæ¥ç¼–ç ç¤¾ä¼šæ–‡åŒ–åè§ã€‚é€šè¿‡äººå·¥æ™ºèƒ½ã€äººç±»å’Œæœºå™¨äººçš„æ²‰æµ¸å¼äº’åŠ¨ï¼Œã€ŠReTracingã€‹é¢ä¸´ç€æˆ‘ä»¬è¿™ä¸ªæ—¶ä»£çš„ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šåœ¨åŒæ ·ä¼šç§»åŠ¨ã€æ€è€ƒå’Œç•™ä¸‹ç—•è¿¹çš„äººå·¥æ™ºèƒ½ä¸­ï¼Œä½œä¸ºäººç±»æ„å‘³ç€ä»€ä¹ˆï¼Ÿ

</details>

---

## 15. Flow caching for autoregressive video generation / ç”¨äºè‡ªå›å½’è§†é¢‘ç”Ÿæˆçš„æµç¼“å­˜

**Date**: 2026-02-11 | **arXiv**: [2602.10825v1](http://arxiv.org/abs/2602.10825v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10825v1)

**Categories**: cs.CV, cs.AI

**Code**: https://github.com/mikeallen39/FlowCache.

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Autoregressive models, often built on Transformer architectures, represent a powerful paradigm for generating ultra-long videos by synthesizing content in sequential chunks. However, this sequential generation process is notoriously slow. While caching strategies have proven effective for accelerating traditional video diffusion models, existing methods assume uniform denoising across all frames-an assumption that breaks down in autoregressive models where different video chunks exhibit varying similarity patterns at identical timesteps. In this paper, we present FlowCache, the first caching framework specifically designed for autoregressive video generation. Our key insight is that each video chunk should maintain independent caching policies, allowing fine-grained control over which chunks require recomputation at each timestep. We introduce a chunkwise caching strategy that dynamically adapts to the unique denoising characteristics of each chunk, complemented by a joint importance-redundancy optimized KV cache compression mechanism that maintains fixed memory bounds while preserving generation quality. Our method achieves remarkable speedups of 2.38 times on MAGI-1 and 6.7 times on SkyReels-V2, with negligible quality degradation (VBench: 0.87 increase and 0.79 decrease respectively). These results demonstrate that FlowCache successfully unlocks the potential of autoregressive models for real-time, ultra-long video generation-establishing a new benchmark for efficient video synthesis at scale. The code is available at https://github.com/mikeallen39/FlowCache.

è‡ªå›å½’æ¨¡å‹é€šå¸¸å»ºç«‹åœ¨ Transformer æ¶æ„ä¹‹ä¸Šï¼Œä»£è¡¨äº†é€šè¿‡åˆæˆè¿ç»­å—ä¸­çš„å†…å®¹æ¥ç”Ÿæˆè¶…é•¿è§†é¢‘çš„å¼ºå¤§èŒƒä¾‹ã€‚ç„¶è€Œï¼Œè¿™ç§é¡ºåºç”Ÿæˆè¿‡ç¨‹æ˜¯å‡ºäº†åçš„æ…¢ã€‚è™½ç„¶ç¼“å­˜ç­–ç•¥å·²è¢«è¯æ˜å¯¹äºåŠ é€Ÿä¼ ç»Ÿè§†é¢‘æ‰©æ•£æ¨¡å‹æ˜¯æœ‰æ•ˆçš„ï¼Œä½†ç°æœ‰æ–¹æ³•å‡è®¾æ‰€æœ‰å¸§éƒ½é‡‡ç”¨ç»Ÿä¸€çš„å»å™ªâ€”â€”è¿™ç§å‡è®¾åœ¨è‡ªå›å½’æ¨¡å‹ä¸­è¢«æ‰“ç ´ï¼Œå…¶ä¸­ä¸åŒçš„è§†é¢‘å—åœ¨ç›¸åŒçš„æ—¶é—´æ­¥é•¿è¡¨ç°å‡ºä¸åŒçš„ç›¸ä¼¼æ€§æ¨¡å¼ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† FlowCacheï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“ä¸ºè‡ªå›å½’è§†é¢‘ç”Ÿæˆè€Œè®¾è®¡çš„ç¼“å­˜æ¡†æ¶ã€‚æˆ‘ä»¬çš„ä¸»è¦è§è§£æ˜¯æ¯ä¸ªè§†é¢‘å—åº”è¯¥ç»´æŠ¤ç‹¬ç«‹çš„ç¼“å­˜ç­–ç•¥ï¼Œä»è€Œå¯ä»¥å¯¹æ¯ä¸ªæ—¶é—´æ­¥éœ€è¦é‡æ–°è®¡ç®—çš„å—è¿›è¡Œç»†ç²’åº¦æ§åˆ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ†å—ç¼“å­˜ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åŠ¨æ€é€‚åº”æ¯ä¸ªå—çš„ç‹¬ç‰¹å»å™ªç‰¹æ€§ï¼Œå¹¶è¾…ä»¥è”åˆé‡è¦æ€§å†—ä½™ä¼˜åŒ–çš„ KV ç¼“å­˜å‹ç¼©æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åœ¨ä¿æŒå›ºå®šå†…å­˜è¾¹ç•Œçš„åŒæ—¶ä¿æŒç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ MAGI-1 ä¸Šå®ç°äº† 2.38 å€çš„æ˜¾ç€åŠ é€Ÿï¼Œåœ¨ SkyReels-V2 ä¸Šå®ç°äº† 6.7 å€çš„æ˜¾ç€åŠ é€Ÿï¼Œè€Œè´¨é‡ä¸‹é™å¯ä»¥å¿½ç•¥ä¸è®¡ï¼ˆVBenchï¼šåˆ†åˆ«å¢åŠ  0.87 å€å’Œå‡å°‘ 0.79 å€ï¼‰ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒFlowCache æˆåŠŸé‡Šæ”¾äº†è‡ªå›å½’æ¨¡å‹åœ¨å®æ—¶ã€è¶…é•¿è§†é¢‘ç”Ÿæˆæ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºå¤§è§„æ¨¡é«˜æ•ˆè§†é¢‘åˆæˆå»ºç«‹äº†æ–°åŸºå‡†ã€‚è¯¥ä»£ç å¯ä» https://github.com/mikeallen39/FlowCache è·å–ã€‚

</details>

---

## 16. TwiFF (Think With Future Frames): A Large-Scale Dataset for Dynamic Visual Reasoning / TwiFFï¼ˆæ€è€ƒæœªæ¥æ¡†æ¶ï¼‰ï¼šç”¨äºåŠ¨æ€è§†è§‰æ¨ç†çš„å¤§è§„æ¨¡æ•°æ®é›†

**Date**: 2026-02-11 | **arXiv**: [2602.10675v1](http://arxiv.org/abs/2602.10675v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10675v1)

**Categories**: cs.CV, cs.AI

**Code**: https://github.com/LiuJunhua02/TwiFF.

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Visual Chain-of-Thought (VCoT) has emerged as a promising paradigm for enhancing multimodal reasoning by integrating visual perception into intermediate reasoning steps. However, existing VCoT approaches are largely confined to static scenarios and struggle to capture the temporal dynamics essential for tasks such as instruction, prediction, and camera motion. To bridge this gap, we propose TwiFF-2.7M, the first large-scale, temporally grounded VCoT dataset derived from $2.7$ million video clips, explicitly designed for dynamic visual question and answer. Accompanying this, we introduce TwiFF-Bench, a high-quality evaluation benchmark of $1,078$ samples that assesses both the plausibility of reasoning trajectories and the correctness of final answers in open-ended dynamic settings. Building on these foundations, we propose the TwiFF model, a unified modal that synergistically leverages pre-trained video generation and image comprehension capabilities to produce temporally coherent visual reasoning cues-iteratively generating future action frames and textual reasoning. Extensive experiments demonstrate that TwiFF significantly outperforms existing VCoT methods and Textual Chain-of-Thought baselines on dynamic reasoning tasks, which fully validates the effectiveness for visual question answering in dynamic scenarios. Our code and data is available at https://github.com/LiuJunhua02/TwiFF.

è§†è§‰æ€ç»´é“¾ï¼ˆVCoTï¼‰å·²æˆä¸ºä¸€ç§æœ‰å‰é€”çš„èŒƒå¼ï¼Œé€šè¿‡å°†è§†è§‰æ„ŸçŸ¥é›†æˆåˆ°ä¸­é—´æ¨ç†æ­¥éª¤æ¥å¢å¼ºå¤šæ¨¡æ€æ¨ç†ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ VCoT æ–¹æ³•ä¸»è¦å±€é™äºé™æ€åœºæ™¯ï¼Œéš¾ä»¥æ•æ‰æŒ‡ä»¤ã€é¢„æµ‹å’Œç›¸æœºè¿åŠ¨ç­‰ä»»åŠ¡æ‰€å¿…éœ€çš„æ—¶é—´åŠ¨æ€ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº† TwiFF-2.7Mï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡ã€åŸºäºæ—¶é—´çš„ VCoT æ•°æ®é›†ï¼Œæºè‡ªä»·å€¼ 270 ä¸‡ç¾å…ƒçš„è§†é¢‘å‰ªè¾‘ï¼Œä¸“é—¨ä¸ºåŠ¨æ€è§†è§‰é—®ç­”è€Œè®¾è®¡ã€‚ä¸æ­¤åŒæ—¶ï¼Œæˆ‘ä»¬æ¨å‡ºäº† TwiFF-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« 1,078 ç¾å…ƒæ ·æœ¬çš„é«˜è´¨é‡è¯„ä¼°åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¼€æ”¾å¼åŠ¨æ€è®¾ç½®ä¸­æ¨ç†è½¨è¿¹çš„åˆç†æ€§å’Œæœ€ç»ˆç­”æ¡ˆçš„æ­£ç¡®æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº† TwiFF æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€æ¨¡å¼ï¼ŒååŒåˆ©ç”¨é¢„å…ˆè®­ç»ƒçš„è§†é¢‘ç”Ÿæˆå’Œå›¾åƒç†è§£åŠŸèƒ½æ¥äº§ç”Ÿæ—¶é—´è¿è´¯çš„è§†è§‰æ¨ç†çº¿ç´¢ï¼Œè¿­ä»£åœ°ç”Ÿæˆæœªæ¥çš„åŠ¨ä½œæ¡†æ¶å’Œæ–‡æœ¬æ¨ç†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTwiFF åœ¨åŠ¨æ€æ¨ç†ä»»åŠ¡ä¸Šæ˜¾ç€ä¼˜äºç°æœ‰çš„ VCoT æ–¹æ³•å’Œæ–‡æœ¬æ€ç»´é“¾åŸºçº¿ï¼Œå……åˆ†éªŒè¯äº†åŠ¨æ€åœºæ™¯ä¸‹è§†è§‰é—®ç­”çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯åœ¨ https://github.com/LiuJunhua02/TwiFF è·å–ã€‚

</details>

---

## 17. VideoSTF: Stress-Testing Output Repetition in Video Large Language Models / VideoSTFï¼šè§†é¢‘å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è¾“å‡ºé‡å¤å‹åŠ›æµ‹è¯•

**Date**: 2026-02-11 | **arXiv**: [2602.10639v1](http://arxiv.org/abs/2602.10639v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10639v1)

**Categories**: cs.CV, cs.CR, cs.MM

**Code**: https://github.com/yuxincao22/VideoSTF_benchmark.

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Video Large Language Models (VideoLLMs) have recently achieved strong performance in video understanding tasks. However, we identify a previously underexplored generation failure: severe output repetition, where models degenerate into self-reinforcing loops of repeated phrases or sentences. This failure mode is not captured by existing VideoLLM benchmarks, which focus primarily on task accuracy and factual correctness. We introduce VideoSTF, the first framework for systematically measuring and stress-testing output repetition in VideoLLMs. VideoSTF formalizes repetition using three complementary n-gram-based metrics and provides a standardized testbed of 10,000 diverse videos together with a library of controlled temporal transformations. Using VideoSTF, we conduct pervasive testing, temporal stress testing, and adversarial exploitation across 10 advanced VideoLLMs. We find that output repetition is widespread and, critically, highly sensitive to temporal perturbations of video inputs. Moreover, we show that simple temporal transformations can efficiently induce repetitive degeneration in a black-box setting, exposing output repetition as an exploitable security vulnerability. Our results reveal output repetition as a fundamental stability issue in modern VideoLLMs and motivate stability-aware evaluation for video-language systems. Our evaluation code and scripts are available at: https://github.com/yuxincao22/VideoSTF_benchmark.

è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideoLLMï¼‰æœ€è¿‘åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­å–å¾—äº†å¼ºåŠ²çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªå…ˆå‰æœªè¢«å……åˆ†æ¢ç´¢çš„ç”Ÿæˆå¤±è´¥ï¼šä¸¥é‡çš„è¾“å‡ºé‡å¤ï¼Œå…¶ä¸­æ¨¡å‹é€€åŒ–ä¸ºé‡å¤çŸ­è¯­æˆ–å¥å­çš„è‡ªæˆ‘å¼ºåŒ–å¾ªç¯ã€‚ç°æœ‰çš„ VideoLLM åŸºå‡†æµ‹è¯•æœªæ•è·æ­¤æ•…éšœæ¨¡å¼ï¼Œè¯¥åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨ä»»åŠ¡å‡†ç¡®æ€§å’Œäº‹å®æ­£ç¡®æ€§ã€‚æˆ‘ä»¬ä»‹ç» VideoSTFï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåœ¨ VideoLLM ä¸­ç³»ç»Ÿæµ‹é‡å’Œå‹åŠ›æµ‹è¯•è¾“å‡ºé‡å¤çš„æ¡†æ¶ã€‚ VideoSTF ä½¿ç”¨ä¸‰ä¸ªäº’è¡¥çš„åŸºäº n-gram çš„æŒ‡æ ‡æ¥å½¢å¼åŒ–é‡å¤ï¼Œå¹¶æä¾›åŒ…å« 10,000 ä¸ªä¸åŒè§†é¢‘çš„æ ‡å‡†åŒ–æµ‹è¯•åºŠä»¥åŠå—æ§æ—¶é—´è½¬æ¢åº“ã€‚ä½¿ç”¨ VideoSTFï¼Œæˆ‘ä»¬åœ¨ 10 ä¸ªé«˜çº§ VideoLLM ä¸­è¿›è¡Œæ™®éæµ‹è¯•ã€æ—¶é—´å‹åŠ›æµ‹è¯•å’Œå¯¹æŠ—æ€§åˆ©ç”¨ã€‚æˆ‘ä»¬å‘ç°è¾“å‡ºé‡å¤å¾ˆæ™®éï¼Œè€Œä¸”è‡³å…³é‡è¦çš„æ˜¯ï¼Œå®ƒå¯¹è§†é¢‘è¾“å…¥çš„æ—¶é—´æ‰°åŠ¨é«˜åº¦æ•æ„Ÿã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¡¨æ˜ç®€å•çš„æ—¶é—´å˜æ¢å¯ä»¥æœ‰æ•ˆåœ°åœ¨é»‘ç›’è®¾ç½®ä¸­å¼•èµ·é‡å¤é€€åŒ–ï¼Œä»è€Œå°†è¾“å‡ºé‡å¤æš´éœ²ä¸ºå¯åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¾“å‡ºé‡å¤æ˜¯ç°ä»£ VideoLLM ä¸­çš„ä¸€ä¸ªåŸºæœ¬ç¨³å®šæ€§é—®é¢˜ï¼Œå¹¶æ¿€å‘äº†å¯¹è§†é¢‘è¯­è¨€ç³»ç»Ÿçš„ç¨³å®šæ€§æ„ŸçŸ¥è¯„ä¼°ã€‚æˆ‘ä»¬çš„è¯„ä¼°ä»£ç å’Œè„šæœ¬ä½äºï¼šhttps://github.com/yuxincao22/VideoSTF_benchmarkã€‚

</details>

---

## 18. H-WM: Robotic Task and Motion Planning Guided by Hierarchical World Model / H-WMï¼šåˆ†å±‚ä¸–ç•Œæ¨¡å‹å¼•å¯¼çš„æœºå™¨äººä»»åŠ¡å’Œè¿åŠ¨è§„åˆ’

**Date**: 2026-02-11 | **arXiv**: [2602.11291v1](http://arxiv.org/abs/2602.11291v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11291v1)

**Categories**: cs.RO

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

World models are becoming central to robotic planning and control, as they enable prediction of future state transitions. Existing approaches often emphasize video generation or natural language prediction, which are difficult to directly ground in robot actions and suffer from compounding errors over long horizons. Traditional task and motion planning relies on symbolic logic world models, such as planning domains, that are robot-executable and robust for long-horizon reasoning. However, these methods typically operate independently of visual perception, preventing synchronized symbolic and perceptual state prediction. We propose a Hierarchical World Model (H-WM) that jointly predicts logical and visual state transitions within a unified bilevel framework. H-WM combines a high-level logical world model with a low-level visual world model, integrating the robot-executable, long-horizon robustness of symbolic reasoning with perceptual grounding from visual observations. The hierarchical outputs provide stable and consistent intermediate guidance for long-horizon tasks, mitigating error accumulation and enabling robust execution across extended task sequences. To train H-WM, we introduce a robotic dataset that aligns robot motion with symbolic states, actions, and visual observations. Experiments across vision-language-action (VLA) control policies demonstrate the effectiveness and generality of the approach.

ä¸–ç•Œæ¨¡å‹æ­£åœ¨æˆä¸ºæœºå™¨äººè§„åˆ’å’Œæ§åˆ¶çš„æ ¸å¿ƒï¼Œå› ä¸ºå®ƒä»¬èƒ½å¤Ÿé¢„æµ‹æœªæ¥çš„çŠ¶æ€è½¬æ¢ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸å¼ºè°ƒè§†é¢‘ç”Ÿæˆæˆ–è‡ªç„¶è¯­è¨€é¢„æµ‹ï¼Œè¿™äº›æ–¹æ³•å¾ˆéš¾ç›´æ¥åæ˜ æœºå™¨äººçš„åŠ¨ä½œï¼Œå¹¶ä¸”åœ¨é•¿æœŸèŒƒå›´å†…ä¼šå‡ºç°å¤åˆé”™è¯¯ã€‚ä¼ ç»Ÿçš„ä»»åŠ¡å’Œè¿åŠ¨è§„åˆ’ä¾èµ–äºç¬¦å·é€»è¾‘ä¸–ç•Œæ¨¡å‹ï¼Œä¾‹å¦‚è§„åˆ’åŸŸï¼Œè¿™äº›æ¨¡å‹æ˜¯æœºå™¨äººå¯æ‰§è¡Œçš„å¹¶ä¸”å¯¹äºé•¿æœŸæ¨ç†æ¥è¯´æ˜¯é²æ£’çš„ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ç‹¬ç«‹äºè§†è§‰æ„ŸçŸ¥è¿›è¡Œæ“ä½œï¼Œä»è€Œé˜»æ­¢äº†åŒæ­¥çš„ç¬¦å·å’Œæ„ŸçŸ¥çŠ¶æ€é¢„æµ‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†å±‚ä¸–ç•Œæ¨¡å‹ï¼ˆH-WMï¼‰ï¼Œå®ƒåœ¨ç»Ÿä¸€çš„åŒå±‚æ¡†æ¶å†…è”åˆé¢„æµ‹é€»è¾‘å’Œè§†è§‰çŠ¶æ€è½¬æ¢ã€‚ H-WM å°†é«˜çº§é€»è¾‘ä¸–ç•Œæ¨¡å‹ä¸ä½çº§è§†è§‰ä¸–ç•Œæ¨¡å‹ç›¸ç»“åˆï¼Œå°†æœºå™¨äººå¯æ‰§è¡Œçš„ç¬¦å·æ¨ç†çš„é•¿æœŸé²æ£’æ€§ä¸è§†è§‰è§‚å¯Ÿçš„æ„ŸçŸ¥åŸºç¡€ç›¸ç»“åˆã€‚åˆ†å±‚è¾“å‡ºä¸ºé•¿æœŸä»»åŠ¡æä¾›ç¨³å®šä¸€è‡´çš„ä¸­é—´æŒ‡å¯¼ï¼Œå‡å°‘é”™è¯¯ç´¯ç§¯å¹¶å®ç°è·¨æ‰©å±•ä»»åŠ¡åºåˆ—çš„ç¨³å¥æ‰§è¡Œã€‚ä¸ºäº†è®­ç»ƒ H-WMï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæœºå™¨äººæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†å°†æœºå™¨äººè¿åŠ¨ä¸ç¬¦å·çŠ¶æ€ã€åŠ¨ä½œå’Œè§†è§‰è§‚å¯Ÿå¯¹é½ã€‚è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ§åˆ¶ç­–ç•¥çš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚

</details>

---

## 19. Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation / è¯´ã€æ¢¦æƒ³å’Œè¡ŒåŠ¨ï¼šå­¦ä¹ æŒ‡ä»¤é©±åŠ¨æœºå™¨äººæ“ä½œçš„è§†é¢‘ä¸–ç•Œæ¨¡å‹

**Date**: 2026-02-11 | **arXiv**: [2602.10717v1](http://arxiv.org/abs/2602.10717v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10717v1)

**Categories**: cs.RO

<details><summary><b>Abstract / æ‘˜è¦</b></summary>

Robotic manipulation requires anticipating how the environment evolves in response to actions, yet most existing systems lack this predictive capability, often resulting in errors and inefficiency. While Vision-Language Models (VLMs) provide high-level guidance, they cannot explicitly forecast future states, and existing world models either predict only short horizons or produce spatially inconsistent frames. To address these challenges, we propose a framework for fast and predictive video-conditioned action. Our approach first selects and adapts a robust video generation model to ensure reliable future predictions, then applies adversarial distillation for fast, few-step video generation, and finally trains an action model that leverages both generated videos and real observations to correct spatial errors. Extensive experiments show that our method produces temporally coherent, spatially accurate video predictions that directly support precise manipulation, achieving significant improvements in embodiment consistency, spatial referring ability, and task completion over existing baselines. Codes & Models will be released.

æœºå™¨äººæ“çºµéœ€è¦é¢„æµ‹ç¯å¢ƒå¦‚ä½•å“åº”è¡ŒåŠ¨è€Œæ¼”å˜ï¼Œä½†å¤§å¤šæ•°ç°æœ‰ç³»ç»Ÿç¼ºä¹è¿™ç§é¢„æµ‹èƒ½åŠ›ï¼Œå¸¸å¸¸å¯¼è‡´é”™è¯¯å’Œä½æ•ˆç‡ã€‚è™½ç„¶è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æä¾›é«˜çº§æŒ‡å¯¼ï¼Œä½†å®ƒä»¬æ— æ³•æ˜ç¡®é¢„æµ‹æœªæ¥çŠ¶æ€ï¼Œå¹¶ä¸”ç°æœ‰çš„ä¸–ç•Œæ¨¡å‹è¦ä¹ˆä»…é¢„æµ‹çŸ­æœŸæƒ…å†µï¼Œè¦ä¹ˆäº§ç”Ÿç©ºé—´ä¸ä¸€è‡´çš„æ¡†æ¶ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¿«é€Ÿã€é¢„æµ‹æ€§è§†é¢‘æ¡ä»¶åŠ¨ä½œæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆé€‰æ‹©å¹¶è°ƒæ•´ä¸€ä¸ªå¼ºå¤§çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œä»¥ç¡®ä¿å¯é çš„æœªæ¥é¢„æµ‹ï¼Œç„¶ååº”ç”¨å¯¹æŠ—æ€§è’¸é¦æ¥å¿«é€Ÿã€å‡ æ­¥è§†é¢‘ç”Ÿæˆï¼Œæœ€åè®­ç»ƒä¸€ä¸ªåŠ¨ä½œæ¨¡å‹ï¼Œåˆ©ç”¨ç”Ÿæˆçš„è§†é¢‘å’ŒçœŸå®è§‚å¯Ÿæ¥çº æ­£ç©ºé—´é”™è¯¯ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•äº§ç”Ÿæ—¶é—´ä¸Šä¸€è‡´ã€ç©ºé—´ä¸Šå‡†ç¡®çš„è§†é¢‘é¢„æµ‹ï¼Œç›´æ¥æ”¯æŒç²¾ç¡®æ“ä½œï¼Œåœ¨ç°æœ‰åŸºçº¿çš„åŸºç¡€ä¸Šå®ç°äº†å®æ–½ä¾‹ä¸€è‡´æ€§ã€ç©ºé—´å‚è€ƒèƒ½åŠ›å’Œä»»åŠ¡å®Œæˆåº¦çš„æ˜¾ç€æ”¹è¿›ã€‚ä»£ç å’Œå‹å·å°†è¢«å‘å¸ƒã€‚

</details>

---



</details>

<!-- PAPERS_CONTENT_END -->

## ğŸš€ å¿«é€Ÿå¼€å§‹

1. **é…ç½® API Key**ï¼šåœ¨ä»“åº“ `Settings -> Secrets and variables -> Actions -> Secrets` ä¸­æ·»åŠ  `DEEPSEEK_API_KEY`ã€‚
2. **æ‰‹åŠ¨è¿è¡Œ**ï¼šåœ¨ `Actions` æ ‡ç­¾é¡µé€‰æ‹© `Daily Video Papers Update` å¹¶ç‚¹å‡» `Run workflow`ã€‚
3. **åˆ‡æ¢ç‰ˆæœ¬**ï¼šåœ¨ `Variables` ä¸­è®¾ç½® `VERSION` ä¸º `v2` å³å¯å¼€å¯ AI æ·±åº¦åˆ†ææ¨¡å¼ã€‚

## ğŸ› ï¸ æŠ€æœ¯ç»†èŠ‚

- **æ•°æ®æº**ï¼šarXiv API (cs.CV, cs.AI, cs.MM, cs.RO, cs.LG)
- **ç¿»è¯‘/åˆ†æ**ï¼šDeepSeek API (ä¼˜å…ˆ) / Gemini (å¤‡ç”¨)
- **è‡ªåŠ¨åŒ–**ï¼šGitHub Actions

---
*æœ¬é¡¹ç›®ç”± Manus è‡ªåŠ¨ç”Ÿæˆå¹¶ç»´æŠ¤ã€‚*
