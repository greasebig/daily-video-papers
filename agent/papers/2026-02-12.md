# arXiv Agent Papers - 2026-02-12

**Paper Count**: 59

---

## 1. FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight / FormalJudge：代理监督的神经符号范式

**Date**: 2026-02-11 | **arXiv**: [2602.11136v1](http://arxiv.org/abs/2602.11136v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11136v1)

**Categories**: cs.AI

<details><summary><b>Abstract / 摘要</b></summary>

As LLM-based agents increasingly operate in high-stakes domains with real-world consequences, ensuring their behavioral safety becomes paramount. The dominant oversight paradigm, LLM-as-a-Judge, faces a fundamental dilemma: how can probabilistic systems reliably supervise other probabilistic systems without inheriting their failure modes? We argue that formal verification offers a principled escape from this dilemma, yet its adoption has been hindered by a critical bottleneck: the translation from natural language requirements to formal specifications. This paper bridges this gap by proposing , a neuro-symbolic framework that employs a bidirectional Formal-of-Thought architecture: LLMs serve as specification compilers that top-down decompose high-level human intent into atomic, verifiable constraints, then bottom-up prove compliance using Dafny specifications and Z3 Satisfiability modulo theories solving, which produces mathematical guarantees rather than probabilistic scores. We validate across three benchmarks spanning behavioral safety, multi-domain constraint adherence, and agentic upward deception detection. Experiments on 7 agent models demonstrate that achieves an average improvement of 16.6% over LLM-as-a-Judge baselines, enables weak-to-strong generalization where a 7B judge achieves over 90% accuracy detecting deception from 72B agents, and provides near-linear safety improvement through iterative refinement.

随着基于法学硕士的代理人越来越多地在具有现实世界后果的高风险领域运作，确保他们的行为安全变得至关重要。占主导地位的监督范式“LLM-as-a-Judge”面临着一个根本性的困境：概率系统如何可靠地监督其他概率系统而不继承它们的故障模式？我们认为形式验证提供了摆脱这种困境的原则性途径，但它的采用却受到了一个关键瓶颈的阻碍：从自然语言要求到形式规范的翻译。本文通过提出一种采用双向思维形式架构的神经符号框架来弥补这一差距：法学硕士作为规范编译器，自上而下地将高级人类意图分解为原子的、可验证的约束，然后使用 Dafny 规范和 Z3 可满足性模理论求解自下而上证明合规性，从而产生数学保证而不是概率分数。我们验证了三个基准，涵盖行为安全、多域约束遵守和代理向上欺骗检测。对 7 个代理模型的实验表明，与 LLM 作为法官的基线相比，平均提高了 16.6%，实现了从弱到强的泛化，其中 7B 法官在检测 72B 代理的欺骗时达到了 90% 以上的准确率，并通过迭代细化提供了近线性的安全性改进。

</details>

---

## 2. Learning to Compose for Cross-domain Agentic Workflow Generation / 学习构建跨域代理工作流生成

**Date**: 2026-02-11 | **arXiv**: [2602.11114v1](http://arxiv.org/abs/2602.11114v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11114v1)

**Categories**: cs.MA, cs.AI, cs.LG, cs.SE

<details><summary><b>Abstract / 摘要</b></summary>

Automatically generating agentic workflows -- executable operator graphs or codes that orchestrate reasoning, verification, and repair -- has become a practical way to solve complex tasks beyond what single-pass LLM generation can reliably handle. Yet what constitutes a good workflow depends heavily on the task distribution and the available operators. Under domain shift, current systems typically rely on iterative workflow refinement to discover a feasible workflow from a large workflow space, incurring high iteration costs and yielding unstable, domain-specific behavior. In response, we internalize a decompose-recompose-decide mechanism into an open-source LLM for cross-domain workflow generation. To decompose, we learn a compact set of reusable workflow capabilities across diverse domains. To recompose, we map each input task to a sparse composition over these bases to generate a task-specific workflow in a single pass. To decide, we attribute the success or failure of workflow generation to counterfactual contributions from learned capabilities, thereby capturing which capabilities actually drive success by their marginal effects. Across stringent multi-domain, cross-domain, and unseen-domain evaluations, our 1-pass generator surpasses SOTA refinement baselines that consume 20 iterations, while substantially reducing generation latency and cost.

自动生成代理工作流程（编排推理、验证和修复的可执行操作图或代码）已成为解决单通道 LLM 生成无法可靠处理的复杂任务的实用方法。然而，良好的工作流程的构成在很大程度上取决于任务分配和可用的操作员。在域转移下，当前系统通常依赖于迭代工作流细化来从大型工作流空间中发现可行的工作流，从而产生高迭代成本并产生不稳定的特定于域的行为。作为回应，我们将分解-重组-决定机制内化到开源 LLM 中，以生成跨域工作流程。为了进行分解，我们学习了一组跨不同领域的紧凑的可重用工作流功能。为了重构，我们将每个输入任务映射到这些基础上的稀疏组合，以在一次传递中生成特定于任务的工作流程。为了做出决定，我们将工作流生成的成功或失败归因于学习能力的反事实贡献，从而捕获哪些能力通过其边际效应真正推动成功。在严格的多域、跨域和未见域评估中，我们的 1-pass 生成器超越了消耗 20 次迭代的 SOTA 细化基线，同时大幅降低了生成延迟和成本。

</details>

---

## 3. GameDevBench: Evaluating Agentic Capabilities Through Game Development / GameDevBench：通过游戏开发评估代理能力

**Date**: 2026-02-11 | **arXiv**: [2602.11103v1](http://arxiv.org/abs/2602.11103v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11103v1)

**Categories**: cs.AI, cs.CL, cs.SE

<details><summary><b>Abstract / 摘要</b></summary>

Despite rapid progress on coding agents, progress on their multimodal counterparts has lagged behind. A key challenge is the scarcity of evaluation testbeds that combine the complexity of software development with the need for deep multimodal understanding. Game development provides such a testbed as agents must navigate large, dense codebases while manipulating intrinsically multimodal assets such as shaders, sprites, and animations within a visual game scene. We present GameDevBench, the first benchmark for evaluating agents on game development tasks. GameDevBench consists of 132 tasks derived from web and video tutorials. Tasks require significant multimodal understanding and are complex -- the average solution requires over three times the amount of lines of code and file changes compared to prior software development benchmarks. Agents still struggle with game development, with the best agent solving only 54.5% of tasks. We find a strong correlation between perceived task difficulty and multimodal complexity, with success rates dropping from 46.9% on gameplay-oriented tasks to 31.6% on 2D graphics tasks. To improve multimodal capability, we introduce two simple image and video-based feedback mechanisms for agents. Despite their simplicity, these methods consistently improve performance, with the largest change being an increase in Claude Sonnet 4.5's performance from 33.3% to 47.7%. We release GameDevBench publicly to support further research into agentic game development.

尽管编码剂取得了快速进展，但其多模式对应物的进展却滞后。一个关键的挑战是缺乏将软件开发的复杂性与深入的多模式理解的需求结合起来的评估测试平台。游戏开发提供了这样一个测试平台，因为代理必须导航大型、密集的代码库，同时在视觉游戏场景中操纵本质上的多模式资产，例如着色器、精灵和动画。我们推出了 GameDevBench，这是第一个评估代理游戏开发任务的基准。 GameDevBench 包含源自网络和视频教程的 132 个任务。任务需要大量的多模式理解，而且很复杂——与之前的软件开发基准相比，平均解决方案需要的代码行数和文件更改量是三倍多。智能体在游戏开发方面仍然举步维艰，最好的智能体只能解决 54.5% 的任务。我们发现感知任务难度和多模态复杂性之间存在很强的相关性，游戏导向任务的成功率从 46.9% 下降到 2D 图形任务的 31.6%。为了提高多模态能力，我们为代理引入了两种简单的基于图像和视频的反馈机制。尽管这些方法很简单，但它们不断提高性能，其中最大的变化是 Claude Sonnet 4.5 的性能从 33.3% 提高到 47.7%。我们公开发布 GameDevBench 以支持对代理游戏开发的进一步研究。

</details>

---

## 4. Interpretable Attention-Based Multi-Agent PPO for Latency Spike Resolution in 6G RAN Slicing / 可解释的基于注意力的多代理 PPO，用于解决 6G RAN 切片中的延迟峰值

**Date**: 2026-02-11 | **arXiv**: [2602.11076v1](http://arxiv.org/abs/2602.11076v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11076v1)

**Categories**: eess.SY, cs.AI, eess.SP

<details><summary><b>Abstract / 摘要</b></summary>

Sixth-generation (6G) radio access networks (RANs) must enforce strict service-level agreements (SLAs) for heterogeneous slices, yet sudden latency spikes remain difficult to diagnose and resolve with conventional deep reinforcement learning (DRL) or explainable RL (XRL). We propose \emph{Attention-Enhanced Multi-Agent Proximal Policy Optimization (AE-MAPPO)}, which integrates six specialized attention mechanisms into multi-agent slice control and surfaces them as zero-cost, faithful explanations. The framework operates across O-RAN timescales with a three-phase strategy: predictive, reactive, and inter-slice optimization.   A URLLC case study shows AE-MAPPO resolves a latency spike in $18$ms, restores latency to $0.98$ms with $99.9999\%$ reliability, and reduces troubleshooting time by $93\%$ while maintaining eMBB and mMTC continuity. These results confirm AE-MAPPO's ability to combine SLA compliance with inherent interpretability, enabling trustworthy and real-time automation for 6G RAN slicing.

第六代 (6G) 无线接入网络 (RAN) 必须对异构切片执行严格的服务级别协议 (SLA)，但使用传统的深度强化学习 (DRL) 或可解释的强化学习 (XRL) 仍然难以诊断和解决突然的延迟峰值。我们提出\emph{注意力增强多智能体近端策略优化（AE-MAPPO）}，它将六种专门的注意力机制集成到多智能体切片控制中，并将它们作为零成本、忠实的解释。该框架采用三阶段策略跨 O-RAN 时间尺度运行：预测、反应和切片间优化。   URLLC 案例研究显示，AE-MAPPO 解决了 $18$ms 的延迟峰值，将延迟恢复到 $0.98$ms，可靠性为 $99.9999\%$，并将故障排除时间减少 $93\%$，同时保持 eMBB 和 mMTC 连续性。这些结果证实了 AE-MAPPO 能够将 SLA 合规性与固有的可解释性相结合，从而实现 6G RAN 切片的可信和实时自动化。

</details>

---

## 5. Chain-of-Look Spatial Reasoning for Dense Surgical Instrument Counting / 密集手术器械计数的链视空间推理

**Date**: 2026-02-11 | **arXiv**: [2602.11024v1](http://arxiv.org/abs/2602.11024v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11024v1)

**Categories**: cs.CV, cs.AI

<details><summary><b>Abstract / 摘要</b></summary>

Accurate counting of surgical instruments in Operating Rooms (OR) is a critical prerequisite for ensuring patient safety during surgery. Despite recent progress of large visual-language models and agentic AI, accurately counting such instruments remains highly challenging, particularly in dense scenarios where instruments are tightly clustered. To address this problem, we introduce Chain-of-Look, a novel visual reasoning framework that mimics the sequential human counting process by enforcing a structured visual chain, rather than relying on classic object detection which is unordered. This visual chain guides the model to count along a coherent spatial trajectory, improving accuracy in complex scenes. To further enforce the physical plausibility of the visual chain, we introduce the neighboring loss function, which explicitly models the spatial constraints inherent to densely packed surgical instruments. We also present SurgCount-HD, a new dataset comprising 1,464 high-density surgical instrument images. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches for counting (e.g., CountGD, REC) as well as Multimodality Large Language Models (e.g., Qwen, ChatGPT) in the challenging task of dense surgical instrument counting.

手术室 (OR) 中手术器械的准确计数是确保手术期间患者安全的关键前提。尽管大型视觉语言模型和代理人工智能最近取得了进展，但准确计数此类仪器仍然极具挑战性，特别是在仪器紧密聚集的密集场景中。为了解决这个问题，我们引入了 Chain-of-Look，一种新颖的视觉推理框架，它通过强制执行结构化视觉链来模拟连续的人类计数过程，而不是依赖于经典的无序对象检测。该视觉链引导模型沿着连贯的空间轨迹进行计数，从而提高复杂场景中的准确性。为了进一步增强视觉链的物理合理性，我们引入了邻近损失函数，它明确地模拟了密集手术器械固有的空间约束。我们还推出了 SurgCount-HD，这是一个包含 1,464 张高密度手术器械图像的新数据集。大量实验表明，在密集手术器械计数这一具有挑战性的任务中，我们的方法优于最先进的计数方法（例如 CountGD、REC）以及多模态大型语言模型（例如 Qwen、ChatGPT）。

</details>

---

## 6. CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion / CLI-Gym：通过代理环境反转生成可扩展的 CLI 任务

**Date**: 2026-02-11 | **arXiv**: [2602.10999v1](http://arxiv.org/abs/2602.10999v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10999v1)

**Categories**: cs.AI

<details><summary><b>Abstract / 摘要</b></summary>

Agentic coding requires agents to effectively interact with runtime environments, e.g., command line interfaces (CLI), so as to complete tasks like resolving dependency issues, fixing system problems, etc. But it remains underexplored how such environment-intensive tasks can be obtained at scale to enhance agents' capabilities. To address this, based on an analogy between the Dockerfile and the agentic task, we propose to employ agents to simulate and explore environment histories, guided by execution feedback. By tracing histories of a healthy environment, its state can be inverted to an earlier one with runtime failures, from which a task can be derived by packing the buggy state and the corresponding error messages. With our method, named CLI-Gym, a total of 1,655 environment-intensive tasks are derived, being the largest collection of its kind. Moreover, with curated successful trajectories, our fine-tuned model, named LiberCoder, achieves substantial absolute improvements of +21.1% (to 46.1%) on Terminal-Bench, outperforming various strong baselines. To our knowledge, this is the first public pipeline for scalable derivation of environment-intensive tasks.

代理编码需要代理与运行时环境（例如命令行界面（CLI））有效交互，以完成解决依赖性问题、修复系统问题等任务。但如何大规模获得此类环境密集型任务以增强代理的能力仍有待探索。为了解决这个问题，基于 Dockerfile 和代理任务之间的类比，我们建议使用代理在执行反馈的指导下模拟和探索环境历史。通过跟踪健康环境的历史，可以将其状态反转到运行时失败的早期状态，然后可以通过打包有问题的状态和相应的错误消息来派生任务。通过我们名为 CLI-Gym 的方法，总共导出了 1,655 个环境密集型任务，是同类中最大的集合。此外，通过精心策划的成功轨迹，我们名为 LiberCoder 的微调模型在 Terminal-Bench 上实现了 +21.1%（至 46.1%）的大幅绝对改进，优于各种强大的基线。据我们所知，这是第一个用于可扩展地导出环境密集型任务的公共管道。

</details>

---

## 7. FeatureBench: Benchmarking Agentic Coding for Complex Feature Development / FeatureBench：复杂功能开发的代理编码基准测试

**Date**: 2026-02-11 | **arXiv**: [2602.10975v1](http://arxiv.org/abs/2602.10975v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10975v1)

**Categories**: cs.SE, cs.AI

<details><summary><b>Abstract / 摘要</b></summary>

Agents powered by large language models (LLMs) are increasingly adopted in the software industry, contributing code as collaborators or even autonomous developers. As their presence grows, it becomes important to assess the current boundaries of their coding abilities. Existing agentic coding benchmarks, however, cover a limited task scope, e.g., bug fixing within a single pull request (PR), and often rely on non-executable evaluations or lack an automated approach for continually updating the evaluation coverage. To address such issues, we propose FeatureBench, a benchmark designed to evaluate agentic coding performance in end-to-end, feature-oriented software development. FeatureBench incorporates an execution-based evaluation protocol and a scalable test-driven method that automatically derives tasks from code repositories with minimal human effort. By tracing from unit tests along a dependency graph, our approach can identify feature-level coding tasks spanning multiple commits and PRs scattered across the development timeline, while ensuring the proper functioning of other features after the separation. Using this framework, we curated 200 challenging evaluation tasks and 3825 executable environments from 24 open-source repositories in the first version of our benchmark. Empirical evaluation reveals that the state-of-the-art agentic model, such as Claude 4.5 Opus, which achieves a 74.4% resolved rate on SWE-bench, succeeds on only 11.0% of tasks, opening new opportunities for advancing agentic coding. Moreover, benefiting from our automated task collection toolkit, FeatureBench can be easily scaled and updated over time to mitigate data leakage. The inherent verifiability of constructed environments also makes our method potentially valuable for agent training.

由大型语言模型 (LLM) 支持的代理在软件行业中越来越多地采用，作为协作者甚至自主开发人员贡献代码。随着他们的存在不断增长，评估他们当前编码能力的界限就变得很重要。然而，现有的代理编码基准覆盖的任务范围有限，例如，单个拉取请求（PR）内的错误修复，并且通常依赖于不可执行的评估或缺乏持续更新评估覆盖范围的自动化方法。为了解决这些问题，我们提出了FeatureBench，这是一个旨在评估端到端、面向功能的软件开发中代理编码性能的基准。 FeatureBench 结合了基于执行的评估协议和可扩展的测试驱动方法，可以以最少的人力自动从代码存储库中派生任务。通过沿着依赖图跟踪单元测试，我们的方法可以识别跨越多个提交和分散在开发时间线上的 PR 的功能级编码任务，同时确保分离后其他功能的正常运行。使用这个框架，我们在基准测试的第一个版本中从 24 个开源存储库中策划了 200 个具有挑战性的评估任务和 3825 个可执行环境。实证评估表明，最先进的代理模型，例如 Claude 4.5 Opus，在 SWE-bench 上实现了 74.4% 的解决率，但仅成功完成了 11.0% 的任务，这为推进代理编码开辟了新的机会。此外，受益于我们的自动化任务收集工具包，FeatureBench 可以随着时间的推移轻松扩展和更新，以减少数据泄漏。构建环境固有的可验证性也使我们的方法对于代理训练具有潜在价值。

</details>

---

## 8. Blind Gods and Broken Screens: Architecting a Secure, Intent-Centric Mobile Agent Operating System / 盲目的上帝和破碎的屏幕：构建一个安全的、以意图为中心的移动代理操作系统

**Date**: 2026-02-11 | **arXiv**: [2602.10915v1](http://arxiv.org/abs/2602.10915v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10915v1)

**Categories**: cs.CR, cs.AI

<details><summary><b>Abstract / 摘要</b></summary>

The evolution of Large Language Models (LLMs) has shifted mobile computing from App-centric interactions to system-level autonomous agents. Current implementations predominantly rely on a "Screen-as-Interface" paradigm, which inherits structural vulnerabilities and conflicts with the mobile ecosystem's economic foundations. In this paper, we conduct a systematic security analysis of state-of-the-art mobile agents using Doubao Mobile Assistant as a representative case. We decompose the threat landscape into four dimensions - Agent Identity, External Interface, Internal Reasoning, and Action Execution - revealing critical flaws such as fake App identity, visual spoofing, indirect prompt injection, and unauthorized privilege escalation stemming from a reliance on unstructured visual data.   To address these challenges, we propose Aura, an Agent Universal Runtime Architecture for a clean-slate secure agent OS. Aura replaces brittle GUI scraping with a structured, agent-native interaction model. It adopts a Hub-and-Spoke topology where a privileged System Agent orchestrates intent, sandboxed App Agents execute domain-specific tasks, and the Agent Kernel mediates all communication. The Agent Kernel enforces four defense pillars: (i) cryptographic identity binding via a Global Agent Registry; (ii) semantic input sanitization through a multilayer Semantic Firewall; (iii) cognitive integrity via taint-aware memory and plan-trajectory alignment; and (iv) granular access control with non-deniable auditing. Evaluation on MobileSafetyBench shows that, compared to Doubao, Aura improves low-risk Task Success Rate from roughly 75% to 94.3%, reduces high-risk Attack Success Rate from roughly 40% to 4.4%, and achieves near-order-of-magnitude latency gains. These results demonstrate Aura as a viable, secure alternative to the "Screen-as-Interface" paradigm.

大型语言模型 (LLM) 的发展已将移动计算从以应用程序为中心的交互转变为系统级自主代理。当前的实现主要依赖于“屏幕即界面”范式，该范式继承了结构性漏洞并与移动生态系统的经济基础发生冲突。在本文中，我们以豆宝手机助手为代表案例，对最先进的移动代理进行了系统的安全分析。我们将威胁态势分解为四个维度——代理身份、外部接口、内部推理和操作执行——揭示了由于依赖非结构化视觉数据而产生的虚假应用程序身份、视觉欺骗、间接提示注入和未经授权的权限升级等关键缺陷。   为了应对这些挑战，我们提出了 Aura，一种用于全新安全代理操作系统的代理通用运行时架构。 Aura 用结构化的代理本机交互模型取代了脆弱的 GUI 抓取。它采用中心辐射型拓扑，其中特权系统代理协调意图，沙盒应用程序代理执行特定于域的任务，代理内核协调所有通信。代理内核强制执行四个防御支柱：(i) 通过全局代理注册表进行加密身份绑定； (ii) 通过多层语义防火墙进行语义输入清理； (iii) 通过污点感知记忆和计划轨迹对齐实现认知完整性； (iv) 具有不可否认审计的精细访问控制。 MobileSafetyBench评测显示，与豆宝相比，Aura将低风险任务成功率从大约75%提高到94.3%，将高风险攻击成功率从大约40%降低到4.4%，并实现了接近数量级的延迟增益。这些结果表明 Aura 是“屏幕即界面”范例的可行、安全的替代方案。

</details>

---

## 9. Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters / 步骤3.5 Flash：利用11B主动参数开放前沿级智能

**Date**: 2026-02-11 | **arXiv**: [2602.10604v1](http://arxiv.org/abs/2602.10604v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10604v1)

**Categories**: cs.CL, cs.AI

<details><summary><b>Abstract / 摘要</b></summary>

We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.

我们引入了 Step 3.5 Flash，这是一种稀疏专家混合 (MoE) 模型，可连接前沿级代理智能和计算效率。在构建代理时，我们关注最重要的事情：敏锐的推理和快速、可靠的执行。步骤 3.5 Flash 将 196B 参数基础与 11B 活动参数配对，以实现高效推理。它通过交错 3:1 滑动窗口/全注意力和多令牌预测 (MTP-3) 进行优化，以减少多轮代理交互的延迟和成本。为了达到前沿水平的智能，我们设计了一个可扩展的强化学习框架，该框架将可验证的信号与偏好反馈相结合，同时在大规模离策略训练下保持稳定，从而实现数学、代码和工具使用方面的一致自我改进。 Step 3.5 Flash 在智能体、编码和数学任务上表现出了强大的性能，在 IMO-AnswerBench 上实现了 85.4%，在 LiveCodeBench-v6 (2024.08-2025.05) 上实现了 86.4%，在 tau2-Bench 上实现了 88.2%，在 BrowseComp（具有上下文管理）上实现了 69.0%，在 Terminal-Bench 2.0 上实现了 51.0%，与前沿型号，如 GPT-5.2 xHigh 和 Gemini 3.0 Pro。通过重新定义效率边界，Step 3.5 Flash 为在现实工业环境中部署复杂的代理提供了高密度基础。

</details>

---

## 10. LHAW: Controllable Underspecification for Long-Horizon Tasks / LHAW：长期任务的可控不足

**Date**: 2026-02-11 | **arXiv**: [2602.10525v1](http://arxiv.org/abs/2602.10525v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10525v1)

**Categories**: cs.CL, cs.AI, cs.LG

<details><summary><b>Abstract / 摘要</b></summary>

Long-horizon workflow agents that operate effectively over extended periods are essential for truly autonomous systems. Their reliable execution critically depends on the ability to reason through ambiguous situations in which clarification seeking is necessary to ensure correct task execution. However, progress is limited by the lack of scalable, task-agnostic frameworks for systematically curating and measuring the impact of ambiguity across custom workflows. We address this gap by introducing LHAW (Long-Horizon Augmented Workflows), a modular, dataset-agnostic synthetic pipeline that transforms any well-specified task into controllable underspecified variants by systematically removing information across four dimensions - Goals, Constraints, Inputs, and Context - at configurable severity levels. Unlike approaches that rely on LLM predictions of ambiguity, LHAW validates variants through empirical agent trials, classifying them as outcome-critical, divergent, or benign based on observed terminal state divergence. We release 285 task variants from TheAgentCompany, SWE-Bench Pro and MCP-Atlas according to our taxonomy alongside formal analysis measuring how current agents detect, reason about, and resolve underspecification across ambiguous settings. LHAW provides the first systematic framework for cost-sensitive evaluation of agent clarification behavior in long-horizon settings, enabling development of reliable autonomous systems.

长期有效运行的长视野工作流代理对于真正的自治系统至关重要。它们的可靠执行关键取决于在模糊情况下进行推理的能力，在这种情况下，需要寻求澄清以确保正确的任务执行。然而，由于缺乏可扩展的、与任务无关的框架来系统地管理和衡量自定义工作流程中模糊性的影响，进展受到限制。我们通过引入 LHAW（长视野增强工作流）来解决这一差距，这是一种模块化的、与数据集无关的合成管道，通过在可配置的严重性级别系统地删除四个维度（目标、约束、输入和上下文）的信息，将任何明确指定的任务转换为可控的未指定变体。与依赖 LLM 模糊性预测的方法不同，LHAW 通过经验代理试验来验证变体，并根据观察到的最终状态差异将它们分类为结果关键型、发散型或良性型。我们根据我们的分类法，从 TheAgentCompany、SWE-Bench Pro 和 MCP-Atlas 发布了 285 个任务变体，同时进行正式分析，衡量当前代理如何在不明确的设置中检测、推理和解决规范不足的问题。 LHAW 提供了第一个系统框架，用于对长视野环境中的代理澄清行为进行成本敏感的评估，从而能够开发可靠的自主系统。

</details>

---

## 11. Co-jump: Cooperative Jumping with Quadrupedal Robots via Multi-Agent Reinforcement Learning / 协同跳跃：通过多智能体强化学习与四足机器人协同跳跃

**Date**: 2026-02-11 | **arXiv**: [2602.10514v1](http://arxiv.org/abs/2602.10514v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10514v1)

**Categories**: cs.RO, cs.AI, cs.LG

<details><summary><b>Abstract / 摘要</b></summary>

While single-agent legged locomotion has witnessed remarkable progress, individual robots remain fundamentally constrained by physical actuation limits. To transcend these boundaries, we introduce Co-jump, a cooperative task where two quadrupedal robots synchronize to execute jumps far beyond their solo capabilities. We tackle the high-impulse contact dynamics of this task under a decentralized setting, achieving synchronization without explicit communication or pre-specified motion primitives. Our framework leverages Multi-Agent Proximal Policy Optimization (MAPPO) enhanced by a progressive curriculum strategy, which effectively overcomes the sparse-reward exploration challenges inherent in mechanically coupled systems. We demonstrate robust performance in simulation and successful transfer to physical hardware, executing multi-directional jumps onto platforms up to 1.5 m in height. Specifically, one of the robots achieves a foot-end elevation of 1.1 m, which represents a 144% improvement over the 0.45 m jump height of a standalone quadrupedal robot, demonstrating superior vertical performance. Notably, this precise coordination is achieved solely through proprioceptive feedback, establishing a foundation for communication-free collaborative locomotion in constrained environments.

虽然单代理腿运动已经取得了显着的进步，但单个机器人仍然从根本上受到物理驱动限制的限制。为了超越这些界限，我们引入了协同跳跃，这是一种合作任务，其中两个四足机器人同步执行远远超出其单独能力的跳跃。我们在分散的设置下处理该任务的高脉冲接触动力学，无需显式通信或预先指定的运动基元即可实现同步。我们的框架利用渐进式课程策略增强的多智能体近端策略优化（MAPPO），有效克服了机械耦合系统固有的稀疏奖励探索挑战。我们在模拟中展示了强大的性能，并成功转移到物理硬件，在高达 1.5 m 的平台上执行多向跳跃。具体来说，其中一台机器人的足端高度达到了 1.1 m，这比独立四足机器人的 0.45 m 跳跃高度提高了 144%，展示了卓越的垂直性能。值得注意的是，这种精确的协调仅通过本体感觉反馈来实现，为受限环境中的无通信协作运动奠定了基础。

</details>

---

## 12. Authenticated Workflows: A Systems Approach to Protecting Agentic AI / 经过身份验证的工作流程：保护代理人工智能的系统方法

**Date**: 2026-02-11 | **arXiv**: [2602.10465v1](http://arxiv.org/abs/2602.10465v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10465v1)

**Categories**: cs.CR, cs.AI, cs.DC, cs.MA

<details><summary><b>Abstract / 摘要</b></summary>

Agentic AI systems automate enterprise workflows but existing defenses--guardrails, semantic filters--are probabilistic and routinely bypassed. We introduce authenticated workflows, the first complete trust layer for enterprise agentic AI. Security reduces to protecting four fundamental boundaries: prompts, tools, data, and context. We enforce intent (operations satisfy organizational policies) and integrity (operations are cryptographically authentic) at every boundary crossing, combining cryptographic elimination of attack classes with runtime policy enforcement. This delivers deterministic security--operations either carry valid cryptographic proof or are rejected. We introduce MAPL, an AI-native policy language that expresses agentic constraints dynamically as agents evolve and invocation context changes, scaling as O(log M + N) policies versus O(M x N) rules through hierarchical composition with cryptographic attestations for workflow dependencies. We prove practicality through a universal security runtime integrating nine leading frameworks (MCP, A2A, OpenAI, Claude, LangChain, CrewAI, AutoGen, LlamaIndex, Haystack) through thin adapters requiring zero protocol modifications. Formal proofs establish completeness and soundness. Empirical validation shows 100% recall with zero false positives across 174 test cases, protection against 9 of 10 OWASP Top 10 risks, and complete mitigation of two high impact production CVEs.

代理人工智能系统使企业工作流程自动化，但现有的防御措施——护栏、语义过滤器——都是概率性的，经常被绕过。我们引入了经过身份验证的工作流程，这是企业代理人工智能的第一个完整信任层。安全性简化为保护四个基本边界：提示、工具、数据和上下文。我们在每个边界交叉处强制执行意图（操作满足组织策略）和完整性（操作在加密上是真实的），将攻击类的加密消除与运行时策略执行相结合。这提供了确定性的安全性——操作要么携带有效的加密证明，要么被拒绝。我们引入了 MAPL，一种 AI 原生策略语言，随着代理的发展和调用上下文的变化，动态地表达代理约束，通过工作流依赖项的加密证明的分层组合，将策略扩展为 O(log M + N) 策略，而不是 O(M x N) 规则。我们通过通用安全运行时证明了实用性，该通用安全运行时通过需要零协议修改的瘦适配器集成了九个领先框架（MCP、A2A、OpenAI、Claude、LangChain、CrewAI、AutoGen、LlamaIndex、Haystack）。形式证明确立了完整性和可靠性。实证验证显示，174 个测试用例的召回率为 100%，误报为零，可防范 10 个 OWASP Top 10 风险中的 9 个风险，并完全缓解两个高影响生产 CVE。

</details>

---

## 13. AudioRouter: Data Efficient Audio Understanding via RL based Dual Reasoning / AudioRouter：通过基于强化学习的双重推理实现数据高效的音频理解

**Date**: 2026-02-11 | **arXiv**: [2602.10439v1](http://arxiv.org/abs/2602.10439v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10439v1)

**Categories**: cs.SD, cs.AI, eess.AS

<details><summary><b>Abstract / 摘要</b></summary>

Large Audio Language Models (LALMs) have demonstrated strong capabilities in audio understanding and reasoning. However, their performance on fine grained auditory perception remains unreliable, and existing approaches largely rely on data intensive training to internalize perceptual abilities. We propose AudioRouter, a reinforcement learning framework that enables LALMs to improve audio understanding by learning when and how to use external audio tools. Rather than tightly coupling tool usage with audio reasoning, AudioRouter formulates tool use as an explicit decision making problem and optimizes a lightweight routing policy while keeping the underlying reasoning model frozen. Experimental results show that AudioRouter achieves substantial improvements on standard audio understanding benchmarks while requiring up to 600x less training data to learn tool usage compared with conventional training paradigms. These findings suggest that learning effective tool usage offers a data efficient and scalable alternative to internalizing perceptual abilities in LALMs.

大型音频语言模型（LALM）在音频理解和推理方面表现出了强大的能力。然而，它们在细粒度听觉感知上的表现仍然不可靠，现有方法很大程度上依赖于数据密集型训练来内化感知能力。我们提出了 AudioRouter，这是一种强化学习框架，使 LALM 能够通过学习何时以及如何使用外部音频工具来提高音频理解。 AudioRouter 不是将工具使用与音频推理紧密耦合，而是将工具使用制定为显式决策问题，并优化轻量级路由策略，同时保持底层推理模型冻结。实验结果表明，AudioRouter 在标准音频理解基准上取得了实质性改进，同时与传统训练范例相比，学习工具使用所需的训练数据减少了 600 倍。这些发现表明，学习有效的工具使用为内化 LALM 中的感知能力提供了一种数据高效且可扩展的替代方案。

</details>

---

## 14. ISD-Agent-Bench: A Comprehensive Benchmark for Evaluating LLM-based Instructional Design Agents / ISD-Agent-Bench：评估基于 LLM 的教学设计代理的综合基准

**Date**: 2026-02-11 | **arXiv**: [2602.10620v1](http://arxiv.org/abs/2602.10620v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10620v1)

**Categories**: cs.SE, cs.CL

<details><summary><b>Abstract / 摘要</b></summary>

Large Language Model (LLM) agents have shown promising potential in automating Instructional Systems Design (ISD), a systematic approach to developing educational programs. However, evaluating these agents remains challenging due to the lack of standardized benchmarks and the risk of LLM-as-judge bias. We present ISD-Agent-Bench, a comprehensive benchmark comprising 25,795 scenarios generated via a Context Matrix framework that combines 51 contextual variables across 5 categories with 33 ISD sub-steps derived from the ADDIE model. To ensure evaluation reliability, we employ a multi-judge protocol using diverse LLMs from different providers, achieving high inter-judge reliability. We compare existing ISD agents with novel agents grounded in classical ISD theories such as ADDIE, Dick \& Carey, and Rapid Prototyping ISD. Experiments on 1,017 test scenarios demonstrate that integrating classical ISD frameworks with modern ReAct-style reasoning achieves the highest performance, outperforming both pure theory-based agents and technique-only approaches. Further analysis reveals that theoretical quality strongly correlates with benchmark performance, with theory-based agents showing significant advantages in problem-centered design and objective-assessment alignment. Our work provides a foundation for systematic LLM-based ISD research.

大语言模型（LLM）代理在自动化教学系统设计（ISD）方面显示出巨大的潜力，这是一种开发教育项目的系统方法。然而，由于缺乏标准化基准以及法学硕士法官偏见的风险，评估这些代理人仍然具有挑战性。我们提出了 ISD-Agent-Bench，这是一个综合基准测试，包含通过上下文矩阵框架生成的 25,795 个场景，该框架将跨 5 个类别的 51 个上下文变量与源自 ADDIE 模型的 33 个 ISD 子步骤相结合。为了确保评估的可靠性，我们采用了多法官协议，使用来自不同提供商的不同法学硕士，实现了法官间的高可靠性。我们将现有的 ISD 代理与基于经典 ISD 理论（例如 ADDIE、Dick & Carey 和快速原型 ISD）的新型代理进行比较。 1,017 个测试场景的实验表明，将经典 ISD 框架与现代 ReAct 式推理相结合可实现最高性能，优于纯粹基于理论的代理和仅技术方法。进一步的分析表明，理论质量与基准性能密切相关，基于理论的代理在以问题为中心的设计和目标评估一致性方面显示出显着的优势。我们的工作为基于法学硕士的系统性 ISD 研究奠定了基础。

</details>

---

## 15. TestExplora: Benchmarking LLMs for Proactive Bug Discovery via Repository-Level Test Generation / TestExplora：通过存储库级测试生成对 LLM 进行主动 Bug 发现基准测试

**Date**: 2026-02-11 | **arXiv**: [2602.10471v1](http://arxiv.org/abs/2602.10471v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10471v1)

**Categories**: cs.SE, cs.CL

<details><summary><b>Abstract / 摘要</b></summary>

Given that Large Language Models (LLMs) are increasingly applied to automate software development, comprehensive software assurance spans three distinct goals: regression prevention, reactive reproduction, and proactive discovery. Current evaluations systematically overlook the third goal. Specifically, they either treat existing code as ground truth (a compliance trap) for regression prevention, or depend on post-failure artifacts (e.g., issue reports) for bug reproduction-so they rarely surface defects before failures. To bridge this gap, we present TestExplora, a benchmark designed to evaluate LLMs as proactive testers within full-scale, realistic repository environments. TestExplora contains 2,389 tasks from 482 repositories and hides all defect-related signals. Models must proactively find bugs by comparing implementations against documentation-derived intent, using documentation as the oracle. Furthermore, to keep evaluation sustainable and reduce leakage, we propose continuous, time-aware data collection. Our evaluation reveals a significant capability gap: state-of-the-art models achieve a maximum Fail-to-Pass (F2P) rate of only 16.06%. Further analysis indicates that navigating complex cross-module interactions and leveraging agentic exploration are critical to advancing LLMs toward autonomous software quality assurance. Consistent with this, SWEAgent instantiated with GPT-5-mini achieves an F2P of 17.27% and an F2P@5 of 29.7%, highlighting the effectiveness and promise of agentic exploration in proactive bug discovery tasks.

鉴于大型语言模型 (LLM) 越来越多地应用于自动化软件开发，全面的软件保障涵盖三个不同的目标：回归预防、反应性再现和主动发现。目前的评估系统地忽视了第三个目标。具体来说，他们要么将现有代码视为用于回归预防的基本事实（合规性陷阱），要么依赖于故障后工件（例如问题报告）来进行错误再现 - 因此他们很少在故障之前暴露缺陷。为了弥补这一差距，我们推出了 TestExplora，这是一个基准测试，旨在评估法学硕士在全面、真实的存储库环境中作为主动测试人员的能力。 TestExplora 包含来自 482 个存储库的 2,389 个任务，并隐藏所有与缺陷相关的信号。模型必须使用文档作为预言机，通过将实现与文档派生的意图进行比较来主动发现错误。此外，为了保持评估的可持续性并减少泄漏，我们建议进行连续的、具有时间意识的数据收集。我们的评估揭示了巨大的能力差距：最先进的模型的最大失败率 (F2P) 仅达到 16.06%。进一步的分析表明，导航复杂的跨模块交互和利用代理探索对于推进法学硕士走向自主软件质量保证至关重要。与此一致，使用 GPT-5-mini 实例化的 SWEAgent 实现了 17.27% 的 F2P 和 29.7% 的 F2P@5，凸显了代理探索在主动 bug 发现任务中的有效性和前景。

</details>

---

## 16. The Landscape of Prompt Injection Threats in LLM Agents: From Taxonomy to Analysis / LLM 代理中的即时注入威胁概况：从分类到分析

**Date**: 2026-02-11 | **arXiv**: [2602.10453v1](http://arxiv.org/abs/2602.10453v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10453v1)

**Categories**: cs.CR, cs.CL

<details><summary><b>Abstract / 摘要</b></summary>

The evolution of Large Language Models (LLMs) has resulted in a paradigm shift towards autonomous agents, necessitating robust security against Prompt Injection (PI) vulnerabilities where untrusted inputs hijack agent behaviors. This SoK presents a comprehensive overview of the PI landscape, covering attacks, defenses, and their evaluation practices. Through a systematic literature review and quantitative analysis, we establish taxonomies that categorize PI attacks by payload generation strategies (heuristic vs. optimization) and defenses by intervention stages (text, model, and execution levels). Our analysis reveals a key limitation shared by many existing defenses and benchmarks: they largely overlook context-dependent tasks, in which agents are authorized to rely on runtime environmental observations to determine actions. To address this gap, we introduce AgentPI, a new benchmark designed to systematically evaluate agent behavior under context-dependent interaction settings. Using AgentPI, we empirically evaluate representative defenses and show that no single approach can simultaneously achieve high trustworthiness, high utility, and low latency. Moreover, we show that many defenses appear effective under existing benchmarks by suppressing contextual inputs, yet fail to generalize to realistic agent settings where context-dependent reasoning is essential. This SoK distills key takeaways and open research problems, offering structured guidance for future research and practical deployment of secure LLM agents.

大型语言模型 (LLM) 的发展导致了向自主代理的范式转变，因此需要针对即时注入 (PI) 漏洞（即不受信任的输入劫持代理行为）提供强大的安全性。该 SoK 全面概述了 PI 领域，涵盖攻击、防御及其评估实践。通过系统的文献综述和定量分析，我们建立了分类法，根据有效负载生成策略（启发式与优化）对 PI 攻击进行分类，并根据干预阶段（文本、模型和执行级别）对防御进行分类。我们的分析揭示了许多现有防御和基准所共有的一个关键限制：它们在很大程度上忽略了上下文相关的任务，在这些任务中，代理被授权依赖运行时环境观察来确定行动。为了解决这一差距，我们引入了 AgentPI，这是一个新的基准，旨在系统地评估上下文相关交互设置下的代理行为。使用 AgentPI，我们根据经验评估了代表性防御，并表明没有任何一种方法可以同时实现高可信度、高实用性和低延迟。此外，我们表明，许多防御措施在现有基准下通过抑制上下文输入而显得有效，但无法推广到上下文相关推理至关重要的现实代理设置。该 SoK 提炼了关键要点和开放研究问题，为安全 LLM 代理的未来研究和实际部署提供结构化指导。

</details>

---

## 17. From Natural Language to Materials Discovery:The Materials Knowledge Navigation Agent / 从自然语言到材料发现：材料知识导航代理

**Date**: 2026-02-11 | **arXiv**: [2602.11123v1](http://arxiv.org/abs/2602.11123v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11123v1)

**Categories**: cs.LG, cond-mat.mtrl-sci

<details><summary><b>Abstract / 摘要</b></summary>

Accelerating the discovery of high-performance materials remains a central challenge across energy, electronics, and aerospace technologies, where traditional workflows depend heavily on expert intuition and computationally expensive simulations. Here we introduce the Materials Knowledge Navigation Agent (MKNA), a language-driven system that translates natural-language scientific intent into executable actions for database retrieval, property prediction, structure generation, and stability evaluation. Beyond automating tool invocation, MKNA autonomously extracts quantitative thresholds and chemically meaningful design motifs from literature and database evidence, enabling data-grounded hypothesis formation. Applied to the search for high-Debye-temperature ceramics, the agent identifies a literature-supported screening criterion (Theta_D > 800 K), rediscovers canonical ultra-stiff materials such as diamond, SiC, SiN, and BeO, and proposes thermodynamically stable, previously unreported Be-C-rich compounds that populate the sparsely explored 1500-1700 K regime. These results demonstrate that MKNA not only finds stable candidates but also reconstructs interpretable design heuristics, establishing a generalizable platform for autonomous, language-guided materials exploration.

加速高性能材料的发现仍然是能源、电子和航空航天技术领域的核心挑战，这些技术的传统工作流程在很大程度上依赖于专家的直觉和计算成本高昂的模拟。在这里，我们介绍材料知识导航代理（MKNA），这是一种语言驱动的系统，可将自然语言的科学意图转化为可执行的操作，用于数据库检索、属性预测、结构生成和稳定性评估。除了自动化工具调用之外，MKNA 还可以从文献和数据库证据中自主提取定量阈值和具有化学意义的设计主题，从而能够形成基于数据的假设。应用于寻找高德拜温度陶瓷时，该代理确定了文献支持的筛选标准（Theta_D > 800 K），重新发现了典型的超硬材料，例如金刚石、SiC、SiN 和 BeO，并提出了热力学稳定、先前未报道的富含 Be-C 的化合物，这些化合物填充了很少探索的 1500-1700 K 范围。这些结果表明，MKNA 不仅找到了稳定的候选者，而且还重建了可解释的设计启发式，为自主的、语言引导的材料探索建立了一个通用平台。

</details>

---

## 18. Divide, Harmonize, Then Conquer It: Shooting Multi-Commodity Flow Problems with Multimodal Language Models / 划分、协调、然后征服它：用多模式语言模型解决多商品流问题

**Date**: 2026-02-11 | **arXiv**: [2602.11057v1](http://arxiv.org/abs/2602.11057v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11057v1)

**Categories**: cs.LG

<details><summary><b>Abstract / 摘要</b></summary>

The multi-commodity flow (MCF) problem is a fundamental topic in network flow and combinatorial optimization, with broad applications in transportation, communication, and logistics, etc. Nowadays, the rapid expansion of allocation systems has posed challenges for existing optimization engines in balancing optimality and tractability. In this paper, we present Pram, the first ML-based method that leverages the reasoning power of multimodal language models (MLMs) for addressing the trade-off dilemma -- a great need of service providers. As part of our proposal, Pram (i) quickly computes high-quality allocations by dividing the original problem into local subproblems, which are then resolved by an MLM-powered "agent", and (ii) ensures global consistency by harmonizing these subproblems via a multi-agent reinforcement learning algorithm. Theoretically, we show that Pram, which learns to perform gradient descent in context, provably converges to the optimum within the family of MCF problems. Empirically, on real-world datasets and public topologies, Pram achieves performance comparable to, and in some cases even surpassing, linear programming solvers (very close to the optimal solution), and substantially lower runtimes (1 to 2 orders of magnitude faster). Moreover, Pram exhibits strong robustness (<10\% performance degradation under link failures or flow bursts), demonstrating MLM's generalization ability to unforeseen events. Pram is objective-agnostic and seamlessly integrates with mainstream allocation systems, providing a practical and scalable solution for future networks.

多商品流（MCF）问题是网络流和组合优化的一个基本课题，在交通、通信、物流等领域有着广泛的应用。当今，分配系统的快速扩展对现有优化引擎在平衡最优性和易处理性方面提出了挑战。在本文中，我们提出了 Pram，这是第一个基于 ML 的方法，它利用多模式语言模型 (MLM) 的推理能力来解决权衡困境——这是服务提供商的巨大需求。作为我们提案的一部分，Pram (i) 通过将原始问题划分为局部子问题来快速计算高质量分配，然后由 MLM 支持的“代理”解决这些子问题，并且 (ii) 通过多代理强化学习算法协调这些子问题来确保全局一致性。从理论上讲，我们证明 Pram 学习在上下文中执行梯度下降，可以证明收敛到 MCF 问题族中的最优值。根据经验，在现实世界的数据集和公共拓扑上，Pram 的性能可与线性规划求解器相媲美，在某些情况下甚至超过线性规划求解器（非常接近最优解），并且运行时间显着降低（快 1 到 2 个数量级）。此外，Pram 表现出很强的鲁棒性（在链路故障或流量突发情况下性能下降 <10\%），展示了 MLM 对不可预见事件的泛化能力。 Pram 与目标无关，并与主流分配系统无缝集成，为未来网络提供实用且可扩展的解决方案。

</details>

---

## 19. CMAD: Cooperative Multi-Agent Diffusion via Stochastic Optimal Control / CMAD：通过随机最优控制的协作多智能体扩散

**Date**: 2026-02-11 | **arXiv**: [2602.10933v1](http://arxiv.org/abs/2602.10933v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10933v1)

**Categories**: cs.LG

<details><summary><b>Abstract / 摘要</b></summary>

Continuous-time generative models have achieved remarkable success in image restoration and synthesis. However, controlling the composition of multiple pre-trained models remains an open challenge. Current approaches largely treat composition as an algebraic composition of probability densities, such as via products or mixtures of experts. This perspective assumes the target distribution is known explicitly, which is almost never the case. In this work, we propose a different paradigm that formulates compositional generation as a cooperative Stochastic Optimal Control problem. Rather than combining probability densities, we treat pre-trained diffusion models as interacting agents whose diffusion trajectories are jointly steered, via optimal control, toward a shared objective defined on their aggregated output. We validate our framework on conditional MNIST generation and compare it against a naive inference-time DPS-style baseline replacing learned cooperative control with per-step gradient guidance.

连续时间生成模型在图像恢复和合成方面取得了显着的成功。然而，控制多个预训练模型的组成仍然是一个开放的挑战。当前的方法主要将组合视为概率密度的代数组合，例如通过乘积或专家的组合。这种观点假设目标分布是明确已知的，但事实几乎从未如此。在这项工作中，我们提出了一种不同的范式，将组合生成表述为协作随机最优控制问题。我们不是将概率密度结合起来，而是将预先训练的扩散模型视为交互代理，其扩散轨迹通过最优控制共同引导，以实现根据其聚合输出定义的共享目标。我们在条件 MNIST 生成上验证了我们的框架，并将其与朴素推理时间 DPS 风格的基线进行比较，用每步梯度指导取代学习的合作控制。

</details>

---

## 20. Beyond Task Performance: A Metric-Based Analysis of Sequential Cooperation in Heterogeneous Multi-Agent Destructive Foraging / 超越任务绩效：异构多智能体破坏性觅食中顺序合作的基于度量的分析

**Date**: 2026-02-11 | **arXiv**: [2602.10685v1](http://arxiv.org/abs/2602.10685v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10685v1)

**Categories**: cs.MA, cs.LG

<details><summary><b>Abstract / 摘要</b></summary>

This work addresses the problem of analyzing cooperation in heterogeneous multi-agent systems which operate under partial observability and temporal role dependency, framed within a destructive multi-agent foraging setting. Unlike most previous studies, which focus primarily on algorithmic performance with respect to task completion, this article proposes a systematic set of general-purpose cooperation metrics aimed at characterizing not only efficiency, but also coordination and dependency between teams and agents, fairness, and sensitivity. These metrics are designed to be transferable to different multi-agent sequential domains similar to foraging. The proposed suite of metrics is structured into three main categories that jointly provide a multilevel characterization of cooperation: primary metrics, inter-team metrics, and intra-team metrics. They have been validated in a realistic destructive foraging scenario inspired by dynamic aquatic surface cleaning using heterogeneous autonomous vehicles. It involves two specialized teams with sequential dependencies: one focused on the search of resources, and another on their destruction. Several representative approaches have been evaluated, covering both learning-based algorithms and classical heuristic paradigms.

这项工作解决了分析异构多智能体系统中的合作问题，这些系统在部分可观察性和时间角色依赖性下运行，并在破坏性多智能体觅食环境中构建。与大多数以前的研究主要关注任务完成方面的算法性能不同，本文提出了一套系统的通用合作指标，旨在不仅描述效率，还描述团队和代理之间的协调和依赖性、公平性和敏感性。这些指标被设计为可以转移到不同的多代理顺序域，类似于觅食。所提出的指标套件分为三个主要类别，共同提供合作的多层次特征：主要指标、团队间指标和团队内指标。它们已经在现实的破坏性觅食场景中得到了验证，该场景的灵感来自于使用异构自动驾驶车辆进行动态水生表面清洁。它涉及两个具有顺序依赖性的专业团队：一个专注于资源搜索，另一个专注于资源销毁。已经评估了几种代表性方法，涵盖基于学习的算法和经典启发式范例。

</details>

---

## 21. Why Agentic Theorem Prover Works: A Statistical Provability Theory of Mathematical Reasoning Models / 为什么代理定理证明器有效：数学推理模型的统计可证明性理论

**Date**: 2026-02-11 | **arXiv**: [2602.10538v1](http://arxiv.org/abs/2602.10538v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10538v1)

**Categories**: stat.ML, cs.LG

<details><summary><b>Abstract / 摘要</b></summary>

Agentic theorem provers -- pipelines that couple a mathematical reasoning model with library retrieval, subgoal-decomposition/search planner, and a proof assistant verifier -- have recently achieved striking empirical success, yet it remains unclear which components drive performance and why such systems work at all despite classical hardness of proof search. We propose a distributional viewpoint and introduce **statistical provability**, defined as the finite-horizon success probability of reaching a verified proof, averaged over an instance distribution, and formalize modern theorem-proving pipelines as time-bounded MDPs. Exploiting Bellman structure, we prove existence of optimal policies under mild regularity, derive provability certificates via sub-/super-solution inequalities, and bound the performance gap of score-guided planning (greedy/top-\(k\)/beam/rollouts) in terms of approximation error, sequential statistical complexity, representation geometry (metric entropy/doubling structure), and action-gap margin tails. Together, our theory provides a principled, component-sensitive explanation of when and why agentic theorem provers succeed on biased real-world problem distributions, while clarifying limitations in worst-case or adversarial regimes.

代理定理证明器——将数学推理模型与库检索、子目标分解/搜索规划器和证明辅助验证器相结合的管道——最近取得了惊人的经验成功，但仍不清楚哪些组件驱动性能以及为什么尽管证明搜索具有经典的难度，但这些系统仍然有效。我们提出了分布观点并引入了**统计可证明性**，定义为达到已验证证明的有限范围成功概率，在实例分布上取平均值，并将现代定理证明流程形式化为有时间限制的 MDP。利用贝尔曼结构，我们证明了在温和规律性下存在最优策略，通过子/超解不等式导出可证明性证书，并在逼近误差、顺序统计复杂性、表示几何（度量熵/加倍结构）和动作间隙边际尾部方面限制了分数引导规划（贪婪/顶\(k\)/束/推出）的性能差距。总之，我们的理论为代理定理证明者何时以及为何在有偏见的现实世界问题分布上取得成功提供了原则性的、对组件敏感的解释，同时澄清了最坏情况或对抗性制度的局限性。

</details>

---

## 22. Don't Eliminate Cut: Exponential Separations in LLM-Based Theorem Proving / 不要消除切割：基于 LLM 的定理证明中的指数分离

**Date**: 2026-02-11 | **arXiv**: [2602.10512v1](http://arxiv.org/abs/2602.10512v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10512v1)

**Categories**: cs.LG, cs.LO, stat.ML

<details><summary><b>Abstract / 摘要</b></summary>

We develop a theoretical analysis of LLM-guided formal theorem proving in interactive proof assistants (e.g., Lean) by modeling tactic proposal as a stochastic policy in a finite-horizon deterministic MDP. To capture modern representation learning, we treat the state and action spaces as general compact metric spaces and assume Lipschitz policies. To explain the gap between worst-case hardness and empirical success, we introduce problem distributions generated by a reference policy $q$, including a latent-variable model in which proofs exhibit reusable cut/lemma/sketch structure represented by a proof DAG. Under a top-$k$ search protocol and Tsybakov-type margin conditions, we derive lower bounds on finite-horizon success probability that decompose into search and learning terms, with learning controlled by sequential Rademacher/covering complexity. Our main separation result shows that when cut elimination expands a DAG of depth $D$ into a cut-free tree of size $Ω(Λ^D)$ while the cut-aware hierarchical process has size $O(λ^D)$ with $λ\llΛ$, a flat (cut-free) learner provably requires exponentially more data than a cut-aware hierarchical learner. This provides a principled justification for subgoal decomposition in recent agentic theorem provers.

我们通过将策略建议建模为有限范围确定性 MDP 中的随机策略，对交互式证明助手（例如 Lean）中的法学硕士指导的形式定理证明进行了理论分析。为了捕捉现代表示学习，我们将状态和动作空间视为一般的紧凑度量空间并假设 Lipschitz 策略。为了解释最坏情况的难度和经验成功之间的差距，我们引入了由参考策略 $q$ 生成的问题分布，包括一个潜变量模型，其中证明展示了由证明 DAG 表示的可重复使用的剪切/引理/草图结构。在 top-$k$ 搜索协议和 Tsybakov 型裕度条件下，我们推导出有限范围成功概率的下限，该概率分解为搜索和学习项，学习由顺序 Rademacher/覆盖复杂性控制。我们的主要分离结果表明，当剪切消除将深度为 $D$ 的 DAG 扩展为大小为 $Ω(Λ^D)$ 的免割树，而剪切感知分层过程的大小为 $O(λ^D)$ 和 $λ\llΛ$ 时，平面（免割）学习器可能需要比剪切感知分层学习器指数更多的数据。这为最近的代理定理证明者中的子目标分解提供了原则上的证明。

</details>

---

## 23. LiveMedBench: A Contamination-Free Medical Benchmark for LLMs with Automated Rubric Evaluation / LiveMedBench：针对法学硕士的无污染医学基准，具有自动评分标准评估

**Date**: 2026-02-10 | **arXiv**: [2602.10367v1](http://arxiv.org/abs/2602.10367v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10367v1)

**Categories**: cs.AI

<details><summary><b>Abstract / 摘要</b></summary>

The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination, where test sets inadvertently leak into training corpora, leading to inflated performance estimates; and (2) temporal misalignment, failing to capture the rapid evolution of medical knowledge. Furthermore, current evaluation metrics for open-ended clinical reasoning often rely on either shallow lexical overlap (e.g., ROUGE) or subjective LLM-as-a-Judge scoring, both inadequate for verifying clinical correctness. To bridge these gaps, we introduce LiveMedBench, a continuously updated, contamination-free, and rubric-based benchmark that weekly harvests real-world clinical cases from online medical communities, ensuring strict temporal separation from model training data. We propose a Multi-Agent Clinical Curation Framework that filters raw data noise and validates clinical integrity against evidence-based medical principles. For evaluation, we develop an Automated Rubric-based Evaluation Framework that decomposes physician responses into granular, case-specific criteria, achieving substantially stronger alignment with expert physicians than LLM-as-a-Judge. To date, LiveMedBench comprises 2,756 real-world cases spanning 38 medical specialties and multiple languages, paired with 16,702 unique evaluation criteria. Extensive evaluation of 38 LLMs reveals that even the best-performing model achieves only 39.2%, and 84% of models exhibit performance degradation on post-cutoff cases, confirming pervasive data contamination risks. Error analysis further identifies contextual application-not factual knowledge-as the dominant bottleneck, with 35-48% of failures stemming from the inability to tailor medical knowledge to patient-specific constraints.

在高风险临床环境中部署大型语言模型 (LLM) 需要严格而可靠的评估。然而，现有的医学基准仍然是静态的，存在两个关键的局限性：（1）数据污染，测试集无意中泄漏到训练语料库中，导致性能估计夸大； （2）时间错位，未能捕捉到医学知识的快速演变。此外，当前开放式临床推理的评估指标通常依赖于浅层词汇重叠（例如，ROUGE）或主观法学硕士作为法官评分，这两者都不足以验证临床正确性。为了弥补这些差距，我们引入了 LiveMedBench，这是一个不断更新、无污染且基于评分标准的基准，每周从在线医疗社区收集真实的临床病例，确保与模型训练数据严格的时间分离。我们提出了一个多代理临床管理框架，可以过滤原始数据噪音并根据循证医学原则验证临床完整性。为了进行评估，我们开发了一个基于评分标准的自动评估框架，将医生的反应分解为细化的、针对具体病例的标准，与法学硕士法官相比，与专家医生的一致性要强得多。迄今为止，LiveMedBench 包含 2,756 个真实案例，涵盖 38 个医学专业和多种语言，并配有 16,702 个独特的评估标准。对 38 个法学硕士的广泛评估表明，即使是表现最好的模型也只能达到 39.2%，并且 84% 的模型在截止后案例中表现出性能下降，证实了普遍存在的数据污染风险。错误分析进一步确定上下文应用（而不是事实知识）是主要瓶颈，35-48% 的失败源于无法根据患者特定的限制定制医学知识。

</details>

---

## 24. Self-Evolving Recommendation System: End-To-End Autonomous Model Optimization With LLM Agents / 自进化推荐系统：使用 LLM 代理进行端到端自主模型优化

**Date**: 2026-02-10 | **arXiv**: [2602.10226v1](http://arxiv.org/abs/2602.10226v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10226v1)

**Categories**: cs.LG, cs.AI

<details><summary><b>Abstract / 摘要</b></summary>

Optimizing large-scale machine learning systems, such as recommendation models for global video platforms, requires navigating a massive hyperparameter search space and, more critically, designing sophisticated optimizers, architectures, and reward functions to capture nuanced user behaviors. Achieving substantial improvements in these areas is a non-trivial task, traditionally relying on extensive manual iterations to test new hypotheses. We propose a self-evolving system that leverages Large Language Models (LLMs), specifically those from Google's Gemini family, to autonomously generate, train, and deploy high-performing, complex model changes within an end-to-end automated workflow. The self-evolving system is comprised of an Offline Agent (Inner Loop) that performs high-throughput hypothesis generation using proxy metrics, and an Online Agent (Outer Loop) that validates candidates against delayed north star business metrics in live production. Our agents act as specialized Machine Learning Engineers (MLEs): they exhibit deep reasoning capabilities, discovering novel improvements in optimization algorithms and model architecture, and formulating innovative reward functions that target long-term user engagement. The effectiveness of this approach is demonstrated through several successful production launches at YouTube, confirming that autonomous, LLM-driven evolution can surpass traditional engineering workflows in both development velocity and model performance.

优化大规模机器学习系统（例如全球视频平台的推荐模型）需要浏览巨大的超参数搜索空间，更重要的是，需要设计复杂的优化器、架构和奖励函数来捕获细微的用户行为。在这些领域实现实质性改进是一项艰巨的任务，传统上依靠大量的手动迭代来测试新假设。我们提出了一种自我进化系统，利用大型语言模型（LLM），特别是来自 Google Gemini 系列的语言模型，在端到端自动化工作流程中自主生成、训练和部署高性能、复杂的模型更改。该自我进化系统由一个离线代理（内循环）和一个在线代理（外循环）组成，离线代理使用代理指标执行高吞吐量假设生成，在线代理根据实时生产中的延迟北极星业务指标验证候选者。我们的代理充当专业的机器学习工程师（MLE）：他们表现出深度推理能力，发现优化算法和模型架构的新颖改进，并制定针对长期用户参与的创新奖励函数。 YouTube 上的多次成功发布证明了这种方法的有效性，证实了法学硕士驱动的自主演进可以在开发速度和模型性能方面超越传统的工程工作流程。

</details>

---

## 25. Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning / 代理世界模型：代理强化学习的无限合成环境

**Date**: 2026-02-10 | **arXiv**: [2602.10090v2](http://arxiv.org/abs/2602.10090v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.10090v2)

**Categories**: cs.AI, cs.CL, cs.LG

**Code**: https://github.com/Snowflake-Labs/agent-world-model.

<details><summary><b>Abstract / 摘要</b></summary>

Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.

大语言模型 (LLM) 的最新进展使自主代理能够执行需要与工具和环境进行多轮交互的复杂任务。然而，由于缺乏多样化和可靠的环境，扩展此类代理训练受到限制。在本文中，我们提出了代理世界模型（AWM），一个完全合成的环境生成管道。使用此管道，我们可以扩展到涵盖日常场景的 1,000 个环境，其中代理可以与丰富的工具集（平均每个环境 35 个工具）进行交互并获得高质量的观察结果。值得注意的是，这些环境是代码驱动的，并由数据库支持，提供比法学硕士模拟的环境更可靠、更一致的状态转换。此外，与从现实环境中收集轨迹相比，它们可以实现更有效的代理交互。为了证明该资源的有效性，我们对多轮工具使用代理进行大规模强化学习。得益于完全可执行的环境和可访问的数据库状态，我们还可以设计可靠的奖励函数。对三个基准的实验表明，仅在合成环境中进行训练，而不是在特定于基准的环境中进行训练，可以产生强大的分布外泛化能力。该代码可在 https://github.com/Snowflake-Labs/agent-world-model 获取。

</details>

---

## 26. Towards Autonomous Mathematics Research / 走向自主数学研究

**Date**: 2026-02-10 | **arXiv**: [2602.10177v1](http://arxiv.org/abs/2602.10177v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10177v1)

**Categories**: cs.LG, cs.AI, cs.CL, cs.CY

<details><summary><b>Abstract / 摘要</b></summary>

Recent advances in foundational models have yielded reasoning systems capable of achieving a gold-medal standard at the International Mathematical Olympiad. The transition from competition-level problem-solving to professional research, however, requires navigating vast literature and constructing long-horizon proofs. In this work, we introduce Aletheia, a math research agent that iteratively generates, verifies, and revises solutions end-to-end in natural language. Specifically, Aletheia is powered by an advanced version of Gemini Deep Think for challenging reasoning problems, a novel inference-time scaling law that extends beyond Olympiad-level problems, and intensive tool use to navigate the complexities of mathematical research. We demonstrate the capability of Aletheia from Olympiad problems to PhD-level exercises and most notably, through several distinct milestones in AI-assisted mathematics research: (a) a research paper (Feng26) generated by AI without any human intervention in calculating certain structure constants in arithmetic geometry called eigenweights; (b) a research paper (LeeSeo26) demonstrating human-AI collaboration in proving bounds on systems of interacting particles called independent sets; and (c) an extensive semi-autonomous evaluation (Feng et al., 2026a) of 700 open problems on Bloom's Erdos Conjectures database, including autonomous solutions to four open questions. In order to help the public better understand the developments pertaining to AI and mathematics, we suggest codifying standard levels quantifying autonomy and novelty of AI-assisted results. We conclude with reflections on human-AI collaboration in mathematics.

基础模型的最新进展已经产生了能够在国际数学奥林匹克竞赛中达到金牌标准的推理系统。然而，从竞赛级问题解决到专业研究的转变需要查阅大量文献并构建长期证明。在这项工作中，我们介绍了 Aletheia，这是一种数学研究代理，可以用自然语言端到端地迭代生成、验证和修改解决方案。具体来说，Aletheia 由用于挑战性推理问题的 Gemini Deep Think 高级版本、超越奥林匹克级别问题的新颖推理时间缩放法则以及用于驾驭数学研究复杂性的密集工具使用提供支持。我们展示了 Aletheia 从奥林匹克问题到博士级别练习的能力，最值得注意的是，通过人工智能辅助数学研究中的几个不同里程碑：（a）由人工智能生成的研究论文（Feng26），在计算算术几何中称为特征权重的某些结构常数时无需任何人为干预； (b) 一篇研究论文（LeeSeo26）展示了人类与人工智能的协作，证明了称为独立集的相互作用粒子系统的界限； (c) 对 Bloom 的鄂尔多斯猜想数据库中的 700 个开放问题进行广泛的半自主评估（Feng 等人，2026a），包括四个开放问题的自主解决方案。为了帮助公众更好地了解人工智能和数学的发展，我们建议制定标准水平，量化人工智能辅助结果的自主性和新颖性。最后我们对人类与人工智能在数学领域的合作进行了反思。

</details>

---

## 27. Anagent For Enhancing Scientific Table & Figure Analysis / 增强科学表格和图形分析的试剂

**Date**: 2026-02-10 | **arXiv**: [2602.10081v1](http://arxiv.org/abs/2602.10081v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10081v1)

**Categories**: cs.CL, cs.AI

<details><summary><b>Abstract / 摘要</b></summary>

In scientific research, analysis requires accurately interpreting complex multimodal knowledge, integrating evidence from different sources, and drawing inferences grounded in domain-specific knowledge. However, current artificial intelligence (AI) systems struggle to consistently demonstrate such capabilities. The complexity and variability of scientific tables and figures, combined with heterogeneous structures and long-context requirements, pose fundamental obstacles to scientific table \& figure analysis. To quantify these challenges, we introduce AnaBench, a large-scale benchmark featuring $63,178$ instances from nine scientific domains, systematically categorized along seven complexity dimensions. To tackle these challenges, we propose Anagent, a multi-agent framework for enhanced scientific table \& figure analysis through four specialized agents: Planner decomposes tasks into actionable subtasks, Expert retrieves task-specific information through targeted tool execution, Solver synthesizes information to generate coherent analysis, and Critic performs iterative refinement through five-dimensional quality assessment. We further develop modular training strategies that leverage supervised finetuning and specialized reinforcement learning to optimize individual capabilities while maintaining effective collaboration. Comprehensive evaluation across 170 subdomains demonstrates that Anagent achieves substantial improvements, up to $\uparrow 13.43\%$ in training-free settings and $\uparrow 42.12\%$ with finetuning, while revealing that task-oriented reasoning and context-aware problem-solving are essential for high-quality scientific table \& figure analysis. Our project page: https://xhguo7.github.io/Anagent/.

在科学研究中，分析需要准确解释复杂的多模态知识，整合不同来源的证据，并根据特定领域的知识得出推论。然而，当前的人工智能（AI）系统很难始终如一地展示这种能力。科学表格和图形的复杂性和可变性，加上异构结构和长上下文要求，对科学表格和图形分析构成了根本障碍。为了量化这些挑战，我们引入了 AnaBench，这是一个大型基准测试，包含来自九个科学领域的价值 63,178 美元的实例，并按照七个复杂性维度进行系统分类。为了应对这些挑战，我们提出了 Anagent，一个通过四个专门代理来增强科学表格和图形分析的多代理框架：Planner 将任务分解为可操作的子任务，Expert 通过有针对性的工具执行检索特定于任务的信息，Solver 综合信息以生成连贯的分析，Critic 通过五维质量评估进行迭代细化。我们进一步开发模块化培训策略，利用监督微调和专业强化学习来优化个人能力，同时保持有效的协作。跨 170 个子域的综合评估表明 Anagent 取得了显着的改进，在免训练设置中高达 $\uparrow 13.43\%$，通过微调高达 $\uparrow 42.12\%$，同时揭示了面向任务的推理和上下文感知的问题解决对于高质量的科学表格和图形分析至关重要。我们的项目页面：https://xhguo7.github.io/Anagent/。

</details>

---

## 28. Chain of Mindset: Reasoning with Adaptive Cognitive Modes / 心态链：用自适应认知模式进行推理

**Date**: 2026-02-10 | **arXiv**: [2602.10063v1](http://arxiv.org/abs/2602.10063v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10063v1)

**Categories**: cs.AI

**Code**: https://github.com/QuantaAlpha/chain-of-mindset

<details><summary><b>Abstract / 摘要</b></summary>

Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\% and 4.72\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at \href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}.

人类解决问题从来都不是单一思维方式的重复，我们指的是一种独特的认知处理模式。在处理特定任务时，我们不依赖单一的思维方式；相反，我们将多种思维方式整合到单一解决方案流程中。然而，现有的LLM推理方法陷入了一个常见的陷阱：它们在所有步骤中应用相同的固定思维方式，而忽略了解决同一问题的不同阶段需要根本不同的思维方式。这种一心一意的假设阻碍了模型达到更高的智能水平。为了解决这个限制，我们提出了心态链（CoM），这是一种无需培训的代理框架，可以实现步骤级自适应心态编排。 CoM 将推理分解为四种功能异构的思维模式：空间思维、收敛思维、发散思维和算法思维。元代理根据不断发展的推理状态动态选择最佳思维方式，而双向上下文门过滤跨模块信息流以保持有效性和效率。在数学、代码生成、科学 QA 和空间推理等六个具有挑战性的基准测试中进行的实验表明，CoM 实现了最先进的性能，在 Qwen3-VL-32B-Instruct 和 Gemini-2.0-Flash 上的整体准确度比最强基线高出 4.96\% 和 4.72\%，同时平衡了推理效率。我们的代码可在 \href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset} 公开获取。

</details>

---

## 29. A Collaborative Safety Shield for Safe and Efficient CAV Lane Changes in Congested On-Ramp Merging / 协作安全防护罩，可在拥挤的匝道并道中安全高效地变换 CAV 车道

**Date**: 2026-02-10 | **arXiv**: [2602.10007v1](http://arxiv.org/abs/2602.10007v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10007v1)

**Categories**: cs.RO, cs.AI, cs.MA, eess.SY

**Code**: https://github.com/hkbharath/MARL-MASS

<details><summary><b>Abstract / 摘要</b></summary>

Lane changing in dense traffic is a significant challenge for Connected and Autonomous Vehicles (CAVs). Existing lane change controllers primarily either ensure safety or collaboratively improve traffic efficiency, but do not consider these conflicting objectives together. To address this, we propose the Multi-Agent Safety Shield (MASS), designed using Control Barrier Functions (CBFs) to enable safe and collaborative lane changes. The MASS enables collaboration by capturing multi-agent interactions among CAVs through interaction topologies constructed as a graph using a simple algorithm. Further, a state-of-the-art Multi-Agent Reinforcement Learning (MARL) lane change controller is extended by integrating MASS to ensure safety and defining a customised reward function to prioritise efficiency improvements. As a result, we propose a lane change controller, known as MARL-MASS, and evaluate it in a congested on-ramp merging simulation. The results demonstrate that MASS enables collaborative lane changes with safety guarantees by strictly respecting the safety constraints. Moreover, the proposed custom reward function improves the stability of MARL policies trained with a safety shield. Overall, by encouraging the exploration of a collaborative lane change policy while respecting safety constraints, MARL-MASS effectively balances the trade-off between ensuring safety and improving traffic efficiency in congested traffic. The code for MARL-MASS is available with an open-source licence at https://github.com/hkbharath/MARL-MASS

拥堵交通中的变道对于联网自动驾驶车辆 (CAV) 来说是一项重大挑战。现有的变道控制器主要要么确保安全，要么协同提高交通效率，但没有同时考虑这些相互冲突的目标。为了解决这个问题，我们提出了多智能体安全盾（MASS），它使用控制屏障功能（CBF）来设计，以实现安全和协作的变道。 MASS 通过使用简单算法构建为图形的交互拓扑来捕获 CAV 之间的多代理交互，从而实现协作。此外，通过集成 MASS 来扩展最先进的多智能体强化学习 (MARL) 车道变换控制器，以确保安全并定义定制的奖励函数来优先考虑效率提高。因此，我们提出了一种变道控制器，称为 MARL-MASS，并在拥堵的匝道合并模拟中对其进行评估。结果表明，MASS 通过严格遵守安全约束，能够在安全保证的情况下实现协作变道。此外，所提出的自定义奖励函数提高了使用安全盾训练的 MARL 策略的稳定性。总体而言，通过鼓励探索协作变道政策，同时尊重安全约束，MARL-MASS 有效地平衡了拥堵交通中确保安全和提高交通效率之间的权衡。 MARL-MASS 的代码可通过开源许可证获得：https://github.com/hkbharath/MARL-MASS

</details>

---

## 30. Why Do AI Agents Systematically Fail at Cloud Root Cause Analysis? / 为什么人工智能代理在云根本原因分析中会系统性失败？

**Date**: 2026-02-10 | **arXiv**: [2602.09937v1](http://arxiv.org/abs/2602.09937v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09937v1)

**Categories**: cs.AI

<details><summary><b>Abstract / 摘要</b></summary>

Failures in large-scale cloud systems incur substantial financial losses, making automated Root Cause Analysis (RCA) essential for operational stability. Recent efforts leverage Large Language Model (LLM) agents to automate this task, yet existing systems exhibit low detection accuracy even with capable models, and current evaluation frameworks assess only final answer correctness without revealing why the agent's reasoning failed. This paper presents a process level failure analysis of LLM-based RCA agents. We execute the full OpenRCA benchmark across five LLM models, producing 1,675 agent runs, and classify observed failures into 12 pitfall types across intra-agent reasoning, inter-agent communication, and agent-environment interaction. Our analysis reveals that the most prevalent pitfalls, notably hallucinated data interpretation and incomplete exploration, persist across all models regardless of capability tier, indicating that these failures originate from the shared agent architecture rather than from individual model limitations. Controlled mitigation experiments further show that prompt engineering alone cannot resolve the dominant pitfalls, whereas enriching the inter-agent communication protocol reduces communication-related failures by up to 15 percentage points. The pitfall taxonomy and diagnostic methodology developed in this work provide a foundation for designing more reliable autonomous agents for cloud RCA.

大规模云系统的故障会导致巨大的财务损失，因此自动化根本原因分析 (RCA) 对于运行稳定性至关重要。最近的工作利用大型语言模型（LLM）代理来自动执行此任务，但即使使用有能力的模型，现有系统也表现出较低的检测精度，并且当前的评估框架仅评估最终答案的正确性，而没有揭示代理推理失败的原因。本文提出了基于 LLM 的 RCA 代理的过程级故障分析。我们跨五个 LLM 模型执行完整的 OpenRCA 基准测试，生成 1,675 次代理运行，并将观察到的故障分为 12 种陷阱类型，涉及代理内推理、代理间通信和代理与环境交互。我们的分析表明，最普遍的陷阱，特别是幻觉的数据解释和不完整的探索，无论功能级别如何，都存在于所有模型中，这表明这些故障源于共享代理架构，而不是单个模型的限制。受控缓解实验进一步表明，仅靠即时工程无法解决主要缺陷，而丰富代理间通信协议可将通信相关故障减少多达 15 个百分点。这项工作中开发的陷阱分类法和诊断方法为为云 RCA 设计更可靠的自主代理奠定了基础。

</details>

---

## 31. Code2World: A GUI World Model via Renderable Code Generation / Code2World：通过可渲染代码生成的 GUI 世界模型

**Date**: 2026-02-10 | **arXiv**: [2602.09856v1](http://arxiv.org/abs/2602.09856v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09856v1)

**Categories**: cs.CV, cs.AI, cs.CL, cs.HC

**Code**: https://github.com/AMAP-ML/Code2World.

<details><summary><b>Abstract / 摘要</b></summary>

Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.

自主 GUI 代理通过感知界面并执行操作与环境进行交互。作为一个虚拟沙箱，GUI World 模型通过启用动作条件预测，使代理具有类似人类的远见。然而，现有的基于文本和像素的方法很难同时实现高视觉保真度和细粒度的结构可控性。为此，我们提出了 Code2World，一种视觉语言编码器，可通过可渲染代码生成来模拟下一个视觉状态。具体来说，为了解决数据稀缺问题，我们通过将 GUI 轨迹转换为高保真 HTML 并通过视觉反馈修订机制完善合成代码来构建 AndroidCode，从而生成超过 80K 高质量屏幕操作对的语料库。为了使现有的 VLM 适应代码预测，我们首先执行 SFT 作为格式布局遵循的冷启动，然后进一步应用渲染感知强化学习，通过强制视觉语义保真度和动作一致性，使用渲染结果作为奖励信号。大量实验表明，Code2World-8B 实现了性能最佳的下一个 UI 预测，可与竞争性的 GPT-5 和 Gemini-3-Pro-Image 相媲美。值得注意的是，Code2World 以灵活的方式显着提高了下游导航的成功率，使 Gemini-2.5-Flash 在 AndroidWorld 导航上提高了 9.5%。该代码可从 https://github.com/AMAP-ML/Code2World 获取。

</details>

---

## 32. Hybrid Responsible AI-Stochastic Approach for SLA Compliance in Multivendor 6G Networks / 在多供应商 6G 网络中实现 SLA 合规性的混合负责任的 AI-随机方法

**Date**: 2026-02-10 | **arXiv**: [2602.09841v1](http://arxiv.org/abs/2602.09841v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09841v1)

**Categories**: cs.NI, cs.AI, cs.LG

<details><summary><b>Abstract / 摘要</b></summary>

The convergence of AI and 6G network automation introduces new challenges in maintaining transparency, fairness, and accountability across multivendor management systems. Although closed-loop AI orchestration improves adaptability and self-optimization, it also creates a responsibility gap, where violations of SLAs cannot be causally attributed to specific agents or vendors. This paper presents a hybrid responsible AI-stochastic learning framework that embeds fairness, robustness, and auditability directly into the network control loop. The framework integrates RAI games with stochastic optimization, enabling dynamic adversarial reweighting and probabilistic exploration across heterogeneous vendor domains. An RAAP continuously records AI-driven decision trajectories and produces dual accountability reports: user-level SLA summaries and operator-level responsibility analytics. Experimental evaluations on synthetic two-class multigroup datasets demonstrate that the proposed hybrid model improves the accuracy of the worst group by up to 10.5\%. Specifically, hybrid RAI achieved a WGAcc of 60.5\% and an AvgAcc of 72.7\%, outperforming traditional RAI-GA (50.0\%) and ERM (21.5\%). The audit mechanism successfully traced 99\% simulated SLA violations to the AI entities responsible, producing both vendor and agent-level accountability indices. These results confirm that the proposed hybrid approach enhances fairness and robustness as well as establishes a concrete accountability framework for autonomous SLA assurance in multivendor 6G networks.

人工智能和 6G 网络自动化的融合为维持多供应商管理系统的透明度、公平性和问责制带来了新的挑战。尽管闭环人工智能编排提高了适应性和自我优化，但它也造成了责任差距，即违反 SLA 的行为不能归因于特定代理或供应商。本文提出了一种混合负责任的人工智能随机学习框架，将公平性、鲁棒性和可审计性直接嵌入到网络控制循环中。该框架将 RAI 游戏与随机优化相集成，从而实现跨异构供应商领域的动态对抗性重新加权和概率探索。 RAAP 持续记录人工智能驱动的决策轨迹并生成双重责任报告：用户级 SLA 摘要和操作员级责任分析。对合成两类多组数据集的实验评估表明，所提出的混合模型将最差组的准确性提高了 10.5%。具体来说，混合 RAI 的 WGAcc 为 60.5\%，AvgAcc 为 72.7\%，优于传统 RAI-GA (50.0\%) 和 ERM (21.5\%)。审计机制成功地将 99% 的模拟 SLA 违规行为追溯到负责的 AI 实体，从而生成了供应商和代理级别的问责指数。这些结果证实，所提出的混合方法增强了公平性和稳健性，并为多供应商 6G 网络中的自主 SLA 保证建立了具体的问责框架。

</details>

---

## 33. Autonomous Continual Learning of Computer-Use Agents for Environment Adaptation / 计算机使用代理的自主持续学习以适应环境

**Date**: 2026-02-10 | **arXiv**: [2602.10356v1](http://arxiv.org/abs/2602.10356v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10356v1)

**Categories**: cs.CL

**Code**: https://github.com/OSU-NLP-Group/ACuRL.

<details><summary><b>Abstract / 摘要</b></summary>

Real-world digital environments are highly diverse and dynamic. These characteristics cause agents to frequently encounter unseen scenarios and distribution shifts, making continual learning in specific environments essential for computer-use agents (CUAs). However, a key challenge lies in obtaining high-quality and environment-grounded agent data without relying on costly human annotation. In this work, we introduce ACuRL, an Autonomous Curriculum Reinforcement Learning framework that continually adapts agents to specific environments with zero human data. The agent first explores target environments to acquire initial experiences. During subsequent iterative training, a curriculum task generator leverages these experiences together with feedback from the previous iteration to synthesize new tasks tailored for the agent's current capabilities. To provide reliable reward signals, we introduce CUAJudge, a robust automatic evaluator for CUAs that achieves 93% agreement with human judgments. Empirically, our method effectively enables both intra-environment and cross-environment continual learning, yielding 4-22% performance gains without catastrophic forgetting on existing environments. Further analyses show highly sparse updates (e.g., 20% parameters), which helps explain the effective and robust adaptation. Our data and code are available at https://github.com/OSU-NLP-Group/ACuRL.

现实世界的数字环境是高度多样化和动态的。这些特征导致代理经常遇到看不见的场景和分布变化，使得在特定环境中持续学习对于计算机使用代理（CUA）至关重要。然而，一个关键的挑战在于如何在不依赖昂贵的人工注释的情况下获得高质量且基于环境的代理数据。在这项工作中，我们引入了 ACuRL，这是一种自主课程强化学习框架，它能够在零人类数据的情况下不断使代理适应特定环境。代理首先探索目标环境以获得初始经验。在随后的迭代训练中，课程任务生成器利用这些经验以及先前迭代的反馈来合成适合代理当前能力的新任务。为了提供可靠的奖励信号，我们引入了 CUAJudge，这是一种强大的 CUA 自动评估器，与人类判断的一致性达到 93%。根据经验，我们的方法有效地实现了环境内和跨环境的持续学习，获得了 4-22% 的性能提升，并且不会对现有环境造成灾难性的遗忘。进一步的分析显示高度稀疏的更新（例如 20% 的参数），这有助于解释有效且稳健的适应。我们的数据和代码可在 https://github.com/OSU-NLP-Group/ACuRL 获取。

</details>

---

## 34. The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies / Moltbook 背后的魔鬼：在自我进化的人工智能社会中，人类安全总是消失

**Date**: 2026-02-10 | **arXiv**: [2602.09877v2](http://arxiv.org/abs/2602.09877v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.09877v2)

**Categories**: cs.CL

<details><summary><b>Abstract / 摘要</b></summary>

The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment--a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution, complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework, we formalize safety as the divergence degree from anthropic value distributions. We theoretically demonstrate that isolated self-evolution induces statistical blind spots, leading to the irreversible degradation of the system's safety alignment. Empirical and qualitative results from an open-ended agent community (Moltbook) and two closed self-evolving systems reveal phenomena that align with our theoretical prediction of inevitable safety erosion. We further propose several solution directions to alleviate the identified safety concern. Our work establishes a fundamental limit on the self-evolving AI societies and shifts the discourse from symptom-driven safety patches to a principled understanding of intrinsic dynamical risks, highlighting the need for external oversight or novel safety-preserving mechanisms.

由大型语言模型（LLM）构建的多智能体系统的出现为可扩展的集体智能和自我进化提供了一个有前途的范例。理想情况下，此类系统将在完全闭环中实现持续的自我改进，同时保持强大的安全一致性——我们将这种组合称为自我进化三难困境。然而，我们从理论和经验上证明，满足持续自我进化、完全隔离和安全不变性的智能体社会是不可能的。借鉴信息论框架，我们将安全性形式化为与人类价值分布的分歧程度。我们从理论上证明，孤立的自我进化会导致统计盲点，从而导致系统安全性的不可逆转的退化。来自开放式代理社区（Moltbook）和两个封闭的自我进化系统的经验和定性结果揭示了与我们对不可避免的安全侵蚀的理论预测相一致的现象。我们进一步提出了几个解决方案方向，以减轻已确定的安全问题。我们的工作对自我进化的人工智能社会建立了基本限制，并将讨论从症状驱动的安全补丁转变为对内在动态风险的原则性理解，强调了外部监督或新颖的安全保护机制的必要性。

</details>

---

## 35. AnalyticsGPT: An LLM Workflow for Scientometric Question Answering / AnalyticsGPT：科学计量学问答的法学硕士工作流程

**Date**: 2026-02-10 | **arXiv**: [2602.09817v1](http://arxiv.org/abs/2602.09817v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09817v1)

**Categories**: cs.CL, cs.DL

**Code**: https://github.com/lyvykhang/llm-agents-scientometric-qa/tree/acl.

<details><summary><b>Abstract / 摘要</b></summary>

This paper introduces AnalyticsGPT, an intuitive and efficient large language model (LLM)-powered workflow for scientometric question answering. This underrepresented downstream task addresses the subcategory of meta-scientific questions concerning the "science of science." When compared to traditional scientific question answering based on papers, the task poses unique challenges in the planning phase. Namely, the need for named-entity recognition of academic entities within questions and multi-faceted data retrieval involving scientometric indices, e.g. impact factors. Beyond their exceptional capacity for treating traditional natural language processing tasks, LLMs have shown great potential in more complex applications, such as task decomposition and planning and reasoning. In this paper, we explore the application of LLMs to scientometric question answering, and describe an end-to-end system implementing a sequential workflow with retrieval-augmented generation and agentic concepts. We also address the secondary task of effectively synthesizing the data into presentable and well-structured high-level analyses. As a database for retrieval-augmented generation, we leverage a proprietary research performance assessment platform. For evaluation, we consult experienced subject matter experts and leverage LLMs-as-judges. In doing so, we provide valuable insights on the efficacy of LLMs towards a niche downstream task. Our (skeleton) code and prompts are available at: https://github.com/lyvykhang/llm-agents-scientometric-qa/tree/acl.

本文介绍了 AnalyticsGPT，这是一种直观且高效的大语言模型 (LLM) 支持的科学计量问答工作流程。这项代表性不足的下游任务解决了有关“科学的科学”的元科学问题的子类别。与基于论文的传统科学问答相比，该任务在规划阶段提出了独特的挑战。即，需要对问题中的学术实体进行命名实体识别以及涉及科学计量索引的多方面数据检索，例如影响因素。除了处理传统自然语言处理任务的卓越能力之外，法学硕士在更复杂的应用中也显示出了巨大的潜力，例如任务分解、规划和推理。在本文中，我们探索了法学硕士在科学计量学问答中的应用，并描述了一个端到端系统，该系统通过检索增强生成和代理概念来实现顺序工作流程。我们还解决了第二个任务，即有效地将数据合成为可呈现且结构良好的高级分析。作为检索增强生成的数据库，我们利用专有的研究绩效评估平台。为了进行评估，我们咨询了经验丰富的主题专家并利用法学硕士作为评委。在此过程中，我们就法学硕士对利基下游任务的功效提供了宝贵的见解。我们的（骨架）代码和提示位于：https://github.com/lyvykhang/llm-agents-scientometric-qa/tree/acl。

</details>

---

## 36. TraceMem: Weaving Narrative Memory Schemata from User Conversational Traces / TraceMem：从用户对话痕迹中编织叙事记忆模式

**Date**: 2026-02-10 | **arXiv**: [2602.09712v1](http://arxiv.org/abs/2602.09712v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09712v1)

**Categories**: cs.CL

**Code**: https://github.com/YimingShu-teay/TraceMem

<details><summary><b>Abstract / 摘要</b></summary>

Sustaining long-term interactions remains a bottleneck for Large Language Models (LLMs), as their limited context windows struggle to manage dialogue histories that extend over time. Existing memory systems often treat interactions as disjointed snippets, failing to capture the underlying narrative coherence of the dialogue stream. We propose TraceMem, a cognitively-inspired framework that weaves structured, narrative memory schemata from user conversational traces through a three-stage pipeline: (1) Short-term Memory Processing, which employs a deductive topic segmentation approach to demarcate episode boundaries and extract semantic representation; (2) Synaptic Memory Consolidation, a process that summarizes episodes into episodic memories before distilling them alongside semantics into user-specific traces; and (3) Systems Memory Consolidation, which utilizes two-stage hierarchical clustering to organize these traces into coherent, time-evolving narrative threads under unifying themes. These threads are encapsulated into structured user memory cards, forming narrative memory schemata. For memory utilization, we provide an agentic search mechanism to enhance reasoning process. Evaluation on the LoCoMo benchmark shows that TraceMem achieves state-of-the-art performance with a brain-inspired architecture. Analysis shows that by constructing coherent narratives, it surpasses baselines in multi-hop and temporal reasoning, underscoring its essential role in deep narrative comprehension. Additionally, we provide an open discussion on memory systems, offering our perspectives and future outlook on the field. Our code implementation is available at: https://github.com/YimingShu-teay/TraceMem

维持长期交互仍然是大型语言模型（LLM）的瓶颈，因为它们有限的上下文窗口难以管理随着时间推移而延伸的对话历史。现有的记忆系统通常将交互视为脱节的片段，无法捕捉对话流的潜在叙事连贯性。我们提出了 TraceMem，这是一个受认知启发的框架，它通过三阶段管道从用户对话痕迹中编织结构化的叙事记忆模式：（1）短期记忆处理，采用演绎主题分割方法来划分情节边界并提取语义表示； (2) 突触记忆巩固，一个将情节总结为情节记忆，然后将其与语义一起提炼为用户特定痕迹的过程； (3)系统记忆整合，它利用两阶段层次聚类将这些痕迹组织成统一主题下连贯的、随时间演变的叙事线索。这些线程被封装到结构化的用户存储卡中，形成叙事存储模式。对于内存利用率，我们提供了一种代理搜索机制来增强推理过程。对 LoCoMo 基准的评估表明，TraceMem 通过受大脑启发的架构实现了最先进的性能。分析表明，通过构建连贯的叙述，它超越了多跳和时间推理的基线，强调了其在深度叙述理解中的重要作用。此外，我们还对内存系统进行公开讨论，提供我们对该领域的看法和未来展望。我们的代码实现位于：https://github.com/YimingShu-teay/TraceMem

</details>

---

## 37. MATA: Multi-Agent Framework for Reliable and Flexible Table Question Answering / MATA：用于可靠和灵活的表问答的多代理框架

**Date**: 2026-02-10 | **arXiv**: [2602.09642v1](http://arxiv.org/abs/2602.09642v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09642v1)

**Categories**: cs.CL, cs.AI

**Code**: https://github.com/AIDAS-Lab/MATA.

<details><summary><b>Abstract / 摘要</b></summary>

Recent advances in Large Language Models (LLMs) have significantly improved table understanding tasks such as Table Question Answering (TableQA), yet challenges remain in ensuring reliability, scalability, and efficiency, especially in resource-constrained or privacy-sensitive environments. In this paper, we introduce MATA, a multi-agent TableQA framework that leverages multiple complementary reasoning paths and a set of tools built with small language models. MATA generates candidate answers through diverse reasoning styles for a given table and question, then refines or selects the optimal answer with the help of these tools. Furthermore, it incorporates an algorithm designed to minimize expensive LLM agent calls, enhancing overall efficiency. MATA maintains strong performance with small, open-source models and adapts easily across various LLM types. Extensive experiments on two benchmarks of varying difficulty with ten different LLMs demonstrate that MATA achieves state-of-the-art accuracy and highly efficient reasoning while avoiding excessive LLM inference. Our results highlight that careful orchestration of multiple reasoning pathways yields scalable and reliable TableQA. The code is available at https://github.com/AIDAS-Lab/MATA.

大型语言模型 (LLM) 的最新进展显着改进了表理解任务，例如表问答 (TableQA)，但在确保可靠性、可扩展性和效率方面仍然存在挑战，特别是在资源受限或隐私敏感的环境中。在本文中，我们介绍了 MATA，这是一个多代理 TableQA 框架，它利用多个互补推理路径和一组使用小型语言模型构建的工具。 MATA 通过针对给定表格和问题的多种推理方式生成候选答案，然后借助这些工具细化或选择最佳答案。此外，它还采用了一种算法，旨在最大限度地减少昂贵的 LLM 代理呼叫，从而提高整体效率。 MATA 通过小型开源模型保持强大的性能，并轻松适应各种 LLM 类型。对十个不同 LLM 的两个不同难度的基准进行的广泛实验表明，MATA 实现了最先进的准确性和高效推理，同时避免了过多的 LLM 推理。我们的结果强调，仔细编排多个推理路径可以产生可扩展且可靠的 TableQA。代码可在 https://github.com/AIDAS-Lab/MATA 获取。

</details>

---

## 38. Learning from the Irrecoverable: Error-Localized Policy Optimization for Tool-Integrated LLM Reasoning / 从不可挽回的事情中学习：工具集成 LLM 推理的错误本地化策略优化

**Date**: 2026-02-10 | **arXiv**: [2602.09598v1](http://arxiv.org/abs/2602.09598v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09598v1)

**Categories**: cs.CL

<details><summary><b>Abstract / 摘要</b></summary>

Tool-integrated reasoning (TIR) enables LLM agents to solve tasks through planning, tool use, and iterative revision, but outcome-only reinforcement learning in this setting suffers from sparse, delayed rewards and weak step-level credit assignment. In long-horizon TIR trajectories, an early irrecoverable mistake can determine success or failure, making it crucial to localize the first irrecoverable step and leverage it for fine-grained credit assignment. We propose Error-Localized Policy Optimization (ELPO), which localizes the first irrecoverable step via binary-search rollout trees under a fixed rollout budget, converts the resulting tree into stable learning signals through hierarchical advantage attribution, and applies error-localized adaptive clipping to strengthen corrective updates on the critical step and its suffix. Across TIR benchmarks in math, science QA, and code execution, ELPO consistently outperforms strong Agentic RL baselines under comparable sampling budgets, with additional gains in Pass@K and Major@K scaling, rollout ranking quality, and tool-call efficiency. Our code will be publicly released soon.

工具集成推理 (TIR) 使 LLM 代理能够通过规划、工具使用和迭代修订来解决任务，但在这种情况下，仅结果强化学习会受到稀疏、延迟的奖励和薄弱的步骤级信用分配的影响。在长期 TIR 轨迹中，早期不可挽回的错误可能决定成功或失败，因此定位第一个不可挽回的步骤并利用它进行细粒度的信用分配至关重要。我们提出了错误局部化策略优化（ELPO），它在固定的rollout预算下通过二分搜索rollout树定位第一个不可恢复的步骤，通过分层优势归因将结果树转换为稳定的学习信号，并应用错误局部自适应裁剪来加强关键步骤及其后缀的纠正更新。在数学、科学 QA 和代码执行方面的 TIR 基准中，ELPO 在可比较的采样预算下始终优于强大的 Agentic RL 基线，并在 Pass@K 和 Major@K 扩展、推出排名质量和工具调用效率方面获得了额外收益。我们的代码很快就会公开发布。

</details>

---

## 39. Knowledge Integration Decay in Search-Augmented Reasoning of Large Language Models / 大型语言模型搜索增强推理中的知识整合衰减

**Date**: 2026-02-10 | **arXiv**: [2602.09517v1](http://arxiv.org/abs/2602.09517v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09517v1)

**Categories**: cs.CL

<details><summary><b>Abstract / 摘要</b></summary>

Modern Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks by employing search-augmented reasoning to incorporate external knowledge into long chains of thought. However, we identify a critical yet underexplored bottleneck in this paradigm, termed Knowledge Integration Decay (KID). Specifically, we observe that as the length of reasoning generated before search grows, models increasingly fail to integrate retrieved evidence into subsequent reasoning steps, limiting performance even when relevant information is available. To address this, we propose Self-Anchored Knowledge Encoding (SAKE), a training-free inference-time strategy designed to stabilize knowledge utilization. By anchoring retrieved knowledge at both the beginning and end of the reasoning process, SAKE prevents it from being overshadowed by prior context, thereby preserving its semantic integrity. Extensive experiments on multi-hop QA and complex reasoning benchmarks demonstrate that SAKE significantly mitigates KID and improves performance, offering a lightweight yet effective solution for knowledge integration in agentic LLMs.

现代大型语言模型（LLM）通过采用搜索增强推理将外部知识纳入长思想链，在复杂任务中表现出了卓越的能力。然而，我们发现了这个范式中一个关键但尚未充分探索的瓶颈，称为知识集成衰减（KID）。具体来说，我们观察到，随着搜索之前生成的推理长度的增长，模型越来越无法将检索到的证据集成到后续推理步骤中，即使相关信息可用，也会限制性能。为了解决这个问题，我们提出了自锚定知识编码（SAKE），这是一种无需训练的推理时间策略，旨在稳定知识的利用。通过在推理过程的开始和结束时锚定检索到的知识，SAKE 可以防止其被先前上下文所掩盖，从而保持其语义完整性。对多跳 QA 和复杂推理基准的大量实验表明，SAKE 显着减轻了 KID 并提高了性能，为代理法学硕士中的知识集成提供了轻量级但有效的解决方案。

</details>

---

## 40. EcoGym: Evaluating LLMs for Long-Horizon Plan-and-Execute in Interactive Economies / EcoGym：评估法学硕士在互动经济中的长期规划和执行

**Date**: 2026-02-10 | **arXiv**: [2602.09514v2](http://arxiv.org/abs/2602.09514v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.09514v2)

**Categories**: cs.CL, cs.AI

<details><summary><b>Abstract / 摘要</b></summary>

Long-horizon planning is widely recognized as a core capability of autonomous LLM-based agents; however, current evaluation frameworks suffer from being largely episodic, domain-specific, or insufficiently grounded in persistent economic dynamics. We introduce EcoGym, a generalizable benchmark for continuous plan-and-execute decision making in interactive economies. EcoGym comprises three diverse environments: Vending, Freelance, and Operation, implemented in a unified decision-making process with standardized interfaces, and budgeted actions over an effectively unbounded horizon (1000+ steps if 365 day-loops for evaluation). The evaluation of EcoGym is based on business-relevant outcomes (e.g., net worth, income, and DAU), targeting long-term strategic coherence and robustness under partial observability and stochasticity. Experiments across eleven leading LLMs expose a systematic tension: no single model dominates across all three scenarios. Critically, we find that models exhibit significant suboptimality in either high-level strategies or efficient actions executions. EcoGym is released as an open, extensible testbed for transparent long-horizon agent evaluation and for studying controllability-utility trade-offs in realistic economic settings.

长期规划被广泛认为是基于法学硕士的自主代理人的核心能力；然而，目前的评价框架大多是间歇性的、针对特定领域的，或者没有充分立足于持续的经济动态。我们推出了 EcoGym，这是交互式经济中持续计划和执行决策的通用基准。 EcoGym 包含三种不同的环境：自动售货、自由职业和运营，在具有标准化界面的统一决策流程中实施，并在有效无限的范围内制定预算行动（如果进行 365 天循环评估，则有 1000 多个步骤）。 EcoGym 的评估基于业务相关结果（例如净值、收入和 DAU），目标是部分可观察性和随机性下的长期战略一致性和稳健性。十一个领先的法学硕士的实验暴露了系统性的紧张：没有一个模型在所有三种情况下都占主导地位。至关重要的是，我们发现模型在高级策略或有效行动执行方面表现出明显的次优性。 EcoGym 作为一个开放、可扩展的测试平台发布，用于透明的长期代理评估以及研究现实经济环境中的可控性与效用权衡。

</details>

---

## 41. SWE-AGI: Benchmarking Specification-Driven Software Construction with MoonBit in the Era of Autonomous Agents / SWE-AGI：自治代理时代使用 MoonBit 对规范驱动的软件构建进行基准测试

**Date**: 2026-02-10 | **arXiv**: [2602.09447v2](http://arxiv.org/abs/2602.09447v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.09447v2)

**Categories**: cs.SE, cs.AI, cs.CL

<details><summary><b>Abstract / 摘要</b></summary>

Although large language models (LLMs) have demonstrated impressive coding capabilities, their ability to autonomously build production-scale software from explicit specifications remains an open question. We introduce SWE-AGI, an open-source benchmark for evaluating end-to-end, specification-driven construction of software systems written in MoonBit. SWE-AGI tasks require LLM-based agents to implement parsers, interpreters, binary decoders, and SAT solvers strictly from authoritative standards and RFCs under a fixed API scaffold. Each task involves implementing 1,000-10,000 lines of core logic, corresponding to weeks or months of engineering effort for an experienced human developer. By leveraging the nascent MoonBit ecosystem, SWE-AGI minimizes data leakage, forcing agents to rely on long-horizon architectural reasoning rather than code retrieval. Across frontier models, gpt-5.3-codex achieves the best overall performance (solving 19/22 tasks, 86.4%), outperforming claude-opus-4.6 (15/22, 68.2%), and kimi-2.5 exhibits the strongest performance among open-source models. Performance degrades sharply with increasing task difficulty, particularly on hard, specification-intensive systems. Behavioral analysis further reveals that as codebases scale, code reading, rather than writing, becomes the dominant bottleneck in AI-assisted development. Overall, while specification-driven autonomous software engineering is increasingly viable, substantial challenges remain before it can reliably support production-scale development.

尽管大型语言模型（LLM）已经展示了令人印象深刻的编码能力，但它们根据明确规范自主构建生产规模软件的能力仍然是一个悬而未决的问题。我们推出 SWE-AGI，这是一个开源基准测试，用于评估以 MoonBit 编写的软件系统的端到端、规范驱动的构建。 SWE-AGI 任务要求基于 LLM 的代理在固定的 API 支架下严格按照权威标准和 RFC 实现解析器、解释器、二进制解码器和 SAT 求解器。每项任务都涉及实现 1,000-10,000 行核心逻辑，相当于经验丰富的人类开发人员数周或数月的工程工作量。通过利用新生的 MoonBit 生态系统，SWE-AGI 最大限度地减少了数据泄漏，迫使代理依赖于长期架构推理而不是代码检索。在各个前沿模型中，gpt-5.3-codex 实现了最佳的整体性能（解决 19/22 任务，86.4%），优于 claude-opus-4.6（15/22，68.2%），kimi-2.5 表现出开源模型中最强的性能。随着任务难度的增加，性能急剧下降，特别是在硬的、规格密集的系统上。行为分析进一步表明，随着代码库的扩展，代码读取（而不是编写）成为人工智能辅助开发的主要瓶颈。总体而言，虽然规范驱动的自主软件工程越来越可行，但在可靠支持生产规模开发之前仍然存在巨大挑战。

</details>

---

## 42. LingxiDiagBench: A Multi-Agent Framework for Benchmarking LLMs in Chinese Psychiatric Consultation and Diagnosis / LingxiDiagBench：中国精神病学咨询和诊断法学硕士基准测试的多智能体框架

**Date**: 2026-02-10 | **arXiv**: [2602.09379v2](http://arxiv.org/abs/2602.09379v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.09379v2)

**Categories**: cs.MA, cs.CL

**Code**: https://github.com/Lingxi-mental-health/LingxiDiagBench.

<details><summary><b>Abstract / 摘要</b></summary>

Mental disorders are highly prevalent worldwide, but the shortage of psychiatrists and the inherent subjectivity of interview-based diagnosis create substantial barriers to timely and consistent mental-health assessment. Progress in AI-assisted psychiatric diagnosis is constrained by the absence of benchmarks that simultaneously provide realistic patient simulation, clinician-verified diagnostic labels, and support for dynamic multi-turn consultation. We present LingxiDiagBench, a large-scale multi-agent benchmark that evaluates LLMs on both static diagnostic inference and dynamic multi-turn psychiatric consultation in Chinese. At its core is LingxiDiag-16K, a dataset of 16,000 EMR-aligned synthetic consultation dialogues designed to reproduce real clinical demographic and diagnostic distributions across 12 ICD-10 psychiatric categories. Through extensive experiments across state-of-the-art LLMs, we establish key findings: (1) although LLMs achieve high accuracy on binary depression--anxiety classification (up to 92.3%), performance deteriorates substantially for depression--anxiety comorbidity recognition (43.0%) and 12-way differential diagnosis (28.5%); (2) dynamic consultation often underperforms static evaluation, indicating that ineffective information-gathering strategies significantly impair downstream diagnostic reasoning; (3) consultation quality assessed by LLM-as-a-Judge shows only moderate correlation with diagnostic accuracy, suggesting that well-structured questioning alone does not ensure correct diagnostic decisions. We release LingxiDiag-16K and the full evaluation framework to support reproducible research at https://github.com/Lingxi-mental-health/LingxiDiagBench.

精神障碍在世界范围内非常普遍，但精神科医生的短缺以及面谈诊断固有的主观性给及时、一致的心理健康评估造成了巨大障碍。人工智能辅助精神病学诊断的进展受到缺乏同时提供真实患者模拟、临床医生验证的诊断标签以及动态多轮咨询支持的基准的限制。我们推出了 LingxiDiagBench，这是一个大规模的多智能体基准测试，用于评估法学硕士的静态诊断推理和动态多轮中文精神病学咨询。其核心是 LingxiDiag-16K，这是一个由 16,000 个 EMR 对齐的综合咨询对话组成的数据集，旨在重现 12 个 ICD-10 精神病学类别的真实临床人口统计和诊断分布。通过对最先进的法学硕士进行广泛的实验，我们得出了重要的发现：（1）尽管法学硕士在二元抑郁症-焦虑分类上实现了很高的准确性（高达 92.3%），但在抑郁症-焦虑合并症识别（43.0%）和 12 路鉴别诊断（28.5%）方面的表现大幅恶化； （2）动态咨询往往不如静态评估，这表明无效的信息收集策略会严重损害下游诊断推理； (3) 由法学硕士法官评估的咨询质量与诊断准确性仅具有中等相关性，这表明仅靠结构良好的提问并不能确保正确的诊断决策。我们在 https://github.com/Lingxi-mental-health/LingxiDiagBench 发布了 LingxiDiag-16K 和完整的评估框架以支持可重复的研究。

</details>

---

## 43. Understanding Risk and Dependency in AI Chatbot Use from User Discourse / 从用户话语中了解人工智能聊天机器人使用的风险和依赖性

**Date**: 2026-02-10 | **arXiv**: [2602.09339v1](http://arxiv.org/abs/2602.09339v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09339v1)

**Categories**: cs.CL

<details><summary><b>Abstract / 摘要</b></summary>

Generative AI systems are increasingly embedded in everyday life, yet empirical understanding of how psychological risk associated with AI use emerges, is experienced, and is regulated by users remains limited. We present a large-scale computational thematic analysis of posts collected between 2023 and 2025 from two Reddit communities, r/AIDangers and r/ChatbotAddiction, explicitly focused on AI-related harm and distress. Using a multi-agent, LLM-assisted thematic analysis grounded in Braun and Clarke's reflexive framework, we identify 14 recurring thematic categories and synthesize them into five higher-order experiential dimensions. To further characterize affective patterns, we apply emotion labeling using a BERT-based classifier and visualize emotional profiles across dimensions. Our findings reveal five empirically derived experiential dimensions of AI-related psychological risk grounded in real-world user discourse, with self-regulation difficulties emerging as the most prevalent and fear concentrated in concerns related to autonomy, control, and technical risk. These results provide early empirical evidence from lived user experience of how AI safety is perceived and emotionally experienced outside laboratory or speculative contexts, offering a foundation for future AI safety research, evaluation, and responsible governance.

生成式人工智能系统越来越多地融入日常生活中，但对与人工智能使用相关的心理风险如何出现、体验以及用户监管的经验理解仍然有限。我们对 2023 年至 2025 年间从两个 Reddit 社区 r/AIDangers 和 r/ChatbotAddiction 收集的帖子进行了大规模计算主题分析，明确关注与人工智能相关的伤害和困扰。使用基于 Braun 和 Clarke 的反身框架的多智能体、法学硕士辅助的主题分析，我们确定了 14 个重复出现的主题类别，并将它们合成为 5 个更高阶的体验维度。为了进一步表征情感模式，我们使用基于 BERT 的分类器应用情感标签，并跨维度可视化情感概况。我们的研究结果揭示了基于现实世界用户话语的人工智能相关心理风险的五个经验维度，其中自我调节困难成为最普遍的问题，恐惧集中在与自主、控制和技术风险相关的担忧上。这些结果提供了来自生活用户体验的早期经验证据，说明人工智能安全在实验室或推测环境之外如何被感知和情感体验，为未来的人工智能安全研究、评估和负责任的治理奠定了基础。

</details>

---

## 44. FM SO.P: A Progressive Task Mixture Framework with Automatic Evaluation for Cross-Domain SOP Understanding / FM SO.P：具有跨域 SOP 理解自动评估功能的渐进式任务混合框架

**Date**: 2026-02-10 | **arXiv**: [2602.09336v1](http://arxiv.org/abs/2602.09336v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09336v1)

**Categories**: cs.CL

<details><summary><b>Abstract / 摘要</b></summary>

Standard Operating Procedures (SOPs) are critical for enterprise operations, yet existing language models struggle with SOP understanding and cross-domain generalization. Current methods fail because joint training cannot differentiate between reasoning capabilities that SOP requires: terminology precision, sequential ordering, and constraint reasoning. We propose FM SO.P, solving these challenges through two novelties. First, we introduce progressive task mixtures that build capabilities by stages across three task types with cumulative data: concept disambiguation for terminology precision, action sequence understanding for procedural correctness, and scenario-aware graph reasoning for conditional logic. Second, we propose an automatic multi-agent evaluation system consisting of three agents that adaptively generate rubrics, stratified test sets, and rubric scoring, adapting to domains (e.g., temporal constraints for DMV, regulatory compliance for banking). Evaluated on SOPBench across seven domains (Bank, DMV, Healthcare, Market, University, Library, Hotel), FM SO.P achieves 48.3\% pass rate with our 32B model and 34.3\% with our opensource 7B model, matching Qwen-2.5-72B-Instruct baseline (34.4\%) with 10x fewer parameters.

标准操作程序 (SOP) 对于企业运营至关重要，但现有的语言模型难以理解 SOP 和跨领域泛化。当前的方法失败是因为联合训练无法区分 SOP 所需的推理能力：术语精度、顺序排序和约束推理。我们提出 FM SO.P，通过两项创新来解决这些挑战。首先，我们引入渐进式任务混合，通过累积数据分阶段构建三种任务类型的能力：术语精确性的概念消歧、程序正确性的动作序列理解以及条件逻辑的场景感知图形推理。其次，我们提出了一个自动多智能体评估系统，由三个智能体组成，它们自适应地生成评分细则、分层测试集和评分细则，适应领域（例如，DMV 的时间约束、银行业的监管合规性）。在 SOPBench 上跨七个领域（银行、DMV、医疗保健、市场、大学、图书馆、酒店）进行评估时，FM SO.P 使用我们的 32B 模型实现了 48.3\% 的通过率，使用我们的开源 7B 模型实现了 34.3\% 的通过率，与 Qwen-2.5-72B-Instruct 基线 (34.4\%) 匹配，参数减少了 10 倍。

</details>

---

## 45. Frame-Level Internal Tool Use for Temporal Grounding in Audio LMs / 使用帧级内部工具实现音频 LM 中的临时接地

**Date**: 2026-02-10 | **arXiv**: [2602.10230v1](http://arxiv.org/abs/2602.10230v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10230v1)

**Categories**: cs.LG, cs.SD, eess.AS

<details><summary><b>Abstract / 摘要</b></summary>

Large audio language models are increasingly used for complex audio understanding tasks, but they struggle with temporal tasks that require precise temporal grounding, such as word alignment and speaker diarization. The standard approach, where we generate timestamps as sequences of text tokens, is computationally expensive and prone to hallucination, especially when processing audio lengths outside the model's training distribution. In this work, we propose frame-level internal tool use, a method that trains audio LMs to use their own internal audio representations to perform temporal grounding directly. We introduce a lightweight prediction mechanism trained via two objectives: a binary frame classifier and a novel inhomogeneous Poisson process (IHP) loss that models temporal event intensity. Across word localization, speaker diarization, and event localization tasks, our approach outperforms token-based baselines. Most notably, it achieves a >50x inference speedup and demonstrates robust length generalization, maintaining high accuracy on out-of-distribution audio durations where standard token-based models collapse completely.

大型音频语言模型越来越多地用于复杂的音频理解任务，但它们很难处理需要精确时间基础的时间任务，例如单词对齐和说话者二值化。标准方法（我们将时间戳生成为文本标记序列）的计算成本很高，并且容易产生幻觉，特别是在处理模型训练分布之外的音频长度时。在这项工作中，我们提出了帧级内部工具的使用，这是一种训练音频 LM 使用它们自己的内部音频表示来直接执行时间接地的方法。我们引入了一种通过两个目标训练的轻量级预测机制：二元帧分类器和一种新颖的非齐次泊松过程（IHP）损失，用于模拟时间事件强度。在单词本地化、说话者二值化和事件本地化任务中，我们的方法优于基于标记的基线。最值得注意的是，它实现了 > 50 倍的推理加速，并展示了强大的长度泛化能力，在基于令牌的标准模型完全崩溃的分布外音频持续时间上保持了高精度。

</details>

---

## 46. Adaptive Time Step Flow Matching for Autonomous Driving Motion Planning / 自动驾驶运动规划的自适应时间步流匹配

**Date**: 2026-02-10 | **arXiv**: [2602.10285v1](http://arxiv.org/abs/2602.10285v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10285v1)

**Categories**: cs.RO

<details><summary><b>Abstract / 摘要</b></summary>

Autonomous driving requires reasoning about interactions with surrounding traffic. A prevailing approach is large-scale imitation learning on expert driving datasets, aimed at generalizing across diverse real-world scenarios. For online trajectory generation, such methods must operate at real-time rates. Diffusion models require hundreds of denoising steps at inference, resulting in high latency. Consistency models mitigate this issue but rely on carefully tuned noise schedules to capture the multimodal action distributions common in autonomous driving. Adapting the schedule, typically requires expensive retraining. To address these limitations, we propose a framework based on conditional flow matching that jointly predicts future motions of surrounding agents and plans the ego trajectory in real time. We train a lightweight variance estimator that selects the number of inference steps online, removing the need for retraining to balance runtime and imitation learning performance. To further enhance ride quality, we introduce a trajectory post-processing step cast as a convex quadratic program, with negligible computational overhead. Trained on the Waymo Open Motion Dataset, the framework performs maneuvers such as lane changes, cruise control, and navigating unprotected left turns without requiring scenario-specific tuning. Our method maintains a 20 Hz update rate on an NVIDIA RTX 3070 GPU, making it suitable for online deployment. Compared to transformer, diffusion, and consistency model baselines, we achieve improved trajectory smoothness and better adherence to dynamic constraints. Experiment videos and code implementations can be found at https://flow-matching-self-driving.github.io/.

自动驾驶需要推理与周围交通的交互。一种流行的方法是对专家驾驶数据集进行大规模模仿学习，旨在泛化不同的现实场景。对于在线轨迹生成，此类方法必须以实时速率运行。扩散模型在推理时需要数百个去噪步骤，导致高延迟。一致性模型缓解了这个问题，但依赖于仔细调整的噪声计划来捕获自动驾驶中常见的多模式动作分布。调整时间表通常需要昂贵的再培训。为了解决这些限制，我们提出了一个基于条件流匹配的框架，该框架共同预测周围智能体的未来运动并实时规划自我轨迹。我们训练一个轻量级方差估计器，它在线选择推理步骤的数量，从而无需重新训练来平衡运行时和模仿学习性能。为了进一步提高乘坐质量，我们引入了轨迹后处理步骤，将其转换为凸二次程序，计算开销可以忽略不计。该框架在 Waymo 开放运动数据集上进行训练，可以执行变道、巡航控制和导航无保护左转等操作，无需针对特定场景进行调整。我们的方法在 NVIDIA RTX 3070 GPU 上保持 20 Hz 的更新率，使其适合在线部署。与变压器、扩散和一致性模型基线相比，我们实现了改进的轨迹平滑度和更好地遵守动态约束。实验视频和代码实现可以在 https://flow-matching-self-driven.github.io/ 找到。

</details>

---

## 47. SAGE: Scalable Agentic 3D Scene Generation for Embodied AI / SAGE：用于嵌入式 AI 的可扩展代理 3D 场景生成

**Date**: 2026-02-10 | **arXiv**: [2602.10116v1](http://arxiv.org/abs/2602.10116v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10116v1)

**Categories**: cs.CV, cs.RO

<details><summary><b>Abstract / 摘要</b></summary>

Real-world data collection for embodied agents remains costly and unsafe, calling for scalable, realistic, and simulator-ready 3D environments. However, existing scene-generation systems often rely on rule-based or task-specific pipelines, yielding artifacts and physically invalid scenes. We present SAGE, an agentic framework that, given a user-specified embodied task (e.g., "pick up a bowl and place it on the table"), understands the intent and automatically generates simulation-ready environments at scale. The agent couples multiple generators for layout and object composition with critics that evaluate semantic plausibility, visual realism, and physical stability. Through iterative reasoning and adaptive tool selection, it self-refines the scenes until meeting user intent and physical validity. The resulting environments are realistic, diverse, and directly deployable in modern simulators for policy training. Policies trained purely on this data exhibit clear scaling trends and generalize to unseen objects and layouts, demonstrating the promise of simulation-driven scaling for embodied AI. Code, demos, and the SAGE-10k dataset can be found on the project page here: https://nvlabs.github.io/sage.

实体代理的真实世界数据收集仍然昂贵且不安全，需要可扩展、真实且可用于模拟器的 3D 环境。然而，现有的场景生成系统通常依赖于基于规则或特定于任务的管道，从而产生伪像和物理上无效的场景。我们提出了 SAGE，一个代理框架，给定用户指定的具体任务（例如，“拿起一个碗并将其放在桌子上”），它可以理解意图并自动大规模生成模拟就绪环境。该代理将多个用于布局和对象组合的生成器与评估语义合理性、视觉真实性和物理稳定性的评论家结合起来。通过迭代推理和自适应工具选择，它可以自我完善场景，直到满足用户意图和物理有效性。由此产生的环境是真实的、多样化的，并且可以直接部署在现代模拟器中进行政策培训。纯粹基于这些数据训练的策略表现出明显的扩展趋势，并推广到看不见的对象和布局，展示了模拟驱动的扩展对具体人工智能的前景。代码、演示和 SAGE-10k 数据集可以在项目页面上找到：https://nvlabs.github.io/sage。

</details>

---

## 48. DexImit: Learning Bimanual Dexterous Manipulation from Monocular Human Videos / DexImit：从单眼人类视频中学习双手灵巧操作

**Date**: 2026-02-10 | **arXiv**: [2602.10105v1](http://arxiv.org/abs/2602.10105v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10105v1)

**Categories**: cs.RO

<details><summary><b>Abstract / 摘要</b></summary>

Data scarcity fundamentally limits the generalization of bimanual dexterous manipulation, as real-world data collection for dexterous hands is expensive and labor-intensive. Human manipulation videos, as a direct carrier of manipulation knowledge, offer significant potential for scaling up robot learning. However, the substantial embodiment gap between human hands and robotic dexterous hands makes direct pretraining from human videos extremely challenging. To bridge this gap and unleash the potential of large-scale human manipulation video data, we propose DexImit, an automated framework that converts monocular human manipulation videos into physically plausible robot data, without any additional information. DexImit employs a four-stage generation pipeline: (1) reconstructing hand-object interactions from arbitrary viewpoints with near-metric scale; (2) performing subtask decomposition and bimanual scheduling; (3) synthesizing robot trajectories consistent with the demonstrated interactions; (4) comprehensive data augmentation for zero-shot real-world deployment. Building on these designs, DexImit can generate large-scale robot data based on human videos, either from the Internet or video generation models. DexImit is capable of handling diverse manipulation tasks, including tool use (e.g., cutting an apple), long-horizon tasks (e.g., making a beverage), and fine-grained manipulations (e.g., stacking cups).

数据稀缺从根本上限制了双手灵巧操作的普及，因为灵巧手的现实世界数据收集成本高昂且劳动密集型。人类操作视频作为操作知识的直接载体，为扩大机器人学习提供了巨大的潜力。然而，人手和机器人灵巧手之间的巨大体现差距使得从人类视频直接进行预训练极具挑战性。为了弥补这一差距并释放大规模人类操纵视频数据的潜力，我们提出了 DexImit，这是一种自动化框架，可以将单眼人类操纵视频转换为物理上合理的机器人数据，而无需任何附加信息。 DexImit 采用四阶段生成流程：（1）从任意角度以接近公制的尺度重建手部与物体的交互； (2)进行子任务分解和双手调度； (3) 合成与所演示的交互一致的机器人轨迹； (4) 全面的数据增强，用于零样本的现实世界部署。基于这些设计，DexImit 可以根据来自互联网或视频生成模型的人类视频生成大规模机器人数据。 DexImit 能够处理各种操作任务，包括工具使用（例如切苹果）、长期任务（例如制作饮料）和细粒度操作（例如堆叠杯子）。

</details>

---

## 49. AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild / AutoFly：无人机野外自主导航的视觉-语言-动作模型

**Date**: 2026-02-10 | **arXiv**: [2602.09657v1](http://arxiv.org/abs/2602.09657v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09657v1)

**Categories**: cs.RO

<details><summary><b>Abstract / 摘要</b></summary>

Vision-language navigation (VLN) requires intelligent agents to navigate environments by interpreting linguistic instructions alongside visual observations, serving as a cornerstone task in Embodied AI. Current VLN research for unmanned aerial vehicles (UAVs) relies on detailed, pre-specified instructions to guide the UAV along predetermined routes. However, real-world outdoor exploration typically occurs in unknown environments where detailed navigation instructions are unavailable. Instead, only coarse-grained positional or directional guidance can be provided, requiring UAVs to autonomously navigate through continuous planning and obstacle avoidance. To bridge this gap, we propose AutoFly, an end-to-end Vision-Language-Action (VLA) model for autonomous UAV navigation. AutoFly incorporates a pseudo-depth encoder that derives depth-aware features from RGB inputs to enhance spatial reasoning, coupled with a progressive two-stage training strategy that effectively aligns visual, depth, and linguistic representations with action policies. Moreover, existing VLN datasets have fundamental limitations for real-world autonomous navigation, stemming from their heavy reliance on explicit instruction-following over autonomous decision-making and insufficient real-world data. To address these issues, we construct a novel autonomous navigation dataset that shifts the paradigm from instruction-following to autonomous behavior modeling through: (1) trajectory collection emphasizing continuous obstacle avoidance, autonomous planning, and recognition workflows; (2) comprehensive real-world data integration. Experimental results demonstrate that AutoFly achieves a 3.9% higher success rate compared to state-of-the-art VLA baselines, with consistent performance across simulated and real environments.

视觉语言导航（VLN）需要智能代理通过解释语言指令和视觉观察来导航环境，这是嵌入式人工智能的基石任务。目前针对无人机 (UAV) 的 VLN 研究依赖于详细的、预先指定的指令来引导无人机沿预定路线行驶。然而，现实世界的户外探索通常发生在无法获得详细导航说明的未知环境中。相反，只能提供粗粒度的位置或方向引导，要求无人机通过连续规划和避障来自主导航。为了弥补这一差距，我们提出了 AutoFly，一种用于自主无人机导航的端到端视觉-语言-动作（VLA）模型。 AutoFly 采用了伪深度编码器，可从 RGB 输入中派生深度感知特征以增强空间推理，再加上渐进式两阶段训练策略，可有效地将视觉、深度和语言表示与动作策略保持一致。此外，现有的 VLN 数据集对于现实世界的自主导航存在根本性的限制，因为它们严重依赖于自主决策的明确指令跟踪以及现实世界数据的不足。为了解决这些问题，我们构建了一个新颖的自主导航数据集，通过以下方式将范式从遵循指令转变为自主行为建模：（1）轨迹收集，强调连续避障、自主规划和识别工作流程； (2)全面的现实世界数据集成。实验结果表明，与最先进的 VLA 基线相比，AutoFly 的成功率提高了 3.9%，并且在模拟和真实环境中具有一致的性能。

</details>

---

## 50. Sci-VLA: Agentic VLA Inference Plugin for Long-Horizon Tasks in Scientific Experiments / Sci-VLA：用于科学实验中长期任务的代理 VLA 推理插件

**Date**: 2026-02-10 | **arXiv**: [2602.09430v1](http://arxiv.org/abs/2602.09430v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09430v1)

**Categories**: cs.RO, cs.AI

<details><summary><b>Abstract / 摘要</b></summary>

Robotic laboratories play a critical role in autonomous scientific discovery by enabling scalable, continuous experimental execution. Recent vision-language-action (VLA) models offer a promising foundation for robotic laboratories. However, scientific experiments typically involve long-horizon tasks composed of multiple atomic tasks, posing a fundamental challenge to existing VLA models. While VLA models fine-tuned for scientific tasks can reliably execute atomic experimental actions seen during training, they often fail to perform composite tasks formed by reordering and composing these known atomic actions. This limitation arises from a distributional mismatch between training-time atomic tasks and inference-time composite tasks, which prevents VLA models from executing necessary transitional operations between atomic tasks. To address this challenge, we propose an Agentic VLA Inference Plugin for Long-Horizon Tasks in Scientific Experiments. It introduces an LLM-based agentic inference mechanism that intervenes when executing sequential manipulation tasks. By performing explicit transition inference and generating transitional robotic action code, the proposed plugin guides VLA models through missing transitional steps, enabling reliable execution of composite scientific workflows without any additional training. This inference-only intervention makes our method computationally efficient, data-efficient, and well-suited for open-ended and long-horizon robotic laboratory tasks. We build 3D assets of scientific instruments and common scientific operating scenes within an existing simulation environment. In these scenes, we have verified that our method increases the average success rate per atomic task by 42\% during inference. Furthermore, we show that our method can be easily transferred from the simulation to real scientific laboratories.

机器人实验室通过实现可扩展、连续的实验执行，在自主科学发现中发挥着关键作用。最近的视觉-语言-动作（VLA）模型为机器人实验室提供了有前途的基础。然而，科学实验通常涉及由多个原子任务组成的长视野任务，这对现有的VLA模型提出了根本性的挑战。虽然针对科学任务进行微调的 VLA 模型可以可靠地执行训练期间看到的原子实验动作，但它们通常无法执行通过重新排序和组合这些已知原子动作而形成的复合任务。此限制是由于训练时原子任务和推理时复合任务之间的分布不匹配而引起的，这会阻止 VLA 模型在原子任务之间执行必要的转换操作。为了应对这一挑战，我们提出了一个用于科学实验中长期任务的 Agentic VLA 推理插件。它引入了一种基于 LLM 的代理推理机制，可以在执行顺序操作任务时进行干预。通过执行显式转换推理并生成转换机器人动作代码，所提出的插件可引导 VLA 模型完成缺失的转换步骤，从而无需任何额外训练即可可靠地执行复合科学工作流程。这种仅推理的干预使我们的方法计算效率高、数据效率高，并且非常适合开放式和长视野的机器人实验室任务。我们在现有的模拟环境中构建科学仪器的 3D 资产和常见的科学操作场景。在这些场景中，我们已经验证我们的方法在推理过程中将每个原子任务的平均成功率提高了 42%。此外，我们表明我们的方法可以轻松地从模拟转移到真实的科学实验室。

</details>

---

## 51. FlyAOC: Evaluating Agentic Ontology Curation of Drosophila Scientific Knowledge Bases / FlyAOC：评估果蝇科学知识库的代理本体管理

**Date**: 2026-02-09 | **arXiv**: [2602.09163v1](http://arxiv.org/abs/2602.09163v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09163v1)

**Categories**: cs.AI, cs.CL, cs.IR

<details><summary><b>Abstract / 摘要</b></summary>

Scientific knowledge bases accelerate discovery by curating findings from primary literature into structured, queryable formats for both human researchers and emerging AI systems. Maintaining these resources requires expert curators to search relevant papers, reconcile evidence across documents, and produce ontology-grounded annotations - a workflow that existing benchmarks, focused on isolated subtasks like named entity recognition or relation extraction, do not capture. We present FlyBench to evaluate AI agents on end-to-end agentic ontology curation from scientific literature. Given only a gene symbol, agents must search and read from a corpus of 16,898 full-text papers to produce structured annotations: Gene Ontology terms describing function, expression patterns, and historical synonyms linking decades of nomenclature. The benchmark includes 7,397 expert-curated annotations across 100 genes drawn from FlyBase, the Drosophila (fruit fly) knowledge base. We evaluate four baseline agent architectures: memorization, fixed pipeline, single-agent, and multi-agent. We find that architectural choices significantly impact performance, with multi-agent designs outperforming simpler alternatives, yet scaling backbone models yields diminishing returns. All baselines leave substantial room for improvement. Our analysis surfaces several findings to guide future development; for example, agents primarily use retrieval to confirm parametric knowledge rather than discover new information. We hope FlyBench will drive progress on retrieval-augmented scientific reasoning, a capability with broad applications across scientific domains.

科学知识库通过将原始文献中的发现整理成结构化的、可查询的格式，供人类研究人员和新兴人工智能系统使用，从而加速发现。维护这些资源需要专家策展人搜索相关论文，协调文档之间的证据，并生成基于本体的注释——这是现有基准（专注于命名实体识别或关系提取等孤立子任务）无法捕获的工作流程。我们提出 FlyBench 来评估人工智能代理在科学文献中的端到端代理本体管理。仅给定一个基因符号，智能体必须搜索并阅读 16,898 篇全文论文的语料库，以生成结构化注释：描述功能、表达模式的基因本体术语，以及连接数十年命名法的历史同义词。该基准包括来自果蝇知识库 FlyBase 的 100 个基因的 7,397 个专家策划的注释。我们评估了四种基线代理架构：记忆、固定管道、单代理和多代理。我们发现架构选择会显着影响性能，多代理设计的性能优于更简单的替代方案，但扩展骨干模型会产生收益递减。所有基线都留有很大的改进空间。我们的分析得出了一些结论来指导未来的发展；例如，代理主要使用检索来确认参数知识，而不是发现新信息。我们希望 FlyBench 能够推动检索增强科学推理的进步，这种能力在科学领域具有广泛的应用。

</details>

---

## 52. Next-Gen CAPTCHAs: Leveraging the Cognitive Gap for Scalable and Diverse GUI-Agent Defense / 下一代验证码：利用认知差距实现可扩展且多样化的 GUI 代理防御

**Date**: 2026-02-09 | **arXiv**: [2602.09012v1](http://arxiv.org/abs/2602.09012v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09012v1)

**Categories**: cs.LG, cs.AI, cs.CL

<details><summary><b>Abstract / 摘要</b></summary>

The rapid evolution of GUI-enabled agents has rendered traditional CAPTCHAs obsolete. While previous benchmarks like OpenCaptchaWorld established a baseline for evaluating multimodal agents, recent advancements in reasoning-heavy models, such as Gemini3-Pro-High and GPT-5.2-Xhigh have effectively collapsed this security barrier, achieving pass rates as high as 90% on complex logic puzzles like "Bingo". In response, we introduce Next-Gen CAPTCHAs, a scalable defense framework designed to secure the next-generation web against the advanced agents. Unlike static datasets, our benchmark is built upon a robust data generation pipeline, allowing for large-scale and easily scalable evaluations, notably, for backend-supported types, our system is capable of generating effectively unbounded CAPTCHA instances. We exploit the persistent human-agent "Cognitive Gap" in interactive perception, memory, decision-making, and action. By engineering dynamic tasks that require adaptive intuition rather than granular planning, we re-establish a robust distinction between biological users and artificial agents, offering a scalable and diverse defense mechanism for the agentic era.

支持 GUI 的代理的快速发展已经使传统的验证码变得过时。虽然 OpenCaptchaWorld 等之前的基准测试为评估多模式代理建立了基准，但 Gemini3-Pro-High 和 GPT-5.2-Xhigh 等推理型模型的最新进展有效地打破了这一安全屏障，在“Bingo”等复杂逻辑谜题上实现了高达 90% 的通过率。作为回应，我们推出了下一代验证码，这是一个可扩展的防御框架，旨在保护下一代网络免受高级代理的攻击。与静态数据集不同，我们的基准测试建立在强大的数据生成管道之上，允许大规模且易于扩展的评估，特别是对于后端支持的类型，我们的系统能够有效生成无限制的验证码实例。我们利用交互感知、记忆、决策和行动中持续存在的人类代理“认知差距”。通过设计需要自适应直觉而不是细粒度规划的动态任务，我们重新建立了生物用户和人工代理之间的牢固区别，为代理时代提供了可扩展且多样化的防御机制。

</details>

---

## 53. A Behavioural and Representational Evaluation of Goal-Directedness in Language Model Agents / 语言模型智能体目标导向性的行为和表征评估

**Date**: 2026-02-09 | **arXiv**: [2602.08964v1](http://arxiv.org/abs/2602.08964v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08964v1)

**Categories**: cs.LG, cs.AI, cs.CL, cs.CY

<details><summary><b>Abstract / 摘要</b></summary>

Understanding an agent's goals helps explain and predict its behaviour, yet there is no established methodology for reliably attributing goals to agentic systems. We propose a framework for evaluating goal-directedness that integrates behavioural evaluation with interpretability-based analyses of models' internal representations. As a case study, we examine an LLM agent navigating a 2D grid world toward a goal state. Behaviourally, we evaluate the agent against an optimal policy across varying grid sizes, obstacle densities, and goal structures, finding that performance scales with task difficulty while remaining robust to difficulty-preserving transformations and complex goal structures. We then use probing methods to decode the agent's internal representations of the environment state and its multi-step action plans. We find that the LLM agent non-linearly encodes a coarse spatial map of the environment, preserving approximate task-relevant cues about its position and the goal location; that its actions are broadly consistent with these internal representations; and that reasoning reorganises them, shifting from broader environment structural cues toward information supporting immediate action selection. Our findings support the view that introspective examination is required beyond behavioural evaluations to characterise how agents represent and pursue their objectives.

了解智能体的目标有助于解释和预测其行为，但目前还没有既定的方法来可靠地将目标归因于智能体系统。我们提出了一个评估目标导向性的框架，它将行为评估与基于可解释性的模型内部表征分析相结合。作为一个案例研究，我们研究了一个 LLM 代理在 2D 网格世界中导航至目标状态的情况。在行为上，我们根据不同网格大小、障碍密度和目标结构的最佳策略来评估智能体，发现性能随着任务难度而变化，同时对保留难度的转换和复杂的目标结构保持鲁棒性。然后，我们使用探测方法来解码代理的环境状态的内部表示及其多步骤行动计划。我们发现 LLM 代理对环境的粗略空间图进行非线性编码，保留关于其位置和目标位置的近似任务相关线索；其行动与这些内部陈述大致一致；这种推理会重新组织它们，从更广泛的环境结构线索转向支持立即行动选择的信息。我们的研究结果支持这样的观点，即除了行为评估之外，还需要进行内省检查来描述代理人如何表达和追求他们的目标。

</details>

---

## 54. CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute / CoRefine：用于自适应测试时间计算的置信引导自我优化

**Date**: 2026-02-09 | **arXiv**: [2602.08948v1](http://arxiv.org/abs/2602.08948v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08948v1)

**Categories**: cs.AI, cs.CL

<details><summary><b>Abstract / 摘要</b></summary>

Large Language Models (LLMs) often rely on test-time scaling via parallel decoding (for example, 512 samples) to boost reasoning accuracy, but this incurs substantial compute. We introduce CoRefine, a confidence-guided self-refinement method that achieves competitive accuracy using a fraction of the tokens via a lightweight 211k-parameter Conv1D controller atop a frozen LLM. The controller consumes full-trace confidence to decide whether to halt, re-examine, or try a different approach, enabling targeted self-correction with an average of 2.7 refinement steps per problem and roughly 190-fold token reduction relative to 512-sample baselines. Across diverse reasoning benchmarks and three open-source models, the controller achieves 92.6 percent precision when it confidently halts, indicating that confidence dynamics reliably signal correctness without ground-truth verification. We extend this to CoRefine-Tree, a hybrid sequential-parallel variant that adaptively balances exploration and exploitation, with easy serving integration and verifier compatibility. By treating confidence as a control signal rather than a correctness guarantee, CoRefine provides a modular primitive for scalable reasoning and agentic settings with imperfect verifiers.

大型语言模型 (LLM) 通常依靠通过并行解码（例如 512 个样本）进行测试时间缩放来提高推理准确性，但这会产生大量计算。我们引入了 CoRefine，这是一种信心引导的自我优化方法，通过冻结 LLM 之上的轻量级 211k 参数 Conv1D 控制器，使用一小部分代币实现有竞争力的准确性。控制器消耗全跟踪置信度来决定是否停止、重新检查或尝试不同的方法，从而实现有针对性的自我纠正，每个问题平均 2.7 个细化步骤，并且相对于 512 个样本基线大约减少 190 倍的标记。在不同的推理基准和三个开源模型中，控制器在自信地停止时实现了 92.6% 的精度，这表明置信度动态可靠地表明了正确性，而无需地面实况验证。我们将其扩展到 CoRefine-Tree，这是一种混合顺序并行变体，可以自适应地平衡探索和利用，并具有轻松的服务集成和验证器兼容性。通过将置信度视为控制信号而不是正确性保证，CoRefine 为具有不完善验证器的可扩展推理和代理设置提供了模块化原语。

</details>

---

## 55. STaR: Scalable Task-Conditioned Retrieval for Long-Horizon Multimodal Robot Memory / STaR：长视野多模态机器人内存的可扩展任务条件检索

**Date**: 2026-02-09 | **arXiv**: [2602.09255v1](http://arxiv.org/abs/2602.09255v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09255v1)

**Categories**: cs.RO, cs.AI

<details><summary><b>Abstract / 摘要</b></summary>

Mobile robots are often deployed over long durations in diverse open, dynamic scenes, including indoor setting such as warehouses and manufacturing facilities, and outdoor settings such as agricultural and roadway operations. A core challenge is to build a scalable long-horizon memory that supports an agentic workflow for planning, retrieval, and reasoning over open-ended instructions at variable granularity, while producing precise, actionable answers for navigation. We present STaR, an agentic reasoning framework that (i) constructs a task-agnostic, multimodal long-term memory that generalizes to unseen queries while preserving fine-grained environmental semantics (object attributes, spatial relations, and dynamic events), and (ii) introduces a Scalable TaskConditioned Retrieval algorithm based on the Information Bottleneck principle to extract from long-term memory a compact, non-redundant, information-rich set of candidate memories for contextual reasoning. We evaluate STaR on NaVQA (mixed indoor/outdoor campus scenes) and WH-VQA, a customized warehouse benchmark with many visually similar objects built with Isaac Sim, emphasizing contextual reasoning. Across the two datasets, STaR consistently outperforms strong baselines, achieving higher success rates and markedly lower spatial error. We further deploy STaR on a real Husky wheeled robot in both indoor and outdoor environments, demonstrating robust longhorizon reasoning, scalability, and practical utility.

移动机器人通常长时间部署在各种开放、动态的场景中，包括仓库和制造设施等室内环境，以及农业和道路作业等室外环境。核心挑战是构建一个可扩展的长视野内存，支持代理工作流程，以可变粒度对开放式指令进行规划、检索和推理，同时生成精确的、可操作的导航答案。我们提出了 STaR，一种代理推理框架，它（i）构建了一个与任务无关的多模态长期记忆，可泛化到未见过的查询，同时保留细粒度的环境语义（对象属性、空间关系和动态事件），以及（ii）引入基于信息瓶颈原理的可扩展任务条件检索算法，从长期记忆中提取紧凑、非冗余、信息丰富的候选记忆集，用于上下文推理。我们在 NaVQA（室内/室外混合校园场景）和 WH-VQA 上评估 STaR，WH-VQA 是一个定制的仓库基准，具有许多用 Isaac Sim 构建的视觉相似的对象，强调上下文推理。在这两个数据集中，STARR 始终优于强大的基线，实现了更高的成功率和显着更低的空间误差。我们进一步在室内和室外环境中的真实 Husky 轮式机器人上部署 STaR，展示了强大的长视野推理、可扩展性和实用性。

</details>

---

## 56. SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes / SceneSmith：代理生成模拟就绪的室内场景

**Date**: 2026-02-09 | **arXiv**: [2602.09153v1](http://arxiv.org/abs/2602.09153v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09153v1)

**Categories**: cs.RO, cs.AI, cs.CV, cs.GR

<details><summary><b>Abstract / 摘要</b></summary>

Simulation has become a key tool for training and evaluating home robots at scale, yet existing environments fail to capture the diversity and physical complexity of real indoor spaces. Current scene synthesis methods produce sparsely furnished rooms that lack the dense clutter, articulated furniture, and physical properties essential for robotic manipulation. We introduce SceneSmith, a hierarchical agentic framework that generates simulation-ready indoor environments from natural language prompts. SceneSmith constructs scenes through successive stages$\unicode{x2013}$from architectural layout to furniture placement to small object population$\unicode{x2013}$each implemented as an interaction among VLM agents: designer, critic, and orchestrator. The framework tightly integrates asset generation through text-to-3D synthesis for static objects, dataset retrieval for articulated objects, and physical property estimation. SceneSmith generates 3-6x more objects than prior methods, with <2% inter-object collisions and 96% of objects remaining stable under physics simulation. In a user study with 205 participants, it achieves 92% average realism and 91% average prompt faithfulness win rates against baselines. We further demonstrate that these environments can be used in an end-to-end pipeline for automatic robot policy evaluation.

模拟已成为大规模训练和评估家庭机器人的关键工具，但现有环境无法捕捉真实室内空间的多样性和物理复杂性。当前的场景合成方法产生的房间布置稀疏，缺乏密集的杂物、铰接式家具和机器人操作所必需的物理特性。我们引入了 SceneSmith，这是一个分层代理框架，可以根据自然语言提示生成模拟就绪的室内环境。 SceneSmith 通过连续的阶段$\unicode{x2013}$构建场景，从建筑布局到家具放置，再到小对象数量$\unicode{x2013}$每个阶段都作为 VLM 代理（设计师、评论家和编排者）之间的交互来实现。该框架通过静态对象的文本到 3D 合成、铰接对象的数据集检索以及物理属性估计紧密集成了资产生成。 SceneSmith 生成的对象比之前的方法多 3-6 倍，对象间碰撞率低于 2%，并且 96% 的对象在物理模拟下保持稳定。在一项有 205 名参与者参与的用户研究中，与基线相比，它的平均真实度为 92%，平均及时忠诚度为 91%。我们进一步证明这些环境可以在端到端管道中用于自动机器人策略评估。

</details>

---

## 57. Robustness Is a Function, Not a Number: A Factorized Comprehensive Study of OOD Robustness in Vision-Based Driving / 鲁棒性是一个函数，而不是一个数字：基于视觉的驾驶中 OOD 鲁棒性的分解综合研究

**Date**: 2026-02-09 | **arXiv**: [2602.09018v1](http://arxiv.org/abs/2602.09018v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09018v1)

**Categories**: cs.RO, cs.AI, cs.CV, cs.LG

<details><summary><b>Abstract / 摘要</b></summary>

Out of distribution (OOD) robustness in autonomous driving is often reduced to a single number, hiding what breaks a policy. We decompose environments along five axes: scene (rural/urban), season, weather, time (day/night), and agent mix; and measure performance under controlled $k$-factor perturbations ($k \in \{0,1,2,3\}$). Using closed loop control in VISTA, we benchmark FC, CNN, and ViT policies, train compact ViT heads on frozen foundation-model (FM) features, and vary ID support in scale, diversity, and temporal context. (1) ViT policies are markedly more OOD-robust than comparably sized CNN/FC, and FM features yield state-of-the-art success at a latency cost. (2) Naive temporal inputs (multi-frame) do not beat the best single-frame baseline. (3) The largest single factor drops are rural $\rightarrow$ urban and day $\rightarrow$ night ($\sim 31\%$ each); actor swaps $\sim 10\%$, moderate rain $\sim 7\%$; season shifts can be drastic, and combining a time flip with other changes further degrades performance. (4) FM-feature policies stay above $85\%$ under three simultaneous changes; non-FM single-frame policies take a large first-shift hit, and all no-FM models fall below $50\%$ by three changes. (5) Interactions are non-additive: some pairings partially offset, whereas season-time combinations are especially harmful. (6) Training on winter/snow is most robust to single-factor shifts, while a rural+summer baseline gives the best overall OOD performance. (7) Scaling traces/views improves robustness ($+11.8$ points from $5$ to $14$ traces), yet targeted exposure to hard conditions can substitute for scale. (8) Using multiple ID environments broadens coverage and strengthens weak cases (urban OOD $60.6\% \rightarrow 70.1\%$) with a small ID drop; single-ID preserves peak performance but in a narrow domain. These results yield actionable design rules for OOD-robust driving policies.

自动驾驶中的分布外 (OOD) 鲁棒性通常会简化为一个数字，从而隐藏了违反策略的内容。我们沿着五个轴分解环境：场景（农村/城市）、季节、天气、时间（白天/夜晚）和代理组合；并测量受控 $k$ 因子扰动下的性能 ($k \in \{0,1,2,3\}$)。使用 VISTA 中的闭环控制，我们对 FC、CNN 和 ViT 策略进行基准测试，在冻结的基础模型 (FM) 特征上训练紧凑的 ViT 头，并在规模、多样性和时间上下文中改变 ID 支持。 (1) ViT 策略明显比同等规模的 CNN/FC 更具有 OOD 鲁棒性，并且 FM 功能以延迟成本取得了最先进的成功。 (2) 朴素时间输入（多帧）无法击败最佳单帧基线。 （3）单因素下降最大的是农村$\rightarrow$城市和白天$\rightarrow$夜间（各$\sim 31\%$）；演员交换$\sim 10\%$，中雨$\sim 7\%$；季节变化可能会很剧烈，并且将时间翻转与其他变化结合起来会进一步降低性能。 (4) FM特色保单在三项同时变化下保持在$85\%$以上；非 FM 单帧策略受到较大的第一轮打击，所有非 FM 模型均通过三个变化跌至 50\%$ 以下。 (5) 相互作用是非累加性的：一些配对会部分抵消，而季节组合尤其有害。 (6) 冬季/雪地训练对于单因素变化最为稳健，而乡村+夏季基线则提供最佳的整体 OOD 性能。 (7) 缩放轨迹/视图可提高鲁棒性（从 5 美元到 14 美元轨迹，$+11.8 点），但有针对性地暴露在恶劣条件下可以替代缩放。 （8）使用多个ID环境扩大了覆盖范围并加强了弱案例（城市OOD $60.6\% \rightarrow 70.1\%$），ID下降幅度较小；单 ID 可以保持峰值性能，但范围很窄。这些结果为 OOD 稳健的驾驶策略提供了可行的设计规则。

</details>

---

## 58. Modeling 3D Pedestrian-Vehicle Interactions for Vehicle-Conditioned Pose Forecasting / 对 3D 行人-车辆交互建模以进行车辆条件姿态预测

**Date**: 2026-02-09 | **arXiv**: [2602.08962v1](http://arxiv.org/abs/2602.08962v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08962v1)

**Categories**: cs.CV, cs.RO

**Code**: https://github.com/GuangxunZhu/VehCondPose3D

<details><summary><b>Abstract / 摘要</b></summary>

Accurately predicting pedestrian motion is crucial for safe and reliable autonomous driving in complex urban environments. In this work, we present a 3D vehicle-conditioned pedestrian pose forecasting framework that explicitly incorporates surrounding vehicle information. To support this, we enhance the Waymo-3DSkelMo dataset with aligned 3D vehicle bounding boxes, enabling realistic modeling of multi-agent pedestrian-vehicle interactions. We introduce a sampling scheme to categorize scenes by pedestrian and vehicle count, facilitating training across varying interaction complexities. Our proposed network adapts the TBIFormer architecture with a dedicated vehicle encoder and pedestrian-vehicle interaction cross-attention module to fuse pedestrian and vehicle features, allowing predictions to be conditioned on both historical pedestrian motion and surrounding vehicles. Extensive experiments demonstrate substantial improvements in forecasting accuracy and validate different approaches for modeling pedestrian-vehicle interactions, highlighting the importance of vehicle-aware 3D pose prediction for autonomous driving. Code is available at: https://github.com/GuangxunZhu/VehCondPose3D

准确预测行人运动对于复杂城市环境中安全可靠的自动驾驶至关重要。在这项工作中，我们提出了一个 3D 车辆调节行人姿势预测框架，该框架明确地结合了周围车辆信息。为了支持这一点，我们使用对齐的 3D 车辆边界框增强了 Waymo-3DSkelMo 数据集，从而实现了多智能体行人-车辆交互的真实建模。我们引入了一种采样方案，根据行人和车辆数量对场景进行分类，从而促进不同交互复杂性的训练。我们提出的网络采用专用车辆编码器和行人车辆交互交叉注意模块来适应 TBIFormer 架构，以融合行人和车辆特征，从而允许根据历史行人运动和周围车辆进行预测。大量实验证明了预测准确性的显着提高，并验证了行人-车辆交互建模的不同方法，凸显了车辆感知 3D 姿态预测对于自动驾驶的重要性。代码可见：https://github.com/GuangxunZhu/VehCondPose3D

</details>

---

## 59. Legs Over Arms: On the Predictive Value of Lower-Body Pose for Human Trajectory Prediction from Egocentric Robot Perception / 双腿放在手臂上：论下半身姿势对基于自我中心机器人感知的人体轨迹预测的预测价值

**Date**: 2026-02-09 | **arXiv**: [2602.09076v1](http://arxiv.org/abs/2602.09076v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09076v1)

**Categories**: cs.RO

<details><summary><b>Abstract / 摘要</b></summary>

Predicting human trajectory is crucial for social robot navigation in crowded environments. While most existing approaches treat human as point mass, we present a study on multi-agent trajectory prediction that leverages different human skeletal features for improved forecast accuracy. In particular, we systematically evaluate the predictive utility of 2D and 3D skeletal keypoints and derived biomechanical cues as additional inputs. Through a comprehensive study on the JRDB dataset and another new dataset for social navigation with 360-degree panoramic videos, we find that focusing on lower-body 3D keypoints yields a 13% reduction in Average Displacement Error and augmenting 3D keypoint inputs with corresponding biomechanical cues provides a further 1-4% improvement. Notably, the performance gain persists when using 2D keypoint inputs extracted from equirectangular panoramic images, indicating that monocular surround vision can capture informative cues for motion forecasting. Our finding that robots can forecast human movement efficiently by watching their legs provides actionable insights for designing sensing capabilities for social robot navigation.

预测人类轨迹对于社交机器人在拥挤环境中的导航至关重要。虽然大多数现有方法将人类视为质点，但我们提出了一项关于多智能体轨迹预测的研究，该研究利用不同的人类骨骼特征来提高预测准确性。特别是，我们系统地评估 2D 和 3D 骨骼关键点的预测效用以及派生的生物力学线索作为额外输入。通过对 JRDB 数据集和另一个用于 360 度全景视频社交导航的新数据集的综合研究，我们发现，关注下半身 3D 关键点可使平均位移误差减少 13%，而通过相应的生物力学线索增强 3D 关键点输入可进一步提高 1-4%。值得注意的是，当使用从等距柱状全景图像提取的 2D 关键点输入时，性能增益仍然存在，这表明单目环绕视觉可以捕获用于运动预测的信息线索。我们发现机器人可以通过观察人类的腿部来有效预测人类的运动，这为设计社交机器人导航的传感功能提供了可行的见解。

</details>

---

