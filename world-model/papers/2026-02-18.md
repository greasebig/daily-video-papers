# arXiv World Model Papers - 2026-02-18

**Paper Count**: 6

---

## 1. VLM-DEWM: Dynamic External World Model for Verifiable and Resilient Vision-Language Planning in Manufacturing / VLM-DEWM：用于制造中可验证和弹性视觉语言规划的动态外部世界模型

**Date**: 2026-02-17 | **arXiv**: [2602.15549v1](http://arxiv.org/abs/2602.15549v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.15549v1)

**Categories**: cs.RO, cs.AI

<details><summary><b>Abstract / 摘要</b></summary>

Vision-language model (VLM) shows promise for high-level planning in smart manufacturing, yet their deployment in dynamic workcells faces two critical challenges: (1) stateless operation, they cannot persistently track out-of-view states, causing world-state drift; and (2) opaque reasoning, failures are difficult to diagnose, leading to costly blind retries. This paper presents VLM-DEWM, a cognitive architecture that decouples VLM reasoning from world-state management through a persistent, queryable Dynamic External World Model (DEWM). Each VLM decision is structured into an Externalizable Reasoning Trace (ERT), comprising action proposal, world belief, and causal assumption, which is validated against DEWM before execution. When failures occur, discrepancy analysis between predicted and observed states enables targeted recovery instead of global replanning. We evaluate VLM-DEWM on multi-station assembly, large-scale facility exploration, and real-robot recovery under induced failures. Compared to baseline memory-augmented VLM systems, VLM DEWM improves state-tracking accuracy from 56% to 93%, increases recovery success rate from below 5% to 95%, and significantly reduces computational overhead through structured memory. These results establish VLM-DEWM as a verifiable and resilient solution for long-horizon robotic operations in dynamic manufacturing environments.

视觉语言模型（VLM）显示了智能制造中高层规划的前景，但它们在动态工作单元中的部署面临着两个关键挑战：（1）无状态操作，它们无法持续跟踪视野外的状态，导致世界状态漂移； (2) 推理不透明，故障难以诊断，导致盲目重试成本高昂。本文提出了 VLM-DEWM，这是一种认知架构，通过持久的、可查询的动态外部世界模型 (DEWM) 将 VLM 推理与世界状态管理解耦。每个 VLM 决策都被构建为外部化推理跟踪 (ERT)，其中包括行动建议、世界信念和因果假设，并在执行前针对 DEWM 进行验证。当发生故障时，预测状态和观察状态之间的差异分析可以实现有针对性的恢复，而不是全局重新规划。我们评估了 VLM-DEWM 的多站装配、大规模设施探索以及诱发故障下的真实机器人恢复。与基线内存增强 VLM 系统相比，VLM DEWM 将状态跟踪精度从 56% 提高到 93%，将恢复成功率从 5% 以下提高到 95%，并通过结构化内存显着降低计算开销。这些结果使 VLM-DEWM 成为动态制造环境中长期机器人操作的可验证且有弹性的解决方案。

</details>

---

## 2. World-Model-Augmented Web Agents with Action Correction / 具有动作校正功能的世界模型增强网络代理

**Date**: 2026-02-17 | **arXiv**: [2602.15384v1](http://arxiv.org/abs/2602.15384v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.15384v1)

**Categories**: cs.AI, cs.CL

<details><summary><b>Abstract / 摘要</b></summary>

Web agents based on large language models have demonstrated promising capability in automating web tasks. However, current web agents struggle to reason out sensible actions due to the limitations of predicting environment changes, and might not possess comprehensive awareness of execution risks, prematurely performing risky actions that cause losses and lead to task failure. To address these challenges, we propose WAC, a web agent that integrates model collaboration, consequence simulation, and feedback-driven action refinement. To overcome the cognitive isolation of individual models, we introduce a multi-agent collaboration process that enables an action model to consult a world model as a web-environment expert for strategic guidance; the action model then grounds these suggestions into executable actions, leveraging prior knowledge of environmental state transition dynamics to enhance candidate action proposal. To achieve risk-aware resilient task execution, we introduce a two-stage deduction chain. A world model, specialized in environmental state transitions, simulates action outcomes, which a judge model then scrutinizes to trigger action corrective feedback when necessary. Experiments show that WAC achieves absolute gains of 1.8% on VisualWebArena and 1.3% on Online-Mind2Web.

基于大型语言模型的 Web 代理在自动化 Web 任务方面表现出了良好的能力。然而，当前的网络代理由于预测环境变化的局限性，难以推理出合理的行动，并且可能不具备全面的执行风险意识，过早地执行风险行动，造成损失并导致任务失败。为了应对这些挑战，我们提出了 WAC，这是一种集成了模型协作、结果模拟和反馈驱动的动作细化的网络代理。为了克服各个模型的认知隔离，我们引入了多智能体协作流程，使行动模型能够作为网络环境专家咨询世界模型以获取战略指导；然后，行动模型将这些建议转化为可执行的行动，利用环境状态转换动态的先验知识来增强候选行动建议。为了实现风险感知的弹性任务执行，我们引入了两阶段推论链。专门研究环境状态转换的世界模型会模拟行动结果，然后法官模型会对其进行仔细检查，以在必要时触发行动纠正反馈。实验表明，WAC 在 VisualWebArena 上获得了 1.8% 的绝对收益，在 Online-Mind2Web 上获得了 1.3% 的绝对收益。

</details>

---

## 3. Feasibility-aware Imitation Learning from Observation with Multimodal Feedback / 通过多模态反馈进行观察的可行性感知模仿学习

**Date**: 2026-02-17 | **arXiv**: [2602.15351v1](http://arxiv.org/abs/2602.15351v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.15351v1)

**Categories**: cs.RO

<details><summary><b>Abstract / 摘要</b></summary>

Imitation learning frameworks that learn robot control policies from demonstrators' motions via hand-mounted demonstration interfaces have attracted increasing attention. However, due to differences in physical characteristics between demonstrators and robots, this approach faces two limitations: i) the demonstration data do not include robot actions, and ii) the demonstrated motions may be infeasible for robots. These limitations make policy learning difficult. To address them, we propose Feasibility-Aware Behavior Cloning from Observation (FABCO). FABCO integrates behavior cloning from observation, which complements robot actions using robot dynamics models, with feasibility estimation. In feasibility estimation, the demonstrated motions are evaluated using a robot-dynamics model, learned from the robot's execution data, to assess reproducibility under the robot's dynamics. The estimated feasibility is used for multimodal feedback and feasibility-aware policy learning to improve the demonstrator's motions and learn robust policies. Multimodal feedback provides feasibility through the demonstrator's visual and haptic senses to promote feasible demonstrated motions. Feasibility-aware policy learning reduces the influence of demonstrated motions that are infeasible for robots, enabling the learning of policies that robots can execute stably. We conducted experiments with 15 participants on two tasks and confirmed that FABCO improves imitation learning performance by more than 3.2 times compared to the case without feasibility feedback.

通过手持式演示界面从演示者的动作中学习机器人控制策略的模仿学习框架引起了越来越多的关注。然而，由于演示者和机器人之间的物理特征差异，这种方法面临两个限制：i）演示数据不包括机器人动作，ii）演示的运动对于机器人来说可能不可行。这些限制使得政策学习变得困难。为了解决这些问题，我们提出了基于观察的可行性感知行为克隆（FABCO）。 FABCO 将观察行为克隆与可行性评估相结合，利用机器人动力学模型补充机器人动作。在可行性评估中，使用从机器人的执行数据中学习的机器人动力学模型来评估演示的运动，以评估机器人动力学下的再现性。估计的可行性用于多模式反馈和可行性感知政策学习，以改进演示者的动作并学习稳健的政策。多模态反馈通过演示者的视觉和触觉提供可行性，以促进可行的演示动作。可行性感知策略学习减少了机器人无法执行的演示动作的影响，从而能够学习机器人可以稳定执行的策略。我们对 15 名参与者进行了两项任务的实验，证实 FABCO 与没有可行性反馈的情况相比，模仿学习性能提高了 3.2 倍以上。

</details>

---

## 4. Cold-Start Personalization via Training-Free Priors from Structured World Models / 通过结构化世界模型的免训练先验进行冷启动个性化

**Date**: 2026-02-16 | **arXiv**: [2602.15012v1](http://arxiv.org/abs/2602.15012v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.15012v1)

**Categories**: cs.CL, cs.AI, cs.LG

<details><summary><b>Abstract / 摘要</b></summary>

Cold-start personalization requires inferring user preferences through interaction when no user-specific historical data is available. The core challenge is a routing problem: each task admits dozens of preference dimensions, yet individual users care about only a few, and which ones matter depends on who is asking. With a limited question budget, asking without structure will miss the dimensions that matter. Reinforcement learning is the natural formulation, but in multi-turn settings its terminal reward fails to exploit the factored, per-criterion structure of preference data, and in practice learned policies collapse to static question sequences that ignore user responses. We propose decomposing cold-start elicitation into offline structure learning and online Bayesian inference. Pep (Preference Elicitation with Priors) learns a structured world model of preference correlations offline from complete profiles, then performs training-free Bayesian inference online to select informative questions and predict complete preference profiles, including dimensions never asked about. The framework is modular across downstream solvers and requires only simple belief models. Across medical, mathematical, social, and commonsense reasoning, Pep achieves 80.8% alignment between generated responses and users' stated preferences versus 68.5% for RL, with 3-5x fewer interactions. When two users give different answers to the same question, Pep changes its follow-up 39-62% of the time versus 0-28% for RL. It does so with ~10K parameters versus 8B for RL, showing that the bottleneck in cold-start elicitation is the capability to exploit the factored structure of preference data.

冷启动个性化需要在没有特定于用户的历史数据可用时通过交互推断用户偏好。核心挑战是路由问题：每个任务都有数十个偏好维度，但个人用户只关心其中几个，而哪些维度重要取决于提出请求的人。由于问题预算有限，没有结构的提问会错过重要的维度。强化学习是自然的公式，但在多轮设置中，其最终奖励无法利用偏好数据的分解的、按标准的结构，并且在实践中学习的策略崩溃为忽略用户响应的静态问题序列。我们建议将冷启动启发分解为离线结构学习和在线贝叶斯推理。 Pep（先验偏好启发）从完整的配置文件中离线学习偏好相关性的结构化世界模型，然后在线执行免训练贝叶斯推理以选择信息性问题并预测完整的偏好配置文件，包括从未询问过的维度。该框架在下游求解器中是模块化的，并且只需要简单的信念模型。在医学、数学、社会和常识推理方面，Pep 在生成的响应和用户陈述的偏好之间实现了 80.8% 的一致性，而 RL 的一致性为 68.5%，交互次数减少了 3-5 倍。当两个用户对同一问题给出不同答案时，Pep 在 39-62% 的情况下会更改其后续内容，而 RL 的这一比例为 0-28%。与 RL 的 8B 参数相比，它使用约 10K 参数来实现这一点，这表明冷启动启发的瓶颈是利用偏好数据的分解结构的能力。

</details>

---

## 5. World Models for Policy Refinement in StarCraft II / 星际争霸 II 中政策细化的世界模型

**Date**: 2026-02-16 | **arXiv**: [2602.14857v1](http://arxiv.org/abs/2602.14857v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.14857v1)

**Categories**: cs.AI

<details><summary><b>Abstract / 摘要</b></summary>

Large Language Models (LLMs) have recently shown strong reasoning and generalization capabilities, motivating their use as decision-making policies in complex environments. StarCraft II (SC2), with its massive state-action space and partial observability, is a challenging testbed. However, existing LLM-based SC2 agents primarily focus on improving the policy itself and overlook integrating a learnable, action-conditioned transition model into the decision loop. To bridge this gap, we propose StarWM, the first world model for SC2 that predicts future observations under partial observability. To facilitate learning SC2's hybrid dynamics, we introduce a structured textual representation that factorizes observations into five semantic modules, and construct SC2-Dynamics-50k, the first instruction-tuning dataset for SC2 dynamics prediction. We further develop a multi-dimensional offline evaluation framework for predicted structured observations. Offline results show StarWM's substantial gains over zero-shot baselines, including nearly 60% improvements in resource prediction accuracy and self-side macro-situation consistency. Finally, we propose StarWM-Agent, a world-model-augmented decision system that integrates StarWM into a Generate--Simulate--Refine decision loop for foresight-driven policy refinement. Online evaluation against SC2's built-in AI demonstrates consistent improvements, yielding win-rate gains of 30%, 15%, and 30% against Hard (LV5), Harder (LV6), and VeryHard (LV7), respectively, alongside improved macro-management stability and tactical risk assessment.

大型语言模型（LLM）最近表现出强大的推理和泛化能力，促使它们在复杂环境中用作决策策略。星际争霸 II (SC2) 具有巨大的状态动作空间和部分可观测性，是一个具有挑战性的测试平台。然而，现有的基于 LLM 的 SC2 智能体主要侧重于改进策略本身，而忽略了将可学习的、以行动为条件的转换模型集成到决策循环中。为了弥补这一差距，我们提出了 StarWM，这是 SC2 的第一个世界模型，可以在部分可观测性下预测未来的观测结果。为了促进学习 SC2 的混合动力学，我们引入了一种结构化文本表示，将观察结果分解为五个语义模块，并构建了 SC2-Dynamics-50k，这是第一个用于 SC2 动力学预测的指令调整数据集。我们进一步开发了一个用于预测结构化观察的多维离线评估框架。离线结果显示，StarWM 较零样本基线有大幅提升，包括资源预测精度和自身宏观情况一致性方面提高了近 60%。最后，我们提出了 StarWM-Agent，这是一个世界模型增强决策系统，它将 StarWM 集成到生成-模拟-细化决策循环中，以实现前瞻驱动的政策细化。针对 SC2 内置 AI 的在线评估显示出持续的改进，相对于 Hard (LV5)、Harder (LV6) 和 VeryHard (LV7) 胜率分别提高了 30%、15% 和 30%，同时改善了宏观管理稳定性和战术风险评估。

</details>

---

## 6. WebWorld: A Large-Scale World Model for Web Agent Training / WebWorld：用于 Web 代理训练的大规模世界模型

**Date**: 2026-02-16 | **arXiv**: [2602.14721v1](http://arxiv.org/abs/2602.14721v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.14721v1)

**Categories**: cs.AI

<details><summary><b>Abstract / 摘要</b></summary>

Web agents require massive trajectories to generalize, yet real-world training is constrained by network latency, rate limits, and safety risks. We introduce \textbf{WebWorld} series, the first open-web simulator trained at scale. While existing simulators are restricted to closed environments with thousands of trajectories, WebWorld leverages a scalable data pipeline to train on 1M+ open-web interactions, supporting reasoning, multi-format data, and long-horizon simulations of 30+ steps. For intrinsic evaluation, we introduce WebWorld-Bench with dual metrics spanning nine dimensions, where WebWorld achieves simulation performance comparable to Gemini-3-Pro. For extrinsic evaluation, Qwen3-14B trained on WebWorld-synthesized trajectories improves by +9.2\% on WebArena, reaching performance comparable to GPT-4o. WebWorld enables effective inference-time search, outperforming GPT-5 as a world model. Beyond web simulation, WebWorld exhibits cross-domain generalization to code, GUI, and game environments, providing a replicable recipe for world model construction.

网络代理需要大量的轨迹来进行概括，但现实世界的训练受到网络延迟、速率限制和安全风险的限制。我们推出 \textbf{WebWorld} 系列，这是第一个大规模训练的开放网络模拟器。虽然现有模拟器仅限于具有数千条轨迹的封闭环境，但 WebWorld 利用可扩展的数据管道来训练超过 100 万次开放网络交互，支持推理、多格式数据和 30 多个步骤的长期模拟。对于内在评估，我们引入了具有跨越九个维度的双指标的WebWorld-Bench，其中WebWorld实现了与Gemini-3-Pro相当的模拟性能。对于外部评估，在 WebWorld 合成轨迹上训练的 Qwen3-14B 在 WebArena 上提高了 +9.2%，达到了与 GPT-4o 相当的性能。 WebWorld 可实现有效的推理时间搜索，作为世界模型，其性能优于 GPT-5。除了网络模拟之外，WebWorld 还展示了对代码、GUI 和游戏环境的跨域泛化，为世界模型构建提供了可复制的方法。

</details>

---

