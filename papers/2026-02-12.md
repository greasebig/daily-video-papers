# arXiv Video Papers - 2026-02-12

**Paper Count**: 27

---

## 1. SurfPhase: 3D Interfacial Dynamics in Two-Phase Flows from Sparse Videos

**中文标题**: SurfPhase：稀疏视频中两相流的 3D 界面动力学

**Date**: 2026-02-11 | **arXiv**: [2602.11154v1](http://arxiv.org/abs/2602.11154v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11154v1)

<details><summary><b>Abstract</b></summary>

Interfacial dynamics in two-phase flows govern momentum, heat, and mass transfer, yet remain difficult to measure experimentally. Classical techniques face intrinsic limitations near moving interfaces, while existing neural rendering methods target single-phase flows with diffuse boundaries and cannot handle sharp, deformable liquid-vapor interfaces. We propose SurfPhase, a novel model for reconstructing 3D interfacial dynamics from sparse camera views. Our approach integrates dynamic Gaussian surfels with a signed distance function formulation for geometric consistency, and leverages a video diffusion model to synthesize novel-view videos to refine reconstruction from sparse observations. We evaluate on a new dataset of high-speed pool boiling videos, demonstrating high-quality view synthesis and velocity estimation from only two camera views. Project website: https://yuegao.me/SurfPhase.

</details>

<details><summary><b>中文摘要</b></summary>

两相流中的界面动力学控制动量、热量和质量传递，但仍然难以通过实验测量。经典技术在移动界面附近面临固有的局限性，而现有的神经渲染方法针对具有扩散边界的单相流，无法处理尖锐、可变形的液-气界面。我们提出了 SurfPhase，一种从稀疏相机视图重建 3D 界面动力学的新颖模型。我们的方法将动态高斯面元与带符号距离函数公式相结合以实现几何一致性，并利用视频扩散模型来合成新颖的视图视频，以改进稀疏观测的重建。我们对高速池沸腾视频的新数据集进行了评估，仅通过两个摄像机视图演示了高质量的视图合成和速度估计。项目网站：https://yuegao.me/SurfPhase。

</details>

---

## 2. HairWeaver: Few-Shot Photorealistic Hair Motion Synthesis with Sim-to-Real Guided Video Diffusion

**中文标题**: HairWeaver：通过模拟到真实引导视频扩散进行少镜头真实感头发运动合成

**Date**: 2026-02-11 | **arXiv**: [2602.11117v1](http://arxiv.org/abs/2602.11117v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11117v1)

<details><summary><b>Abstract</b></summary>

We present HairWeaver, a diffusion-based pipeline that animates a single human image with realistic and expressive hair dynamics. While existing methods successfully control body pose, they lack specific control over hair, and as a result, fail to capture the intricate hair motions, resulting in stiff and unrealistic animations. HairWeaver overcomes this limitation using two specialized modules: a Motion-Context-LoRA to integrate motion conditions and a Sim2Real-Domain-LoRA to preserve the subject's photoreal appearance across different data domains. These lightweight components are designed to guide a video diffusion backbone while maintaining its core generative capabilities. By training on a specialized dataset of dynamic human motion generated from a CG simulator, HairWeaver affords fine control over hair motion and ultimately learns to produce highly realistic hair that responds naturally to movement. Comprehensive evaluations demonstrate that our approach sets a new state of the art, producing lifelike human hair animations with dynamic details.

</details>

<details><summary><b>中文摘要</b></summary>

我们推出了 HairWeaver，这是一种基于扩散的管道，可以通过逼真且富有表现力的头发动态来对单个人类图像进行动画处理。虽然现有方法成功地控制了身体姿势，但它们缺乏对头发的具体控制，因此无法捕捉复杂的头发运动，导致动画僵硬且不切实际。 HairWeaver 使用两个专用模块克服了这一限制：一个用于集成运动条件的 Motion-Context-LoRA，另一个是 Sim2Real-Domain-LoRA，用于在不同数据域中保留主体的真实外观。这些轻量级组件旨在指导视频传播主干，同时保持其核心生成功能。通过对 CG 模拟器生成的动态人体运动的专门数据集进行训练，HairWeaver 可以对头发运动进行精细控制，并最终学会生成对运动做出自然响应的高度逼真的头发。综合评估表明，我们的方法树立了新的技术水平，可以制作具有动态细节的逼真的人发动画。

</details>

---

## 3. FastFlow: Accelerating The Generative Flow Matching Models with Bandit Inference

**中文标题**: FastFlow：通过强盗推理加速生成流匹配模型

**Date**: 2026-02-11 | **arXiv**: [2602.11105v1](http://arxiv.org/abs/2602.11105v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.11105v1)

**Code**: https://github.com/Div290/FastFlow.

<details><summary><b>Abstract</b></summary>

Flow-matching models deliver state-of-the-art fidelity in image and video generation, but the inherent sequential denoising process renders them slower. Existing acceleration methods like distillation, trajectory truncation, and consistency approaches are static, require retraining, and often fail to generalize across tasks. We propose FastFlow, a plug-and-play adaptive inference framework that accelerates generation in flow matching models. FastFlow identifies denoising steps that produce only minor adjustments to the denoising path and approximates them without using the full neural network models used for velocity predictions. The approximation utilizes finite-difference velocity estimates from prior predictions to efficiently extrapolate future states, enabling faster advancements along the denoising path at zero compute cost. This enables skipping computation at intermediary steps. We model the decision of how many steps to safely skip before requiring a full model computation as a multi-armed bandit problem. The bandit learns the optimal skips to balance speed with performance. FastFlow integrates seamlessly with existing pipelines and generalizes across image generation, video generation, and editing tasks. Experiments demonstrate a speedup of over 2.6x while maintaining high-quality outputs. The source code for this work can be found at https://github.com/Div290/FastFlow.

</details>

<details><summary><b>中文摘要</b></summary>

流匹配模型在图像和视频生成中提供最先进的保真度，但固有的顺序去噪过程使它们速度变慢。现有的加速方法（例如蒸馏、轨迹截断和一致性方法）是静态的，需要重新训练，并且通常无法跨任务泛化。我们提出了 FastFlow，一种即插即用的自适应推理框架，可加速流匹配模型的生成。 FastFlow 识别仅对去噪路径产生微小调整的去噪步骤，并在不使用用于速度预测的完整神经网络模型的情况下对其进行近似。该近似利用先前预测的有限差分速度估计来有效地推断未来状态，从而以零计算成本沿着去噪路径实现更快的进展。这使得能够跳过中间步骤的计算。我们将在需要完整模型计算之前安全跳过多少步骤的决策建模为多臂老虎机问题。老虎机学习最佳跳跃以平衡速度与性能。 FastFlow 与现有管道无缝集成，并可泛化图像生成、视频生成和编辑任务。实验表明，在保持高质量输出的同时，速度提高了 2.6 倍以上。这项工作的源代码可以在 https://github.com/Div290/FastFlow 找到。

</details>

---

## 4. Flow caching for autoregressive video generation

**中文标题**: 用于自回归视频生成的流缓存

**Date**: 2026-02-11 | **arXiv**: [2602.10825v1](http://arxiv.org/abs/2602.10825v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10825v1)

**Code**: https://github.com/mikeallen39/FlowCache.

<details><summary><b>Abstract</b></summary>

Autoregressive models, often built on Transformer architectures, represent a powerful paradigm for generating ultra-long videos by synthesizing content in sequential chunks. However, this sequential generation process is notoriously slow. While caching strategies have proven effective for accelerating traditional video diffusion models, existing methods assume uniform denoising across all frames-an assumption that breaks down in autoregressive models where different video chunks exhibit varying similarity patterns at identical timesteps. In this paper, we present FlowCache, the first caching framework specifically designed for autoregressive video generation. Our key insight is that each video chunk should maintain independent caching policies, allowing fine-grained control over which chunks require recomputation at each timestep. We introduce a chunkwise caching strategy that dynamically adapts to the unique denoising characteristics of each chunk, complemented by a joint importance-redundancy optimized KV cache compression mechanism that maintains fixed memory bounds while preserving generation quality. Our method achieves remarkable speedups of 2.38 times on MAGI-1 and 6.7 times on SkyReels-V2, with negligible quality degradation (VBench: 0.87 increase and 0.79 decrease respectively). These results demonstrate that FlowCache successfully unlocks the potential of autoregressive models for real-time, ultra-long video generation-establishing a new benchmark for efficient video synthesis at scale. The code is available at https://github.com/mikeallen39/FlowCache.

</details>

<details><summary><b>中文摘要</b></summary>

自回归模型通常建立在 Transformer 架构之上，代表了通过合成连续块中的内容来生成超长视频的强大范例。然而，这种顺序生成过程是出了名的慢。虽然缓存策略已被证明对于加速传统视频扩散模型是有效的，但现有方法假设所有帧都采用统一的去噪——这种假设在自回归模型中被打破，其中不同的视频块在相同的时间步长表现出不同的相似性模式。在本文中，我们介绍了 FlowCache，这是第一个专为自回归视频生成而设计的缓存框架。我们的主要见解是每个视频块应该维护独立的缓存策略，从而可以对每个时间步需要重新计算的块进行细粒度控制。我们引入了一种分块缓存策略，该策略动态适应每个块的独特去噪特性，并辅以联合重要性冗余优化的 KV 缓存压缩机制，该机制在保持固定内存边界的同时保持生成质量。我们的方法在 MAGI-1 上实现了 2.38 倍的显着加速，在 SkyReels-V2 上实现了 6.7 倍的显着加速，而质量下降可以忽略不计（VBench：分别增加 0.87 倍和减少 0.79 倍）。这些结果表明，FlowCache 成功释放了自回归模型在实时、超长视频生成方面的潜力，为大规模高效视频合成建立了新基准。该代码可从 https://github.com/mikeallen39/FlowCache 获取。

</details>

---

## 5. TwiFF (Think With Future Frames): A Large-Scale Dataset for Dynamic Visual Reasoning

**中文标题**: TwiFF（思考未来框架）：用于动态视觉推理的大规模数据集

**Date**: 2026-02-11 | **arXiv**: [2602.10675v1](http://arxiv.org/abs/2602.10675v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10675v1)

**Code**: https://github.com/LiuJunhua02/TwiFF.

<details><summary><b>Abstract</b></summary>

Visual Chain-of-Thought (VCoT) has emerged as a promising paradigm for enhancing multimodal reasoning by integrating visual perception into intermediate reasoning steps. However, existing VCoT approaches are largely confined to static scenarios and struggle to capture the temporal dynamics essential for tasks such as instruction, prediction, and camera motion. To bridge this gap, we propose TwiFF-2.7M, the first large-scale, temporally grounded VCoT dataset derived from $2.7$ million video clips, explicitly designed for dynamic visual question and answer. Accompanying this, we introduce TwiFF-Bench, a high-quality evaluation benchmark of $1,078$ samples that assesses both the plausibility of reasoning trajectories and the correctness of final answers in open-ended dynamic settings. Building on these foundations, we propose the TwiFF model, a unified modal that synergistically leverages pre-trained video generation and image comprehension capabilities to produce temporally coherent visual reasoning cues-iteratively generating future action frames and textual reasoning. Extensive experiments demonstrate that TwiFF significantly outperforms existing VCoT methods and Textual Chain-of-Thought baselines on dynamic reasoning tasks, which fully validates the effectiveness for visual question answering in dynamic scenarios. Our code and data is available at https://github.com/LiuJunhua02/TwiFF.

</details>

<details><summary><b>中文摘要</b></summary>

视觉思维链（VCoT）已成为一种有前途的范式，通过将视觉感知集成到中间推理步骤来增强多模态推理。然而，现有的 VCoT 方法主要局限于静态场景，难以捕捉指令、预测和相机运动等任务所必需的时间动态。为了弥补这一差距，我们提出了 TwiFF-2.7M，这是第一个大规模、基于时间的 VCoT 数据集，源自价值 270 万美元的视频剪辑，专门为动态视觉问答而设计。与此同时，我们推出了 TwiFF-Bench，这是一个包含 1,078 美元样本的高质量评估基准，用于评估开放式动态设置中推理轨迹的合理性和最终答案的正确性。在此基础上，我们提出了 TwiFF 模型，这是一种统一模式，协同利用预先训练的视频生成和图像理解功能来产生时间连贯的视觉推理线索，迭代地生成未来的动作框架和文本推理。大量实验表明，TwiFF 在动态推理任务上显着优于现有的 VCoT 方法和文本思维链基线，充分验证了动态场景下视觉问答的有效性。我们的代码和数据可在 https://github.com/LiuJunhua02/TwiFF 获取。

</details>

---

## 6. VideoSTF: Stress-Testing Output Repetition in Video Large Language Models

**中文标题**: VideoSTF：视频大语言模型中的输出重复压力测试

**Date**: 2026-02-11 | **arXiv**: [2602.10639v1](http://arxiv.org/abs/2602.10639v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10639v1)

**Code**: https://github.com/yuxincao22/VideoSTF_benchmark.

<details><summary><b>Abstract</b></summary>

Video Large Language Models (VideoLLMs) have recently achieved strong performance in video understanding tasks. However, we identify a previously underexplored generation failure: severe output repetition, where models degenerate into self-reinforcing loops of repeated phrases or sentences. This failure mode is not captured by existing VideoLLM benchmarks, which focus primarily on task accuracy and factual correctness. We introduce VideoSTF, the first framework for systematically measuring and stress-testing output repetition in VideoLLMs. VideoSTF formalizes repetition using three complementary n-gram-based metrics and provides a standardized testbed of 10,000 diverse videos together with a library of controlled temporal transformations. Using VideoSTF, we conduct pervasive testing, temporal stress testing, and adversarial exploitation across 10 advanced VideoLLMs. We find that output repetition is widespread and, critically, highly sensitive to temporal perturbations of video inputs. Moreover, we show that simple temporal transformations can efficiently induce repetitive degeneration in a black-box setting, exposing output repetition as an exploitable security vulnerability. Our results reveal output repetition as a fundamental stability issue in modern VideoLLMs and motivate stability-aware evaluation for video-language systems. Our evaluation code and scripts are available at: https://github.com/yuxincao22/VideoSTF_benchmark.

</details>

<details><summary><b>中文摘要</b></summary>

视频大语言模型（VideoLLM）最近在视频理解任务中取得了强劲的性能。然而，我们发现了一个先前未被充分探索的生成失败：严重的输出重复，其中模型退化为重复短语或句子的自我强化循环。现有的 VideoLLM 基准测试未捕获此故障模式，该基准测试主要关注任务准确性和事实正确性。我们介绍 VideoSTF，这是第一个在 VideoLLM 中系统测量和压力测试输出重复的框架。 VideoSTF 使用三个互补的基于 n-gram 的指标来形式化重复，并提供包含 10,000 个不同视频的标准化测试床以及受控时间转换库。使用 VideoSTF，我们在 10 个高级 VideoLLM 中进行普遍测试、时间压力测试和对抗性利用。我们发现输出重复很普遍，而且至关重要的是，它对视频输入的时间扰动高度敏感。此外，我们表明简单的时间变换可以有效地在黑盒设置中引起重复退化，从而将输出重复暴露为可利用的安全漏洞。我们的结果表明，输出重复是现代 VideoLLM 中的一个基本稳定性问题，并激发了对视频语言系统的稳定性感知评估。我们的评估代码和脚本位于：https://github.com/yuxincao22/VideoSTF_benchmark。

</details>

---

## 7. Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation

**中文标题**: 说、梦想和行动：学习指令驱动机器人操作的视频世界模型

**Date**: 2026-02-11 | **arXiv**: [2602.10717v1](http://arxiv.org/abs/2602.10717v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10717v1)

<details><summary><b>Abstract</b></summary>

Robotic manipulation requires anticipating how the environment evolves in response to actions, yet most existing systems lack this predictive capability, often resulting in errors and inefficiency. While Vision-Language Models (VLMs) provide high-level guidance, they cannot explicitly forecast future states, and existing world models either predict only short horizons or produce spatially inconsistent frames. To address these challenges, we propose a framework for fast and predictive video-conditioned action. Our approach first selects and adapts a robust video generation model to ensure reliable future predictions, then applies adversarial distillation for fast, few-step video generation, and finally trains an action model that leverages both generated videos and real observations to correct spatial errors. Extensive experiments show that our method produces temporally coherent, spatially accurate video predictions that directly support precise manipulation, achieving significant improvements in embodiment consistency, spatial referring ability, and task completion over existing baselines. Codes & Models will be released.

</details>

<details><summary><b>中文摘要</b></summary>

机器人操纵需要预测环境如何响应行动而演变，但大多数现有系统缺乏这种预测能力，常常导致错误和低效率。虽然视觉语言模型（VLM）提供高级指导，但它们无法明确预测未来状态，并且现有的世界模型要么仅预测短期情况，要么产生空间不一致的框架。为了应对这些挑战，我们提出了一个快速、预测性视频条件动作框架。我们的方法首先选择并调整一个强大的视频生成模型，以确保可靠的未来预测，然后应用对抗性蒸馏来快速、几步视频生成，最后训练一个动作模型，利用生成的视频和真实观察来纠正空间错误。大量的实验表明，我们的方法产生时间上一致、空间上准确的视频预测，直接支持精确操作，在现有基线的基础上实现了实施例一致性、空间参考能力和任务完成度的显着改进。代码和型号将被发布。

</details>

---

## 8. TVCACHE: A Stateful Tool-Value Cache for Post-Training LLM Agents

**中文标题**: TVCACHE：用于训练后 LLM 代理的状态工具值缓存

**Date**: 2026-02-11 | **arXiv**: [2602.10986v1](http://arxiv.org/abs/2602.10986v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10986v1)

<details><summary><b>Abstract</b></summary>

In RL post-training of LLM agents, calls to external tools take several seconds or even minutes, leaving allocated GPUs idle and inflating post-training time and cost. While many tool invocations repeat across parallel rollouts and could in principle be cached, naively caching their outputs for reuse is incorrect since tool outputs depend on the environment state induced by prior agent interactions. We present TVCACHE, a stateful tool-value cache for LLM agent post-training. TVCACHE maintains a tree of observed tool-call sequences and performs longest-prefix matching for cache lookups: a hit occurs only when the agent's full tool history matches a previously executed sequence, guaranteeing identical environment state. On three diverse workloads-terminal-based tasks, SQL generation, and video understanding. TVCACHE achieves cache hit rates of up to 70% and reduces median tool call execution time by up to 6.9X, with no degradation in post-training reward accumulation.

</details>

<details><summary><b>中文摘要</b></summary>

在 LLM 代理的强化学习后期训练中，调用外部工具需要几秒钟甚至几分钟的时间，导致分配的 GPU 闲置，并增加训练后的时间和成本。虽然许多工具调用在并行部署中重复，并且原则上可以缓存，但天真地缓存其输出以供重用是不正确的，因为工具输出取决于先前代理交互引起的环境状态。我们推出了 TVCACHE，这是一种用于 LLM 代理后期训练的有状态工具值缓存。 TVCACHE 维护观察到的工具调用序列树，并执行缓存查找的最长前缀匹配：仅当代理的完整工具历史记录与先前执行的序列匹配时才会发生命中，从而保证相同的环境状态。关于三种不同的工作负载——基于终端的任务、SQL 生成和视频理解。 TVCACHE 实现了高达 70% 的缓存命中率，并将工具调用执行时间中位数减少了 6.9 倍，并且训练后奖励积累没有下降。

</details>

---

## 9. ConsID-Gen: View-Consistent and Identity-Preserving Image-to-Video Generation

**中文标题**: ConsID-Gen：视图一致且保留身份的图像到视频生成

**Date**: 2026-02-10 | **arXiv**: [2602.10113v1](http://arxiv.org/abs/2602.10113v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10113v1)

<details><summary><b>Abstract</b></summary>

Image-to-Video generation (I2V) animates a static image into a temporally coherent video sequence following textual instructions, yet preserving fine-grained object identity under changing viewpoints remains a persistent challenge. Unlike text-to-video models, existing I2V pipelines often suffer from appearance drift and geometric distortion, artifacts we attribute to the sparsity of single-view 2D observations and weak cross-modal alignment. Here we address this problem from both data and model perspectives. First, we curate ConsIDVid, a large-scale object-centric dataset built with a scalable pipeline for high-quality, temporally aligned videos, and establish ConsIDVid-Bench, where we present a novel benchmarking and evaluation framework for multi-view consistency using metrics sensitive to subtle geometric and appearance deviations. We further propose ConsID-Gen, a view-assisted I2V generation framework that augments the first frame with unposed auxiliary views and fuses semantic and structural cues via a dual-stream visual-geometric encoder as well as a text-visual connector, yielding unified conditioning for a Diffusion Transformer backbone. Experiments across ConsIDVid-Bench demonstrate that ConsID-Gen consistently outperforms in multiple metrics, with the best overall performance surpassing leading video generation models like Wan2.1 and HunyuanVideo, delivering superior identity fidelity and temporal coherence under challenging real-world scenarios. We will release our model and dataset at https://myangwu.github.io/ConsID-Gen.

</details>

<details><summary><b>中文摘要</b></summary>

图像到视频生成 (I2V) 将静态图像按照文本指令动画化为时间连贯的视频序列，但在不断变化的视点下保留细粒度的对象身份仍然是一个持续的挑战。与文本到视频模型不同，现有的 I2V 管道经常遭受外观漂移和几何失真的影响，我们将这些伪影归因于单视图 2D 观察的稀疏性和弱的跨模式对齐。这里我们从数据和模型两个角度来解决这个问题。首先，我们策划 ConsIDVid，这是一个以可扩展管道构建的大规模以对象为中心的数据集，用于高质量、时间对齐的视频，并建立 ConsIDVid-Bench，在其中我们使用对细微几何和外观偏差敏感的指标，提出了一种新颖的多视图一致性基准测试和评估框架。我们进一步提出 ConsID-Gen，一种视图辅助的 I2V 生成框架，它使用未设置的辅助视图增强第一帧，并通过双流视觉几何编码器以及文本视觉连接器融合语义和结构线索，从而为 Diffusion Transformer 主干产生统一条件。 ConsIDVid-Bench 的实验表明，ConsID-Gen 在多个指标上始终表现出色，其最佳整体性能超越了 Wan2.1 和 HunyuanVideo 等领先的视频生成模型，在具有挑战性的现实场景下提供卓越的身份保真度和时间一致性。我们将在 https://myangwu.github.io/ConsID-Gen 发布我们的模型和数据集。

</details>

---

## 10. VideoWorld 2: Learning Transferable Knowledge from Real-world Videos

**中文标题**: VideoWorld 2：从现实世界的视频中学习可转移的知识

**Date**: 2026-02-10 | **arXiv**: [2602.10102v1](http://arxiv.org/abs/2602.10102v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10102v1)

<details><summary><b>Abstract</b></summary>

Learning transferable knowledge from unlabeled video data and applying it in new environments is a fundamental capability of intelligent agents. This work presents VideoWorld 2, which extends VideoWorld and offers the first investigation into learning transferable knowledge directly from raw real-world videos. At its core, VideoWorld 2 introduces a dynamic-enhanced Latent Dynamics Model (dLDM) that decouples action dynamics from visual appearance: a pretrained video diffusion model handles visual appearance modeling, enabling the dLDM to learn latent codes that focus on compact and meaningful task-related dynamics. These latent codes are then modeled autoregressively to learn task policies and support long-horizon reasoning. We evaluate VideoWorld 2 on challenging real-world handcraft making tasks, where prior video generation and latent-dynamics models struggle to operate reliably. Remarkably, VideoWorld 2 achieves up to 70% improvement in task success rate and produces coherent long execution videos. In robotics, we show that VideoWorld 2 can acquire effective manipulation knowledge from the Open-X dataset, which substantially improves task performance on CALVIN. This study reveals the potential of learning transferable world knowledge directly from raw videos, with all code, data, and models to be open-sourced for further research.

</details>

<details><summary><b>中文摘要</b></summary>

从未标记的视频数据中学习可转移的知识并将其应用到新环境中是智能代理的基本能力。这项工作提出了 VideoWorld 2，它扩展了 VideoWorld，并首次对直接从原始现实世界视频中学习可转移知识进行了研究。 VideoWorld 2 的核心引入了动态增强的潜在动态模型 (dLDM)，它将动作动态与视觉外观分离：预训练的视频扩散模型处理视觉外观建模，使 dLDM 能够学习专注于紧凑且有意义的任务相关动态的潜在代码。然后对这些潜在代码进行自回归建模，以学习任务策略并支持长期推理。我们在具有挑战性的现实世界手工制作任务中评估了 VideoWorld 2，其中先前的视频生成和潜在动态模型难以可靠运行。值得注意的是，VideoWorld 2 将任务成功率提高了 70%，并生成连贯的长执行视频。在机器人技术中，我们证明 VideoWorld 2 可以从 Open-X 数据集中获取有效的操作知识，这大大提高了 CALVIN 上的任务性能。这项研究揭示了直接从原始视频中学习可转移的世界知识的潜力，所有代码、数据和模型都将开源以供进一步研究。

</details>

---

## 11. ArtisanGS: Interactive Tools for Gaussian Splat Selection with AI and Human in the Loop

**中文标题**: ArtisanGS：利用 AI 和 Human in the Loop 进行高斯 Splat 选择的交互式工具

**Date**: 2026-02-10 | **arXiv**: [2602.10173v1](http://arxiv.org/abs/2602.10173v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10173v1)

<details><summary><b>Abstract</b></summary>

Representation in the family of 3D Gaussian Splats (3DGS) are growing into a viable alternative to traditional graphics for an expanding number of application, including recent techniques that facilitate physics simulation and animation. However, extracting usable objects from in-the-wild captures remains challenging and controllable editing techniques for this representation are limited. Unlike the bulk of emerging techniques, focused on automatic solutions or high-level editing, we introduce an interactive suite of tools centered around versatile Gaussian Splat selection and segmentation. We propose a fast AI-driven method to propagate user-guided 2D selection masks to 3DGS selections. This technique allows for user intervention in the case of errors and is further coupled with flexible manual selection and segmentation tools. These allow a user to achieve virtually any binary segmentation of an unstructured 3DGS scene. We evaluate our toolset against the state-of-the-art for Gaussian Splat selection and demonstrate their utility for downstream applications by developing a user-guided local editing approach, leveraging a custom Video Diffusion Model. With flexible selection tools, users have direct control over the areas that the AI can modify. Our selection and editing tools can be used for any in-the-wild capture without additional optimization.

</details>

<details><summary><b>中文摘要</b></summary>

3D 高斯图 (3DGS) 系列中的表示正在发展成为传统图形的可行替代品，其应用范围不断扩大，包括促进物理模拟和动画的最新技术。然而，从野外捕获中提取可用对象仍然具有挑战性，并且这种表示的可控编辑技术是有限的。与大量专注于自动解决方案或高级编辑的新兴技术不同，我们引入了一套以多功能高斯 Splat 选择和分割为中心的交互式工具。我们提出了一种快速 AI 驱动的方法，将用户引导的 2D 选择掩模传播到 3DGS 选择。该技术允许用户在出现错误时进行干预，并进一步与灵活的手动选择和分段工具相结合。这些允许用户实现非结构化 3DGS 场景的几乎任何二进制分割。我们根据最先进的高斯 Splat 选择来评估我们的工具集，并通过开发用户引导的本地编辑方法，利用自定义视频扩散模型来展示它们对下游应用程序的实用性。借助灵活的选择工具，用户可以直接控制人工智能可以修改的区域。我们的选择和编辑工具可用于任何野外捕捉，无需额外优化。

</details>

---

## 12. Monocular Normal Estimation via Shading Sequence Estimation

**中文标题**: 通过阴影序列估计进行单目法线估计

**Date**: 2026-02-10 | **arXiv**: [2602.09929v2](http://arxiv.org/abs/2602.09929v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.09929v2)

<details><summary><b>Abstract</b></summary>

Monocular normal estimation aims to estimate the normal map from a single RGB image of an object under arbitrary lights. Existing methods rely on deep models to directly predict normal maps. However, they often suffer from 3D misalignment: while the estimated normal maps may appear to have a correct appearance, the reconstructed surfaces often fail to align with the geometric details. We argue that this misalignment stems from the current paradigm: the model struggles to distinguish and reconstruct varying geometry represented in normal maps, as the differences in underlying geometry are reflected only through relatively subtle color variations. To address this issue, we propose a new paradigm that reformulates normal estimation as shading sequence estimation, where shading sequences are more sensitive to various geometric information. Building on this paradigm, we present RoSE, a method that leverages image-to-video generative models to predict shading sequences. The predicted shading sequences are then converted into normal maps by solving a simple ordinary least-squares problem. To enhance robustness and better handle complex objects, RoSE is trained on a synthetic dataset, MultiShade, with diverse shapes, materials, and light conditions. Experiments demonstrate that RoSE achieves state-of-the-art performance on real-world benchmark datasets for object-based monocular normal estimation.

</details>

<details><summary><b>中文摘要</b></summary>

单目法线估计旨在从任意光照下物体的单个 RGB 图像估计法线图。现有方法依赖深度模型来直接预测法线贴图。然而，它们经常遭受 3D 未对准的影响：虽然估计的法线贴图可能看起来具有正确的外观，但重建的表面通常无法与几何细节对齐。我们认为这种错位源于当前的范式：模型难以区分和重建法线贴图中表示的不同几何形状，因为底层几何形状的差异仅通过相对微妙的颜色变化反映出来。为了解决这个问题，我们提出了一种新的范式，将法线估计重新表述为着色序列估计，其中着色序列对各种几何信息更加敏感。在此范例的基础上，我们提出了 RoSE，一种利用图像到视频生成模型来预测着色序列的方法。然后通过解决简单的普通最小二乘问题将预测的着色序列转换为法线贴图。为了增强鲁棒性并更好地处理复杂对象，RoSE 在具有不同形状、材料和光照条件的合成数据集 MultiShade 上进行训练。实验表明，RoSE 在基于对象的单目法线估计的真实世界基准数据集上实现了最先进的性能。

</details>

---

## 13. AdaTSQ: Pushing the Pareto Frontier of Diffusion Transformers via Temporal-Sensitivity Quantization

**中文标题**: AdaTSQ：通过时间敏感性量化推动扩散变压器的帕累托前沿

**Date**: 2026-02-10 | **arXiv**: [2602.09883v1](http://arxiv.org/abs/2602.09883v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09883v1)

**Code**: https://github.com/Qiushao-E/AdaTSQ.

<details><summary><b>Abstract</b></summary>

Diffusion Transformers (DiTs) have emerged as the state-of-the-art backbone for high-fidelity image and video generation. However, their massive computational cost and memory footprint hinder deployment on edge devices. While post-training quantization (PTQ) has proven effective for large language models (LLMs), directly applying existing methods to DiTs yields suboptimal results due to the neglect of the unique temporal dynamics inherent in diffusion processes. In this paper, we propose AdaTSQ, a novel PTQ framework that pushes the Pareto frontier of efficiency and quality by exploiting the temporal sensitivity of DiTs. First, we propose a Pareto-aware timestep-dynamic bit-width allocation strategy. We model the quantization policy search as a constrained pathfinding problem. We utilize a beam search algorithm guided by end-to-end reconstruction error to dynamically assign layer-wise bit-widths across different timesteps. Second, we propose a Fisher-guided temporal calibration mechanism. It leverages temporal Fisher information to prioritize calibration data from highly sensitive timesteps, seamlessly integrating with Hessian-based weight optimization. Extensive experiments on four advanced DiTs (e.g., Flux-Dev, Flux-Schnell, Z-Image, and Wan2.1) demonstrate that AdaTSQ significantly outperforms state-of-the-art methods like SVDQuant and ViDiT-Q. Our code will be released at https://github.com/Qiushao-E/AdaTSQ.

</details>

<details><summary><b>中文摘要</b></summary>

扩散变压器 (DiT) 已成为高保真图像和视频生成的最先进的支柱。然而，它们巨大的计算成本和内存占用阻碍了在边缘设备上的部署。虽然训练后量化 (PTQ) 已被证明对大型语言模型 (LLM) 有效，但由于忽略了扩散过程中固有的独特时间动态，直接将现有方法应用于 DiT 会产生次优结果。在本文中，我们提出了 AdaTSQ，这是一种新颖的 PTQ 框架，它通过利用 DiT 的时间敏感性来推动效率和质量的帕累托前沿。首先，我们提出了帕累托感知时间步动态位宽分配策略。我们将量化策略搜索建模为受限寻路问题。我们利用由端到端重建误差引导的波束搜索算法来跨不同时间步动态分配分层位宽。其次，我们提出了费舍尔引导的时间校准机制。它利用时态 Fisher 信息对来自高度敏感时间步长的校准数据进行优先级排序，与基于 Hessian 的权重优化无缝集成。对四种先进 DiT（例如 Flux-Dev、Flux-Schnell、Z-Image 和 Wan2.1）的大量实验表明，AdaTSQ 的性能显着优于 SVDQuant 和 ViDiT-Q 等最先进的方法。我们的代码将发布在https://github.com/Qiushao-E/AdaTSQ。

</details>

---

## 14. Free-GVC: Towards Training-Free Extreme Generative Video Compression with Temporal Coherence

**中文标题**: Free-GVC：实现具有时间一致性的免训练极端生成视频压缩

**Date**: 2026-02-10 | **arXiv**: [2602.09868v1](http://arxiv.org/abs/2602.09868v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09868v1)

<details><summary><b>Abstract</b></summary>

Building on recent advances in video generation, generative video compression has emerged as a new paradigm for achieving visually pleasing reconstructions. However, existing methods exhibit limited exploitation of temporal correlations, causing noticeable flicker and degraded temporal coherence at ultra-low bitrates. In this paper, we propose Free-GVC, a training-free generative video compression framework that reformulates video coding as latent trajectory compression guided by a video diffusion prior. Our method operates at the group-of-pictures (GOP) level, encoding video segments into a compact latent space and progressively compressing them along the diffusion trajectory. To ensure perceptually consistent reconstruction across GOPs, we introduce an Adaptive Quality Control module that dynamically constructs an online rate-perception surrogate model to predict the optimal diffusion step for each GOP. In addition, an Inter-GOP Alignment module establishes frame overlap and performs latent fusion between adjacent groups, thereby mitigating flicker and enhancing temporal coherence. Experiments show that Free-GVC achieves an average of 93.29% BD-Rate reduction in DISTS over the latest neural codec DCVC-RT, and a user study further confirms its superior perceptual quality and temporal coherence at ultra-low bitrates.

</details>

<details><summary><b>中文摘要</b></summary>

基于视频生成领域的最新进展，生成视频压缩已成为实现视觉上令人愉悦的重建的新范例。然而，现有方法对时间相关性的利用有限，导致在超低比特率下出现明显的闪烁和时间相干性下降。在本文中，我们提出了 Free-GVC，这是一种免训练的生成视频压缩框架，它将视频编码重新表述为由视频扩散先验引导的潜在轨迹压缩。我们的方法在图片组（GOP）级别上运行，将视频片段编码到紧凑的潜在空间中，并沿着扩散轨迹逐步压缩它们。为了确保跨 GOP 的感知一致重建，我们引入了自适应质量控制模块，该模块动态构建在线速率感知代理模型来预测每个 GOP 的最佳扩散步骤。此外，GOP 间对齐模块可建立帧重叠并在相邻组之间执行潜在融合，从而减轻闪烁并增强时间一致性。实验表明，与最新的神经编解码器 DCVC-RT 相比，Free-GVC 在 DISTS 中实现了平均 93.29% 的 BD-Rate 降低，并且用户研究进一步证实了其在超低比特率下的卓越感知质量和时间一致性。

</details>

---

## 15. Toward Fine-Grained Facial Control in 3D Talking Head Generation

**中文标题**: 实现 3D 头部说话中的细粒度面部控制

**Date**: 2026-02-10 | **arXiv**: [2602.09736v1](http://arxiv.org/abs/2602.09736v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09736v1)

<details><summary><b>Abstract</b></summary>

Audio-driven talking head generation is a core component of digital avatars, and 3D Gaussian Splatting has shown strong performance in real-time rendering of high-fidelity talking heads. However, achieving precise control over fine-grained facial movements remains a significant challenge, particularly due to lip-synchronization inaccuracies and facial jitter, both of which can contribute to the uncanny valley effect. To address these challenges, we propose Fine-Grained 3D Gaussian Splatting (FG-3DGS), a novel framework that enables temporally consistent and high-fidelity talking head generation. Our method introduces a frequency-aware disentanglement strategy to explicitly model facial regions based on their motion characteristics. Low-frequency regions, such as the cheeks, nose, and forehead, are jointly modeled using a standard MLP, while high-frequency regions, including the eyes and mouth, are captured separately using a dedicated network guided by facial area masks. The predicted motion dynamics, represented as Gaussian deltas, are applied to the static Gaussians to generate the final head frames, which are rendered via a rasterizer using frame-specific camera parameters. Additionally, a high-frequency-refined post-rendering alignment mechanism, learned from large-scale audio-video pairs by a pretrained model, is incorporated to enhance per-frame generation and achieve more accurate lip synchronization. Extensive experiments on widely used datasets for talking head generation demonstrate that our method outperforms recent state-of-the-art approaches in producing high-fidelity, lip-synced talking head videos.

</details>

<details><summary><b>中文摘要</b></summary>

音频驱动的头像生成是数字化身的核心组成部分，3D Gaussian Splatting 在高保真头像实时渲染方面表现出了强大的性能。然而，实现对细粒度面部运动的精确控制仍然是一个重大挑战，特别是由于口型同步不准确和面部抖动，这两者都可能导致恐怖谷效应。为了应对这些挑战，我们提出了细粒度 3D 高斯分布 (FG-3DGS)，这是一种新颖的框架，可以实现时间一致和高保真头部说话。我们的方法引入了频率感知的解缠结策略，以根据运动特征显式地建模面部区域。低频区域（例如脸颊、鼻子和前额）使用标准 MLP 联合建模，而高频区域（包括眼睛和嘴巴）则使用由面部区域掩模引导的专用网络单独捕获。预测的运动动态（表示为高斯增量）应用于静态高斯以生成最终的头部帧，该头部帧通过光栅化器使用特定于帧的相机参数进行渲染。此外，还采用了通过预训练模型从大规模音频-视频对中学习的高频细化后渲染对齐机制，以增强每帧生成并实现更准确的唇形同步。对广泛使用的头像生成数据集进行的大量实验表明，我们的方法在生成高保真、口型同步的头像视频方面优于最新的最先进方法。

</details>

---

## 16. Tele-Omni: a Unified Multimodal Framework for Video Generation and Editing

**中文标题**: Tele-Omni：用于视频生成和编辑的统一多模式框架

**Date**: 2026-02-10 | **arXiv**: [2602.09609v1](http://arxiv.org/abs/2602.09609v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09609v1)

<details><summary><b>Abstract</b></summary>

Recent advances in diffusion-based video generation have substantially improved visual fidelity and temporal coherence. However, most existing approaches remain task-specific and rely primarily on textual instructions, limiting their ability to handle multimodal inputs, contextual references, and diverse video generation and editing scenarios within a unified framework. Moreover, many video editing methods depend on carefully engineered pipelines tailored to individual operations, which hinders scalability and composability. In this paper, we propose Tele-Omni, a unified multimodal framework for video generation and editing that follows multimodal instructions, including text, images, and reference videos, within a single model. Tele-Omni leverages pretrained multimodal large language models to parse heterogeneous instructions and infer structured generation or editing intents, while diffusion-based generators perform high-quality video synthesis conditioned on these structured signals. To enable joint training across heterogeneous video tasks, we introduce a task-aware data processing pipeline that unifies multimodal inputs into a structured instruction format while preserving task-specific constraints. Tele-Omni supports a wide range of video-centric tasks, including text-to-video generation, image-to-video generation, first-last-frame video generation, in-context video generation, and in-context video editing. By decoupling instruction parsing from video synthesis and combining it with task-aware data design, Tele-Omni achieves flexible multimodal control while maintaining strong temporal coherence and visual consistency. Experimental results demonstrate that Tele-Omni achieves competitive performance across multiple tasks.

</details>

<details><summary><b>中文摘要</b></summary>

基于扩散的视频生成的最新进展极大地提高了视觉保真度和时间连贯性。然而，大多数现有方法仍然是特定于任务的，并且主要依赖于文本指令，限制了它们在统一框架内处理多模式输入、上下文参考以及不同视频生成和编辑场景的能力。此外，许多视频编辑方法依赖于针对单独操作精心设计的管道，这阻碍了可扩展性和可组合性。在本文中，我们提出了 Tele-Omni，这是一种用于视频生成和编辑的统一多模式框架，它遵循单一模型中的多模式指令，包括文本、图像和参考视频。 Tele-Omni 利用预训练的多模态大语言模型来解析异构指令并推断结构化生成或编辑意图，而基于扩散的生成器则根据这些结构化信号执行高质量视频合成。为了实现跨异构视频任务的联合训练，我们引入了任务感知数据处理管道，它将多模态输入统一为结构化指令格式，同时保留特定于任务的约束。 Tele-Omni 支持各种以视频为中心的任务，包括文本到视频生成、图像到视频生成、首尾帧视频生成、上下文视频生成和上下文视频编辑。通过将指令解析与视频合成解耦并将其与任务感知数据设计相结合，Tele-Omni 实现了灵活的多模式控制，同时保持了强大的时间连贯性和视觉一致性。实验结果表明，Tele-Omni 在多项任务中实现了具有竞争力的性能。

</details>

---

## 17. Hand2World: Autoregressive Egocentric Interaction Generation via Free-Space Hand Gestures

**中文标题**: Hand2World：通过自由空间手势生成自回归自我中心交互

**Date**: 2026-02-10 | **arXiv**: [2602.09600v1](http://arxiv.org/abs/2602.09600v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09600v1)

<details><summary><b>Abstract</b></summary>

Egocentric interactive world models are essential for augmented reality and embodied AI, where visual generation must respond to user input with low latency, geometric consistency, and long-term stability. We study egocentric interaction generation from a single scene image under free-space hand gestures, aiming to synthesize photorealistic videos in which hands enter the scene, interact with objects, and induce plausible world dynamics under head motion. This setting introduces fundamental challenges, including distribution shift between free-space gestures and contact-heavy training data, ambiguity between hand motion and camera motion in monocular views, and the need for arbitrary-length video generation. We present Hand2World, a unified autoregressive framework that addresses these challenges through occlusion-invariant hand conditioning based on projected 3D hand meshes, allowing visibility and occlusion to be inferred from scene context rather than encoded in the control signal. To stabilize egocentric viewpoint changes, we inject explicit camera geometry via per-pixel Plücker-ray embeddings, disentangling camera motion from hand motion and preventing background drift. We further develop a fully automated monocular annotation pipeline and distill a bidirectional diffusion model into a causal generator, enabling arbitrary-length synthesis. Experiments on three egocentric interaction benchmarks show substantial improvements in perceptual quality and 3D consistency while supporting camera control and long-horizon interactive generation.

</details>

<details><summary><b>中文摘要</b></summary>

以自我为中心的交互式世界模型对于增强现实和嵌入式人工智能至关重要，其中视觉生成必须以低延迟、几何一致性和长期稳定性响应用户输入。我们研究自由空间手势下单个场景图像的以自我为中心的交互生成，旨在合成逼真的视频，其中手进入场景，与物体交互，并在头部运动下诱导可信的世界动态。这种设置带来了根本性的挑战，包括自由空间手势和大量接触训练数据之间的分布变化、单目视图中手部运动和相机运动之间的模糊性，以及任意长度视频生成的需要。我们提出了 Hand2World，这是一个统一的自回归框架，它通过基于投影 3D 手部网格的遮挡不变手调节来解决这些挑战，允许从场景上下文中推断可见性和遮挡，而不是在控制信号中进行编码。为了稳定以自我为中心的视点变化，我们通过每像素 Plücker 射线嵌入注入显式相机几何形状，将相机运动与手部运动分开并防止背景漂移。我们进一步开发了一个全自动的单目注释管道，并将双向扩散模型提炼成因果生成器，从而实现任意长度的合成。对三个以自我为中心的交互基准进行的实验表明，感知质量和 3D 一致性得到了显着改善，同时支持相机控制和长视距交互生成。

</details>

---

## 18. AUHead: Realistic Emotional Talking Head Generation via Action Units Control

**中文标题**: AUHead：通过动作单元控制生成逼真的情感头部

**Date**: 2026-02-10 | **arXiv**: [2602.09534v1](http://arxiv.org/abs/2602.09534v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09534v1)

**Code**: https://github.com/laura990501/AUHead_ICLR

<details><summary><b>Abstract</b></summary>

Realistic talking-head video generation is critical for virtual avatars, film production, and interactive systems. Current methods struggle with nuanced emotional expressions due to the lack of fine-grained emotion control. To address this issue, we introduce a novel two-stage method (AUHead) to disentangle fine-grained emotion control, i.e. , Action Units (AUs), from audio and achieve controllable generation. In the first stage, we explore the AU generation abilities of large audio-language models (ALMs), by spatial-temporal AU tokenization and an "emotion-then-AU" chain-of-thought mechanism. It aims to disentangle AUs from raw speech, effectively capturing subtle emotional cues. In the second stage, we propose an AU-driven controllable diffusion model that synthesizes realistic talking-head videos conditioned on AU sequences. Specifically, we first map the AU sequences into the structured 2D facial representation to enhance spatial fidelity, and then model the AU-vision interaction within cross-attention modules. To achieve flexible AU-quality trade-off control, we introduce an AU disentanglement guidance strategy during inference, further refining the emotional expressiveness and identity consistency of the generated videos. Results on benchmark datasets demonstrate that our approach achieves competitive performance in emotional realism, accurate lip synchronization, and visual coherence, significantly surpassing existing techniques. Our implementation is available at https://github.com/laura990501/AUHead_ICLR

</details>

<details><summary><b>中文摘要</b></summary>

逼真的头部说话视频生成对于虚拟化身、电影制作和交互系统至关重要。由于缺乏细粒度的情绪控制，当前的方法难以处理微妙的情绪表达。为了解决这个问题，我们引入了一种新颖的两阶段方法（AUHead）来从音频中分离出细粒度的情感控制，即动作单元（AU），并实现可控生成。在第一阶段，我们通过时空AU标记化和“情感然后AU”的思想链机制来探索大型音频语言模型（ALM）的AU生成能力。它的目的是将 AU 与原始语音分开，有效捕捉微妙的情感线索。在第二阶段，我们提出了一种由 AU 驱动的可控扩散模型，该模型可以合成以 AU 序列为条件的逼真的头部说话视频。具体来说，我们首先将 AU 序列映射到结构化 2D 面部表示中以增强空间保真度，然后在交叉注意模块内对 AU 视觉交互进行建模。为了实现灵活的 AU 质量权衡控制，我们在推理过程中引入了 AU 解纠缠指导策略，进一步细化生成视频的情感表达和身份一致性。基准数据集的结果表明，我们的方法在情感真实性、准确的口型同步和视觉连贯性方面实现了竞争性能，显着超越了现有技术。我们的实现可在 https://github.com/laura990501/AUHead_ICLR 获取

</details>

---

## 19. Beyond Closed-Pool Video Retrieval: A Benchmark and Agent Framework for Real-World Video Search and Moment Localization

**中文标题**: 超越闭池视频检索：现实世界视频搜索和时刻定位的基准和代理框架

**Date**: 2026-02-10 | **arXiv**: [2602.10159v1](http://arxiv.org/abs/2602.10159v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10159v1)

<details><summary><b>Abstract</b></summary>

Traditional video retrieval benchmarks focus on matching precise descriptions to closed video pools, failing to reflect real-world searches characterized by fuzzy, multi-dimensional memories on the open web. We present \textbf{RVMS-Bench}, a comprehensive system for evaluating real-world video memory search. It consists of \textbf{1,440 samples} spanning \textbf{20 diverse categories} and \textbf{four duration groups}, sourced from \textbf{real-world open-web videos}. RVMS-Bench utilizes a hierarchical description framework encompassing \textbf{Global Impression, Key Moment, Temporal Context, and Auditory Memory} to mimic realistic multi-dimensional search cues, with all samples strictly verified via a human-in-the-loop protocol. We further propose \textbf{RACLO}, an agentic framework that employs abductive reasoning to simulate the human ``Recall-Search-Verify'' cognitive process, effectively addressing the challenge of searching for videos via fuzzy memories in the real world. Experiments reveal that existing MLLMs still demonstrate insufficient capabilities in real-world Video Retrieval and Moment Localization based on fuzzy memories. We believe this work will facilitate the advancement of video retrieval robustness in real-world unstructured scenarios.

</details>

<details><summary><b>中文摘要</b></summary>

传统的视频检索基准侧重于将精确描述与封闭视频池相匹配，无法反映开放网络上以模糊、多维记忆为特征的现实世界搜索。我们提出了 \textbf{RVMS-Bench}，一个用于评估现实世界视频内存搜索的综合系统。它由 \textbf{1,440 个样本}组成，涵盖 \textbf{20 个不同类别}和 \textbf{四个持续时间组}，源自 \textbf{真实世界的开放网络视频}。 RVMS-Bench 利用包含 \textbf{全局印象、关键时刻、时间上下文和听觉记忆}的分层描述框架来模拟现实的多维搜索线索，所有样本都通过人机交互协议进行严格验证。我们进一步提出\textbf{RACLO}，一种代理框架，采用溯因推理来模拟人类的“回忆-搜索-验证”认知过程，有效解决通过现实世界中的模糊记忆搜索视频的挑战。实验表明，现有的 MLLM 在基于模糊记忆的现实视频检索和时刻定位方面仍然表现出不足的能力。我们相信这项工作将有助于提高现实世界非结构化场景中视频检索的鲁棒性。

</details>

---

## 20. RoboSubtaskNet: Temporal Sub-task Segmentation for Human-to-Robot Skill Transfer in Real-World Environments

**中文标题**: RoboSubtaskNet：现实环境中人机技能转移的时间子任务分割

**Date**: 2026-02-10 | **arXiv**: [2602.10015v2](http://arxiv.org/abs/2602.10015v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.10015v2)

<details><summary><b>Abstract</b></summary>

Temporally locating and classifying fine-grained sub-task segments in long, untrimmed videos is crucial to safe human-robot collaboration. Unlike generic activity recognition, collaborative manipulation requires sub-task labels that are directly robot-executable. We present RoboSubtaskNet, a multi-stage human-to-robot sub-task segmentation framework that couples attention-enhanced I3D features (RGB plus optical flow) with a modified MS-TCN employing a Fibonacci dilation schedule to capture better short-horizon transitions such as reach-pick-place. The network is trained with a composite objective comprising cross-entropy and temporal regularizers (truncated MSE and a transition-aware term) to reduce over-segmentation and to encourage valid sub-task progressions. To close the gap between vision benchmarks and control, we introduce RoboSubtask, a dataset of healthcare and industrial demonstrations annotated at the sub-task level and designed for deterministic mapping to manipulator primitives. Empirically, RoboSubtaskNet outperforms MS-TCN and MS-TCN++ on GTEA and our RoboSubtask benchmark (boundary-sensitive and sequence metrics), while remaining competitive on the long-horizon Breakfast benchmark. Specifically, RoboSubtaskNet attains F1 @ 50 = 79.5%, Edit = 88.6%, Acc = 78.9% on GTEA; F1 @ 50 = 30.4%, Edit = 52.0%, Acc = 53.5% on Breakfast; and F1 @ 50 = 94.2%, Edit = 95.6%, Acc = 92.2% on RoboSubtask. We further validate the full perception-to-execution pipeline on a 7-DoF Kinova Gen3 manipulator, achieving reliable end-to-end behavior in physical trials (overall task success approx 91.25%). These results demonstrate a practical path from sub-task level video understanding to deployed robotic manipulation in real-world settings.

</details>

<details><summary><b>中文摘要</b></summary>

在未经修剪的长视频中临时定位和分类细粒度的子任务片段对于安全的人机协作至关重要。与通用活动识别不同，协作操作需要机器人可以直接执行的子任务标签。我们提出了 RoboSubtaskNet，一个多阶段的人机子任务分割框架，它将注意力增强的 I3D 特征（RGB 加光流）与改进的 MS-TCN 结合起来，采用斐波那契扩张计划来捕获更好的短视野过渡，例如到达-拾取-放置。该网络采用由交叉熵和时间正则化器（截断的 MSE 和转换感知项）组成的复合目标进行训练，以减少过度分割并鼓励有效的子任务进展。为了缩小视觉基准和控制之间的差距，我们引入了 RoboSubtask，这是一个在子任务级别注释的医疗保健和工业演示数据集，旨在用于确定性映射到操纵器基元。根据经验，RoboSubtaskNet 在 GTEA 和我们的 RoboSubtask 基准（边界敏感和序列指标）上优于 MS-TCN 和 MS-TCN++，同时在长期早餐基准上保持竞争力。具体来说，RoboSubtaskNet 在 GTEA 上达到 F1 @ 50 = 79.5%，Edit = 88.6%，Acc = 78.9%；早餐时 F1 @ 50 = 30.4%，编辑 = 52.0%，Acc = 53.5%； RoboSubtask 上的 F1 @ 50 = 94.2%，编辑 = 95.6%，Acc = 92.2%。我们进一步验证了 7-DoF Kinova Gen3 机械臂上的完整感知到执行流程，在物理试验中实现了可靠的端到端行为（总体任务成功率约为 91.25%）。这些结果展示了从子任务级视频理解到在现实环境中部署机器人操作的实用路径。

</details>

---

## 21. DexImit: Learning Bimanual Dexterous Manipulation from Monocular Human Videos

**中文标题**: DexImit：从单眼人类视频中学习双手灵巧操作

**Date**: 2026-02-10 | **arXiv**: [2602.10105v1](http://arxiv.org/abs/2602.10105v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.10105v1)

<details><summary><b>Abstract</b></summary>

Data scarcity fundamentally limits the generalization of bimanual dexterous manipulation, as real-world data collection for dexterous hands is expensive and labor-intensive. Human manipulation videos, as a direct carrier of manipulation knowledge, offer significant potential for scaling up robot learning. However, the substantial embodiment gap between human hands and robotic dexterous hands makes direct pretraining from human videos extremely challenging. To bridge this gap and unleash the potential of large-scale human manipulation video data, we propose DexImit, an automated framework that converts monocular human manipulation videos into physically plausible robot data, without any additional information. DexImit employs a four-stage generation pipeline: (1) reconstructing hand-object interactions from arbitrary viewpoints with near-metric scale; (2) performing subtask decomposition and bimanual scheduling; (3) synthesizing robot trajectories consistent with the demonstrated interactions; (4) comprehensive data augmentation for zero-shot real-world deployment. Building on these designs, DexImit can generate large-scale robot data based on human videos, either from the Internet or video generation models. DexImit is capable of handling diverse manipulation tasks, including tool use (e.g., cutting an apple), long-horizon tasks (e.g., making a beverage), and fine-grained manipulations (e.g., stacking cups).

</details>

<details><summary><b>中文摘要</b></summary>

数据稀缺从根本上限制了双手灵巧操作的普及，因为灵巧手的现实世界数据收集成本高昂且劳动密集型。人类操作视频作为操作知识的直接载体，为扩大机器人学习提供了巨大的潜力。然而，人手和机器人灵巧手之间的巨大体现差距使得从人类视频直接进行预训练极具挑战性。为了弥补这一差距并释放大规模人类操纵视频数据的潜力，我们提出了 DexImit，这是一种自动化框架，可以将单眼人类操纵视频转换为物理上合理的机器人数据，而无需任何附加信息。 DexImit 采用四阶段生成流程：（1）从任意角度以接近公制的尺度重建手部与物体的交互； (2)进行子任务分解和双手调度； (3) 合成与所演示的交互一致的机器人轨迹； (4) 全面的数据增强，用于零样本的现实世界部署。基于这些设计，DexImit 可以根据来自互联网或视频生成模型的人类视频生成大规模机器人数据。 DexImit 能够处理各种操作任务，包括工具使用（例如切苹果）、长期任务（例如制作饮料）和细粒度操作（例如堆叠杯子）。

</details>

---

## 22. Rethinking Global Text Conditioning in Diffusion Transformers

**中文标题**: 重新思考扩散变压器中的全局文本调节

**Date**: 2026-02-09 | **arXiv**: [2602.09268v1](http://arxiv.org/abs/2602.09268v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09268v1)

<details><summary><b>Abstract</b></summary>

Diffusion transformers typically incorporate textual information via attention layers and a modulation mechanism using a pooled text embedding. Nevertheless, recent approaches discard modulation-based text conditioning and rely exclusively on attention. In this paper, we address whether modulation-based text conditioning is necessary and whether it can provide any performance advantage. Our analysis shows that, in its conventional usage, the pooled embedding contributes little to overall performance, suggesting that attention alone is generally sufficient for faithfully propagating prompt information. However, we reveal that the pooled embedding can provide significant gains when used from a different perspective-serving as guidance and enabling controllable shifts toward more desirable properties. This approach is training-free, simple to implement, incurs negligible runtime overhead, and can be applied to various diffusion models, bringing improvements across diverse tasks, including text-to-image/video generation and image editing.

</details>

<details><summary><b>中文摘要</b></summary>

扩散变压器通常通过注意力层和使用池文本嵌入的调制机制合并文本信息。然而，最近的方法放弃了基于调制的文本调节并完全依赖于注意力。在本文中，我们讨论基于调制的文本调节是否必要以及它是否可以提供任何性能优势。我们的分析表明，在常规用法中，池化嵌入对整体性能贡献不大，这表明仅注意通常足以忠实地传播提示信息。然而，我们发现，从不同的角度使用时，池化嵌入可以提供显着的收益——作为指导并实现向更理想的属性的可控转变。这种方法无需培训，易于实现，运行时开销可以忽略不计，并且可以应用于各种扩散模型，从而改进各种任务，包括文本到图像/视频生成和图像编辑。

</details>

---

## 23. SemanticMoments: Training-Free Motion Similarity via Third Moment Features

**中文标题**: SemanticMoments：通过第三矩特征进行免训练运动相似性

**Date**: 2026-02-09 | **arXiv**: [2602.09146v1](http://arxiv.org/abs/2602.09146v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09146v1)

<details><summary><b>Abstract</b></summary>

Retrieving videos based on semantic motion is a fundamental, yet unsolved, problem. Existing video representation approaches overly rely on static appearance and scene context rather than motion dynamics, a bias inherited from their training data and objectives. Conversely, traditional motion-centric inputs like optical flow lack the semantic grounding needed to understand high-level motion. To demonstrate this inherent bias, we introduce the SimMotion benchmarks, combining controlled synthetic data with a new human-annotated real-world dataset. We show that existing models perform poorly on these benchmarks, often failing to disentangle motion from appearance. To address this gap, we propose SemanticMoments, a simple, training-free method that computes temporal statistics (specifically, higher-order moments) over features from pre-trained semantic models. Across our benchmarks, SemanticMoments consistently outperforms existing RGB, flow, and text-supervised methods. This demonstrates that temporal statistics in a semantic feature space provide a scalable and perceptually grounded foundation for motion-centric video understanding.

</details>

<details><summary><b>中文摘要</b></summary>

基于语义运动检索视频是一个基本但尚未解决的问题。现有的视频表示方法过度依赖静态外观和场景上下文，而不是运动动态，这是从其训练数据和目标继承的偏见。相反，传统的以运动为中心的输入（例如光流）缺乏理解高级运动所需的语义基础。为了证明这种固有偏差，我们引入了 SimMotion 基准，将受控合成数据与新的人工注释的真实世界数据集相结合。我们表明，现有模型在这些基准测试中表现不佳，通常无法将运动与外观分开。为了解决这一差距，我们提出了 SemanticMoments，这是一种简单的、免训练的方法，可以根据预先训练的语义模型的特征计算时间统计数据（特别是高阶矩）。在我们的基准测试中，SemanticMoments 始终优于现有的 RGB、流和文本监督方法。这表明语义特征空间中的时间统计为以运动为中心的视频理解提供了可扩展且基于感知的基础。

</details>

---

## 24. WorldCompass: Reinforcement Learning for Long-Horizon World Models

**中文标题**: WorldCompass：长期世界模型的强化学习

**Date**: 2026-02-09 | **arXiv**: [2602.09022v1](http://arxiv.org/abs/2602.09022v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.09022v1)

<details><summary><b>Abstract</b></summary>

This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively "steer" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.

</details>

<details><summary><b>中文摘要</b></summary>

这项工作提出了 WorldCompass，这是一种新颖的强化学习 (RL) 后训练框架，适用于长视野、基于交互式视频的世界模型，使他们能够根据交互信号更准确、一致地探索世界。为了有效地“引导”世界模型的探索，我们引入了针对自回归视频生成范式量身定制的三项核心创新：1）剪辑级推出策略：我们在单个目标剪辑上生成并评估多个样本，这显着提高了推出效率并提供细粒度的奖励信号。 2）补充奖励函数：我们设计了针对交互跟踪准确性和视觉质量的奖励函数，提供直接监督并有效抑制奖励黑客行为。 3）高效的强化学习算法：我们采用负感知微调策略结合各种效率优化来高效且有效地增强模型容量。对SoTA开源世界模型WorldPlay的评估表明，WorldCompass显着提高了各种场景下的交互准确性和视觉保真度。

</details>

---

## 25. GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing

**中文标题**: GOT-Edit：通过在线模型编辑进行几何感知通用对象跟踪

**Date**: 2026-02-09 | **arXiv**: [2602.08550v1](http://arxiv.org/abs/2602.08550v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08550v1)

<details><summary><b>Abstract</b></summary>

Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geometric cues, which makes them susceptible to partial occlusion, distractors, and variations in geometry and appearance. To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to enable geometric cue inference from only a few 2D images. To tackle the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing with null-space constrained updates that incorporate geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios. Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking.

</details>

<details><summary><b>中文摘要</b></summary>

人类对 2D 视频流中有效对象跟踪的感知源自对先验 3D 知识与语义推理相结合的隐式使用。相比之下，大多数通用对象跟踪 (GOT) 方法主要依赖于目标及其周围环境的 2D 特征，而忽略 3D 几何线索，这使得它们容易受到部分遮挡、干扰以及几何和外观变化的影响。为了解决这个限制，我们引入了 GOT-Edit，这是一种在线跨模态模型编辑方法，它将几何感知线索集成到来自 2D 视频流的通用对象跟踪器中。我们的方法利用预先训练的 Visual Geometry Grounded Transformer 的功能，仅从少量 2D 图像即可进行几何线索推断。为了应对无缝结合几何和语义的挑战，GOT-Edit 通过零空间约束更新执行在线模型编辑，在保留语义区分的同时合并几何信息，从而在不同场景中始终获得更好的性能。对多个 GOT 基准的大量实验表明，GOT-Edit 实现了卓越的鲁棒性和准确性，特别是在遮挡和杂乱的情况下，建立了将 2D 语义与 3D 几何推理相结合以进行通用对象跟踪的新范例。

</details>

---

## 26. T2VTree: User-Centered Visual Analytics for Agent-Assisted Thought-to-Video Authoring

**中文标题**: T2VTree：以用户为中心的可视化分析，用于代理辅助的思想到视频创作

**Date**: 2026-02-09 | **arXiv**: [2602.08368v1](http://arxiv.org/abs/2602.08368v1) | **PDF**: [Link](http://arxiv.org/pdf/2602.08368v1)

**Code**: https://github.com/tezuka0210/T2VTree.

<details><summary><b>Abstract</b></summary>

Generative models have substantially expanded video generation capabilities, yet practical thought-to-video creation remains a multi-stage, multi-modal, and decision-intensive process. However, existing tools either hide intermediate decisions behind repeated reruns or expose operator-level workflows that make exploration traces difficult to manage, compare, and reuse. We present T2VTree, a user-centered visual analytics approach for agent-assisted thought-to-video authoring. T2VTree represents the authoring process as a tree visualization. Each node in the tree binds an editable specification (intent, referenced inputs, workflow choice, prompts, and parameters) with the resulting multimodal outputs, making refinement, branching, and provenance inspection directly operable. To reduce the burden of deciding what to do next, a set of collaborating agents translates step-level intent into an executable plan that remains visible and user-editable before execution. We further implement a visual analytics system that integrates branching authoring with in-place preview and stitching for convergent assembly, enabling end-to-end multi-scene creation without leaving the authoring context. We demonstrate T2VTreeVA through two multi-scene case studies and a comparative user study, showing how the T2VTree visualization and editable agent planning support reliable refinement, localized comparison, and practical reuse in real authoring workflows. T2VTree is available at: https://github.com/tezuka0210/T2VTree.

</details>

<details><summary><b>中文摘要</b></summary>

生成模型大大扩展了视频生成功能，但实际的思想到视频创作仍然是一个多阶段、多模式和决策密集型的过程。然而，现有工具要么隐藏重复重新运行背后的中间决策，要么暴露操作员级别的工作流程，这使得探索跟踪难以管理、比较和重用。我们提出了 T2VTree，一种以用户为中心的可视化分析方法，用于代理辅助的思想到视频创作。 T2VTree 将创作过程表示为树可视化。树中的每个节点都将可编辑的规范（意图、引用的输入、工作流选择、提示和参数）与生成的多模式输出绑定在一起，从而使细化、分支和来源检查可直接操作。为了减轻决定下一步做什么的负担，一组协作代理将步骤级意图转换为可执行计划，该计划在执行前保持可见且用户可编辑。我们进一步实现了一个可视化分析系统，该系统将分支创作与就地预览和拼接集成在一起以进行聚合组装，从而无需离开创作上下文即可实现端到端的多场景创建。我们通过两个多场景案例研究和比较用户研究来演示 T2VTreeVA，展示 T2VTree 可视化和可编辑代理规划如何支持真实创作工作流程中的可靠细化、本地化比较和实际重用。 T2VTree 位于：https://github.com/tezuka0210/T2VTree。

</details>

---

## 27. WorldArena: A Unified Benchmark for Evaluating Perception and Functional Utility of Embodied World Models

**中文标题**: WorldArena：评估具体世界模型的感知和功能效用的统一基准

**Date**: 2026-02-09 | **arXiv**: [2602.08971v2](http://arxiv.org/abs/2602.08971v2) | **PDF**: [Link](http://arxiv.org/pdf/2602.08971v2)

<details><summary><b>Abstract</b></summary>

While world models have emerged as a cornerstone of embodied intelligence by enabling agents to reason about environmental dynamics through action-conditioned prediction, their evaluation remains fragmented. Current evaluation of embodied world models has largely focused on perceptual fidelity (e.g., video generation quality), overlooking the functional utility of these models in downstream decision-making tasks. In this work, we introduce WorldArena, a unified benchmark designed to systematically evaluate embodied world models across both perceptual and functional dimensions. WorldArena assesses models through three dimensions: video perception quality, measured with 16 metrics across six sub-dimensions; embodied task functionality, which evaluates world models as data engines, policy evaluators, and action planners integrating with subjective human evaluation. Furthermore, we propose EWMScore, a holistic metric integrating multi-dimensional performance into a single interpretable index. Through extensive experiments on 14 representative models, we reveal a significant perception-functionality gap, showing that high visual quality does not necessarily translate into strong embodied task capability. WorldArena benchmark with the public leaderboard is released at https://world-arena.ai, providing a framework for tracking progress toward truly functional world models in embodied AI.

</details>

<details><summary><b>中文摘要</b></summary>

虽然世界模型已成为体现智能的基石，使智能体能够通过行动条件预测来推理环境动态，但它们的评估仍然支离破碎。目前对具体世界模型的评估主要集中在感知保真度（例如视频生成质量），而忽视了这些模型在下游决策任务中的功能效用。在这项工作中，我们介绍了 WorldArena，这是一个统一的基准，旨在跨感知和功能维度系统地评估具体世界模型。 WorldArena 通过三个维度评估模型：视频感知质量，通过 6 个子维度的 16 个指标进行衡量；体现任务功能，将世界模型评估为数据引擎、政策评估者和与主观人类评估相结合的行动规划者。此外，我们提出了 EWMScore，这是一种将多维性能集成到单个可解释指数中的整体指标。通过对 14 个代表性模型的广泛实验，我们揭示了显着的感知功能差距，表明高视觉质量并不一定转化为强大的具体任务能力。 WorldArena 基准测试和公共排行榜在 https://world-arena.ai 上发布，提供了一个框架，用于跟踪具体人工智能中真正功能性世界模型的进展。

</details>

---

